{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Docs This directory contains current, accepted documentation underpinning Carbon. These documents cover all aspects of Carbon ranging from the project down to detailed designs for specific language features. If you're trying to learn more about Carbon, we recommend starting at /README.md . Design The design of the Carbon language, and the rationale for that design, is documented in the design/ directory . This documentation is intended to support these audiences: People who wish to determine whether Carbon would be the right choice to use for a project compared to other existing languages. People working on the evolution of the Carbon language who wish to understanding the rationale and motivation for existing design decisions. People working on a specification or implementation of the Carbon language who need a detailed understanding of the intended design. People writing Carbon code who wish to understand why the language rules are the way they are. This is in contrast to proposals , which document the individual decisions that led to this design (along with other changes to the Carbon project), including the rationale and alternatives considered. Project The project/ directory contains project-related documentation for Carbon, including: goals , and the principles and roadmap derived from those goals, how the project works, and how to contribute. Guides The guides/ directory contains to-be-written end-user documentation for developers writing programs in Carbon. Spec The spec/ directory contains the to-be-written formal specification of the Carbon language. This is for implementers of compilers or other tooling. This is intended to complement the interactive language explorer tool .","title":"Docs"},{"location":"#docs","text":"This directory contains current, accepted documentation underpinning Carbon. These documents cover all aspects of Carbon ranging from the project down to detailed designs for specific language features. If you're trying to learn more about Carbon, we recommend starting at /README.md .","title":"Docs"},{"location":"#design","text":"The design of the Carbon language, and the rationale for that design, is documented in the design/ directory . This documentation is intended to support these audiences: People who wish to determine whether Carbon would be the right choice to use for a project compared to other existing languages. People working on the evolution of the Carbon language who wish to understanding the rationale and motivation for existing design decisions. People working on a specification or implementation of the Carbon language who need a detailed understanding of the intended design. People writing Carbon code who wish to understand why the language rules are the way they are. This is in contrast to proposals , which document the individual decisions that led to this design (along with other changes to the Carbon project), including the rationale and alternatives considered.","title":"Design"},{"location":"#project","text":"The project/ directory contains project-related documentation for Carbon, including: goals , and the principles and roadmap derived from those goals, how the project works, and how to contribute.","title":"Project"},{"location":"#guides","text":"The guides/ directory contains to-be-written end-user documentation for developers writing programs in Carbon.","title":"Guides"},{"location":"#spec","text":"The spec/ directory contains the to-be-written formal specification of the Carbon language. This is for implementers of compilers or other tooling. This is intended to complement the interactive language explorer tool .","title":"Spec"},{"location":"design/","text":"Language design Table of contents Overview This document is provisional Hello, Carbon Code and comments Build modes Types are values Primitive types bool Integer types Integer literals Floating-point types Floating-point literals String types String literals Value categories and value phases Composite types Tuples Struct types Pointer types Arrays and slices Expressions Declarations, Definitions, and Scopes Patterns Binding patterns Destructuring patterns Refutable patterns Name-binding declarations Constant let declarations Variable var declarations auto Functions Parameters auto return type Blocks and statements Assignment statements Control flow if and else Loops while for break continue return returned var match User-defined types Classes Assignment Class functions and factory functions Methods Inheritance Access control Destructors const Unformed state Move Mixins Choice types Names Files, libraries, packages Package declaration Imports Name visibility Package scope Namespaces Naming conventions Aliases Name lookup Name lookup for common types Generics Checked and template parameters Interfaces and implementations Combining constraints Associated types Generic entities Generic Classes Generic choice types Generic interfaces Generic implementations Other features Generic type equality and observe declarations Operator overloading Common type Bidirectional interoperability with C and C++ Goals Non-goals Importing and #include ABI and dynamic linking Operator overloading Templates Standard types Inheritance Enums Unfinished tales Safety Lifetime and move semantics Metaprogramming Pattern matching as function overload resolution Error handling Execution abstractions Abstract machine and execution model Lambdas Co-routines Concurrency Overview This documentation describes the design of the Carbon language, and the rationale for that design. This documentation is an overview of the Carbon project in its current state, written for the builders of Carbon and for those interested in learning more about Carbon. This document is not a complete programming manual, and, nor does it provide detailed and comprehensive justification for design decisions. These descriptions are found in linked dedicated designs. This document is provisional This document includes much that is provisional or placeholder. This means that the syntax used, language rules, standard library, and other aspects of the design have things that have not been decided through the Carbon process. This preliminary material fills in gaps until aspects of the design can be filled in. Hello, Carbon Here is a simple function showing some Carbon code: import Console; // Prints the Fibonacci numbers less than `limit`. fn Fibonacci(limit: i64) { var (a: i64, b: i64) = (0, 1); while (a < limit) { Console.Print(a, \" \"); let next: i64 = a + b; a = b; b = next; } Console.Print(\"\\n\"); } Carbon is a language that should feel familiar to C++ and C developers. This example has familiar constructs like imports , function definitions , typed arguments , and curly braces . A few other features that are unlike C or C++ may stand out. First, declarations start with introducer keywords. fn introduces a function declaration, and var introduces a variable declaration . You can also see a tuple , a composite type written as a comma-separated list inside parentheses. Unlike, say, Python, these types are strongly-typed as well. Code and comments All source code is UTF-8 encoded text. Comments, identifiers, and strings are allowed to have non-ASCII characters. var r\u00e9sultat: String = \"Succ\u00e8s\"; Comments start with two slashes // and go to the end of the line. They are required to be the only non-whitespace on the line. // Compute an approximation of \u03c0 References: Source files lexical conventions Proposal #142: Unicode source files Proposal #198: Comments Build modes The behavior of the Carbon compiler depends on the build mode : In a development build , the priority is diagnosing problems and fast build time. In a performance build , the priority is fastest execution time and lowest memory usage. In a hardened build , the first priority is safety and second is performance. References: Safety strategy Types are values Expressions compute values in Carbon, and these values are always strongly typed much like in C++. However, an important difference from C++ is that types are themselves modeled as values; specifically, compile-time constant values. This means that the grammar for writing a type is the expression grammar. Expressions written where a type is expected must be able to be evaluated at compile-time and must evaluate to a type value. Primitive types Primitive types fall into the following categories: the boolean type bool , signed and unsigned integer types, IEEE-754 floating-point types, and string types. These are made available through the prelude . References: Primitive types bool The type bool is a boolean type with two possible values: true and false . Comparison expressions produce bool values. The condition arguments in control-flow statements , like if and while , and if - then - else conditional expressions take bool values. Integer types The signed-integer type with bit width N may be written Carbon.Int(N) . For convenience and brevity, the common power-of-two sizes may be written with an i followed by the size: i8 , i16 , i32 , i64 , i128 , or i256 . Signed-integer overflow is a programming error: In a development build, overflow will be caught immediately when it happens at runtime. In a performance build, the optimizer can assume that such conditions don't occur. As a consequence, if they do, the behavior of the program is not defined. In a hardened build, overflow does not result in undefined behavior. Instead, either the program will be aborted, or the arithmetic will evaluate to a mathematically incorrect result, such as a two's complement result or zero. The unsigned-integer types are: u8 , u16 , u32 , u64 , u128 , u256 , and Carbon.UInt(N) . Unsigned integer types wrap around on overflow, we strongly advise that they are not used except when those semantics are desired. These types are intended for bit manipulation or modular arithmetic as often found in hashing , cryptography , and PRNG use cases. Values which can never be negative, like sizes, but for which wrapping does not make sense should use signed integer types . References: Question-for-leads issue #543: pick names for fixed-size integer types Proposal #820: Implicit conversions Proposal #1083: Arithmetic expressions Integer literals Integers may be written in decimal, hexadecimal, or binary: 12345 (decimal) 0x1FE (hexadecimal) 0b1010 (binary) Underscores _ may be used as digit separators, but for decimal and hexadecimal literals, they can only appear in conventional locations. Numeric literals are case-sensitive: 0x , 0b must be lowercase, whereas hexadecimal digits must be uppercase. Integer literals never contain a . . Unlike in C++, literals do not have a suffix to indicate their type. Instead, numeric literals have a type derived from their value, and can be implicitly converted to any type that can represent that value. References: Integer literals Proposal #143: Numeric literals Proposal #144: Numeric literal semantics Proposal #820: Implicit conversions Floating-point types Floating-point types in Carbon have IEEE 754 semantics, use the round-to-nearest rounding mode, and do not set any floating-point exception state. They are named with an f and the number of bits: f16 , f32 , f64 , and f128 . BFloat16 is also provided. References: Question-for-leads issue #543: pick names for fixed-size integer types Proposal #820: Implicit conversions Proposal #1083: Arithmetic expressions Floating-point literals Floating-point types along with user-defined types may initialized from real-number literals . Decimal and hexadecimal real-number literals are supported: 123.456 (digits on both sides of the . ) 123.456e789 (optional + or - after the e ) 0x1.Ap123 (optional + or - after the p ) Real-number literals always have a period ( . ) and a digit on each side of the period. When a real-number literal is interpreted as a value of a floating-point type, its value is the representable real number closest to the value of the literal. In the case of a tie, the nearest value whose mantissa is even is selected. References: Real-number literals Proposal #143: Numeric literals Proposal #144: Numeric literal semantics Proposal #820: Implicit conversions Proposal #866: Allow ties in floating literals String types There are two string types: String - a byte sequence treated as containing UTF-8 encoded text. StringView - a read-only reference to a byte sequence treated as containing UTF-8 encoded text. String literals String literals may be written on a single line using a double quotation mark ( \" ) at the beginning and end of the string, as in \"example\" . Multi-line string literals, called block string literals , begin and end with three double quotation marks ( \"\"\" ), and may have a file type indicator after the first \"\"\" . // Block string literal: var block: String = \"\"\" The winds grow high; so do your stomachs, lords. How irksome is this music to my heart! When such strings jar, what hope of harmony? I pray, my lords, let me compound this strife. -- History of Henry VI, Part II, Act II, Scene 1, W. Shakespeare \"\"\"; The indentation of a block string literal's terminating line is removed from all preceding lines. Strings may contain escape sequences introduced with a backslash ( \\ ). Raw string literals are available for representing strings with \\ s and \" s. References: String literals Proposal #199: String literals Value categories and value phases FIXME: Should this be moved together with Types are values ? Every value has a value category , similar to C++ , that is either l-value or r-value . Carbon will automatically convert an l-value to an r-value, but not in the other direction. L-values have storage and a stable address. They may be modified, assuming their type is not const . R-values may not have dedicated storage. This means they cannot be modified and their address generally cannot be taken. R-values are broken down into three kinds, called value phases : A constant has a value known at compile time, and that value is available during type checking, for example to use as the size of an array. These include literals ( integer , floating-point , string ), concrete type values (like f64 or Optional(i32*) ), expressions in terms of constants, and values of template parameters . A symbolic value has a value that will be known at the code generation stage of compilation when monomorphization happens, but is not known during type checking. This includes checked-generic parameters , and type expressions with checked-generic arguments, like Optional(T*) . A runtime value has a dynamic value only known at runtime. Carbon will automatically convert a constant to a symbolic value, or any value to a runtime value: graph TD; A(constant)-->B(symbolic value)-->C(runtime value); D(l-value)-->C; Constants convert to symbolic values and to runtime values. Symbolic values will generally convert into runtime values if an operation that inspects the value is performed on them. Runtime values will convert into constants or to symbolic values if constant evaluation of the runtime expression succeeds. Composite types Tuples A tuple is a fixed-size collection of values that can have different types, where each value is identified by its position in the tuple. An example use of tuples is to return multiple values from a function: fn DoubleBoth(x: i32, y: i32) -> (i32, i32) { return (2 * x, 2 * y); } Breaking this example apart: The return type is a tuple of two i32 types. The expression uses tuple syntax to build a tuple of two i32 values. Both of these are expressions using the tuple syntax (<expression>, <expression>) . The only difference is the type of the tuple expression: one is a tuple of types, the other a tuple of values. In other words, a tuple type is a tuple of types. The components of a tuple are accessed positionally, so element access uses subscript syntax, but the index must be a compile-time constant: fn DoubleTuple(x: (i32, i32)) -> (i32, i32) { return (2 * x[0], 2 * x[1]); } Tuple types are structural . References: Tuples Struct types Carbon also has structural types whose members are identified by name instead of position. These are called structural data classes , also known as a struct types or structs . Both struct types and values are written inside curly braces ( { ... } ). In both cases, they have a comma-separated list of members that start with a period ( . ) followed by the field name. In a struct type, the field name is followed by a colon ( : ) and the type, as in: {.name: String, .count: i32} . In a struct value, called a structural data class literal or a struct literal , the field name is followed by an equal sign ( = ) and the value, as in {.key = \"Joe\", .count = 3} . References: Struct types Proposal #561: Basic classes: use cases, struct literals, struct types, and future work Proposal #981: Implicit conversions for aggregates Proposal #710: Default comparison for data classes Pointer types The type of pointers-to-values-of-type- T is written T* . Carbon pointers do not support pointer arithmetic ; the only pointer operations are: Dereference: given a pointer p , *p gives the value p points to as an l-value . p->m is syntactic sugar for (*p).m . Address-of: given an l-value x , &x returns a pointer to x . There are no null pointers in Carbon. To represent a pointer that may not refer to a valid object, use the type Optional(T*) . TODO: Perhaps Carbon will have stricter pointer provenance or restrictions on casts between pointers and integers. References: Question-for-leads issue #520: should we use whitespace-sensitive operator fixity? Question-for-leads issue #523: what syntax should we use for pointer types? Arrays and slices The type of an array of holding 4 i32 values is written [i32; 4] . There is an implicit conversion from tuples to arrays of the same length as long as every component of the tuple may be implicitly converted to the destination element type. In cases where the size of the array may be deduced, it may be omitted, as in: var i: i32 = 1; // `[i32;]` equivalent to `[i32; 3]` here. var a: [i32;] = (i, i, i); Elements of an array may be accessed using square brackets ( [ ... ] ), as in a[i] : a[i] = 2; Console.Print(a[0]); TODO: Slices Expressions Expressions describe some computed value. The simplest example would be a literal number like 42 : an expression that computes the integer value 42. Some common expressions in Carbon include: Literals: boolean : true , false integer : 42 , -7 real-number : 3.1419 , 6.022e+23 string : \"Hello World!\" tuple : (1, 2, 3) struct : {.word = \"the\", .count = 56} Names and member access Operators : Arithmetic : -x , 1 + 2 , 3 - 4 , 2 * 5 , 6 / 3 , 5 % 3 Bitwise : 2 & 3 , 2 | 4 , 3 ^ 1 , ^7 Bit shift : 1 << 3 , 8 >> 1 Comparison : 2 == 2 , 3 != 4 , 5 < 6 , 7 > 6 , 8 <= 8 , 8 >= 8 Conversion : 2 as i32 Logical : a and b , c or d , not e Indexing : a[3] Function call: f(4) Pointer : *p , p->m , &x Move : ~x Conditionals : if c then t else f Parentheses: (7 + 8) * (3 - 1) When an expression appears in a context in which an expression of a specific type is expected, implicit conversions are applied to convert the expression to the target type. References: Expressions Proposal #162: Basic Syntax Proposal #555: Operator precedence Proposal #601: Operator tokens Proposal #680: And, or, not Proposal #702: Comparison operators Proposal #845: as expressions Proposal #911: Conditional expressions Proposal #1083: Arithmetic expressions Declarations, Definitions, and Scopes Declarations introduce a new name and say what that name represents. For some kinds of entities, like functions , there are two kinds of declarations: forward declarations and definitions . For those entities, there should be exactly one definition for the name, and at most one additional forward declaration that introduces the name before it is defined, plus any number of declarations in a match_first block . Forward declarations can be used to separate interface from implementation, such as to declare a name in an api file that is defined in an impl file . Forward declarations also allow entities to be used before they are defined, such as to allow cyclic references. A name that has been declared but not defined is called incomplete , and in some cases there are limitations on what can be done with an incomplete name. Within a definition, the defined name is incomplete until the end of the definition is reached, but is complete in the bodies of member functions because they are parsed as if they appeared after the definition . A name is valid until the end of the innermost enclosing scope . There are a few kinds of scopes: the outermost scope, which includes the whole file, scopes that are enclosed in curly braces ( { ... } ), and scopes that encompass a single declaration. For example, the names of the parameters of a function or class are valid until the end of the declaration. The name of the function or class itself is visible until the end of the enclosing scope. References: Principle: Information accumulation Proposal #875: Principle: information accumulation Question-for-leads issue #472: Open question: Calling functions defined later in the same file Patterns A pattern says how to receive some data that is being matched against. There are two kinds of patterns: Refutable patterns can fail to match based on the runtime value being matched. Irrefutable patterns are guaranteed to match, so long as the code type-checks. Irrefutable patterns are used in function parameters , variable var declarations , and constant let declarations . match statements can include both refutable patterns and irrefutable patterns. References: Pattern matching Proposal #162: Basic Syntax Binding patterns The most common irrefutable pattern is a binding pattern , consisting of a new name, a colon ( : ), and a type. It binds the matched value of that type to that name. It can only match values that may be implicitly converted to that type. A underscore ( _ ) may be used instead of the name to match a value but without binding any name to it. Binding patterns default to let bindings . The var keyword is used to make it a var binding . The result of a let binding is the name is bound to an r-value . This means the value cannot be modified, and its address generally cannot be taken. A var binding has dedicated storage, and so the name is an l-value which can be modified and has a stable address. A let -binding may trigger a copy of the original value, or a move if the original value is a temporary, or the binding may be a pointer to the original value, like a const reference in C++ . Which option must not be observable to the programmer. For example, Carbon will not allow modifications to the original value when it is through a pointer. This choice may also be influenced by the type. For example, types that don't support being copied will be passed by pointer instead. A generic binding uses :! instead of a colon ( : ) and can only match constant or symbolic values , not run-time values. The keyword auto may be used in place of the type in a binding pattern, as long as the type can be deduced from the type of a value in the same declaration. Destructuring patterns There are also irrefutable destructuring patterns , such as tuple destructuring . A tuple destructuring pattern looks like a tuple of patterns. It may only be used to match tuple values whose components match the component patterns of the tuple. An example use is: // `Bar()` returns a tuple consisting of an // `i32` value and 2-tuple of `f32` values. fn Bar() -> (i32, (f32, f32)); fn Foo() -> i64 { // Pattern in `var` declaration: var (p: i64, _: auto) = Bar(); return p; } The pattern used in the var declaration destructures the tuple value returned by Bar() . The first component pattern, p: i64 , corresponds to the first component of the value returned by Bar() , which has type i32 . This is allowed since there is an implicit conversion from i32 to i64 . The result of this conversion is assigned to the name p . The second component pattern, _: auto , matches the second component of the value returned by Bar() , which has type (f32, f32) . Refutable patterns Additional kinds of patterns are allowed in match statements , that may or may not match based on the runtime value of the match expression: An expression pattern is an expression, such as 42 , whose value must be equal to match. A choice pattern matches one case from a choice type, as described in the choice types section . A dynamic cast pattern is tests the dynamic type, as described in inheritance . See match for examples of refutable patterns. References: Pattern matching Question-for-leads issue #1283: how should pattern matching and implicit conversion interact? Name-binding declarations There are two kinds of name-binding declarations: constant declarations, introduced with let , and variable declarations, introduced with var . There are no forward declarations of these; all name-binding declarations are definitions . Constant let declarations A let declaration matches an irrefutable pattern to a value. In this example, the name x is bound to the value 42 with type i64 : let x: i64 = 42; Here x: i64 is the pattern, which is followed by an equal sign ( = ) and the value to match, 42 . The names from binding patterns are introduced into the enclosing scope . Variable var declarations A var declaration is similar, except with var bindings, so x here is an l-value with storage and an address, and so may be modified: var x: i64 = 42; x = 7; Variables with a type that has an unformed state do not need to be initialized in the variable declaration, but do need to be assigned before they are used. References: Variables Proposal #162: Basic Syntax Proposal #257: Initialization of memory and variables Proposal #339: Add var <type> <identifier> [ = <value> ]; syntax for variables Proposal #618: var ordering auto If auto is used as the type in a var or let declaration, the type is the static type of the initializer expression, which is required. var x: i64 = 2; // The type of `y` is inferred to be `i64`. let y: auto = x + 3; // The type of `z` is inferred to be `bool`. var z: auto = (y > 1); References: Type inference Proposal #851: auto keyword for vars Functions Functions are the core unit of behavior. For example, this is a forward declaration of a function that adds two 64-bit integers: fn Add(a: i64, b: i64) -> i64; Breaking this apart: fn is the keyword used to introduce a function. Its name is Add . This is the name added to the enclosing scope . The parameter list in parentheses ( ( ... ) ) is a comma-separated list of irrefutable patterns . It returns an i64 result. Functions that return nothing omit the -> and return type. You would call this function like Add(1, 2) . A function definition is a function declaration that has a body block instead of a semicolon: fn Add(a: i64, b: i64) -> i64 { return a + b; } The names of the parameters are in scope until the end of the definition or declaration. The parameter names in a forward declaration may be omitted using _ , but must match the definition if they are specified. References: Functions Proposal #162: Basic Syntax Proposal #438: Add statement syntax for function declarations Question-for-leads issue #476: Optional argument names (unused arguments) Question-for-leads issue #1132: How do we match forward declarations with their definitions? Parameters The bindings in the parameter list default to let bindings , and so the parameter names are treated as r-values . This is appropriate for input parameters. This binding will be implemented using a pointer, unless it is legal to copy and copying is cheaper. If the var keyword is added before the binding, then the arguments will be copied (or moved from a temporary) to new storage, and so can be mutated in the function body. The copy ensures that any mutations will not be visible to the caller. Use a pointer parameter type to represent an input/output parameter , allowing a function to modify a variable of the caller's. This makes the possibility of those modifications visible: by taking the address using & in the caller, and dereferencing using * in the callee. Outputs of a function should prefer to be returned. Multiple values may be returned using a tuple or struct type. auto return type If auto is used in place of the return type, the return type of the function is inferred from the function body. It is set to common type of the static type of arguments to the return statements in the function. This is not allowed in a forward declaration. // Return type is inferred to be `bool`, the type of `a > 0`. fn Positive(a: i64) -> auto { return a > 0; } References: Type inference Function return clause Proposal #826: Function return type inference Blocks and statements A block is a sequence of statements . A block defines a scope and, like other scopes, is enclosed in curly braces ( { ... } ). Each statement is terminated by a semicolon or block. Expressions and var and let are valid statements. Statements within a block are normally executed in the order they appear in the source code, except when modified by control-flow statements. The body of a function is defined by a block, and some control-flow statements have their own blocks of code. These are nested within the enclosing scope. For example, here is a function definition with a block of statements defining the body of the function, and a nested block as part of a while statement: fn Foo() { Bar(); while (Baz()) { Quux(); } } References: Blocks and statements Proposal #162: Basic Syntax Assignment statements Assignment statements mutate the value of the l-value described on the left-hand side of the assignment. Assignment: x = y; . x is assigned the value of y . Increment and decrement: ++i; , --j; . i is set to i + 1 , j is set to j - 1 . Compound assignment: x += y; , x -= y; , x *= y; , x /= y; , x &= y; , x |= y; , x ^= y; , x <<= y; , x >>= y; . x @= y; is equivalent to x = x @ y; for each operator @ . Unlike C++, these assignments are statements, not expressions, and don't return a value. Control flow Blocks of statements are generally executed sequentially. Control-flow statements give additional control over the flow of execution and which statements are executed. Some control-flow statements include blocks . Those blocks will always be within curly braces { ... } . // Curly braces { ... } are required. if (condition) { ExecutedWhenTrue(); } else { ExecutedWhenFalse(); } This is unlike C++, which allows control-flow constructs to omit curly braces around a single statement. References: Control flow Proposal #162: Basic Syntax Proposal #623: Require braces if and else if and else provide conditional execution of statements. An if statement consists of: An if introducer followed by a condition in parentheses. If the condition evaluates to true , the block following the condition is executed, otherwise it is skipped. This may be followed by zero or more else if clauses, whose conditions are evaluated if all prior conditions evaluate to false , with a block that is executed if that evaluation is to true . A final optional else clause, with a block that is executed if all conditions evaluate to false . For example: if (fruit.IsYellow()) { Console.Print(\"Banana!\"); } else if (fruit.IsOrange()) { Console.Print(\"Orange!\"); } else { Console.Print(\"Vegetable!\"); } This code will: Print Banana! if fruit.IsYellow() is true . Print Orange! if fruit.IsYellow() is false and fruit.IsOrange() is true . Print Vegetable! if both of the above return false . References: Control flow Proposal #285: if/else Loops References: Loops while while statements loop for as long as the passed expression returns true . For example, this prints 0 , 1 , 2 , then Done! : var x: i32 = 0; while (x < 3) { Console.Print(x); ++x; } Console.Print(\"Done!\"); References: while loops Proposal #340: Add C++-like while loops for for statements support range-based looping, typically over containers. For example, this prints each String value in names : for (var name: String in names) { Console.Print(name); } References: for loops Proposal #353: Add C++-like for loops break The break statement immediately ends a while or for loop. Execution will continue starting from the end of the loop's scope. For example, this processes steps until a manual step is hit (if no manual step is hit, all steps are processed): for (var step: Step in steps) { if (step.IsManual()) { Console.Print(\"Reached manual step!\"); break; } step.Process(); } References: break continue The continue statement immediately goes to the next loop of a while or for . In a while , execution continues with the while expression. For example, this prints all non-empty lines of a file, using continue to skip empty lines: var f: File = OpenFile(path); while (!f.EOF()) { var line: String = f.ReadLine(); if (line.IsEmpty()) { continue; } Console.Print(line); } References: continue return The return statement ends the flow of execution within a function, returning execution to the caller. // Prints the integers 1 .. `n` and then // returns to the caller. fn PrintFirstN(n: i32) { var i: i32 = 0; while (true) { i += 1; if (i > n) { // None of the rest of the function is // executed after a `return`. return; } Console.Print(i); } } If the function returns a value to the caller, that value is provided by an expression in the return statement. For example: fn Sign(i: i32) -> i32 { if (i > 0) { return 1; } if (i < 0) { return -1; } return 0; } Assert(Sign(-3) == -1); References: return return statements Proposal #415: return Proposal #538: return with no argument returned var To avoid a copy when returning a variable, add a returned prefix to the variable's declaration and use return var instead of returning an expression, as in: fn MakeCircle(radius: i32) -> Circle { returned var c: Circle; c.radius = radius; // `return c` would be invalid because `returned` is in use. return var; } This is instead of the \"named return value optimization\" of C++ . References: returned var Proposal #257: Initialization of memory and variables match match is a control flow similar to switch of C and C++ and mirrors similar constructs in other languages, such as Swift. The match keyword is followed by an expression in parentheses, whose value is matched against the case declarations, each of which contains a refutable pattern , in order. The refutable pattern may optionally be followed by an if expression, which may use the names from bindings in the pattern. The code for the first matching case is executed. An optional default block may be placed after the case declarations, it will be executed if none of the case declarations match. An example match is: fn Bar() -> (i32, (f32, f32)); fn Foo() -> f32 { match (Bar()) { case (42, (x: f32, y: f32)) => { return x - y; } case (p: i32, (x: f32, _: f32)) if (p < 13) => { return p * x; } case (p: i32, _: auto) if (p > 3) => { return p * Pi; } default => { return Pi; } } } References: Pattern matching Question-for-leads issue #1283: how should pattern matching and implicit conversion interact? User-defined types TODO: Maybe rename to \"nominal types\"? Classes Nominal classes , or just classes , are a way for users to define their own data structures or record types . This is an example of a class definition : class Widget { var x: i32; var y: i32; var payload: String; } Breaking this apart: This defines a class named Widget . Widget is the name added to the enclosing scope . The name Widget is followed by curly braces ( { ... } ) containing the class body , making this a definition . A forward declaration would instead have a semicolon( ; ). Those braces delimit the class' scope . Fields, or instances variables , are defined using var declarations . Widget has two i32 fields ( x and y ), and one String field ( payload ). The order of the field declarations determines the fields' memory-layout order. Classes may have other kinds of members beyond fields declared in its scope: Class functions Methods alias let to define class constants. TODO: Another syntax to define constants associated with the class like class let or static let ? class , to define a member class or nested class Within the scope of a class, the unqualified name Self can be used to refer to the class itself. Members of a class are accessed using the dot ( . ) notation, so given an instance dial of type Widget , dial.payload refers to its payload field. Both structural data classes and nominal classes are considered class types , but they are commonly referred to as \"structs\" and \"classes\" respectively when that is not confusing. Like structs, classes refer to their members by name. Unlike structs, classes are nominal types . References: Classes Proposal #722: Nominal classes and methods Proposal #989: Member access expressions Assignment There is an implicit conversions defined between a struct literal and a class type with the same fields, in any scope that has access to all of the class' fields. This may be used to assign or initialize a variable with a class type, as in: var sprocket: Widget = {.x = 3, .y = 4, .payload = \"Sproing\"}; sprocket = {.x = 2, .y = 1, .payload = \"Bounce\"}; References: Classes: Construction Proposal #981: Implicit conversions for aggregates Class functions and factory functions Classes may also contain class functions . These are functions that are accessed as members of the type, like static member functions in C++ , as opposed to methods that are members of instances. They are commonly used to define a function that creates instances. Carbon does not have separate constructors like C++ does. class Point { // Class function that instantiates `Point`. // `Self` in class scope means the class currently being defined. fn Origin() -> Self { return {.x = 0, .y = 0}; } var x: i32; var y: i32; } Note that if the definition of a function is provided inside the class scope, the body is treated as if it was defined immediately after the outermost class definition. This means that members such as the fields will be considered declared even if their declarations are later in the source than the class function. The returned var feature can be used if the address of the instance being created is needed in a factory function, as in: class Registered { fn Create() -> Self { returned var result: Self = {...}; StoreMyPointerSomewhere(&result); return var; } } This approach can also be used for types that can't be copied or moved. Methods Class type definitions can include methods: class Point { // Method defined inline fn Distance[me: Self](x2: i32, y2: i32) -> f32 { var dx: i32 = x2 - me.x; var dy: i32 = y2 - me.y; return Math.Sqrt(dx * dx - dy * dy); } // Mutating method declaration fn Offset[addr me: Self*](dx: i32, dy: i32); var x: i32; var y: i32; } // Out-of-line definition of method declared inline fn Point.Offset[addr me: Self*](dx: i32, dy: i32) { me->x += dx; me->y += dy; } var origin: Point = {.x = 0, .y = 0}; Assert(Math.Abs(origin.Distance(3, 4) - 5.0) < 0.001); origin.Offset(3, 4); Assert(origin.Distance(3, 4) == 0.0); This defines a Point class type with two integer data members x and y and two methods Distance and Offset : Methods are defined as class functions with a me parameter inside square brackets [ ... ] before the regular explicit parameter list in parens ( ... ) . Methods are called using using the member syntax, origin.Distance( ... ) and origin.Offset( ... ) . Distance computes and returns the distance to another point, without modifying the Point . This is signified using [me: Self] in the method declaration. origin.Offset( ... ) does modify the value of origin . This is signified using [addr me: Self*] in the method declaration. Since calling this method requires taking the address of origin , it may only be called on non- const l-values . Methods may be declared lexically inline like Distance , or lexically out of line like Offset . References: Methods Proposal #722: Nominal classes and methods Inheritance The philosophy of inheritance support in Carbon is to focus on use cases where inheritance is a good match, and use other features for other cases. For example, mixins for implementation reuse and generics for separating interface from implementation. This allows Carbon to move away from multiple inheritance , which doesn't have as efficient of an implementation strategy. Classes by default are final , which means they may not be extended. A class may be declared as allowing extension using either the base class or abstract class introducer instead of class . An abstract class is a base class that may not itself be instantiated. base class MyBaseClass { ... } Either kind of base class may be extended to get a derived class . Derived classes are final unless they are themselved declared base or abstract . Classes may only extend a single class. Carbon only supports single inheritance, and will use mixins instead of multiple inheritance. base class MiddleDerived extends MyBaseClass { ... } class FinalDerived extends MiddleDerived { ... } // \u274c Forbidden: class Illegal extends FinalDerived { ... } A base class may define virtual methods . These are methods whose implementation may be overridden in a derived class. By default methods are non-virtual , the declaration of a virtual method must be prefixed by one of these three keywords: A method marked virtual has a definition in this class but not in any base. A method marked abstract does not have have a definition in this class, but must have a definition in any non- abstract derived class. A method marked impl has a definition in this class, overriding any definition in a base class. A pointer to a derived class may be cast to a pointer to one of its base classes. Calling a virtual method through a pointer to a base class will use the overridden definition provided in the derived class. Base classes with virtual methods may use run-time type information in a match statement to dynamically test whether the dynamic type of a value is some derived class, as in: var base_ptr: MyBaseType* = ...; match (base_ptr) { case dyn p: MiddleDerived* => { ... } } For purposes of construction, a derived class acts like its first field is called base with the type of its immediate base class. class MyDerivedType extends MyBaseType { fn Create() -> MyDerivedType { return {.base = MyBaseType.Create(), .derived_field = 7}; } var derived_field: i32; } Abstract classes can't be instantiated, so instead they should define class functions returning partial Self . Those functions should be marked protected so they may only be used by derived classes. abstract class AbstractClass { protected fn Create() -> partial Self { return {.field_1 = 3, .field_2 = 9}; } // ... var field_1: i32; var field_2: i32; } // \u274c Error: can't instantiate abstract class var abc: AbstractClass = ...; class DerivedFromAbstract extends AbstractClass { fn Create() -> Self { // AbstractClass.Create() returns a // `partial AbstractClass` that can be used as // the `.base` member when constructing a value // of a derived class. return {.base = AbstractClass.Create(), .derived_field = 42 }; } var derived_field: i32; } References: Inheritance Proposal #777: Inheritance Proposal #820: Implicit conversions Access control Class members are by default publicly accessible. The private keyword prefix can be added to the member's declaration to restrict it to members of the class or any friends. A private virtual or private abstract method may be implemented in derived classes, even though it may not be called. Friends may be declared using a friend declaration inside the class naming an existing function or type. Unlike C++, friend declarations may only refer to names resolvable by the compiler, and don't act like forward declarations. protected is like private , but also gives access to derived classes. References: Access control for class members Question-for-leads issue #665: private vs public syntax strategy, as well as other visibility tools like external / api /etc. Question-for-leads issue #971: Private interfaces in public API files Destructors A destructor for a class is custom code executed when the lifetime of a value of that type ends. They are defined with the destructor keyword followed by either [me: Self] or [addr me: Self*] (as is done with methods ) and the block of code in the class definition, as in: class MyClass { destructor [me: Self] { ... } } or: class MyClass { // Can modify `me` in the body. destructor [addr me: Self*] { ... } } The destructor for a class is run before the destructors of its data members. The data members are destroyed in reverse order of declaration. Derived classes are destroyed before their base classes. A destructor in an abstract or base class may be declared virtual like with methods . Destructors in classes derived from one with a virtual destructor must be declared with the impl keyword prefix. It is illegal to delete an instance of a derived class through a pointer to a base class unless the base class is declared virtual or impl . To delete a pointer to a non-abstract base class when it is known not to point to a value with a derived type, use UnsafeDelete . References: Destructors Proposal #1154: Destructors const Note: This is provisional, no design for const has been through the proposal process yet. For every type MyClass , there is the type const MyClass such that: The data representation is the same, so a MyClass* value may be implicitly converted to a (const MyClass)* . A const MyClass l-value may automatically convert to a MyClass r-value, the same way that a MyClass l-value can. If member x of MyClass has type T , then member x of const MyClass has type const T . The API of a const MyClass is a subset of MyClass , excluding all methods taking [addr me: Self*] . Note that const binds more tightly than postfix- * for forming a pointer type, so const MyClass* is equal to (const MyClass)* . This example uses the definition of Point from the \"methods\" section : var origin: Point = {.x = 0, .y = 0}; // \u2705 Allowed conversion from `Point*` to // `const Point*`: let p: const Point* = &origin; // \u2705 Allowed conversion of `const Point` l-value // to `Point` r-value. let five: f32 = p->Distance(3, 4); // \u274c Error: mutating method `Offset` excluded // from `const Point` API. p->Offset(3, 4); // \u274c Error: mutating method `AssignAdd.Op` // excluded from `const i32` API. p->x += 2; Unformed state Types indicate that they support unformed states by implementing a particular interface , otherwise variables of that type must be explicitly initialized when they are declared. An unformed state for an object is one that satisfies the following properties: Assignment from a fully formed value is correct using the normal assignment implementation for the type. Destruction must be correct using the type's normal destruction implementation. Destruction must be optional. The behavior of the program must be equivalent whether the destructor is run or not for an unformed object, including not leaking resources. A type might have more than one in-memory representation for the unformed state, and those representations may be the same as valid fully formed values for that type. For example, all values are legal representations of the unformed state for any type with a trivial destructor like i32 . Types may define additional initialization for the hardened build mode . For example, this causes integers to be set to 0 when in unformed state in this mode. Any operation on an unformed object other than destruction or assignment from a fully formed value is an error, even if its in-memory representation is that of a valid value for that type. References: Proposal #257: Initialization of memory and variables Move Carbon will allow types to define if and how they are moved. This can happen when returning a value from a function or by using the move operator ~x . This leaves x in an unformed state and returns its old value. Mixins Mixins allow reuse with different trade-offs compared to inheritance . Mixins focus on implementation reuse, such as might be done using CRTP or multiple inheritance in C++. TODO: The design for mixins is still under development. Choice types A choice type is a tagged union , that can store different types of data in a storage space that can hold the largest. A choice type has a name, and a list of cases separated by commas ( , ). Each case has a name and an optional parameter list. choice IntResult { Success(value: i32), Failure(error: String), Cancelled } The value of a choice type is one of the cases, plus the values of the parameters to that case, if any. A value can be constructed by naming the case and providing values for the parameters, if any: fn ParseAsInt(s: String) -> IntResult { var r: i32 = 0; for (c: i32 in s) { if (not IsDigit(c)) { // Equivalent to `IntResult.Failure(...)` return .Failure(\"Invalid character\"); } // ... } return .Success(r); } Choice type values may be consumed using a match statement : match (ParseAsInt(s)) { case .Success(value: i32) => { return value; } case .Failure(error: String) => { Display(error); } case .Cancelled => { Terminate(); } } They can also represent an enumerated type , if no additional data is associated with the choices, as in: choice LikeABoolean { False, True } References: Proposal #157: Design direction for sum types Proposal #162: Basic Syntax Names Names are introduced by declarations and are valid until the end of the scope in which they appear. Code may not refer to names earlier in the source than they are declared. In executable scopes such as function bodies, names declared later are not found. In declarative scopes such as packages, classes, and interfaces, it is an error to refer to names declared later, except that inline class member function bodies are parsed as if they appeared after the class . A name in Carbon is formed from a sequence of letters, numbers, and underscores, and starts with a letter. We intend to follow Unicode's Annex 31 in selecting valid identifier characters, but a concrete set of valid characters has not been selected yet. References: Lexical conventions Principle: Information accumulation Proposal #142: Unicode source files Question-for-leads issue #472: Open question: Calling functions defined later in the same file Proposal #875: Principle: information accumulation Files, libraries, packages Files are grouped into libraries, which are in turn grouped into packages. Libraries are the granularity of code reuse through imports. Packages are the unit of distribution. Each library must have exactly one api file. This file includes declarations for all public names of the library. Definitions for those declarations must be in some file in the library, either the api file or an impl file. Every package has its own namespace. This means libraries within a package need to coordinate to avoid name conflicts, but not across packages. References: Code and name organization Proposal #107: Code and name organization Package declaration Files start with an optional package declaration, consisting of: the package keyword introducer, an optional identifier specifying the package name, optional library followed by a string with the library name, either api or impl , and a terminating semicolon ( ; ). For example: // Package name is `Geometry`. // Library name is \"Shapes\". // This file is an `api` file, not an `impl` file. package Geometry library \"Shapes\" api; Parts of this declaration may be omitted: If the package name is omitted, as in package library \"Main\" api; , the file contributes to the default package. No other package may import from the default package. If the library keyword is not specified, as in package Geometry api; , this file contributes to the default library. If a file has no package declaration at all, it is the api file belonging to the default package and default library. This is particularly for tests and smaller examples. No other library can import this library even from within the default package. It can be split across multiple impl files using a package impl; package declaration. A program need not use the default package, but if it does, it should contain the entry-point function. By default, the entry-point function is Run from the default package. References: Code and name organization Proposal #107: Code and name organization Imports After the package declaration, files may include import declarations. These include the package name and optionally library followed by the library name. If the library is omitted, the default library for that package is imported. // Import the \"Vector\" library from the // `LinearAlgebra` package. import LinearAlgebra library \"Vector\"; // Import the default library from the // `ArbitraryPrecision` package. import ArbitraryPrecision; The syntax import PackageName ... introduces the name PackageName as a private name naming the given package. It cannot be used to import libraries of the current package. Importing additional libraries from that package makes additional members of PackageName visible. Libraries from the current package are imported by omitting the package name. // Import the \"Vertex\" library from the same package. import library \"Vertex\"; // Import the default library from the same package. import library default; The import library ... syntax adds all the public top-level names within the given library to the top-level scope of the current file as private names, and similarly for names in namespaces . Every impl file automatically imports the api file for its library. All import declarations must appear before all other non- package declarations in the file. References: Code and name organization Proposal #107: Code and name organization Name visibility The names visible from an imported library are determined by these rules: Declarations in an api file are by default public , which means visible to any file that imports that library. This matches class members, which are also default public . A private prefix on a declaration in an api file makes the name library private . This means the name is visible in the file and all impl files for the same library. The visibility of a name is determined by its first declaration, considering api files before impl files. The private prefix is only allowed on the first declaration. A name declared in an impl file and not the corresponding api file is file private , meaning visible in just that file. Its first declaration must be marked with a private prefix. TODO: This needs to be finalized in a proposal to resolve inconsistency between #665 and #1136 . Private names don't conflict with names outside the region they're private to: two different libraries can have different private names foo without conflict, but a private name conflicts with a public name in the same scope. At most one api file in a package transitively used in a program may declare a given name public. References: Exporting entities from an API file Question-for-leads issue #665: private vs public syntax strategy, as well as other visibility tools like external / api /etc. Proposal #752: api file default public Proposal #931: Generic impls access (details 4) Question-for-leads issue #1136: what is the top-level scope in a source file, and what names are found there? Package scope The top-level scope in a package is the scope of the package. This means: Within this scope (and its sub-namespaces), all visible names from the same package appear. This includes names from the same file, names from the api file of a library when inside an impl file, and names from imported libraries of the same package. In scopes where package members might have a name conflict with something else, the syntax package.Foo can be used to name the Foo member of the current package. In this example, the names F and P are used in a scope where they could mean two different things, and qualifications are needed to disambiguate : import P; fn F(); class C { fn F(); class P { fn H(); } fn G() { // \u274c Error: ambiguous whether `F` means // `package.F` or `package.C.F`. F(); // \u2705 Allowed: fully qualified package.F(); package.C.F(); // \u2705 Allowed: unambiguous C.F(); // \u274c Error: ambiguous whether `P` means // `package.P` or `package.P.F`. P.H(); // \u2705 Allowed package.P.H(); package.C.P.H(); C.P.H(); } } References: Code and name organization Proposal #107: Code and name organization Proposal #752: api file default public Question-for-leads issue #1136: what is the top-level scope in a source file, and what names are found there? Namespaces A namespace declaration defines a name that may be used as a prefix of names declared afterward. When defining a member of a namespace, other members of that namespace are considered in scope and may be found by name lookup without the namespace prefix. In this example, package P defines some of its members inside a namespace N : package P api; // Defines namespace `N` within the current package. namespace N; // Defines namespaces `M` and `M.L`. namespace M.L; fn F(); // \u2705 Allowed: Declares function `G` in namespace `N`. private fn N.G(); // \u274c Error: `Bad` hasn't been declared. fn Bad.H(); fn J() { // \u274c Error: No `package.G` G(); } fn N.K() { // \u2705 Allowed: Looks in both `package` and `package.N`. // Finds `package.F` and `package.N.G`. F(); G(); } // \u2705 Allowed: Declares function `R` in namespace `M.L`. fn M.L.R(); // \u2705 Allowed: Declares function `Q` in namespace `M`. fn M.Q(); Another package importing P can refer to the public members of that namespace by prefixing with the package name P followed by the namespace: import P; // \u2705 Allowed: `F` is public member of `P`. P.F(); // \u274c Error: `N.G` is a private member of `P`. P.N.G(); // \u2705 Allowed: `N.K` is public member of `P`. P.N.K(); // \u2705 Allowed: `M.L.R` is public member of `P`. P.M.L.R(); // \u2705 Allowed: `M.Q` is public member of `P`. P.M.Q(); References: \"Namespaces\" in \"Code and name organization\" \"Package and namespace members\" in \"Qualified names and member access\" Naming conventions Our naming conventions are: For idiomatic Carbon code: UpperCamelCase will be used when the named entity cannot have a dynamically varying value. For example, functions, namespaces, or compile-time constant values. Note that virtual methods are named the same way to be consistent with other functions and methods. lower_snake_case will be used when the named entity's value won't be known until runtime, such as for variables. For Carbon-provided features: Keywords and type literals will use lower_snake_case . Other code will use the conventions for idiomatic Carbon code. References: Naming conventions Proposal #861: Naming conventions Aliases alias declares a name as equivalent to another name, for example: alias NewName = SomePackage.OldName; Note that the right-hand side of the equal sign ( = ) is a name not a value, so alias four = 4; is not allowed. This allows alias to work with entities like namespaces, which aren't values in Carbon. This can be used during an incremental migration when changing a name, or to include a name in a public API. For example, alias may be used to include a name from an interface implementation as a member of a class or named constraint , possibly renamed: class ContactInfo { external impl as Printable; external impl as ToPrinterDevice; alias PrintToScreen = Printable.Print; alias PrintToPrinter = ToPrinterDevice.Print; ... } References: Aliases \"Aliasing\" in \"Code and name organization\" alias a name from an external impl alias a name in a named constraint Proposal #553: Generics details part 1 Question-for-leads issue #749: Alias syntax Name lookup The general principle of Carbon name lookup is that we look up names in all relevant scopes, and report an error if the name is found to refer to more than one different entity. So Carbon requires disambiguation by adding qualifiers instead of doing any shadowing of names. For an example, see the \"package scope\" section . Unqualified name lookup walks the semantically-enclosing scopes, not only the lexically-enclosing ones. So when a lookup is performed within fn MyNamespace.MyClass.MyNestedClass.MyFunction() , we will look in MyNestedClass , MyClass , MyNamespace , and the package scope, even when the lexically-enclosing scope is the package scope. This means that the definition of a method will look for names in the class' scope even if it is written lexically out of line: class C { fn F(); fn G(); } fn C.G() { // \u2705 Allowed: resolves to `package.C.F`. F(); } Member name lookup follows a similar philosophy. If a checked-generic type parameter is known to implement multiple interfaces due to a constraint using & or where clauses , member name lookup into that type will look in all of the interfaces. If it is found in multiple, the name must be disambiguated by qualifying using compound member access ( 1 , 2 ). A template-generic type parameter performs look up into the caller's type in addition to the constraint. Carbon also rejects cases that would be invalid if all declarations in the file, including ones appearing later, were visible everywhere, not only after their point of appearance: class C { fn F(); fn G(); } fn C.G() { F(); } // Error: use of `F` in `C.G` would be ambiguous // if this declaration was earlier. fn F(); References: Name lookup \"Qualified names and member access\" section of \"Expressions\" Qualified names and member access Principle: Information accumulation Proposal #875: Principle: information accumulation Proposal #989: Member access expressions Question-for-leads issue #1136: what is the top-level scope in a source file, and what names are found there? Name lookup for common types Common types that we expect to be used universally will be provided for every file are made available as if there was a special \"prelude\" package that was imported automatically into every api file. Dedicated type literal syntaxes like i32 and bool refer to types defined within this package, based on the \"all APIs are library APIs\" principle . References: Name lookup Principle: All APIs are library APIs Question-for-leads issue #750: Naming conventions for Carbon-provided features Question-for-leads issue #1058: How should interfaces for core functionality be named? Proposal #1280: Principle: All APIs are library APIs Generics Generics allow Carbon constructs like functions and classes to be written with compile-time parameters and apply generically to different types using those parameters. For example, this Min function has a type parameter T that can be any type that implements the Ordered interface. fn Min[T:! Ordered](x: T, y: T) -> T { // Can compare `x` and `y` since they have // type `T` known to implement `Ordered`. return if x <= y then x else y; } var a: i32 = 1; var b: i32 = 2; // `T` is deduced to be `i32` Assert(Min(a, b) == 1); // `T` is deduced to be `String` Assert(Min(\"abc\", \"xyz\") == \"abc\"); Since the T type parameter is in the deduced parameter list in square brackets ( [ ... ] ) before the explicit parameter list in parentheses ( ( ... ) ), the value of T is determined from the types of the explicit arguments instead of being passed as a separate explicit argument. References: TODO: Revisit Generics: Overview Proposal #524: Generics overview Proposal #553: Generics details part 1 Proposal #950: Generic details 6: remove facets Checked and template parameters The :! indicates that T is a checked parameter passed at compile time. \"Checked\" here means that the body of Min is type checked when the function is defined, independent of the specific type values T is instantiated with, and name lookup is delegated to the constraint on T ( Ordered in this case). This type checking is equivalent to saying the function would pass type checking given any type T that implements the Ordered interface. Then calls to Min only need to check that the deduced type value of T implements Ordered . The parameter could alternatively be declared to be a template parameter by prefixing with the template keyword, as in template T:! Type . fn Convert[template T:! Type](source: T, template U:! Type) -> U { var converted: U = source; return converted; } fn Foo(i: i32) -> f32 { // Instantiates with the `T` implicit argument set to `i32` and the `U` // explicit argument set to `f32`, then calls with the runtime value `i`. return Convert(i, f32); } Carbon templates follow the same fundamental paradigm as C++ templates : they are instantiated when called, resulting in late type checking, duck typing, and lazy binding. One difference from C++ templates, Carbon template instantiation is not controlled by the SFINAE rule of C++ ( 1 , 2 ) but by explicit if clauses evaluated at compile-time. The if clause is at the end of the declaration, and the condition can only use constant values known at type-checking time, including template parameters. class Array(template T:! Type, template N:! i64) if N >= 0 and N < MaxArraySize / sizeof(T); Member lookup into a template type parameter is done in the actual type value provided by the caller, in addition to any constraints. This means member name lookup and type checking for anything dependent on the template parameter can't be completed until the template is instantiated with a specific concrete type. When the constraint is just Type , this gives semantics similar to C++ templates. Constraints can then be added incrementally, with the compiler verifying that the semantics stay the same. Once all constraints have been added, removing the word template to switch to a checked parameter is safe. The value phase of a checked parameter is a symbolic value whereas the value phase of a template parameter is constant. Although checked generics are generally preferred, templates enable translation of code between C++ and Carbon, and address some cases where the type checking rigor of generics are problematic. References: Templates Proposal #553: Generics details part 1 Question-for-leads issue #949: Constrained template name lookup Proposal #989: Member access expressions Interfaces and implementations Interfaces specify a set of requirements that a types might satisfy. Interfaces act both as constraints on types a caller might supply and capabilities that may be assumed of types that satisfy that constraint. interface Printable { // Inside an interface definition `Self` means // \"the type implementing this interface\". fn Print[me: Self](); } In addition to function requirements, interfaces can contain: requirements that other interfaces be implemented or interfaces that this interface extends associated types and other associated constants interface defaults final interface members Types only implement an interface if there is an explicit impl declaration that they do. Simply having a Print function with the right signature is not sufficient. class Circle { var radius: f32; impl as Printable { fn Print[me: Self]() { Console.WriteLine(\"Circle with radius: {0}\", me.radius); } } } In this case, Print is a member of Circle . Interfaces may also be implemented externally , which means the members of the interface are not direct members of the type. Those methods may still be called using compound member access syntax ( 1 , 2 ) to qualify the name of the member, as in x.(Printable.Print)() . External implementations don't have to be in the same library as the type definition, subject to the orphan rule ( 1 , 2 ) for coherence . Interfaces and implementations may be forward declared by replacing the definition scope in curly braces ( { ... } ) with a semicolon. References: Generics: Interfaces Generics: Implementing interfaces Proposal #553: Generics details part 1 Proposal #731: Generics details 2: adapters, associated types, parameterized interfaces Proposal #624: Coherence: terminology, rationale, alternatives considered Proposal #990: Generics details 8: interface default and final members Proposal #1084: Generics details 9: forward declarations Question-for-leads issue #1132: How do we match forward declarations with their definitions? Combining constraints A function can require calling types to implement multiple interfaces by combining them using an ampersand ( & ): fn PrintMin[T:! Ordered & Printable](x: T, y: T) { // Can compare since type `T` implements `Ordered`. if (x <= y) { // Can call `Print` since type `T` implements `Printable`. x.Print(); } else { y.Print(); } } The body of the function may call functions that are in either interface, except for names that are members of both. In that case, use the compound member access syntax ( 1 , 2 ) to qualify the name of the member, as in: fn DrawTies[T:! Renderable & GameResult](x: T) { if (x.(GameResult.Draw)()) { x.(Renderable.Draw)(); } } References: Combining interfaces by anding type-of-types Question-for-leads issue #531: Combine interfaces with + or & Proposal #553: Generics details part 1 Associated types An associated type is a type member of an interface whose value is determined by the implementation of that interface for a specific type. These values are set to compile-time values in implementations, and so use the :! generic syntax inside a let declaration without an initializer. This allows types in the signatures of functions in the interface to vary. For example, an interface describing a stack might use an associated type to represent the type of elements stored in the stack. interface StackInterface { let ElementType:! Movable; fn Push[addr me: Self*](value: ElementType); fn Pop[addr me: Self*]() -> ElementType; fn IsEmpty[addr me: Self*]() -> bool; } Then different types implementing StackInterface can specify different type values for the ElementType member of the interface using a where clause: class IntStack { impl as StackInterface where .ElementType == i32 { fn Push[addr me: Self*](value: i32); // ... } } class FruitStack { impl as StackInterface where .ElementType == Fruit { fn Push[addr me: Self*](value: Fruit); // ... } } References: Generics: Associated types Proposal #731: Generics details 2: adapters, associated types, parameterized interfaces Proposal #1013: Generics: Set associated constants using where constraints Generic entities Many Carbon entities, not just functions, may be made generic by adding checked or template parameters . Generic Classes Classes may be defined with an optional explicit parameter list. All parameters to a class must be generic, and so defined with :! , either with or without the template prefix. For example, to define a stack that can hold values of any type T : class Stack(T:! Type) { fn Push[addr me: Self*](value: T); fn Pop[addr me: Self*]() -> T; var storage: Array(T); } var int_stack: Stack(i32); In this example: Stack is a type parameterized by a type T . T may be used within the definition of Stack anywhere a normal type would be used. Array(T) instantiates generic type Array with its parameter set to T . Stack(i32) instantiates Stack with T set to i32 . The values of type parameters are part of a type's value, and so may be deduced in a function call, as in this example: fn PeekTopOfStack[T:! Type](s: Stack(T)*) -> T { var top: T = s->Pop(); s->Push(top); return top; } // `int_stack` has type `Stack(i32)`, so `T` is deduced to be `i32`. PeekTopOfStack(&int_stack); References: Generic or parameterized types Proposal #1146: Generic details 12: parameterized types Generic choice types Choice types may be parameterized similarly to classes: choice Result(T:! Type, Error:! Type) { Success(value: T), Failure(error: Error) } Generic interfaces Interfaces are always parameterized by a Self type, but in some cases they will have additional parameters. interface AddWith(U:! Type); Interfaces without parameters may only be implemented once for a given type, but a type can have distinct implementations of AddWith(i32) and AddWith(BigInt) . Parameters to an interface determine which implementation is selected for a type, in contrast to associated types which are determined by the implementation of an interface for a type. References: Generic or parameterized interfaces Proposal #731: Generics details 2: adapters, associated types, parameterized interfaces Generic implementations An impl declaration may be parameterized by adding forall [ generic parameter list ] after the impl keyword introducer, as in: external impl forall [T:! Printable] Vector(T) as Printable; external impl forall [Key:! Hashable, Value:! Type] HashMap(Key, Value) as Has(Key); external impl forall [T:! Ordered] T as PartiallyOrdered; external impl forall [T:! ImplicitAs(i32)] BigInt as AddWith(T); external impl forall [U:! Type, T:! As(U)] Optional(T) as As(Optional(U)); Generic implementations can create a situation where multiple impl definitions apply to a given type and interface query. The specialization rules pick which definition is selected. These rules ensure: Implementations have coherence , so the same implementation is always selected for a given query. Libraries will work together as long as they pass their separate checks. A generic function can assume that some impl will be successfully selected if it can see an impl that applies, even though another more specific impl may be selected. Implementations may be marked final to indicate that they may not be specialized, subject to some restrictions . References: Generic or parameterized impls Proposal #624: Coherence: terminology, rationale, alternatives considered Proposal #920: Generic parameterized impls (details 5) Proposal #983: Generics details 7: final impls Other features Carbon generics have a number of other features, including: Named constraints may be used to disambiguate when combining two interfaces that have name conflicts. Named constraints may be implemented and otherwise used in place of an interface. Template constraints are a kind of named constraint that can contain structural requirements. For example, a template constraint could match any type that has a function with a specific name and signature without any explicit declaration that the type implements the constraint. Template constraints may only be used as requirements for template parameters. An adapter type is a type with the same data representation as an existing type, so you may cast between the two types, but can implement different interfaces or implement interfaces differently. Additional requirements can be placed on the associated types of an interface using where constraints . Implied constraints allows some constraints to be deduced and omitted from a function signature. Dynamic erased types can hold any value with a type implementing an interface, and allows the functions in that interface to be called using dynamic dispatch , for some interfaces marked \" dyn -safe\". Variadics supports variable-length parameter lists. References: Generics details Proposal #553: Generics details part 1 Proposal #731: Generics details 2: adapters, associated types, parameterized interfaces Proposal #818: Constraints for generics (generics details 3) Generic type equality and observe declarations Determining whether two types must be equal in a generic context is in general undecidable, as has been shown in Swift . To make compilation fast, the Carbon compiler will limit its search to a depth of 1, only identifying types as equal if there is an explicit declaration that they are equal in the code, such as in a where constraint . There will be situations where two types must be equal as the result of combining these facts, but the compiler will return a type error since it did not realize they are equal due to the limit of the search. An observe ... == declaration may be added to describe how two types are equal, allowing more code to pass type checking. An observe declaration showing types are equal can increase the set of interfaces the compiler knows that a type implements. It is also possible that knowing a type implements one interface implies that it implements another, from an interface requirement or generic implementation . An observe ... is declaration may be used to observe that a type implements an interface . References: Generics: observe declarations Generics: Observing a type implements an interface Proposal #818: Constraints for generics (generics details 3) Proposal #1088: Generic details 10: interface-implemented requirements Operator overloading Uses of an operator in an expression is translated into a call to a method of an interface. For example, if x has type T and y has type U , then x + y is translated into a call to x.(AddWith(U).Op)(y) . So overloading of the + operator is accomplished by implementing interface AddWith(U) for type T . In order to support implicit conversion of the first operand to type T and the second argument to type U , add the like keyword to both types in the impl declaration, as in: external impl like T as AddWith(like U) where .Result == V { // `Self` is `T` here fn Op[me: Self](other: U) -> V { ... } } When the operand types and result type are all the same, this is equivalent to implementing the Add interface: external impl T as Add { fn Op[me: Self](other: Self) -> Self { ... } } The interfaces that correspond to each operator are given by: Arithmetic : -x : Negate x + y : Add or AddWith(U) x - y : Sub or SubWith(U) x * y : Mul or MulWith(U) x / y : Div or DivWith(U) x % y : Mod or ModWith(U) Bitwise and shift operators : ^x : BitComplement x & y : BitAnd or BitAndWith(U) x | y : BitOr or BitOrWith(U) x ^ y : BitXor or BitXorWith(U) x << y : LeftShift or LeftShiftWith(U) x >> y : RightShift or RightShiftWith(U) Comparison: x == y , x != y overloaded by implementing Eq or EqWith(U) x < y , x > y , x <= y , x >= y overloaded by implementing Ordered or OrderedWith(U) Conversion: x as U is rewritten to use the As(U) interface Implicit conversions use ImplicitAs(U) TODO: Assignment : x = y , ++x , x += y , and so on TODO: Dereference: *p TODO: Move : ~x TODO: Indexing: a[3] TODO: Function call: f(4) The logical operators can not be overloaded . Operators that result in l-values , such as dereferencing *p and indexing a[3] , have interfaces that return the address of the value. Carbon automatically dereferences the pointer to get the l-value. Operators that can take multiple arguments, such as function calling operator f(4) , have a variadic parameter list. Whether and how a value supports other operations, such as being copied, swapped, or set into an unformed state , is also determined by implementing corresponding interfaces for the value's type. References: Operator overloading Proposal #702: Comparison operators Proposal #820: Implicit conversions Proposal #845: as expressions Question-for-leads issue #1058: How should interfaces for core functionality be named? Proposal #1083: Arithmetic expressions Proposal #1191: Bitwise operators Proposal #1178: Rework operator interfaces Common type There are some situations where the common type for two types is needed: A conditional expression like if c then t else f returns a value with the common type of t and f . If there are multiple parameters to a function with a type parameter, it will be set to the common type of the corresponding arguments, as in: ```carbon fn F T:! Type ; // Calls F with T set to the // common type of G() and H() : F(G(), H()); ``` The inferred return type of a function with auto return type is the common type of its return statements. The common type is specified by implementing the CommonTypeWith interface: // Common type of `A` and `B` is `C`. impl A as CommonTypeWith(B) where .Result == C { } The common type is required to be a type that both types have an implicit conversion to. References: if expressions Proposal #911: Conditional expressions Question-for-leads issue #1077: find a way to permit impls of CommonTypeWith where the LHS and RHS type overlap Bidirectional interoperability with C and C++ Interoperability, or interop , is the ability to call C and C++ code from Carbon code and the other way around. This ability achieves two goals: Allows sharing a code and library ecosystem with C and C++. Allows incremental migration to Carbon from C and C++. Carbon's approach to interopp is most similar to Java/Kotlin interop , where the two languages are different, but share enough of runtime model that data from one side can be used from the other. For example, C++ and Carbon will use the same memory model . The design for interoperability between Carbon and C++ hinges on: The ability to interoperate with a wide variety of code, such as classes/structs and templates , not just free functions. A willingness to expose the idioms of C++ into Carbon code, and the other way around, when necessary to maximize performance of the interoperability layer. The use of wrappers and generic programming, including templates, to minimize or eliminate runtime overhead. This feature will have some restrictions; only a subset of Carbon APIs will be available to C++ and a subset of C++ APIs will be available to Carbon. To achieve simplification in Carbon, its programming model will exclude some rarely used and complex features of C++. For example, there will be limitations on multiple inheritance . C or C++ features that compromise the performance of code that don't use that feature, like RTTI and exceptions , are in particular subject to revision in Carbon. References: Bidirectional interoperability with C/C++ Proposal #175: C++ interoperability goals Goals The goals for interop include: Support mixing Carbon and C++ toolchains Compatibility with the C++ memory model Minimize bridge code Unsurprising mappings between C++ and Carbon types Allow C++ bridge code in Carbon files Carbon inheritance from C++ types Support use of advanced C++ features Support basic C interoperability References: Interoperability: Goals Non-goals The non-goals for interop include: Full parity between a Carbon-only toolchain and mixing C++/Carbon toolchains Never require bridge code Convert all C++ types to Carbon types Support for C++ exceptions without bridge code Cross-language metaprogramming Offer equivalent support for languages other than C++ References: Interoperability: Non-goals Importing and #include A C++ library header file may be imported into Carbon using an import declaration of the special Cpp package. // like `#include \"circle.h\"` in C++ import Cpp library \"circle.h\"; This adds the names from circle.h into the Cpp namespace. If circle.h defines some names in a namespace shapes { ... } scope, those will be found in Carbon's Cpp.shapes namespace. In the other direction, Carbon packages can export a header file to be #include d from C++ files. // like `import Geometry` in Carbon #include \"geometry.carbon.h\" Generally Carbon entities will be usable from C++ and C++ entities will be usable from Carbon. This includes types, function, and constants. Some entities, such as Carbon interfaces, won't be able to be translated directly. C and C++ macros that are defining constants will be imported as constants. Otherwise, C and C++ macros will be unavailable in Carbon. C and C++ typedef s would be translated into type constants, as if declared using a let . Carbon functions and types that satisfy some restrictions may be annotated as exported to C as well, like C++'s extern \"C\" marker. ABI and dynamic linking Carbon itself will not have a stable ABI for the language as a whole, and most language features will be designed around not having any ABI stability. Instead, we expect to add dedicated language features that are specifically designed to provide an ABI-stable boundary between two separate parts of a Carbon program. These ABI-resilient language features and API boundaries will be opt-in and explicit. They may also have functionality restrictions to make them easy to implement with strong ABI resilience. When interoperating with already compiled C++ object code or shared libraries, the C++ interop may be significantly less feature rich than otherwise. This is an open area for us to explore, but we expect to require re-compiling C++ code in order to get the full ergonomic and performance benefits when interoperating with Carbon. For example, recompilation lets us ensure Carbon and C++ can use the same representation for key vocabulary types. However, we expect to have full support for the C ABI when interoperating with already-compiled C object code or shared libraries. We expect Carbon's bridge code functionality to cover similar use cases as C++'s extern \"C\" marker in order to provide full bi-directional support here. The functionality available across this interop boundary will of course be restricted to what is expressible in the C ABI, and types may need explicit markers to have guaranteed ABI compatibility. Operator overloading Operator overloading is supported in Carbon, but is done by implementing an interface instead of defining a method or nonmember function as in C++. Carbon types implementing an operator overload using an interface should get the corresponding operator overload in C++. So implementing ModWith(U) in Carbon for a type effectively implements operator% in C++ for that type. This also works in the other direction, so C++ types implementing an operator overload are automatically considered to implement the corresponding Carbon interface. So implementing operator% in C++ for a type also implements interface ModWith(U) in Carbon. However, there may be edge cases around implicit conversions or overload selection that don't map completely into Carbon. In some cases, the operation might be written differently in the two languages. In those cases, they are matched according to which operation has the most similar semantics rather than using the same symbols. For example, the ^x operation and BitComplement interface in Carbon corresponds to the ~x operation and operator~ function in C++. Similarly, the ImplicitAs(U) Carbon interface corresponds to implicit conversions in C++, which can be written in multiple different ways. Other C++ customization points like swap will correspond to a Carbon interface, on a case-by-case basis. Some operators will only exist or be overridable in C++, such as logical operators or the comma operator. In the unlikely situation where those operators need to be overridden for a Carbon type, that can be done with a nonmember C++ function. Carbon interfaces with no C++ equivalent, such as CommonTypeWith(U) , may be implemented for C++ types externally in Carbon code. To satisfy the orphan rule ( 1 , 2 ), each C++ library will have a corresponding Carbon wrapper library that must be imported instead of the C++ library if the Carbon wrapper exists. TODO: Perhaps it will automatically be imported, so a wrapper may be added without requiring changes to importers? Templates Carbon supports both checked and template generics . This provides a migration path for C++ template code: C++ template -> Carbon template: This involves migrating the code from C++ to Carbon. If that migration is faithful, the change should be transparent to callers. -> Carbon template with constraints: Constraints may be added one at a time. Adding a constraint never changes the meaning of the code as long as it continues to compile. Compile errors will point to types for which an implementation of missing interfaces is needed. A temporary template implementation of that interface can act as a bridge during the transition. -> Carbon checked generic: Once all callers work after all constraints have been added, the template parameter may be switched to a checked generic. Carbon will also provide direct interop with C++ templates in many ways: Ability to call C++ templates and use C++ templated types from Carbon. Ability to instantiate a C++ template with a Carbon type. Ability to instantiate a Carbon generic with a C++ type. We expect the best interop in these areas to be based on a Carbon-provided C++ toolchain. However, even when using Carbon's generated C++ headers for interop, we will include the ability where possible to use a Carbon generic from C++ as if it were a C++ template. Standard types The Carbon integer types, like i32 and u64 , are considered equal to the corresponding fixed-width integer types in C++, like int32_t and uint64_t , provided by <stdint.h> or <cstdint> . The basic C and C++ integer types like int , char , and unsigned long are available in Carbon inside the Cpp namespace given an import Cpp; declaration, with names like Cpp.int , Cpp.char , and Cpp.unsigned_long . C++ types are considered different if C++ considers them different, so C++ overloads are resolved the same way. Carbon conventions for implicit conversions between integer types apply here, allowing them whenever the numerical value for all inputs may be preserved by the conversion. Other C and C++ types are equal to Carbon types as follows: C or C++ Carbon bool bool float f32 double f64 T* Optional(T*) T[4] [T; 4] Further, C++ reference types like T& will be translated to T* in Carbon, which is Carbon's non-null pointer type. Carbon will work to have idiomatic vocabulary view types for common data structures, like std::string_view and std::span , map transparently between C++ and the Carbon equivalents. This will include data layout so that even pointers to these types translate seamlessly, contingent on a suitable C++ ABI for those types, potentially by re-compiling the C++ code with a customized ABI. We will also explore how to expand coverage to similar view types in other libraries. However, Carbon's containers will be distinct from the C++ standard library containers in order to maximize our ability to improve performance and leverage language features like checked generics in their design and implementation. Where possible, we will also try to provide implementations of Carbon's standard library container interfaces for the relevant C++ container types so that they can be directly used with generic Carbon code. This should allow generic code in Carbon to work seamlessly with both Carbon and C++ containers without performance loss or constraining the Carbon container implementations. In the other direction, Carbon containers will satisfy C++ container requirements, so templated C++ code can operate directly on Carbon containers as well. Inheritance Carbon has single inheritance allowing C++ classes using inheritance to be migrated. The data representation will be consistent so that Carbon classes may inherit from C++ classes, and the other way around, even with virtual methods. C++ multiple inheritance and CRTP will be migrated using a combination of Carbon features. Carbon mixins support implementation reuse and Carbon interfaces allow a type to implement multiple APIs. However, there may be limits on the degree of interop available with multiple inheritance across the C++ <-> Carbon boundaries. Carbon dyn-safe interfaces may be exported to C++ as an abstract base class . The reverse operation is also possible using a proxy object implementing a C++ abstract base class and holding a pointer to a type implementing the corresponding interface. Enums TODO Unfinished tales Safety Carbon's premise is that C++ users can't give up performance to get safety. Even if some isolated users can make that tradeoff, they share code with performance-sensitive users. Any path to safety must preserve performance of C++ today. This rules out garbage collection, and many other options. The only well understood mechanism of achieving safety without giving up performance is compile-time safety. The leading example of how to achieve this is Rust. The difference between Rust's approach and Carbon's is that Rust starts with safety and Carbons starts with migration. Rust supports interop with C, and there is ongoing work to improve the C++-interop story and develop migration tools. However, there is a large gap in programming models between the two languages, generally requiring a revision to the architecture. So, thus far the common pattern in the Rust community is to \"rewrite it in Rust\" ( 1 , 2 , 3 ). Carbon's approach is to focus on migration from C++, including seamless interop, and then incrementally improve safety. The first impact on Carbon's design to support its safety strategy are the necessary building blocks for this level of compile-time safety. We look at existing languages like Rust and Swift to understand what fundamental capabilities they ended up needing. The two components that stand out are: Expanded type system that includes more semantic information. More pervasive use of type system abstractions (typically generics). For migrating C++ code, we also need the ability to add features and migrate code to use those new features incrementally and over time. This requires designing the language with evolution baked in on day one. This impacts a wide range of features: At the lowest level, a simple and extensible syntax and grammar. Tools and support for adding and removing APIs. Scalable migration strategies, including tooling support. Rust shows the value of expanded semantic information in the type system such as precise lifetimes. This is hard to do in C++ since it has too many kinds of references and pointers, which increases the complexity in the type system multiplicatively. Carbon is attempting to compress C++'s type variations into just values and pointers . Rust also shows the value of functions parameterized by lifetimes. Since lifetimes are only used to establish safety properties of the code, there is no reason to pay the cost of monomorphization for those parameters. So we need a generics system that can reason about code before it is instantiated, unlike C++ templates. In conclusion, there are two patterns in how Carbon diverges from C++: Simplify and removing things to create space for new safety features. This trivially requires breaking backwards compatibility. Re-engineer foundations to model and enforce safety. This has complex and difficulty in C++ without first simplifying the language. This leads to Carbon's incremental path to safety: Keep your performance, your existing codebase, and your developers. Adopt Carbon through a scalable, tool-assisted migration from C++. Address initial, easy safety improvements starting day one. Shift the Carbon code onto an incremental path towards memory safety over the next decade. References: Safety strategy Lifetime and move semantics TODO: Metaprogramming TODO: References need to be evolved. Needs a detailed design and a high level summary provided inline. Carbon provides metaprogramming facilities that look similar to regular Carbon code. These are structured, and do not offer arbitrary inclusion or preprocessing of source text such as C and C++ do. References: Metaprogramming Pattern matching as function overload resolution TODO: References need to be evolved. Needs a detailed design and a high level summary provided inline. References: Pattern matching Error handling For now, Carbon does not have language features dedicated to error handling, but we would consider adding some in the future. At this point, errors are represented using choice types like Result and Optional . This is similar to the story for Rust, which started using Result , then added ? operator for convenience, and is now considering ( 1 , 2 ) adding more. Execution abstractions Carbon provides some higher-order abstractions of program execution, as well as the critical underpinnings of such abstractions. Abstract machine and execution model TODO: Lambdas TODO: Co-routines TODO: Concurrency TODO:","title":"Language design"},{"location":"design/#language-design","text":"","title":"Language design"},{"location":"design/#table-of-contents","text":"Overview This document is provisional Hello, Carbon Code and comments Build modes Types are values Primitive types bool Integer types Integer literals Floating-point types Floating-point literals String types String literals Value categories and value phases Composite types Tuples Struct types Pointer types Arrays and slices Expressions Declarations, Definitions, and Scopes Patterns Binding patterns Destructuring patterns Refutable patterns Name-binding declarations Constant let declarations Variable var declarations auto Functions Parameters auto return type Blocks and statements Assignment statements Control flow if and else Loops while for break continue return returned var match User-defined types Classes Assignment Class functions and factory functions Methods Inheritance Access control Destructors const Unformed state Move Mixins Choice types Names Files, libraries, packages Package declaration Imports Name visibility Package scope Namespaces Naming conventions Aliases Name lookup Name lookup for common types Generics Checked and template parameters Interfaces and implementations Combining constraints Associated types Generic entities Generic Classes Generic choice types Generic interfaces Generic implementations Other features Generic type equality and observe declarations Operator overloading Common type Bidirectional interoperability with C and C++ Goals Non-goals Importing and #include ABI and dynamic linking Operator overloading Templates Standard types Inheritance Enums Unfinished tales Safety Lifetime and move semantics Metaprogramming Pattern matching as function overload resolution Error handling Execution abstractions Abstract machine and execution model Lambdas Co-routines Concurrency","title":"Table of contents"},{"location":"design/#overview","text":"This documentation describes the design of the Carbon language, and the rationale for that design. This documentation is an overview of the Carbon project in its current state, written for the builders of Carbon and for those interested in learning more about Carbon. This document is not a complete programming manual, and, nor does it provide detailed and comprehensive justification for design decisions. These descriptions are found in linked dedicated designs.","title":"Overview"},{"location":"design/#this-document-is-provisional","text":"This document includes much that is provisional or placeholder. This means that the syntax used, language rules, standard library, and other aspects of the design have things that have not been decided through the Carbon process. This preliminary material fills in gaps until aspects of the design can be filled in.","title":"This document is provisional"},{"location":"design/#hello-carbon","text":"Here is a simple function showing some Carbon code: import Console; // Prints the Fibonacci numbers less than `limit`. fn Fibonacci(limit: i64) { var (a: i64, b: i64) = (0, 1); while (a < limit) { Console.Print(a, \" \"); let next: i64 = a + b; a = b; b = next; } Console.Print(\"\\n\"); } Carbon is a language that should feel familiar to C++ and C developers. This example has familiar constructs like imports , function definitions , typed arguments , and curly braces . A few other features that are unlike C or C++ may stand out. First, declarations start with introducer keywords. fn introduces a function declaration, and var introduces a variable declaration . You can also see a tuple , a composite type written as a comma-separated list inside parentheses. Unlike, say, Python, these types are strongly-typed as well.","title":"Hello, Carbon"},{"location":"design/#code-and-comments","text":"All source code is UTF-8 encoded text. Comments, identifiers, and strings are allowed to have non-ASCII characters. var r\u00e9sultat: String = \"Succ\u00e8s\"; Comments start with two slashes // and go to the end of the line. They are required to be the only non-whitespace on the line. // Compute an approximation of \u03c0 References: Source files lexical conventions Proposal #142: Unicode source files Proposal #198: Comments","title":"Code and comments"},{"location":"design/#build-modes","text":"The behavior of the Carbon compiler depends on the build mode : In a development build , the priority is diagnosing problems and fast build time. In a performance build , the priority is fastest execution time and lowest memory usage. In a hardened build , the first priority is safety and second is performance. References: Safety strategy","title":"Build modes"},{"location":"design/#types-are-values","text":"Expressions compute values in Carbon, and these values are always strongly typed much like in C++. However, an important difference from C++ is that types are themselves modeled as values; specifically, compile-time constant values. This means that the grammar for writing a type is the expression grammar. Expressions written where a type is expected must be able to be evaluated at compile-time and must evaluate to a type value.","title":"Types are values"},{"location":"design/#primitive-types","text":"Primitive types fall into the following categories: the boolean type bool , signed and unsigned integer types, IEEE-754 floating-point types, and string types. These are made available through the prelude . References: Primitive types","title":"Primitive types"},{"location":"design/#bool","text":"The type bool is a boolean type with two possible values: true and false . Comparison expressions produce bool values. The condition arguments in control-flow statements , like if and while , and if - then - else conditional expressions take bool values.","title":"bool"},{"location":"design/#integer-types","text":"The signed-integer type with bit width N may be written Carbon.Int(N) . For convenience and brevity, the common power-of-two sizes may be written with an i followed by the size: i8 , i16 , i32 , i64 , i128 , or i256 . Signed-integer overflow is a programming error: In a development build, overflow will be caught immediately when it happens at runtime. In a performance build, the optimizer can assume that such conditions don't occur. As a consequence, if they do, the behavior of the program is not defined. In a hardened build, overflow does not result in undefined behavior. Instead, either the program will be aborted, or the arithmetic will evaluate to a mathematically incorrect result, such as a two's complement result or zero. The unsigned-integer types are: u8 , u16 , u32 , u64 , u128 , u256 , and Carbon.UInt(N) . Unsigned integer types wrap around on overflow, we strongly advise that they are not used except when those semantics are desired. These types are intended for bit manipulation or modular arithmetic as often found in hashing , cryptography , and PRNG use cases. Values which can never be negative, like sizes, but for which wrapping does not make sense should use signed integer types . References: Question-for-leads issue #543: pick names for fixed-size integer types Proposal #820: Implicit conversions Proposal #1083: Arithmetic expressions","title":"Integer types"},{"location":"design/#integer-literals","text":"Integers may be written in decimal, hexadecimal, or binary: 12345 (decimal) 0x1FE (hexadecimal) 0b1010 (binary) Underscores _ may be used as digit separators, but for decimal and hexadecimal literals, they can only appear in conventional locations. Numeric literals are case-sensitive: 0x , 0b must be lowercase, whereas hexadecimal digits must be uppercase. Integer literals never contain a . . Unlike in C++, literals do not have a suffix to indicate their type. Instead, numeric literals have a type derived from their value, and can be implicitly converted to any type that can represent that value. References: Integer literals Proposal #143: Numeric literals Proposal #144: Numeric literal semantics Proposal #820: Implicit conversions","title":"Integer literals"},{"location":"design/#floating-point-types","text":"Floating-point types in Carbon have IEEE 754 semantics, use the round-to-nearest rounding mode, and do not set any floating-point exception state. They are named with an f and the number of bits: f16 , f32 , f64 , and f128 . BFloat16 is also provided. References: Question-for-leads issue #543: pick names for fixed-size integer types Proposal #820: Implicit conversions Proposal #1083: Arithmetic expressions","title":"Floating-point types"},{"location":"design/#floating-point-literals","text":"Floating-point types along with user-defined types may initialized from real-number literals . Decimal and hexadecimal real-number literals are supported: 123.456 (digits on both sides of the . ) 123.456e789 (optional + or - after the e ) 0x1.Ap123 (optional + or - after the p ) Real-number literals always have a period ( . ) and a digit on each side of the period. When a real-number literal is interpreted as a value of a floating-point type, its value is the representable real number closest to the value of the literal. In the case of a tie, the nearest value whose mantissa is even is selected. References: Real-number literals Proposal #143: Numeric literals Proposal #144: Numeric literal semantics Proposal #820: Implicit conversions Proposal #866: Allow ties in floating literals","title":"Floating-point literals"},{"location":"design/#string-types","text":"There are two string types: String - a byte sequence treated as containing UTF-8 encoded text. StringView - a read-only reference to a byte sequence treated as containing UTF-8 encoded text.","title":"String types"},{"location":"design/#string-literals","text":"String literals may be written on a single line using a double quotation mark ( \" ) at the beginning and end of the string, as in \"example\" . Multi-line string literals, called block string literals , begin and end with three double quotation marks ( \"\"\" ), and may have a file type indicator after the first \"\"\" . // Block string literal: var block: String = \"\"\" The winds grow high; so do your stomachs, lords. How irksome is this music to my heart! When such strings jar, what hope of harmony? I pray, my lords, let me compound this strife. -- History of Henry VI, Part II, Act II, Scene 1, W. Shakespeare \"\"\"; The indentation of a block string literal's terminating line is removed from all preceding lines. Strings may contain escape sequences introduced with a backslash ( \\ ). Raw string literals are available for representing strings with \\ s and \" s. References: String literals Proposal #199: String literals","title":"String literals"},{"location":"design/#value-categories-and-value-phases","text":"FIXME: Should this be moved together with Types are values ? Every value has a value category , similar to C++ , that is either l-value or r-value . Carbon will automatically convert an l-value to an r-value, but not in the other direction. L-values have storage and a stable address. They may be modified, assuming their type is not const . R-values may not have dedicated storage. This means they cannot be modified and their address generally cannot be taken. R-values are broken down into three kinds, called value phases : A constant has a value known at compile time, and that value is available during type checking, for example to use as the size of an array. These include literals ( integer , floating-point , string ), concrete type values (like f64 or Optional(i32*) ), expressions in terms of constants, and values of template parameters . A symbolic value has a value that will be known at the code generation stage of compilation when monomorphization happens, but is not known during type checking. This includes checked-generic parameters , and type expressions with checked-generic arguments, like Optional(T*) . A runtime value has a dynamic value only known at runtime. Carbon will automatically convert a constant to a symbolic value, or any value to a runtime value: graph TD; A(constant)-->B(symbolic value)-->C(runtime value); D(l-value)-->C; Constants convert to symbolic values and to runtime values. Symbolic values will generally convert into runtime values if an operation that inspects the value is performed on them. Runtime values will convert into constants or to symbolic values if constant evaluation of the runtime expression succeeds.","title":"Value categories and value phases"},{"location":"design/#composite-types","text":"","title":"Composite types"},{"location":"design/#tuples","text":"A tuple is a fixed-size collection of values that can have different types, where each value is identified by its position in the tuple. An example use of tuples is to return multiple values from a function: fn DoubleBoth(x: i32, y: i32) -> (i32, i32) { return (2 * x, 2 * y); } Breaking this example apart: The return type is a tuple of two i32 types. The expression uses tuple syntax to build a tuple of two i32 values. Both of these are expressions using the tuple syntax (<expression>, <expression>) . The only difference is the type of the tuple expression: one is a tuple of types, the other a tuple of values. In other words, a tuple type is a tuple of types. The components of a tuple are accessed positionally, so element access uses subscript syntax, but the index must be a compile-time constant: fn DoubleTuple(x: (i32, i32)) -> (i32, i32) { return (2 * x[0], 2 * x[1]); } Tuple types are structural . References: Tuples","title":"Tuples"},{"location":"design/#struct-types","text":"Carbon also has structural types whose members are identified by name instead of position. These are called structural data classes , also known as a struct types or structs . Both struct types and values are written inside curly braces ( { ... } ). In both cases, they have a comma-separated list of members that start with a period ( . ) followed by the field name. In a struct type, the field name is followed by a colon ( : ) and the type, as in: {.name: String, .count: i32} . In a struct value, called a structural data class literal or a struct literal , the field name is followed by an equal sign ( = ) and the value, as in {.key = \"Joe\", .count = 3} . References: Struct types Proposal #561: Basic classes: use cases, struct literals, struct types, and future work Proposal #981: Implicit conversions for aggregates Proposal #710: Default comparison for data classes","title":"Struct types"},{"location":"design/#pointer-types","text":"The type of pointers-to-values-of-type- T is written T* . Carbon pointers do not support pointer arithmetic ; the only pointer operations are: Dereference: given a pointer p , *p gives the value p points to as an l-value . p->m is syntactic sugar for (*p).m . Address-of: given an l-value x , &x returns a pointer to x . There are no null pointers in Carbon. To represent a pointer that may not refer to a valid object, use the type Optional(T*) . TODO: Perhaps Carbon will have stricter pointer provenance or restrictions on casts between pointers and integers. References: Question-for-leads issue #520: should we use whitespace-sensitive operator fixity? Question-for-leads issue #523: what syntax should we use for pointer types?","title":"Pointer types"},{"location":"design/#arrays-and-slices","text":"The type of an array of holding 4 i32 values is written [i32; 4] . There is an implicit conversion from tuples to arrays of the same length as long as every component of the tuple may be implicitly converted to the destination element type. In cases where the size of the array may be deduced, it may be omitted, as in: var i: i32 = 1; // `[i32;]` equivalent to `[i32; 3]` here. var a: [i32;] = (i, i, i); Elements of an array may be accessed using square brackets ( [ ... ] ), as in a[i] : a[i] = 2; Console.Print(a[0]); TODO: Slices","title":"Arrays and slices"},{"location":"design/#expressions","text":"Expressions describe some computed value. The simplest example would be a literal number like 42 : an expression that computes the integer value 42. Some common expressions in Carbon include: Literals: boolean : true , false integer : 42 , -7 real-number : 3.1419 , 6.022e+23 string : \"Hello World!\" tuple : (1, 2, 3) struct : {.word = \"the\", .count = 56} Names and member access Operators : Arithmetic : -x , 1 + 2 , 3 - 4 , 2 * 5 , 6 / 3 , 5 % 3 Bitwise : 2 & 3 , 2 | 4 , 3 ^ 1 , ^7 Bit shift : 1 << 3 , 8 >> 1 Comparison : 2 == 2 , 3 != 4 , 5 < 6 , 7 > 6 , 8 <= 8 , 8 >= 8 Conversion : 2 as i32 Logical : a and b , c or d , not e Indexing : a[3] Function call: f(4) Pointer : *p , p->m , &x Move : ~x Conditionals : if c then t else f Parentheses: (7 + 8) * (3 - 1) When an expression appears in a context in which an expression of a specific type is expected, implicit conversions are applied to convert the expression to the target type. References: Expressions Proposal #162: Basic Syntax Proposal #555: Operator precedence Proposal #601: Operator tokens Proposal #680: And, or, not Proposal #702: Comparison operators Proposal #845: as expressions Proposal #911: Conditional expressions Proposal #1083: Arithmetic expressions","title":"Expressions"},{"location":"design/#declarations-definitions-and-scopes","text":"Declarations introduce a new name and say what that name represents. For some kinds of entities, like functions , there are two kinds of declarations: forward declarations and definitions . For those entities, there should be exactly one definition for the name, and at most one additional forward declaration that introduces the name before it is defined, plus any number of declarations in a match_first block . Forward declarations can be used to separate interface from implementation, such as to declare a name in an api file that is defined in an impl file . Forward declarations also allow entities to be used before they are defined, such as to allow cyclic references. A name that has been declared but not defined is called incomplete , and in some cases there are limitations on what can be done with an incomplete name. Within a definition, the defined name is incomplete until the end of the definition is reached, but is complete in the bodies of member functions because they are parsed as if they appeared after the definition . A name is valid until the end of the innermost enclosing scope . There are a few kinds of scopes: the outermost scope, which includes the whole file, scopes that are enclosed in curly braces ( { ... } ), and scopes that encompass a single declaration. For example, the names of the parameters of a function or class are valid until the end of the declaration. The name of the function or class itself is visible until the end of the enclosing scope. References: Principle: Information accumulation Proposal #875: Principle: information accumulation Question-for-leads issue #472: Open question: Calling functions defined later in the same file","title":"Declarations, Definitions, and Scopes"},{"location":"design/#patterns","text":"A pattern says how to receive some data that is being matched against. There are two kinds of patterns: Refutable patterns can fail to match based on the runtime value being matched. Irrefutable patterns are guaranteed to match, so long as the code type-checks. Irrefutable patterns are used in function parameters , variable var declarations , and constant let declarations . match statements can include both refutable patterns and irrefutable patterns. References: Pattern matching Proposal #162: Basic Syntax","title":"Patterns"},{"location":"design/#binding-patterns","text":"The most common irrefutable pattern is a binding pattern , consisting of a new name, a colon ( : ), and a type. It binds the matched value of that type to that name. It can only match values that may be implicitly converted to that type. A underscore ( _ ) may be used instead of the name to match a value but without binding any name to it. Binding patterns default to let bindings . The var keyword is used to make it a var binding . The result of a let binding is the name is bound to an r-value . This means the value cannot be modified, and its address generally cannot be taken. A var binding has dedicated storage, and so the name is an l-value which can be modified and has a stable address. A let -binding may trigger a copy of the original value, or a move if the original value is a temporary, or the binding may be a pointer to the original value, like a const reference in C++ . Which option must not be observable to the programmer. For example, Carbon will not allow modifications to the original value when it is through a pointer. This choice may also be influenced by the type. For example, types that don't support being copied will be passed by pointer instead. A generic binding uses :! instead of a colon ( : ) and can only match constant or symbolic values , not run-time values. The keyword auto may be used in place of the type in a binding pattern, as long as the type can be deduced from the type of a value in the same declaration.","title":"Binding patterns"},{"location":"design/#destructuring-patterns","text":"There are also irrefutable destructuring patterns , such as tuple destructuring . A tuple destructuring pattern looks like a tuple of patterns. It may only be used to match tuple values whose components match the component patterns of the tuple. An example use is: // `Bar()` returns a tuple consisting of an // `i32` value and 2-tuple of `f32` values. fn Bar() -> (i32, (f32, f32)); fn Foo() -> i64 { // Pattern in `var` declaration: var (p: i64, _: auto) = Bar(); return p; } The pattern used in the var declaration destructures the tuple value returned by Bar() . The first component pattern, p: i64 , corresponds to the first component of the value returned by Bar() , which has type i32 . This is allowed since there is an implicit conversion from i32 to i64 . The result of this conversion is assigned to the name p . The second component pattern, _: auto , matches the second component of the value returned by Bar() , which has type (f32, f32) .","title":"Destructuring patterns"},{"location":"design/#refutable-patterns","text":"Additional kinds of patterns are allowed in match statements , that may or may not match based on the runtime value of the match expression: An expression pattern is an expression, such as 42 , whose value must be equal to match. A choice pattern matches one case from a choice type, as described in the choice types section . A dynamic cast pattern is tests the dynamic type, as described in inheritance . See match for examples of refutable patterns. References: Pattern matching Question-for-leads issue #1283: how should pattern matching and implicit conversion interact?","title":"Refutable patterns"},{"location":"design/#name-binding-declarations","text":"There are two kinds of name-binding declarations: constant declarations, introduced with let , and variable declarations, introduced with var . There are no forward declarations of these; all name-binding declarations are definitions .","title":"Name-binding declarations"},{"location":"design/#constant-let-declarations","text":"A let declaration matches an irrefutable pattern to a value. In this example, the name x is bound to the value 42 with type i64 : let x: i64 = 42; Here x: i64 is the pattern, which is followed by an equal sign ( = ) and the value to match, 42 . The names from binding patterns are introduced into the enclosing scope .","title":"Constant let declarations"},{"location":"design/#variable-var-declarations","text":"A var declaration is similar, except with var bindings, so x here is an l-value with storage and an address, and so may be modified: var x: i64 = 42; x = 7; Variables with a type that has an unformed state do not need to be initialized in the variable declaration, but do need to be assigned before they are used. References: Variables Proposal #162: Basic Syntax Proposal #257: Initialization of memory and variables Proposal #339: Add var <type> <identifier> [ = <value> ]; syntax for variables Proposal #618: var ordering","title":"Variable var declarations"},{"location":"design/#auto","text":"If auto is used as the type in a var or let declaration, the type is the static type of the initializer expression, which is required. var x: i64 = 2; // The type of `y` is inferred to be `i64`. let y: auto = x + 3; // The type of `z` is inferred to be `bool`. var z: auto = (y > 1); References: Type inference Proposal #851: auto keyword for vars","title":"auto"},{"location":"design/#functions","text":"Functions are the core unit of behavior. For example, this is a forward declaration of a function that adds two 64-bit integers: fn Add(a: i64, b: i64) -> i64; Breaking this apart: fn is the keyword used to introduce a function. Its name is Add . This is the name added to the enclosing scope . The parameter list in parentheses ( ( ... ) ) is a comma-separated list of irrefutable patterns . It returns an i64 result. Functions that return nothing omit the -> and return type. You would call this function like Add(1, 2) . A function definition is a function declaration that has a body block instead of a semicolon: fn Add(a: i64, b: i64) -> i64 { return a + b; } The names of the parameters are in scope until the end of the definition or declaration. The parameter names in a forward declaration may be omitted using _ , but must match the definition if they are specified. References: Functions Proposal #162: Basic Syntax Proposal #438: Add statement syntax for function declarations Question-for-leads issue #476: Optional argument names (unused arguments) Question-for-leads issue #1132: How do we match forward declarations with their definitions?","title":"Functions"},{"location":"design/#parameters","text":"The bindings in the parameter list default to let bindings , and so the parameter names are treated as r-values . This is appropriate for input parameters. This binding will be implemented using a pointer, unless it is legal to copy and copying is cheaper. If the var keyword is added before the binding, then the arguments will be copied (or moved from a temporary) to new storage, and so can be mutated in the function body. The copy ensures that any mutations will not be visible to the caller. Use a pointer parameter type to represent an input/output parameter , allowing a function to modify a variable of the caller's. This makes the possibility of those modifications visible: by taking the address using & in the caller, and dereferencing using * in the callee. Outputs of a function should prefer to be returned. Multiple values may be returned using a tuple or struct type.","title":"Parameters"},{"location":"design/#auto-return-type","text":"If auto is used in place of the return type, the return type of the function is inferred from the function body. It is set to common type of the static type of arguments to the return statements in the function. This is not allowed in a forward declaration. // Return type is inferred to be `bool`, the type of `a > 0`. fn Positive(a: i64) -> auto { return a > 0; } References: Type inference Function return clause Proposal #826: Function return type inference","title":"auto return type"},{"location":"design/#blocks-and-statements","text":"A block is a sequence of statements . A block defines a scope and, like other scopes, is enclosed in curly braces ( { ... } ). Each statement is terminated by a semicolon or block. Expressions and var and let are valid statements. Statements within a block are normally executed in the order they appear in the source code, except when modified by control-flow statements. The body of a function is defined by a block, and some control-flow statements have their own blocks of code. These are nested within the enclosing scope. For example, here is a function definition with a block of statements defining the body of the function, and a nested block as part of a while statement: fn Foo() { Bar(); while (Baz()) { Quux(); } } References: Blocks and statements Proposal #162: Basic Syntax","title":"Blocks and statements"},{"location":"design/#assignment-statements","text":"Assignment statements mutate the value of the l-value described on the left-hand side of the assignment. Assignment: x = y; . x is assigned the value of y . Increment and decrement: ++i; , --j; . i is set to i + 1 , j is set to j - 1 . Compound assignment: x += y; , x -= y; , x *= y; , x /= y; , x &= y; , x |= y; , x ^= y; , x <<= y; , x >>= y; . x @= y; is equivalent to x = x @ y; for each operator @ . Unlike C++, these assignments are statements, not expressions, and don't return a value.","title":"Assignment statements"},{"location":"design/#control-flow","text":"Blocks of statements are generally executed sequentially. Control-flow statements give additional control over the flow of execution and which statements are executed. Some control-flow statements include blocks . Those blocks will always be within curly braces { ... } . // Curly braces { ... } are required. if (condition) { ExecutedWhenTrue(); } else { ExecutedWhenFalse(); } This is unlike C++, which allows control-flow constructs to omit curly braces around a single statement. References: Control flow Proposal #162: Basic Syntax Proposal #623: Require braces","title":"Control flow"},{"location":"design/#if-and-else","text":"if and else provide conditional execution of statements. An if statement consists of: An if introducer followed by a condition in parentheses. If the condition evaluates to true , the block following the condition is executed, otherwise it is skipped. This may be followed by zero or more else if clauses, whose conditions are evaluated if all prior conditions evaluate to false , with a block that is executed if that evaluation is to true . A final optional else clause, with a block that is executed if all conditions evaluate to false . For example: if (fruit.IsYellow()) { Console.Print(\"Banana!\"); } else if (fruit.IsOrange()) { Console.Print(\"Orange!\"); } else { Console.Print(\"Vegetable!\"); } This code will: Print Banana! if fruit.IsYellow() is true . Print Orange! if fruit.IsYellow() is false and fruit.IsOrange() is true . Print Vegetable! if both of the above return false . References: Control flow Proposal #285: if/else","title":"if and else"},{"location":"design/#loops","text":"References: Loops","title":"Loops"},{"location":"design/#while","text":"while statements loop for as long as the passed expression returns true . For example, this prints 0 , 1 , 2 , then Done! : var x: i32 = 0; while (x < 3) { Console.Print(x); ++x; } Console.Print(\"Done!\"); References: while loops Proposal #340: Add C++-like while loops","title":"while"},{"location":"design/#for","text":"for statements support range-based looping, typically over containers. For example, this prints each String value in names : for (var name: String in names) { Console.Print(name); } References: for loops Proposal #353: Add C++-like for loops","title":"for"},{"location":"design/#break","text":"The break statement immediately ends a while or for loop. Execution will continue starting from the end of the loop's scope. For example, this processes steps until a manual step is hit (if no manual step is hit, all steps are processed): for (var step: Step in steps) { if (step.IsManual()) { Console.Print(\"Reached manual step!\"); break; } step.Process(); } References: break","title":"break"},{"location":"design/#continue","text":"The continue statement immediately goes to the next loop of a while or for . In a while , execution continues with the while expression. For example, this prints all non-empty lines of a file, using continue to skip empty lines: var f: File = OpenFile(path); while (!f.EOF()) { var line: String = f.ReadLine(); if (line.IsEmpty()) { continue; } Console.Print(line); } References: continue","title":"continue"},{"location":"design/#return","text":"The return statement ends the flow of execution within a function, returning execution to the caller. // Prints the integers 1 .. `n` and then // returns to the caller. fn PrintFirstN(n: i32) { var i: i32 = 0; while (true) { i += 1; if (i > n) { // None of the rest of the function is // executed after a `return`. return; } Console.Print(i); } } If the function returns a value to the caller, that value is provided by an expression in the return statement. For example: fn Sign(i: i32) -> i32 { if (i > 0) { return 1; } if (i < 0) { return -1; } return 0; } Assert(Sign(-3) == -1); References: return return statements Proposal #415: return Proposal #538: return with no argument","title":"return"},{"location":"design/#returned-var","text":"To avoid a copy when returning a variable, add a returned prefix to the variable's declaration and use return var instead of returning an expression, as in: fn MakeCircle(radius: i32) -> Circle { returned var c: Circle; c.radius = radius; // `return c` would be invalid because `returned` is in use. return var; } This is instead of the \"named return value optimization\" of C++ . References: returned var Proposal #257: Initialization of memory and variables","title":"returned var"},{"location":"design/#match","text":"match is a control flow similar to switch of C and C++ and mirrors similar constructs in other languages, such as Swift. The match keyword is followed by an expression in parentheses, whose value is matched against the case declarations, each of which contains a refutable pattern , in order. The refutable pattern may optionally be followed by an if expression, which may use the names from bindings in the pattern. The code for the first matching case is executed. An optional default block may be placed after the case declarations, it will be executed if none of the case declarations match. An example match is: fn Bar() -> (i32, (f32, f32)); fn Foo() -> f32 { match (Bar()) { case (42, (x: f32, y: f32)) => { return x - y; } case (p: i32, (x: f32, _: f32)) if (p < 13) => { return p * x; } case (p: i32, _: auto) if (p > 3) => { return p * Pi; } default => { return Pi; } } } References: Pattern matching Question-for-leads issue #1283: how should pattern matching and implicit conversion interact?","title":"match"},{"location":"design/#user-defined-types","text":"TODO: Maybe rename to \"nominal types\"?","title":"User-defined types"},{"location":"design/#classes","text":"Nominal classes , or just classes , are a way for users to define their own data structures or record types . This is an example of a class definition : class Widget { var x: i32; var y: i32; var payload: String; } Breaking this apart: This defines a class named Widget . Widget is the name added to the enclosing scope . The name Widget is followed by curly braces ( { ... } ) containing the class body , making this a definition . A forward declaration would instead have a semicolon( ; ). Those braces delimit the class' scope . Fields, or instances variables , are defined using var declarations . Widget has two i32 fields ( x and y ), and one String field ( payload ). The order of the field declarations determines the fields' memory-layout order. Classes may have other kinds of members beyond fields declared in its scope: Class functions Methods alias let to define class constants. TODO: Another syntax to define constants associated with the class like class let or static let ? class , to define a member class or nested class Within the scope of a class, the unqualified name Self can be used to refer to the class itself. Members of a class are accessed using the dot ( . ) notation, so given an instance dial of type Widget , dial.payload refers to its payload field. Both structural data classes and nominal classes are considered class types , but they are commonly referred to as \"structs\" and \"classes\" respectively when that is not confusing. Like structs, classes refer to their members by name. Unlike structs, classes are nominal types . References: Classes Proposal #722: Nominal classes and methods Proposal #989: Member access expressions","title":"Classes"},{"location":"design/#assignment","text":"There is an implicit conversions defined between a struct literal and a class type with the same fields, in any scope that has access to all of the class' fields. This may be used to assign or initialize a variable with a class type, as in: var sprocket: Widget = {.x = 3, .y = 4, .payload = \"Sproing\"}; sprocket = {.x = 2, .y = 1, .payload = \"Bounce\"}; References: Classes: Construction Proposal #981: Implicit conversions for aggregates","title":"Assignment"},{"location":"design/#class-functions-and-factory-functions","text":"Classes may also contain class functions . These are functions that are accessed as members of the type, like static member functions in C++ , as opposed to methods that are members of instances. They are commonly used to define a function that creates instances. Carbon does not have separate constructors like C++ does. class Point { // Class function that instantiates `Point`. // `Self` in class scope means the class currently being defined. fn Origin() -> Self { return {.x = 0, .y = 0}; } var x: i32; var y: i32; } Note that if the definition of a function is provided inside the class scope, the body is treated as if it was defined immediately after the outermost class definition. This means that members such as the fields will be considered declared even if their declarations are later in the source than the class function. The returned var feature can be used if the address of the instance being created is needed in a factory function, as in: class Registered { fn Create() -> Self { returned var result: Self = {...}; StoreMyPointerSomewhere(&result); return var; } } This approach can also be used for types that can't be copied or moved.","title":"Class functions and factory functions"},{"location":"design/#methods","text":"Class type definitions can include methods: class Point { // Method defined inline fn Distance[me: Self](x2: i32, y2: i32) -> f32 { var dx: i32 = x2 - me.x; var dy: i32 = y2 - me.y; return Math.Sqrt(dx * dx - dy * dy); } // Mutating method declaration fn Offset[addr me: Self*](dx: i32, dy: i32); var x: i32; var y: i32; } // Out-of-line definition of method declared inline fn Point.Offset[addr me: Self*](dx: i32, dy: i32) { me->x += dx; me->y += dy; } var origin: Point = {.x = 0, .y = 0}; Assert(Math.Abs(origin.Distance(3, 4) - 5.0) < 0.001); origin.Offset(3, 4); Assert(origin.Distance(3, 4) == 0.0); This defines a Point class type with two integer data members x and y and two methods Distance and Offset : Methods are defined as class functions with a me parameter inside square brackets [ ... ] before the regular explicit parameter list in parens ( ... ) . Methods are called using using the member syntax, origin.Distance( ... ) and origin.Offset( ... ) . Distance computes and returns the distance to another point, without modifying the Point . This is signified using [me: Self] in the method declaration. origin.Offset( ... ) does modify the value of origin . This is signified using [addr me: Self*] in the method declaration. Since calling this method requires taking the address of origin , it may only be called on non- const l-values . Methods may be declared lexically inline like Distance , or lexically out of line like Offset . References: Methods Proposal #722: Nominal classes and methods","title":"Methods"},{"location":"design/#inheritance","text":"The philosophy of inheritance support in Carbon is to focus on use cases where inheritance is a good match, and use other features for other cases. For example, mixins for implementation reuse and generics for separating interface from implementation. This allows Carbon to move away from multiple inheritance , which doesn't have as efficient of an implementation strategy. Classes by default are final , which means they may not be extended. A class may be declared as allowing extension using either the base class or abstract class introducer instead of class . An abstract class is a base class that may not itself be instantiated. base class MyBaseClass { ... } Either kind of base class may be extended to get a derived class . Derived classes are final unless they are themselved declared base or abstract . Classes may only extend a single class. Carbon only supports single inheritance, and will use mixins instead of multiple inheritance. base class MiddleDerived extends MyBaseClass { ... } class FinalDerived extends MiddleDerived { ... } // \u274c Forbidden: class Illegal extends FinalDerived { ... } A base class may define virtual methods . These are methods whose implementation may be overridden in a derived class. By default methods are non-virtual , the declaration of a virtual method must be prefixed by one of these three keywords: A method marked virtual has a definition in this class but not in any base. A method marked abstract does not have have a definition in this class, but must have a definition in any non- abstract derived class. A method marked impl has a definition in this class, overriding any definition in a base class. A pointer to a derived class may be cast to a pointer to one of its base classes. Calling a virtual method through a pointer to a base class will use the overridden definition provided in the derived class. Base classes with virtual methods may use run-time type information in a match statement to dynamically test whether the dynamic type of a value is some derived class, as in: var base_ptr: MyBaseType* = ...; match (base_ptr) { case dyn p: MiddleDerived* => { ... } } For purposes of construction, a derived class acts like its first field is called base with the type of its immediate base class. class MyDerivedType extends MyBaseType { fn Create() -> MyDerivedType { return {.base = MyBaseType.Create(), .derived_field = 7}; } var derived_field: i32; } Abstract classes can't be instantiated, so instead they should define class functions returning partial Self . Those functions should be marked protected so they may only be used by derived classes. abstract class AbstractClass { protected fn Create() -> partial Self { return {.field_1 = 3, .field_2 = 9}; } // ... var field_1: i32; var field_2: i32; } // \u274c Error: can't instantiate abstract class var abc: AbstractClass = ...; class DerivedFromAbstract extends AbstractClass { fn Create() -> Self { // AbstractClass.Create() returns a // `partial AbstractClass` that can be used as // the `.base` member when constructing a value // of a derived class. return {.base = AbstractClass.Create(), .derived_field = 42 }; } var derived_field: i32; } References: Inheritance Proposal #777: Inheritance Proposal #820: Implicit conversions","title":"Inheritance"},{"location":"design/#access-control","text":"Class members are by default publicly accessible. The private keyword prefix can be added to the member's declaration to restrict it to members of the class or any friends. A private virtual or private abstract method may be implemented in derived classes, even though it may not be called. Friends may be declared using a friend declaration inside the class naming an existing function or type. Unlike C++, friend declarations may only refer to names resolvable by the compiler, and don't act like forward declarations. protected is like private , but also gives access to derived classes. References: Access control for class members Question-for-leads issue #665: private vs public syntax strategy, as well as other visibility tools like external / api /etc. Question-for-leads issue #971: Private interfaces in public API files","title":"Access control"},{"location":"design/#destructors","text":"A destructor for a class is custom code executed when the lifetime of a value of that type ends. They are defined with the destructor keyword followed by either [me: Self] or [addr me: Self*] (as is done with methods ) and the block of code in the class definition, as in: class MyClass { destructor [me: Self] { ... } } or: class MyClass { // Can modify `me` in the body. destructor [addr me: Self*] { ... } } The destructor for a class is run before the destructors of its data members. The data members are destroyed in reverse order of declaration. Derived classes are destroyed before their base classes. A destructor in an abstract or base class may be declared virtual like with methods . Destructors in classes derived from one with a virtual destructor must be declared with the impl keyword prefix. It is illegal to delete an instance of a derived class through a pointer to a base class unless the base class is declared virtual or impl . To delete a pointer to a non-abstract base class when it is known not to point to a value with a derived type, use UnsafeDelete . References: Destructors Proposal #1154: Destructors","title":"Destructors"},{"location":"design/#const","text":"Note: This is provisional, no design for const has been through the proposal process yet. For every type MyClass , there is the type const MyClass such that: The data representation is the same, so a MyClass* value may be implicitly converted to a (const MyClass)* . A const MyClass l-value may automatically convert to a MyClass r-value, the same way that a MyClass l-value can. If member x of MyClass has type T , then member x of const MyClass has type const T . The API of a const MyClass is a subset of MyClass , excluding all methods taking [addr me: Self*] . Note that const binds more tightly than postfix- * for forming a pointer type, so const MyClass* is equal to (const MyClass)* . This example uses the definition of Point from the \"methods\" section : var origin: Point = {.x = 0, .y = 0}; // \u2705 Allowed conversion from `Point*` to // `const Point*`: let p: const Point* = &origin; // \u2705 Allowed conversion of `const Point` l-value // to `Point` r-value. let five: f32 = p->Distance(3, 4); // \u274c Error: mutating method `Offset` excluded // from `const Point` API. p->Offset(3, 4); // \u274c Error: mutating method `AssignAdd.Op` // excluded from `const i32` API. p->x += 2;","title":"const"},{"location":"design/#unformed-state","text":"Types indicate that they support unformed states by implementing a particular interface , otherwise variables of that type must be explicitly initialized when they are declared. An unformed state for an object is one that satisfies the following properties: Assignment from a fully formed value is correct using the normal assignment implementation for the type. Destruction must be correct using the type's normal destruction implementation. Destruction must be optional. The behavior of the program must be equivalent whether the destructor is run or not for an unformed object, including not leaking resources. A type might have more than one in-memory representation for the unformed state, and those representations may be the same as valid fully formed values for that type. For example, all values are legal representations of the unformed state for any type with a trivial destructor like i32 . Types may define additional initialization for the hardened build mode . For example, this causes integers to be set to 0 when in unformed state in this mode. Any operation on an unformed object other than destruction or assignment from a fully formed value is an error, even if its in-memory representation is that of a valid value for that type. References: Proposal #257: Initialization of memory and variables","title":"Unformed state"},{"location":"design/#move","text":"Carbon will allow types to define if and how they are moved. This can happen when returning a value from a function or by using the move operator ~x . This leaves x in an unformed state and returns its old value.","title":"Move"},{"location":"design/#mixins","text":"Mixins allow reuse with different trade-offs compared to inheritance . Mixins focus on implementation reuse, such as might be done using CRTP or multiple inheritance in C++. TODO: The design for mixins is still under development.","title":"Mixins"},{"location":"design/#choice-types","text":"A choice type is a tagged union , that can store different types of data in a storage space that can hold the largest. A choice type has a name, and a list of cases separated by commas ( , ). Each case has a name and an optional parameter list. choice IntResult { Success(value: i32), Failure(error: String), Cancelled } The value of a choice type is one of the cases, plus the values of the parameters to that case, if any. A value can be constructed by naming the case and providing values for the parameters, if any: fn ParseAsInt(s: String) -> IntResult { var r: i32 = 0; for (c: i32 in s) { if (not IsDigit(c)) { // Equivalent to `IntResult.Failure(...)` return .Failure(\"Invalid character\"); } // ... } return .Success(r); } Choice type values may be consumed using a match statement : match (ParseAsInt(s)) { case .Success(value: i32) => { return value; } case .Failure(error: String) => { Display(error); } case .Cancelled => { Terminate(); } } They can also represent an enumerated type , if no additional data is associated with the choices, as in: choice LikeABoolean { False, True } References: Proposal #157: Design direction for sum types Proposal #162: Basic Syntax","title":"Choice types"},{"location":"design/#names","text":"Names are introduced by declarations and are valid until the end of the scope in which they appear. Code may not refer to names earlier in the source than they are declared. In executable scopes such as function bodies, names declared later are not found. In declarative scopes such as packages, classes, and interfaces, it is an error to refer to names declared later, except that inline class member function bodies are parsed as if they appeared after the class . A name in Carbon is formed from a sequence of letters, numbers, and underscores, and starts with a letter. We intend to follow Unicode's Annex 31 in selecting valid identifier characters, but a concrete set of valid characters has not been selected yet. References: Lexical conventions Principle: Information accumulation Proposal #142: Unicode source files Question-for-leads issue #472: Open question: Calling functions defined later in the same file Proposal #875: Principle: information accumulation","title":"Names"},{"location":"design/#files-libraries-packages","text":"Files are grouped into libraries, which are in turn grouped into packages. Libraries are the granularity of code reuse through imports. Packages are the unit of distribution. Each library must have exactly one api file. This file includes declarations for all public names of the library. Definitions for those declarations must be in some file in the library, either the api file or an impl file. Every package has its own namespace. This means libraries within a package need to coordinate to avoid name conflicts, but not across packages. References: Code and name organization Proposal #107: Code and name organization","title":"Files, libraries, packages"},{"location":"design/#package-declaration","text":"Files start with an optional package declaration, consisting of: the package keyword introducer, an optional identifier specifying the package name, optional library followed by a string with the library name, either api or impl , and a terminating semicolon ( ; ). For example: // Package name is `Geometry`. // Library name is \"Shapes\". // This file is an `api` file, not an `impl` file. package Geometry library \"Shapes\" api; Parts of this declaration may be omitted: If the package name is omitted, as in package library \"Main\" api; , the file contributes to the default package. No other package may import from the default package. If the library keyword is not specified, as in package Geometry api; , this file contributes to the default library. If a file has no package declaration at all, it is the api file belonging to the default package and default library. This is particularly for tests and smaller examples. No other library can import this library even from within the default package. It can be split across multiple impl files using a package impl; package declaration. A program need not use the default package, but if it does, it should contain the entry-point function. By default, the entry-point function is Run from the default package. References: Code and name organization Proposal #107: Code and name organization","title":"Package declaration"},{"location":"design/#imports","text":"After the package declaration, files may include import declarations. These include the package name and optionally library followed by the library name. If the library is omitted, the default library for that package is imported. // Import the \"Vector\" library from the // `LinearAlgebra` package. import LinearAlgebra library \"Vector\"; // Import the default library from the // `ArbitraryPrecision` package. import ArbitraryPrecision; The syntax import PackageName ... introduces the name PackageName as a private name naming the given package. It cannot be used to import libraries of the current package. Importing additional libraries from that package makes additional members of PackageName visible. Libraries from the current package are imported by omitting the package name. // Import the \"Vertex\" library from the same package. import library \"Vertex\"; // Import the default library from the same package. import library default; The import library ... syntax adds all the public top-level names within the given library to the top-level scope of the current file as private names, and similarly for names in namespaces . Every impl file automatically imports the api file for its library. All import declarations must appear before all other non- package declarations in the file. References: Code and name organization Proposal #107: Code and name organization","title":"Imports"},{"location":"design/#name-visibility","text":"The names visible from an imported library are determined by these rules: Declarations in an api file are by default public , which means visible to any file that imports that library. This matches class members, which are also default public . A private prefix on a declaration in an api file makes the name library private . This means the name is visible in the file and all impl files for the same library. The visibility of a name is determined by its first declaration, considering api files before impl files. The private prefix is only allowed on the first declaration. A name declared in an impl file and not the corresponding api file is file private , meaning visible in just that file. Its first declaration must be marked with a private prefix. TODO: This needs to be finalized in a proposal to resolve inconsistency between #665 and #1136 . Private names don't conflict with names outside the region they're private to: two different libraries can have different private names foo without conflict, but a private name conflicts with a public name in the same scope. At most one api file in a package transitively used in a program may declare a given name public. References: Exporting entities from an API file Question-for-leads issue #665: private vs public syntax strategy, as well as other visibility tools like external / api /etc. Proposal #752: api file default public Proposal #931: Generic impls access (details 4) Question-for-leads issue #1136: what is the top-level scope in a source file, and what names are found there?","title":"Name visibility"},{"location":"design/#package-scope","text":"The top-level scope in a package is the scope of the package. This means: Within this scope (and its sub-namespaces), all visible names from the same package appear. This includes names from the same file, names from the api file of a library when inside an impl file, and names from imported libraries of the same package. In scopes where package members might have a name conflict with something else, the syntax package.Foo can be used to name the Foo member of the current package. In this example, the names F and P are used in a scope where they could mean two different things, and qualifications are needed to disambiguate : import P; fn F(); class C { fn F(); class P { fn H(); } fn G() { // \u274c Error: ambiguous whether `F` means // `package.F` or `package.C.F`. F(); // \u2705 Allowed: fully qualified package.F(); package.C.F(); // \u2705 Allowed: unambiguous C.F(); // \u274c Error: ambiguous whether `P` means // `package.P` or `package.P.F`. P.H(); // \u2705 Allowed package.P.H(); package.C.P.H(); C.P.H(); } } References: Code and name organization Proposal #107: Code and name organization Proposal #752: api file default public Question-for-leads issue #1136: what is the top-level scope in a source file, and what names are found there?","title":"Package scope"},{"location":"design/#namespaces","text":"A namespace declaration defines a name that may be used as a prefix of names declared afterward. When defining a member of a namespace, other members of that namespace are considered in scope and may be found by name lookup without the namespace prefix. In this example, package P defines some of its members inside a namespace N : package P api; // Defines namespace `N` within the current package. namespace N; // Defines namespaces `M` and `M.L`. namespace M.L; fn F(); // \u2705 Allowed: Declares function `G` in namespace `N`. private fn N.G(); // \u274c Error: `Bad` hasn't been declared. fn Bad.H(); fn J() { // \u274c Error: No `package.G` G(); } fn N.K() { // \u2705 Allowed: Looks in both `package` and `package.N`. // Finds `package.F` and `package.N.G`. F(); G(); } // \u2705 Allowed: Declares function `R` in namespace `M.L`. fn M.L.R(); // \u2705 Allowed: Declares function `Q` in namespace `M`. fn M.Q(); Another package importing P can refer to the public members of that namespace by prefixing with the package name P followed by the namespace: import P; // \u2705 Allowed: `F` is public member of `P`. P.F(); // \u274c Error: `N.G` is a private member of `P`. P.N.G(); // \u2705 Allowed: `N.K` is public member of `P`. P.N.K(); // \u2705 Allowed: `M.L.R` is public member of `P`. P.M.L.R(); // \u2705 Allowed: `M.Q` is public member of `P`. P.M.Q(); References: \"Namespaces\" in \"Code and name organization\" \"Package and namespace members\" in \"Qualified names and member access\"","title":"Namespaces"},{"location":"design/#naming-conventions","text":"Our naming conventions are: For idiomatic Carbon code: UpperCamelCase will be used when the named entity cannot have a dynamically varying value. For example, functions, namespaces, or compile-time constant values. Note that virtual methods are named the same way to be consistent with other functions and methods. lower_snake_case will be used when the named entity's value won't be known until runtime, such as for variables. For Carbon-provided features: Keywords and type literals will use lower_snake_case . Other code will use the conventions for idiomatic Carbon code. References: Naming conventions Proposal #861: Naming conventions","title":"Naming conventions"},{"location":"design/#aliases","text":"alias declares a name as equivalent to another name, for example: alias NewName = SomePackage.OldName; Note that the right-hand side of the equal sign ( = ) is a name not a value, so alias four = 4; is not allowed. This allows alias to work with entities like namespaces, which aren't values in Carbon. This can be used during an incremental migration when changing a name, or to include a name in a public API. For example, alias may be used to include a name from an interface implementation as a member of a class or named constraint , possibly renamed: class ContactInfo { external impl as Printable; external impl as ToPrinterDevice; alias PrintToScreen = Printable.Print; alias PrintToPrinter = ToPrinterDevice.Print; ... } References: Aliases \"Aliasing\" in \"Code and name organization\" alias a name from an external impl alias a name in a named constraint Proposal #553: Generics details part 1 Question-for-leads issue #749: Alias syntax","title":"Aliases"},{"location":"design/#name-lookup","text":"The general principle of Carbon name lookup is that we look up names in all relevant scopes, and report an error if the name is found to refer to more than one different entity. So Carbon requires disambiguation by adding qualifiers instead of doing any shadowing of names. For an example, see the \"package scope\" section . Unqualified name lookup walks the semantically-enclosing scopes, not only the lexically-enclosing ones. So when a lookup is performed within fn MyNamespace.MyClass.MyNestedClass.MyFunction() , we will look in MyNestedClass , MyClass , MyNamespace , and the package scope, even when the lexically-enclosing scope is the package scope. This means that the definition of a method will look for names in the class' scope even if it is written lexically out of line: class C { fn F(); fn G(); } fn C.G() { // \u2705 Allowed: resolves to `package.C.F`. F(); } Member name lookup follows a similar philosophy. If a checked-generic type parameter is known to implement multiple interfaces due to a constraint using & or where clauses , member name lookup into that type will look in all of the interfaces. If it is found in multiple, the name must be disambiguated by qualifying using compound member access ( 1 , 2 ). A template-generic type parameter performs look up into the caller's type in addition to the constraint. Carbon also rejects cases that would be invalid if all declarations in the file, including ones appearing later, were visible everywhere, not only after their point of appearance: class C { fn F(); fn G(); } fn C.G() { F(); } // Error: use of `F` in `C.G` would be ambiguous // if this declaration was earlier. fn F(); References: Name lookup \"Qualified names and member access\" section of \"Expressions\" Qualified names and member access Principle: Information accumulation Proposal #875: Principle: information accumulation Proposal #989: Member access expressions Question-for-leads issue #1136: what is the top-level scope in a source file, and what names are found there?","title":"Name lookup"},{"location":"design/#name-lookup-for-common-types","text":"Common types that we expect to be used universally will be provided for every file are made available as if there was a special \"prelude\" package that was imported automatically into every api file. Dedicated type literal syntaxes like i32 and bool refer to types defined within this package, based on the \"all APIs are library APIs\" principle . References: Name lookup Principle: All APIs are library APIs Question-for-leads issue #750: Naming conventions for Carbon-provided features Question-for-leads issue #1058: How should interfaces for core functionality be named? Proposal #1280: Principle: All APIs are library APIs","title":"Name lookup for common types"},{"location":"design/#generics","text":"Generics allow Carbon constructs like functions and classes to be written with compile-time parameters and apply generically to different types using those parameters. For example, this Min function has a type parameter T that can be any type that implements the Ordered interface. fn Min[T:! Ordered](x: T, y: T) -> T { // Can compare `x` and `y` since they have // type `T` known to implement `Ordered`. return if x <= y then x else y; } var a: i32 = 1; var b: i32 = 2; // `T` is deduced to be `i32` Assert(Min(a, b) == 1); // `T` is deduced to be `String` Assert(Min(\"abc\", \"xyz\") == \"abc\"); Since the T type parameter is in the deduced parameter list in square brackets ( [ ... ] ) before the explicit parameter list in parentheses ( ( ... ) ), the value of T is determined from the types of the explicit arguments instead of being passed as a separate explicit argument. References: TODO: Revisit Generics: Overview Proposal #524: Generics overview Proposal #553: Generics details part 1 Proposal #950: Generic details 6: remove facets","title":"Generics"},{"location":"design/#checked-and-template-parameters","text":"The :! indicates that T is a checked parameter passed at compile time. \"Checked\" here means that the body of Min is type checked when the function is defined, independent of the specific type values T is instantiated with, and name lookup is delegated to the constraint on T ( Ordered in this case). This type checking is equivalent to saying the function would pass type checking given any type T that implements the Ordered interface. Then calls to Min only need to check that the deduced type value of T implements Ordered . The parameter could alternatively be declared to be a template parameter by prefixing with the template keyword, as in template T:! Type . fn Convert[template T:! Type](source: T, template U:! Type) -> U { var converted: U = source; return converted; } fn Foo(i: i32) -> f32 { // Instantiates with the `T` implicit argument set to `i32` and the `U` // explicit argument set to `f32`, then calls with the runtime value `i`. return Convert(i, f32); } Carbon templates follow the same fundamental paradigm as C++ templates : they are instantiated when called, resulting in late type checking, duck typing, and lazy binding. One difference from C++ templates, Carbon template instantiation is not controlled by the SFINAE rule of C++ ( 1 , 2 ) but by explicit if clauses evaluated at compile-time. The if clause is at the end of the declaration, and the condition can only use constant values known at type-checking time, including template parameters. class Array(template T:! Type, template N:! i64) if N >= 0 and N < MaxArraySize / sizeof(T); Member lookup into a template type parameter is done in the actual type value provided by the caller, in addition to any constraints. This means member name lookup and type checking for anything dependent on the template parameter can't be completed until the template is instantiated with a specific concrete type. When the constraint is just Type , this gives semantics similar to C++ templates. Constraints can then be added incrementally, with the compiler verifying that the semantics stay the same. Once all constraints have been added, removing the word template to switch to a checked parameter is safe. The value phase of a checked parameter is a symbolic value whereas the value phase of a template parameter is constant. Although checked generics are generally preferred, templates enable translation of code between C++ and Carbon, and address some cases where the type checking rigor of generics are problematic. References: Templates Proposal #553: Generics details part 1 Question-for-leads issue #949: Constrained template name lookup Proposal #989: Member access expressions","title":"Checked and template parameters"},{"location":"design/#interfaces-and-implementations","text":"Interfaces specify a set of requirements that a types might satisfy. Interfaces act both as constraints on types a caller might supply and capabilities that may be assumed of types that satisfy that constraint. interface Printable { // Inside an interface definition `Self` means // \"the type implementing this interface\". fn Print[me: Self](); } In addition to function requirements, interfaces can contain: requirements that other interfaces be implemented or interfaces that this interface extends associated types and other associated constants interface defaults final interface members Types only implement an interface if there is an explicit impl declaration that they do. Simply having a Print function with the right signature is not sufficient. class Circle { var radius: f32; impl as Printable { fn Print[me: Self]() { Console.WriteLine(\"Circle with radius: {0}\", me.radius); } } } In this case, Print is a member of Circle . Interfaces may also be implemented externally , which means the members of the interface are not direct members of the type. Those methods may still be called using compound member access syntax ( 1 , 2 ) to qualify the name of the member, as in x.(Printable.Print)() . External implementations don't have to be in the same library as the type definition, subject to the orphan rule ( 1 , 2 ) for coherence . Interfaces and implementations may be forward declared by replacing the definition scope in curly braces ( { ... } ) with a semicolon. References: Generics: Interfaces Generics: Implementing interfaces Proposal #553: Generics details part 1 Proposal #731: Generics details 2: adapters, associated types, parameterized interfaces Proposal #624: Coherence: terminology, rationale, alternatives considered Proposal #990: Generics details 8: interface default and final members Proposal #1084: Generics details 9: forward declarations Question-for-leads issue #1132: How do we match forward declarations with their definitions?","title":"Interfaces and implementations"},{"location":"design/#combining-constraints","text":"A function can require calling types to implement multiple interfaces by combining them using an ampersand ( & ): fn PrintMin[T:! Ordered & Printable](x: T, y: T) { // Can compare since type `T` implements `Ordered`. if (x <= y) { // Can call `Print` since type `T` implements `Printable`. x.Print(); } else { y.Print(); } } The body of the function may call functions that are in either interface, except for names that are members of both. In that case, use the compound member access syntax ( 1 , 2 ) to qualify the name of the member, as in: fn DrawTies[T:! Renderable & GameResult](x: T) { if (x.(GameResult.Draw)()) { x.(Renderable.Draw)(); } } References: Combining interfaces by anding type-of-types Question-for-leads issue #531: Combine interfaces with + or & Proposal #553: Generics details part 1","title":"Combining constraints"},{"location":"design/#associated-types","text":"An associated type is a type member of an interface whose value is determined by the implementation of that interface for a specific type. These values are set to compile-time values in implementations, and so use the :! generic syntax inside a let declaration without an initializer. This allows types in the signatures of functions in the interface to vary. For example, an interface describing a stack might use an associated type to represent the type of elements stored in the stack. interface StackInterface { let ElementType:! Movable; fn Push[addr me: Self*](value: ElementType); fn Pop[addr me: Self*]() -> ElementType; fn IsEmpty[addr me: Self*]() -> bool; } Then different types implementing StackInterface can specify different type values for the ElementType member of the interface using a where clause: class IntStack { impl as StackInterface where .ElementType == i32 { fn Push[addr me: Self*](value: i32); // ... } } class FruitStack { impl as StackInterface where .ElementType == Fruit { fn Push[addr me: Self*](value: Fruit); // ... } } References: Generics: Associated types Proposal #731: Generics details 2: adapters, associated types, parameterized interfaces Proposal #1013: Generics: Set associated constants using where constraints","title":"Associated types"},{"location":"design/#generic-entities","text":"Many Carbon entities, not just functions, may be made generic by adding checked or template parameters .","title":"Generic entities"},{"location":"design/#generic-classes","text":"Classes may be defined with an optional explicit parameter list. All parameters to a class must be generic, and so defined with :! , either with or without the template prefix. For example, to define a stack that can hold values of any type T : class Stack(T:! Type) { fn Push[addr me: Self*](value: T); fn Pop[addr me: Self*]() -> T; var storage: Array(T); } var int_stack: Stack(i32); In this example: Stack is a type parameterized by a type T . T may be used within the definition of Stack anywhere a normal type would be used. Array(T) instantiates generic type Array with its parameter set to T . Stack(i32) instantiates Stack with T set to i32 . The values of type parameters are part of a type's value, and so may be deduced in a function call, as in this example: fn PeekTopOfStack[T:! Type](s: Stack(T)*) -> T { var top: T = s->Pop(); s->Push(top); return top; } // `int_stack` has type `Stack(i32)`, so `T` is deduced to be `i32`. PeekTopOfStack(&int_stack); References: Generic or parameterized types Proposal #1146: Generic details 12: parameterized types","title":"Generic Classes"},{"location":"design/#generic-choice-types","text":"Choice types may be parameterized similarly to classes: choice Result(T:! Type, Error:! Type) { Success(value: T), Failure(error: Error) }","title":"Generic choice types"},{"location":"design/#generic-interfaces","text":"Interfaces are always parameterized by a Self type, but in some cases they will have additional parameters. interface AddWith(U:! Type); Interfaces without parameters may only be implemented once for a given type, but a type can have distinct implementations of AddWith(i32) and AddWith(BigInt) . Parameters to an interface determine which implementation is selected for a type, in contrast to associated types which are determined by the implementation of an interface for a type. References: Generic or parameterized interfaces Proposal #731: Generics details 2: adapters, associated types, parameterized interfaces","title":"Generic interfaces"},{"location":"design/#generic-implementations","text":"An impl declaration may be parameterized by adding forall [ generic parameter list ] after the impl keyword introducer, as in: external impl forall [T:! Printable] Vector(T) as Printable; external impl forall [Key:! Hashable, Value:! Type] HashMap(Key, Value) as Has(Key); external impl forall [T:! Ordered] T as PartiallyOrdered; external impl forall [T:! ImplicitAs(i32)] BigInt as AddWith(T); external impl forall [U:! Type, T:! As(U)] Optional(T) as As(Optional(U)); Generic implementations can create a situation where multiple impl definitions apply to a given type and interface query. The specialization rules pick which definition is selected. These rules ensure: Implementations have coherence , so the same implementation is always selected for a given query. Libraries will work together as long as they pass their separate checks. A generic function can assume that some impl will be successfully selected if it can see an impl that applies, even though another more specific impl may be selected. Implementations may be marked final to indicate that they may not be specialized, subject to some restrictions . References: Generic or parameterized impls Proposal #624: Coherence: terminology, rationale, alternatives considered Proposal #920: Generic parameterized impls (details 5) Proposal #983: Generics details 7: final impls","title":"Generic implementations"},{"location":"design/#other-features","text":"Carbon generics have a number of other features, including: Named constraints may be used to disambiguate when combining two interfaces that have name conflicts. Named constraints may be implemented and otherwise used in place of an interface. Template constraints are a kind of named constraint that can contain structural requirements. For example, a template constraint could match any type that has a function with a specific name and signature without any explicit declaration that the type implements the constraint. Template constraints may only be used as requirements for template parameters. An adapter type is a type with the same data representation as an existing type, so you may cast between the two types, but can implement different interfaces or implement interfaces differently. Additional requirements can be placed on the associated types of an interface using where constraints . Implied constraints allows some constraints to be deduced and omitted from a function signature. Dynamic erased types can hold any value with a type implementing an interface, and allows the functions in that interface to be called using dynamic dispatch , for some interfaces marked \" dyn -safe\". Variadics supports variable-length parameter lists. References: Generics details Proposal #553: Generics details part 1 Proposal #731: Generics details 2: adapters, associated types, parameterized interfaces Proposal #818: Constraints for generics (generics details 3)","title":"Other features"},{"location":"design/#generic-type-equality-and-observe-declarations","text":"Determining whether two types must be equal in a generic context is in general undecidable, as has been shown in Swift . To make compilation fast, the Carbon compiler will limit its search to a depth of 1, only identifying types as equal if there is an explicit declaration that they are equal in the code, such as in a where constraint . There will be situations where two types must be equal as the result of combining these facts, but the compiler will return a type error since it did not realize they are equal due to the limit of the search. An observe ... == declaration may be added to describe how two types are equal, allowing more code to pass type checking. An observe declaration showing types are equal can increase the set of interfaces the compiler knows that a type implements. It is also possible that knowing a type implements one interface implies that it implements another, from an interface requirement or generic implementation . An observe ... is declaration may be used to observe that a type implements an interface . References: Generics: observe declarations Generics: Observing a type implements an interface Proposal #818: Constraints for generics (generics details 3) Proposal #1088: Generic details 10: interface-implemented requirements","title":"Generic type equality and observe declarations"},{"location":"design/#operator-overloading","text":"Uses of an operator in an expression is translated into a call to a method of an interface. For example, if x has type T and y has type U , then x + y is translated into a call to x.(AddWith(U).Op)(y) . So overloading of the + operator is accomplished by implementing interface AddWith(U) for type T . In order to support implicit conversion of the first operand to type T and the second argument to type U , add the like keyword to both types in the impl declaration, as in: external impl like T as AddWith(like U) where .Result == V { // `Self` is `T` here fn Op[me: Self](other: U) -> V { ... } } When the operand types and result type are all the same, this is equivalent to implementing the Add interface: external impl T as Add { fn Op[me: Self](other: Self) -> Self { ... } } The interfaces that correspond to each operator are given by: Arithmetic : -x : Negate x + y : Add or AddWith(U) x - y : Sub or SubWith(U) x * y : Mul or MulWith(U) x / y : Div or DivWith(U) x % y : Mod or ModWith(U) Bitwise and shift operators : ^x : BitComplement x & y : BitAnd or BitAndWith(U) x | y : BitOr or BitOrWith(U) x ^ y : BitXor or BitXorWith(U) x << y : LeftShift or LeftShiftWith(U) x >> y : RightShift or RightShiftWith(U) Comparison: x == y , x != y overloaded by implementing Eq or EqWith(U) x < y , x > y , x <= y , x >= y overloaded by implementing Ordered or OrderedWith(U) Conversion: x as U is rewritten to use the As(U) interface Implicit conversions use ImplicitAs(U) TODO: Assignment : x = y , ++x , x += y , and so on TODO: Dereference: *p TODO: Move : ~x TODO: Indexing: a[3] TODO: Function call: f(4) The logical operators can not be overloaded . Operators that result in l-values , such as dereferencing *p and indexing a[3] , have interfaces that return the address of the value. Carbon automatically dereferences the pointer to get the l-value. Operators that can take multiple arguments, such as function calling operator f(4) , have a variadic parameter list. Whether and how a value supports other operations, such as being copied, swapped, or set into an unformed state , is also determined by implementing corresponding interfaces for the value's type. References: Operator overloading Proposal #702: Comparison operators Proposal #820: Implicit conversions Proposal #845: as expressions Question-for-leads issue #1058: How should interfaces for core functionality be named? Proposal #1083: Arithmetic expressions Proposal #1191: Bitwise operators Proposal #1178: Rework operator interfaces","title":"Operator overloading"},{"location":"design/#common-type","text":"There are some situations where the common type for two types is needed: A conditional expression like if c then t else f returns a value with the common type of t and f . If there are multiple parameters to a function with a type parameter, it will be set to the common type of the corresponding arguments, as in: ```carbon fn F T:! Type ; // Calls F with T set to the // common type of G() and H() : F(G(), H()); ``` The inferred return type of a function with auto return type is the common type of its return statements. The common type is specified by implementing the CommonTypeWith interface: // Common type of `A` and `B` is `C`. impl A as CommonTypeWith(B) where .Result == C { } The common type is required to be a type that both types have an implicit conversion to. References: if expressions Proposal #911: Conditional expressions Question-for-leads issue #1077: find a way to permit impls of CommonTypeWith where the LHS and RHS type overlap","title":"Common type"},{"location":"design/#bidirectional-interoperability-with-c-and-c","text":"Interoperability, or interop , is the ability to call C and C++ code from Carbon code and the other way around. This ability achieves two goals: Allows sharing a code and library ecosystem with C and C++. Allows incremental migration to Carbon from C and C++. Carbon's approach to interopp is most similar to Java/Kotlin interop , where the two languages are different, but share enough of runtime model that data from one side can be used from the other. For example, C++ and Carbon will use the same memory model . The design for interoperability between Carbon and C++ hinges on: The ability to interoperate with a wide variety of code, such as classes/structs and templates , not just free functions. A willingness to expose the idioms of C++ into Carbon code, and the other way around, when necessary to maximize performance of the interoperability layer. The use of wrappers and generic programming, including templates, to minimize or eliminate runtime overhead. This feature will have some restrictions; only a subset of Carbon APIs will be available to C++ and a subset of C++ APIs will be available to Carbon. To achieve simplification in Carbon, its programming model will exclude some rarely used and complex features of C++. For example, there will be limitations on multiple inheritance . C or C++ features that compromise the performance of code that don't use that feature, like RTTI and exceptions , are in particular subject to revision in Carbon. References: Bidirectional interoperability with C/C++ Proposal #175: C++ interoperability goals","title":"Bidirectional interoperability with C and C++"},{"location":"design/#goals","text":"The goals for interop include: Support mixing Carbon and C++ toolchains Compatibility with the C++ memory model Minimize bridge code Unsurprising mappings between C++ and Carbon types Allow C++ bridge code in Carbon files Carbon inheritance from C++ types Support use of advanced C++ features Support basic C interoperability References: Interoperability: Goals","title":"Goals"},{"location":"design/#non-goals","text":"The non-goals for interop include: Full parity between a Carbon-only toolchain and mixing C++/Carbon toolchains Never require bridge code Convert all C++ types to Carbon types Support for C++ exceptions without bridge code Cross-language metaprogramming Offer equivalent support for languages other than C++ References: Interoperability: Non-goals","title":"Non-goals"},{"location":"design/#importing-and-include","text":"A C++ library header file may be imported into Carbon using an import declaration of the special Cpp package. // like `#include \"circle.h\"` in C++ import Cpp library \"circle.h\"; This adds the names from circle.h into the Cpp namespace. If circle.h defines some names in a namespace shapes { ... } scope, those will be found in Carbon's Cpp.shapes namespace. In the other direction, Carbon packages can export a header file to be #include d from C++ files. // like `import Geometry` in Carbon #include \"geometry.carbon.h\" Generally Carbon entities will be usable from C++ and C++ entities will be usable from Carbon. This includes types, function, and constants. Some entities, such as Carbon interfaces, won't be able to be translated directly. C and C++ macros that are defining constants will be imported as constants. Otherwise, C and C++ macros will be unavailable in Carbon. C and C++ typedef s would be translated into type constants, as if declared using a let . Carbon functions and types that satisfy some restrictions may be annotated as exported to C as well, like C++'s extern \"C\" marker.","title":"Importing and #include"},{"location":"design/#abi-and-dynamic-linking","text":"Carbon itself will not have a stable ABI for the language as a whole, and most language features will be designed around not having any ABI stability. Instead, we expect to add dedicated language features that are specifically designed to provide an ABI-stable boundary between two separate parts of a Carbon program. These ABI-resilient language features and API boundaries will be opt-in and explicit. They may also have functionality restrictions to make them easy to implement with strong ABI resilience. When interoperating with already compiled C++ object code or shared libraries, the C++ interop may be significantly less feature rich than otherwise. This is an open area for us to explore, but we expect to require re-compiling C++ code in order to get the full ergonomic and performance benefits when interoperating with Carbon. For example, recompilation lets us ensure Carbon and C++ can use the same representation for key vocabulary types. However, we expect to have full support for the C ABI when interoperating with already-compiled C object code or shared libraries. We expect Carbon's bridge code functionality to cover similar use cases as C++'s extern \"C\" marker in order to provide full bi-directional support here. The functionality available across this interop boundary will of course be restricted to what is expressible in the C ABI, and types may need explicit markers to have guaranteed ABI compatibility.","title":"ABI and dynamic linking"},{"location":"design/#operator-overloading_1","text":"Operator overloading is supported in Carbon, but is done by implementing an interface instead of defining a method or nonmember function as in C++. Carbon types implementing an operator overload using an interface should get the corresponding operator overload in C++. So implementing ModWith(U) in Carbon for a type effectively implements operator% in C++ for that type. This also works in the other direction, so C++ types implementing an operator overload are automatically considered to implement the corresponding Carbon interface. So implementing operator% in C++ for a type also implements interface ModWith(U) in Carbon. However, there may be edge cases around implicit conversions or overload selection that don't map completely into Carbon. In some cases, the operation might be written differently in the two languages. In those cases, they are matched according to which operation has the most similar semantics rather than using the same symbols. For example, the ^x operation and BitComplement interface in Carbon corresponds to the ~x operation and operator~ function in C++. Similarly, the ImplicitAs(U) Carbon interface corresponds to implicit conversions in C++, which can be written in multiple different ways. Other C++ customization points like swap will correspond to a Carbon interface, on a case-by-case basis. Some operators will only exist or be overridable in C++, such as logical operators or the comma operator. In the unlikely situation where those operators need to be overridden for a Carbon type, that can be done with a nonmember C++ function. Carbon interfaces with no C++ equivalent, such as CommonTypeWith(U) , may be implemented for C++ types externally in Carbon code. To satisfy the orphan rule ( 1 , 2 ), each C++ library will have a corresponding Carbon wrapper library that must be imported instead of the C++ library if the Carbon wrapper exists. TODO: Perhaps it will automatically be imported, so a wrapper may be added without requiring changes to importers?","title":"Operator overloading"},{"location":"design/#templates","text":"Carbon supports both checked and template generics . This provides a migration path for C++ template code: C++ template -> Carbon template: This involves migrating the code from C++ to Carbon. If that migration is faithful, the change should be transparent to callers. -> Carbon template with constraints: Constraints may be added one at a time. Adding a constraint never changes the meaning of the code as long as it continues to compile. Compile errors will point to types for which an implementation of missing interfaces is needed. A temporary template implementation of that interface can act as a bridge during the transition. -> Carbon checked generic: Once all callers work after all constraints have been added, the template parameter may be switched to a checked generic. Carbon will also provide direct interop with C++ templates in many ways: Ability to call C++ templates and use C++ templated types from Carbon. Ability to instantiate a C++ template with a Carbon type. Ability to instantiate a Carbon generic with a C++ type. We expect the best interop in these areas to be based on a Carbon-provided C++ toolchain. However, even when using Carbon's generated C++ headers for interop, we will include the ability where possible to use a Carbon generic from C++ as if it were a C++ template.","title":"Templates"},{"location":"design/#standard-types","text":"The Carbon integer types, like i32 and u64 , are considered equal to the corresponding fixed-width integer types in C++, like int32_t and uint64_t , provided by <stdint.h> or <cstdint> . The basic C and C++ integer types like int , char , and unsigned long are available in Carbon inside the Cpp namespace given an import Cpp; declaration, with names like Cpp.int , Cpp.char , and Cpp.unsigned_long . C++ types are considered different if C++ considers them different, so C++ overloads are resolved the same way. Carbon conventions for implicit conversions between integer types apply here, allowing them whenever the numerical value for all inputs may be preserved by the conversion. Other C and C++ types are equal to Carbon types as follows: C or C++ Carbon bool bool float f32 double f64 T* Optional(T*) T[4] [T; 4] Further, C++ reference types like T& will be translated to T* in Carbon, which is Carbon's non-null pointer type. Carbon will work to have idiomatic vocabulary view types for common data structures, like std::string_view and std::span , map transparently between C++ and the Carbon equivalents. This will include data layout so that even pointers to these types translate seamlessly, contingent on a suitable C++ ABI for those types, potentially by re-compiling the C++ code with a customized ABI. We will also explore how to expand coverage to similar view types in other libraries. However, Carbon's containers will be distinct from the C++ standard library containers in order to maximize our ability to improve performance and leverage language features like checked generics in their design and implementation. Where possible, we will also try to provide implementations of Carbon's standard library container interfaces for the relevant C++ container types so that they can be directly used with generic Carbon code. This should allow generic code in Carbon to work seamlessly with both Carbon and C++ containers without performance loss or constraining the Carbon container implementations. In the other direction, Carbon containers will satisfy C++ container requirements, so templated C++ code can operate directly on Carbon containers as well.","title":"Standard types"},{"location":"design/#inheritance_1","text":"Carbon has single inheritance allowing C++ classes using inheritance to be migrated. The data representation will be consistent so that Carbon classes may inherit from C++ classes, and the other way around, even with virtual methods. C++ multiple inheritance and CRTP will be migrated using a combination of Carbon features. Carbon mixins support implementation reuse and Carbon interfaces allow a type to implement multiple APIs. However, there may be limits on the degree of interop available with multiple inheritance across the C++ <-> Carbon boundaries. Carbon dyn-safe interfaces may be exported to C++ as an abstract base class . The reverse operation is also possible using a proxy object implementing a C++ abstract base class and holding a pointer to a type implementing the corresponding interface.","title":"Inheritance"},{"location":"design/#enums","text":"TODO","title":"Enums"},{"location":"design/#unfinished-tales","text":"","title":"Unfinished tales"},{"location":"design/#safety","text":"Carbon's premise is that C++ users can't give up performance to get safety. Even if some isolated users can make that tradeoff, they share code with performance-sensitive users. Any path to safety must preserve performance of C++ today. This rules out garbage collection, and many other options. The only well understood mechanism of achieving safety without giving up performance is compile-time safety. The leading example of how to achieve this is Rust. The difference between Rust's approach and Carbon's is that Rust starts with safety and Carbons starts with migration. Rust supports interop with C, and there is ongoing work to improve the C++-interop story and develop migration tools. However, there is a large gap in programming models between the two languages, generally requiring a revision to the architecture. So, thus far the common pattern in the Rust community is to \"rewrite it in Rust\" ( 1 , 2 , 3 ). Carbon's approach is to focus on migration from C++, including seamless interop, and then incrementally improve safety. The first impact on Carbon's design to support its safety strategy are the necessary building blocks for this level of compile-time safety. We look at existing languages like Rust and Swift to understand what fundamental capabilities they ended up needing. The two components that stand out are: Expanded type system that includes more semantic information. More pervasive use of type system abstractions (typically generics). For migrating C++ code, we also need the ability to add features and migrate code to use those new features incrementally and over time. This requires designing the language with evolution baked in on day one. This impacts a wide range of features: At the lowest level, a simple and extensible syntax and grammar. Tools and support for adding and removing APIs. Scalable migration strategies, including tooling support. Rust shows the value of expanded semantic information in the type system such as precise lifetimes. This is hard to do in C++ since it has too many kinds of references and pointers, which increases the complexity in the type system multiplicatively. Carbon is attempting to compress C++'s type variations into just values and pointers . Rust also shows the value of functions parameterized by lifetimes. Since lifetimes are only used to establish safety properties of the code, there is no reason to pay the cost of monomorphization for those parameters. So we need a generics system that can reason about code before it is instantiated, unlike C++ templates. In conclusion, there are two patterns in how Carbon diverges from C++: Simplify and removing things to create space for new safety features. This trivially requires breaking backwards compatibility. Re-engineer foundations to model and enforce safety. This has complex and difficulty in C++ without first simplifying the language. This leads to Carbon's incremental path to safety: Keep your performance, your existing codebase, and your developers. Adopt Carbon through a scalable, tool-assisted migration from C++. Address initial, easy safety improvements starting day one. Shift the Carbon code onto an incremental path towards memory safety over the next decade. References: Safety strategy","title":"Safety"},{"location":"design/#lifetime-and-move-semantics","text":"TODO:","title":"Lifetime and move semantics"},{"location":"design/#metaprogramming","text":"TODO: References need to be evolved. Needs a detailed design and a high level summary provided inline. Carbon provides metaprogramming facilities that look similar to regular Carbon code. These are structured, and do not offer arbitrary inclusion or preprocessing of source text such as C and C++ do. References: Metaprogramming","title":"Metaprogramming"},{"location":"design/#pattern-matching-as-function-overload-resolution","text":"TODO: References need to be evolved. Needs a detailed design and a high level summary provided inline. References: Pattern matching","title":"Pattern matching as function overload resolution"},{"location":"design/#error-handling","text":"For now, Carbon does not have language features dedicated to error handling, but we would consider adding some in the future. At this point, errors are represented using choice types like Result and Optional . This is similar to the story for Rust, which started using Result , then added ? operator for convenience, and is now considering ( 1 , 2 ) adding more.","title":"Error handling"},{"location":"design/#execution-abstractions","text":"Carbon provides some higher-order abstractions of program execution, as well as the critical underpinnings of such abstractions.","title":"Execution abstractions"},{"location":"design/#abstract-machine-and-execution-model","text":"TODO:","title":"Abstract machine and execution model"},{"location":"design/#lambdas","text":"TODO:","title":"Lambdas"},{"location":"design/#co-routines","text":"TODO:","title":"Co-routines"},{"location":"design/#concurrency","text":"TODO:","title":"Concurrency"},{"location":"design/aliases/","text":"Aliases Table of contents TODO Overview Alternatives TODO This is a skeletal design, added to support the overview . It should not be treated as accepted by the core team; rather, it is a placeholder until we have more time to examine this detail. Please feel welcome to rewrite and update as appropriate. Overview Naming is one of the things that most often requires careful management over time -- things tend to get renamed and moved around. Carbon provides a fully general name aliasing facility to declare a new name as an alias for a value; everything is a value in Carbon. This is a fully general facility because everything is a value in Carbon, including types. For example: alias MyInt = Int; This creates an alias called MyInt for whatever Int resolves to. Code textually after this can refer to MyInt , and it will transparently refer to Int . Alternatives The syntax here is not at all in a good state yet. We've considered a few alternatives, but they all end up being confusing in some way. We need to figure out a good and clean syntax that can be used here.","title":"Aliases"},{"location":"design/aliases/#aliases","text":"","title":"Aliases"},{"location":"design/aliases/#table-of-contents","text":"TODO Overview Alternatives","title":"Table of contents"},{"location":"design/aliases/#todo","text":"This is a skeletal design, added to support the overview . It should not be treated as accepted by the core team; rather, it is a placeholder until we have more time to examine this detail. Please feel welcome to rewrite and update as appropriate.","title":"TODO"},{"location":"design/aliases/#overview","text":"Naming is one of the things that most often requires careful management over time -- things tend to get renamed and moved around. Carbon provides a fully general name aliasing facility to declare a new name as an alias for a value; everything is a value in Carbon. This is a fully general facility because everything is a value in Carbon, including types. For example: alias MyInt = Int; This creates an alias called MyInt for whatever Int resolves to. Code textually after this can refer to MyInt , and it will transparently refer to Int .","title":"Overview"},{"location":"design/aliases/#alternatives","text":"The syntax here is not at all in a good state yet. We've considered a few alternatives, but they all end up being confusing in some way. We need to figure out a good and clean syntax that can be used here.","title":"Alternatives"},{"location":"design/blocks_and_statements/","text":"Blocks and statements Table of contents TODO Overview TODO This is a skeletal design, added to support the overview . It should not be treated as accepted by the core team; rather, it is a placeholder until we have more time to examine this detail. Please feel welcome to rewrite and update as appropriate. Overview The body or definition of a function is provided by a block of code containing statements, much like in C or C++. The body of a function is also a new, nested scope inside the function's scope (meaning that parameter names are available). Statements within a block are terminated by a semicolon. Each statement can, among other things, be an expression. Here is a trivial example of a function definition using a block of statements: fn Foo() { Bar(); Baz(); } Statements can also themselves be a block of statements, which provide scopes and nesting: fn Foo() { Bar(); { Baz(); } }","title":"Blocks and statements"},{"location":"design/blocks_and_statements/#blocks-and-statements","text":"","title":"Blocks and statements"},{"location":"design/blocks_and_statements/#table-of-contents","text":"TODO Overview","title":"Table of contents"},{"location":"design/blocks_and_statements/#todo","text":"This is a skeletal design, added to support the overview . It should not be treated as accepted by the core team; rather, it is a placeholder until we have more time to examine this detail. Please feel welcome to rewrite and update as appropriate.","title":"TODO"},{"location":"design/blocks_and_statements/#overview","text":"The body or definition of a function is provided by a block of code containing statements, much like in C or C++. The body of a function is also a new, nested scope inside the function's scope (meaning that parameter names are available). Statements within a block are terminated by a semicolon. Each statement can, among other things, be an expression. Here is a trivial example of a function definition using a block of statements: fn Foo() { Bar(); Baz(); } Statements can also themselves be a block of statements, which provide scopes and nesting: fn Foo() { Bar(); { Baz(); } }","title":"Overview"},{"location":"design/classes/","text":"Classes Table of contents Overview Use cases Data classes Encapsulated types Without inheritance With inheritance and subtyping Polymorphic types Interface as base class Non-polymorphic inheritance Interop with C++ multiple inheritance Mixins Background Members Data members have an order Struct types Literals Type expression Assignment and initialization Operations performed field-wise Nominal class types Forward declaration Self Construction Assignment Member functions Class functions Methods Name lookup in member function definitions Nominal data classes Member type Let Alias Inheritance Virtual methods Virtual override keywords Subtyping Constructors Partial facet Usage Assignment with inheritance Destructors Access control Private access Protected access Friends Test friendship Access control for construction Operator overloading Future work Struct literal shortcut Optional named parameters Field defaults for struct types Destructuring in pattern matching Discussion Inheritance C++ abstract base classes interoperating with object-safe interfaces Overloaded methods Interop with C++ inheritance Virtual base classes Mixins Memory layout No static variables Computed properties Interfaces implemented for data classes References Overview A Carbon class is a user-defined record type . A class has members that are referenced by their names, in contrast to a Carbon tuple which defines a product type whose members are referenced positionally. Classes are the primary mechanism for users to extend the Carbon type system and are deeply rooted in C++ and its history (C and Simula). We call them classes rather than other terms as that is both familiar to existing programmers and accurately captures their essence: they define the types of objects with (optional) support for methods, encapsulation, and so on. Carbon supports both named, or \"nominal\", and unnamed, anonymous, or \"structural\", class types. Nominal class types are all distinct, but structural types are equal if they have the same sequence of member types and names. Structural class literals may be used to initialize or assign values to nominal class variables. A class type defines the interpretation of the bytes of a value of that type, including the size, data members, and layout. It defines the operations that may be performed on those values, including what methods may be called. A class type may directly have constant members. The type itself is a compile-time immutable constant value. Use cases The use cases for classes include both cases motivated by C++ interop, and cases that we expect to be included in idiomatic Carbon-only code. This design currently only attempts to address the \"data classes\" and \"encapsulated types\" use cases. Addressing the \"interface as base class\", \"interop with C++ multiple inheritance\" and \"mixin\" use cases is future work. Data classes Data classes are types that consist of data fields that are publicly accessible and directly read and manipulated by client code. They have few if any methods, and generally are not involved in inheritance at all. Examples include: a key and value pair returned from a SortedMap or HashMap a 2D point that might be used in a rendering API Properties: Operations like copy, move, destroy, unformed, and so on are defined field-wise. Anonymous classes types and literals should match data class semantics. Expected in idiomatic Carbon-only code. Background: Kotlin has a dedicated concise syntax for defining data classes that avoids boilerplate. Python has a data class library , proposed in PEP 557 , that fills a similar role. Encapsulated types There are several categories of types that support encapsulation . This is done by making their data fields private so access and modification of values are all done through methods defined on the type. Without inheritance The common case for encapsulated types are those that do not participate in inheritance. These types neither support being inherited from (they are \"final\" ) nor do they extend other types. Examples of this use case include: strings, containers, iterators types with invariants such as Date RAII types that are movable but not copyable like C++'s std::unique_ptr or a file handle non-movable types like Mutex We expect two kinds of methods on these types: public methods defining the API for accessing and manipulating values of the type, and private helper methods used as an implementation detail of the public methods. These types are expected in idiomatic Carbon-only code. With inheritance and subtyping The subtyping you get with inheritance is that you may assign the address of an object of a derived type to a pointer to its base type. For this to work, the compiler needs implementation strategies that allow operations performed through the pointer to the base type work independent of which derived type it actually points to. These strategies include: Arranging for the the data layout of derived types to start with the data layout of the base type as a prefix. Putting a pointer to a table of function pointers, a vtable , as the first data member of the object. This allows methods to be virtual and have a derived-type-specific implementation, an override , that is used even when invoking the method on a pointer to a base type. Non-virtual methods implemented on a base type should be applicable to all derived types. In general, derived types should not attempt to overload or override non-virtual names defined in the base type. Note that these subtyping implementation strategies generally rely on encapsulation, but encapsulation is not a strict requirement in all cases. This subtyping relationship also creates safety concerns, which Carbon should protect against. Slicing problems can arise when the source or target of an assignment is a dereferenced pointer to the base type. It is also incorrect to delete an object with a non-virtual destructor through a pointer to a base type. Polymorphic types Carbon will fully support single-inheritance type hierarchies with polymorphic types. Polymorphic types support dynamic dispatch using a vtable , and data members, but only single inheritance. Individual methods opt in to using dynamic dispatch, so types will have a mix of \"virtual\" and non-virtual methods. Polymorphic types support traditional object-oriented single inheritance , a mix of subtyping and implementation and code reuse . We exclude complex multiple inheritance schemes, virtual inheritance, and so on from this use case. This is to avoid the complexity and overhead they bring, particularly since the use of these features in C++ is generally discouraged. The rule is that every type has at most one base type with data members for subtyping purposes. Carbon will support additional base types as long as they don't have data members or don't support subtyping . Background: The \"Nothing is Something\" talk by Sandi Metz and the Composition Over Inheritance Principle describe design patterns to use instead of multiple inheritance to support types that vary over multiple axes. In rare cases where the complex multiple inheritance schemes of C++ are truly needed, they can be effectively approximated using a combination of these simpler building blocks. Polymorphic types support a number of different kinds of methods: They will have virtual methods: Polymorphic types will typically include virtual destructors. The virtual methods types may have default implementations or be abstract (or pure virtual ). In the latter case, they must be implemented in any derived class that can be instantiated. Virtual methods may be protected or private , intended to be called by methods in the base type but implemented in the descendant. They may have non-virtual public or private helper methods, like encapsulated types without inheritance . These avoid the overhead of a virtual function call, and can be written when the base class has sufficient data members. They may have protected helper methods, typically non-virtual, provided by the base type to be called by the descendant. Note that there are two uses for protected methods: those implemented in the base and called in the descendant, and the other way around. \"The End Of Object Inheritance & The Beginning Of A New Modularity\" talk by Augie Fackler and Nathaniel Manista discusses design patterns that split up types to reduce the number of kinds of calls between base and derived types, and make sure calls only go in one direction. We expect polymorphic types in idiomatic Carbon-only code, at least for the medium term. Extending this design to support polymorphic types is future work. Interface as base class We distinguish the specific case of polymorphic base classes that have no data members: From an implementation perspective, the lack of data members removes most of the problems with supporting multiple inheritance. They are about decoupling two pieces of code instead of collaborating. As a use case, they are used primarily for subtyping and much less implementation reuse than other polymorphic types. This case overlaps with the interface concept introduced for Carbon generics . Removing support for data fields greatly simplifies supporting multiple inheritance. For example, it removes the need for a mechanism to figure out the offset of those data fields in the object. Similarly we don't need C++'s virtual inheritance to avoid duplicating those fields. Some complexities still remain, such as pointers changing values when casting to a secondary parent type, but these seem manageable given the benefits of supporting this useful case of multiple inheritance. While an interface base class is generally for providing an API that allows decoupling two pieces of code, a polymorphic type is a collaboration between a base and derived type to provide some functionality. This is a bit like the difference between a library and a framework, where you might use many of the former but only one of the latter. Interface base classes are primarily used for subtyping. The extent of implementation reuse is generally limited by the lack of data members, and the decoupling role they play is usually about defining an API as a set of public pure-virtual methods. Compared to other polymorphic types, they more rarely have methods with implementations (virtual or not), or have methods with restricted access. The main use case is when there is a method that is implemented in terms of pure-virtual methods. Those pure-virtual methods may be marked as protected to ensure they are only called through the non-abstract API, but can still be implemented in descendants. While it is typical for this case to be associated with single-level inheritance hierarchies, there are some cases where there is an interface at the root of a type hierarchy and polymorphic types as interior branches of the tree. The case of generic interfaces extending or requiring other interface would also be modeled by deeper inheritance hierarchies. An interface as base class needs to either have a virtual destructor or forbid deallocation. There is significant overlap between interface base classes and Carbon interfaces . Both represent APIs as a collection of method names and signatures to implement. The subset of interfaces that support dynamic dispatch are called object-safe , following Rust : They don't have a Self in the signature of a method in a contravariant position like a parameter. They don't have free associated types or other associated items used in a method signature. The restrictions on object-safe interfaces match the restrictions on base class methods. The main difference is the representation in memory. A type extending a base class with virtual methods includes a pointer to the table of methods in the object value itself, while a type implementing an interface would store the pointer alongside the pointer to the value in a DynPtr(MyInterface) . Of course, the interface option also allows the method table to be passed at compile time. Note: This presumes that we include some concept of final methods in interfaces to match non-virtual functions in base classes. We expect idiomatic Carbon-only code to generally use Carbon interfaces instead of interface base classes. We may still support interface base classes long term if we determine that the ability to put the pointer to the method implementations in the object value is important for users, particularly with a single parent as in the polymorphic type case . Extending this design to support interface base classes is future work. Background: C++ abstract base classes that don't have data members and Java interfaces model this case. Non-polymorphic inheritance While it is not common, there are cases where C++ code uses inheritance without dynamic dispatch or a vtable . Instead, methods are never overridden, and derived types only add data and methods. There are some cases where this is done in C++ but would be done differently in Carbon: For implementation reuse without subtyping, Carbon code should use mixins or composition. Carbon won't support private inheritance. Carbon will allow data members to have size zero, so the empty-base optimization is unnecessary. For cases where the derived type does not add any data members, in Carbon you can potentially use adapter types instead of inheritance. However, there are still some cases where non-virtual inheritance makes sense. One is a parameterized type where a prefix of the data is the same independent of the parameter. An example of this is containers with a small-buffer optimization , as described in the talk CppCon 2016: Chandler Carruth \"High Performance Code 201: Hybrid Data Structures\" . By moving the data and methods that don't depend on the buffer size to a base class, we reduce the instantiation overhead for monomorphization. The base type is also useful for reducing instantiation for consumers of the container, as long as they only need to access methods defined in the base. Another case for non-virtual inheritance is for different node types within a data structure that have some data members in common. This is done in LLVM's map, red-black tree , and list data structure types. In a linked list, the base type might have the next and previous pointers, which is enough for a sentinel node, and there would also be a derived type with the actual data member. The base type can define operations like \"splice\" that only operate on the pointers not the data, and this is in fact enforced by the type system. Only the derived node type needs to be parameterized by the element type, saving on instantiation costs as before. Many of the concerns around non-polymorphic inheritance are the same as for the non-virtual methods of polymorphic types . Assignment and destruction are examples of operations that need particular care to be sure they are only done on values of the correct type, rather than through a subtyping relationship. This means having some extrinsic way of knowing when it is safe to downcast before performing one of those operations, or performing them on pointers that were never upcast to the base type. Interop with C++ multiple inheritance While Carbon won't support all the C++ forms of multiple inheritance, Carbon code will still need to interoperate with C++ code that does. Of particular concern are the std::iostream family of types. Most uses of those types are the input and output variations or could be migrated to use those variations, not the harder bidirectional cases. Much of the complexity of this interoperation could be alleviated by adopting the restriction that Carbon code can't directly access the fields of a virtual base class. In the cases where such access is needed, the workaround is to access them through C++ functions. We do not expect idiomatic Carbon-only code to use multiple inheritance. Extending this design to support interoperating with C++ types using multiple inheritance is future work. Mixins A mixin is a declaration of data, methods, and interface implementations that can be added to another type, called the \"main type\". The methods of a mixin may also use data, methods, and interface implementations provided by the main type. Mixins are designed around implementation reuse rather than subtyping, and so don't need to use a vtable. A mixin might be an implementation detail of a data class , or encapsulated type . A mixin might partially implement an interface as base class . Examples: intrusive linked list , intrusive reference count In both of these examples, the mixin needs the ability to convert between a pointer to the mixin's data (like a \"next\" pointer or reference count) and a pointer to the containing object with the main type. Mixins are expected in idiomatic Carbon-only code. Extending this design to support mixins is future work. Background: Mixins are typically implemented using the curiously recurring template pattern in C++, but other languages support them directly. In Dart, the mixin defines an interface that the destination type ends up implementing, which restores a form of subtyping. See Dart: What are mixins? . Swift is considering a proposal to add mixin support . Background See how other languages tackle this problem: Swift has two different concepts: classes support inheritance and use reference counting while structs have value semantics may have constructor functions called \"initializers\" and destructors called \"deinitializers\" supports properties , including computed & lazy properties methods are const by default unless marked mutating supports extensions has per-field access control Rust has no support for inheritance has no special constructor functions, instead has literal syntax has some convenience syntax for common cases: variable and field names matching , updating a subset of fields can have unnamed fields supports structs with size 0 Zig explicitly mark structs as packed to manually control layout has a struct literal syntax, including for anonymous structs no special constructor functions supports fields with undefined values supports structs with size 0 supports generics by way of memoized compile time functions accepting and returning types supports default field values has no properties or operator overloading -- Zig does not like hidden control flow Members The members of a class are named, and are accessed with the . notation. For example: var p: Point2D = ...; // Data member access p.x = 1; p.y = 2; // Method call Print(p.DistanceFromOrigin()); Tuples are used for cases where accessing the members positionally is more appropriate. Data members have an order The data members of a class, or fields , have an order that matches the order they are declared in. This determines the order of those fields in memory, and the order that the fields are destroyed when a value goes out of scope or is deallocated. Struct types Structural data classes , or struct types , are convenient for defining data classes in an ad-hoc manner. They would commonly be used: as the return type of a function that returns multiple values and wants those values to have names so a tuple is inappropriate as an initializer for other class variables or values as a type parameter to a container Note that struct types are examples of data class types and are still classes. The \"nominal data classes\" section describes another way to define a data class type. Also note that there is no struct keyword, \"struct\" is just convenient shorthand terminology for a structural data class. Literals Structural data class literals , or struct literals , are written using this syntax: var kvpair: auto = {.key = \"the\", .value = 27}; This produces a struct value with two fields: The first field is named \" key \" and has the value \"the\" . The type of the field is set to the type of the value, and so is String . The second field is named \" value \" and has the value 27 . The type of the field is set to the type of the value, and so is i32 . Note: A comma , may optionally be included after the last field: var kvpair: auto = {.key = \"the\", .value = 27,}; Open question: To keep the literal syntax from being ambiguous with compound statements, Carbon will adopt some combination of: looking ahead after a { to see if it is followed by .name ; not allowing a struct literal at the beginning of a statement; only allowing { to introduce a compound statement in contexts introduced by a keyword where they are required, like requiring { ... } around the cases of an if...else statement. Type expression The type of kvpair in the last example would be represented by this expression: {.key: String, .value: i32} This syntax is intended to parallel the literal syntax, and so uses commas ( , ) to separate fields instead of a semicolon ( ; ) terminator. This choice also reflects the expected use inline in function signature declarations. Struct types may only have data members, so the type declaration is just a list of field names and types. The result of a struct type expression is an immutable compile-time type value. Note: Like with struct literal expressions, a comma , may optionally be included after the last field: {.key: String, .value: i32,} Also note that {} represents both the empty struct literal and its type. Assignment and initialization When initializing or assigning a variable with a data class such as a struct type to a struct value on the right hand side, the order of the fields does not have to match, just the names. var different_order: {.x: i32, .y: i32} = {.y = 2, .x = 3}; Assert(different_order.x == 3); Assert(different_order.y == 2); Initialization and assignment occur field-by-field. The order of fields is determined from the target on the left side of the = . This rule matches what we expect for classes with encapsulation more generally. Open question: What operations and in what order happen for assignment and initialization? Is assignment just destruction followed by initialization? Is that destruction completed for the whole object before initializing, or is it interleaved field-by-field? When initializing to a literal value, is a temporary containing the literal value constructed first or are the fields initialized directly? The latter approach supports types that can't be moved or copied, such as mutex. Perhaps some operations are not ordered with respect to each other? Operations performed field-wise Generally speaking, the operations that are available on a data class value, such as a value with a struct type, are dependent on those operations being available for all the types of the fields. For example, two values of the same data class type may be compared for equality or inequality if equality is supported for every member of the type: var p: auto = {.x = 2, .y = 3}; Assert(p == {.x = 2, .y = 3}); Assert(p != {.x = 2, .y = 4}); Assert({.x = 2, .y = 4} != {.x = 5, .y = 3}); Equality and inequality comparisons are also allowed between different data class types when: At least one is a struct type. They have the same set of field names, though the order may be different. Equality comparison is defined between the pairs of member types with the same field names. For example, since comparison between i32 and u32 is defined , equality comparison between values of types {.x: i32, .y: i32} and {.y: u32, .x: u32} is as well. Equality and inequality comparisons compare fields using the field order of the left-hand operand and stop once the outcome of the comparison is determined. However, the comparison order and short-circuiting are generally expected to affect only the performance characteristics of the comparison and not its meaning. Ordering comparisons, such as < and <= , use the order of the fields to do a lexicographical comparison . The argument types must have a matching order of the field names. Otherwise, the restrictions on ordering comparisons between different data class types are analogous to equality comparisons: At least one is a struct type. Ordering comparison is defined between the pairs of member types with the same field names. Implicit conversion from a struct type to a data class type is allowed when the set of field names is the same and implicit conversion is defined between the pairs of member types with the same field names. So calling a function effectively performs an assignment from each of the caller's arguments to the function's parameters, and will be valid when those assignments are all valid. A data class has an unformed state if all its members do. Treatment of unformed state follows proposal #257 . Destruction is performed field-wise in reverse order. Extending user-defined operations on the fields to an operation on an entire data class is future work . References: The rules for assignment, comparison, and implicit conversion for argument passing were decided in question-for-leads issue #710 . Nominal class types The declarations for nominal class types will have: an optional abstract or base prefix class introducer the name of the class an optional extends followed by the name of the immediate base class { , an open curly brace a sequence of declarations } , a close curly brace Declarations should generally match declarations that can be declared in other contexts, for example variable declarations with var will define instance variables : class TextLabel { var x: i32; var y: i32; var text: String = \"default\"; } The main difference here is that \"default\" is a default instead of an initializer, and will be ignored if another value is supplied for that field when constructing a value. Defaults must be constants whose value can be determined at compile time. Forward declaration To support circular references between class types, we allow forward declaration of types. Forward declarations end with semicolon ; after the name of the class, instead of any extends clause and the block of declarations in curly braces { ... } . A type that is forward declared is considered incomplete until the end of a definition with the same name. // Forward declaration of `GraphNode`. class GraphNode; class GraphEdge { var head: GraphNode*; var tail: GraphNode*; } class GraphNode { var edges: Vector(GraphEdge*); } // `GraphNode` is first complete here. Open question: What is specifically allowed and forbidden with an incomplete type has not yet been decided. Self A class definition may provisionally include references to its own name in limited ways. These limitations arise from the type not being complete until the end of its definition is reached. class IntListNode { var data: i32; var next: IntListNode*; } An equivalent definition of IntListNode , since Self is an alias for the current type, is: class IntListNode { var data: i32; var next: Self*; } Self refers to the innermost type declaration: class IntList { class IntListNode { var data: i32; var next: Self*; } var first: IntListNode*; } Construction Any function with access to all the data fields of a class can construct one by converting a struct value to the class type: var tl1: TextLabel = {.x = 1, .y = 2}; var tl2: auto = {.x = 1, .y = 2} as TextLabel; Assert(tl1.x == tl2.x); fn ReturnsATextLabel() -> TextLabel { return {.x = 1, .y = 2}; } var tl3: TextLabel = ReturnsATextLabel(); fn AcceptsATextLabel(tl: TextLabel) -> i32 { return tl.x + tl.y; } Assert(AcceptsATextLabel({.x = 2, .y = 4}) == 6); Note that a nominal class, unlike a struct type , can define default values for fields, and so may be initialized with a struct value that omits some or all of those fields. Assignment Assignment to a struct value is also allowed in a function with access to all the data fields of a class. Assignment always overwrites all of the field members. var tl: TextLabel = {.x = 1, .y = 2}; Assert(tl.text == \"default\"); // \u2705 Allowed: assigns all fields tl = {.x = 3, .y = 4, .text = \"new\"}; // \u2705 Allowed: This statement is evaluated in two steps: // 1. {.x = 5, .y = 6} is converted into a new TextLabel value, // using default for field `text`. // 2. tl is assigned to a TextLabel, which has values for all // fields. tl = {.x = 5, .y = 6}; Assert(tl.text == \"default\"); Open question: This behavior might be surprising because there is an ambiguity about whether to use the default value or the previous value for a field. We could require all fields to be specified when assigning, and only use field defaults when initializing a new value. // \u274c Forbidden: should tl.text == \"default\" or \"new\"? tl = {.x = 5, .y = 6}; Member functions Member functions can either be class functions or methods. Class functions are members of the type, while methods can only be called on instances. Class functions A class function is like a C++ static member function or method , and is declared like a function at file scope. The declaration can include a definition of the function body, or that definition can be provided out of line after the class definition is finished. A common use is for constructor functions. class Point { fn Origin() -> Self { return {.x = 0, .y = 0}; } fn CreateCentered() -> Self; var x: i32; var y: i32; } fn Point.CreateCentered() -> Self { return {.x = ScreenWidth() / 2, .y = ScreenHeight() / 2}; } Class functions are members of the type, and may be accessed as using dot . member access either the type or any instance. var p1: Point = Point.Origin(); var p2: Point = p1.CreateCentered(); Methods Method declarations are distinguished from class function declarations by having a me parameter in square brackets [ ... ] before the explicit parameter list in parens ( ... ) . There is no implicit member access in methods, so inside the method body members are accessed through the me parameter. Methods may be written lexically inline or after the class declaration. class Circle { fn Diameter[me: Self]() -> f32 { return me.radius * 2; } fn Expand[addr me: Self*](distance: f32); var center: Point; var radius: f32; } fn Circle.Expand[addr me: Self*](distance: f32) { me->radius += distance; } var c: Circle = {.center = Point.Origin(), .radius = 1.5 }; Assert(Math.Abs(c.Diameter() - 3.0) < 0.001); c.Expand(0.5); Assert(Math.Abs(c.Diameter() - 4.0) < 0.001); Methods are called using using the dot . member syntax, c.Diameter() and c.Expand( ... ) . Diameter computes and returns the diameter of the circle without modifying the Circle instance. This is signified using [me: Self] in the method declaration. c.Expand( ... ) does modify the value of c . This is signified using [addr me: Self*] in the method declaration. The pattern ' addr patt ' means \"first take the address of the argument, which must be an l-value , and then match pattern patt against it\". If the method declaration also includes deduced generic parameters , the me parameter must be in the same list in square brackets [ ... ] . The me parameter may appear in any position in that list, as long as it appears after any names needed to describe its type. Name lookup in member function definitions When defining a member function lexically inline, we delay type checking of the function body until the definition of the current type is complete. This means that name lookup for members of objects is also delayed. That means that you can reference me.F() in a lexically inline method definition even before the declaration of F in that class definition. However, other names still need to be declared before they are used. This includes unqualified names, names within namespaces, and names for members of types . class Point { fn Distance[me: Self]() -> f32 { // \u2705 Allowed: `x` and `y` are names for members of an object, // and so lookup is delayed until `type_of(me) == Self` is complete. return Math.Sqrt(me.x * me.x + me.y * me.y); } fn CreatePolarInvalid(r: f32, theta: f32) -> Point { // \u274c Forbidden: unqualified name used before declaration. return Create(r * Math.Cos(theta), r * Math.Sin(theta)); } fn CreatePolarValid1(r: f32, theta: f32) -> Point { // \u274c Forbidden: `Create` is not yet declared. return Point.Create(r * Math.Cos(theta), r * Math.Sin(theta)); } fn CreatePolarValid2(r: f32, theta: f32) -> Point { // \u274c Forbidden: `Create` is not yet declared. return Self.Create(r * Math.Cos(theta), r * Math.Sin(theta)); } fn Create(x: f32, y: f32) -> Point { // \u2705 Allowed: checking that conversion of `{.x: f32, .y: f32}` // to `Point` is delayed until `Point` is complete. return {.x = x, .y = y}; } fn CreateXEqualsY(xy: f32) -> Point { // \u2705 Allowed: `Create` is declared earlier. return Create(xy, xy); } fn CreateXAxis(x: f32) -> Point; fn Angle[me: Self]() -> f32; var x: f32; var y: f32; } fn Point.CreateXAxis(x: f32) -> Point; // \u2705 Allowed: `Point` type is complete. // Members of `Point` like `Create` are in scope. return Create(x, 0); } fn Point.Angle[me: Self]() -> f32 { // \u2705 Allowed: `Point` type is complete. // Function is checked immediately. return Math.ATan2(me.y, me.x); } Note: The details of name lookup are still being decided in issue #472: Open question: Calling functions defined later in the same file . Nominal data classes We will mark data classes with an impl as Data {} line. class TextLabel { var x: i32; var y: i32; var text: String; // This line makes `TextLabel` a data class, which defines // a number of operations field-wise. impl as Data {} } The fields of data classes must all be public. That line will add field-wise implementations and operations of all interfaces that a struct with the same fields would get by default . The word Data here refers to an empty interface in the Carbon prologue. That interface would then be part of our strategy for defining how other interfaces are implemented for data classes . References: Rationale for this approach is given in proposal #722 . Member type Additional types may be defined in the scope of a class definition. class StringCounts { class Node { var key: String; var count: i32; } var counts: Vector(Node); } The inner type is a member of the type, and is given the name StringCounts.Node . This case is called a member class since the type is a class, but other kinds of type declarations, like choice types, are allowed. Let Other type constants can be defined using a let declaration: class MyClass { let Pi:! f32 = 3.141592653589793; let IndexType:! Type = i32; } The :! indicates that this is defining a compile-time constant, and so does not affect the storage of instances of that class. Alias You may declare aliases of the names of class members. This is to allow them to be renamed in multiple steps or support alternate names. class StringPair { var key: String; var value: String; alias first = key; alias second = value; } var sp1: StringPair = {.key = \"K\", .value = \"1\"}; var sp2: StringPair = {.first = \"K\", .second = \"2\"}; Assert(sp1.first == sp2.key); Assert(&sp1.first == &sp1.key); Future work: This needs to be connected to the broader design of aliases, once that lands. Inheritance Carbon supports inheritance using a class hierarchy , on an opt-in basis. Classes by default are final , which means they may not be extended. To declare a class as allowing extension, use either the base class or abstract class introducer: base class MyBaseClass { ... } A base class may be extended to get a derived class : base class MiddleDerived extends MyBaseClass { ... } class FinalDerived extends MiddleDerived { ... } // \u274c Forbidden: class Illegal extends FinalDerived { ... } An abstract class or abstract base class is a base class that may not be instantiated. abstract class MyAbstractClass { ... } // \u274c Forbidden: var a: MyAbstractClass = ...; Future work: For now, the Carbon design only supports single inheritance. In the future, Carbon will support multiple inheritance with limitations on all base classes except the one listed first. Terminology: We say MiddleDerived and FinalDerived are derived classes , transitively extending or derived from MyBaseClass . Similarly FinalDerived is derived from or extends MiddleDerived . MiddleDerived is FinalDerived 's immediate base class , and both MiddleDerived and MyBaseClass are base classes of FinalDerived . Base classes that are not abstract are called extensible classes . A derived class has all the members of the class it extends, including data members and methods, though it may not be able to access them if they were declared private . Virtual methods A base class may define virtual methods . These are methods whose implementation may be overridden in a derived class. Only methods defined in the scope of the class definition may be virtual, not any defined in external interface impls . Interface methods may be implemented using virtual methods when the impl is internal , and calls to those methods by way of the interface will do virtual dispatch just like a direct call to the method does. Class functions may not be declared virtual. Virtual override keywords A method is declared as virtual by using a virtual override keyword in its declaration before fn . base class MyBaseClass { virtual fn Overridable[me: Self]() -> i32 { return 7; } } This matches C++, and makes it relatively easy for authors of derived classes to find the functions that can be overridden. If no keyword is specified, the default for methods is that they are non-virtual . This means: they can't override methods in bases of this class; they can't be overridden in derived classes; and they have an implementation in the current class, and that implementation must work for all derived classes. There are three virtual override keywords: virtual - This marks a method as not present in bases of this class and having an implementation in this class. That implementation may be overridden in derived classes. abstract - This marks a method that must be overridden in a derived class since it has no implementation in this class. This is short for \"abstract virtual\" but is called \"pure virtual\" in C++ . Only abstract classes may have unimplemented abstract methods. impl - This marks a method that overrides a method marked virtual or abstract in the base class with an implementation specific to -- and defined within -- this class. The method is still virtual and may be overridden again in subsequent derived classes if this is a base class. See method overriding in Wikipedia . Requiring a keyword when overriding allows the compiler to diagnose when the derived class accidentally uses the wrong signature or spelling and so doesn't match the base class. We intentionally use the same keyword here as for implementing interfaces, to emphasize that they are similar operations. Keyword on method in C Allowed in abstract class C Allowed in base class C Allowed in final class C in B where C extends B in D where D extends C virtual \u2705 \u2705 \u274c not present abstract impl not mentioned abstract \u2705 \u274c \u274c not present virtual abstract impl abstract impl may not be mentioned if D is not final impl \u2705 \u2705 \u2705 virtual abstract impl abstract impl Subtyping A pointer to a base class, like MyBaseClass* is actually considered to be a pointer to that type or any derived class, like MiddleDerived or FinalDerived . This means that a FinalDerived* value may be implicitly cast to type MiddleDerived* or MyBaseClass* . This is accomplished by making the data layout of a type extending MyBaseClass have MyBaseClass as a prefix. In addition, the first class in the inheritance chain with a virtual method will include a virtual pointer, or vptr , pointing to a virtual method table , or vtable . Any calls to virtual methods will perform dynamic dispatch by calling the method using the function pointer in the vtable, to get the overridden implementation from the most derived class that implements the method. Since a final class may not be extended, the compiler can bypass the vtable and use static dispatch . In general, you can use a combination of an abstract base class and a final class instead of an extensible class if you need to distinguish between exactly a type and possibly a subtype. base class Extensible { ... } // Can be replaced by: abstract class ExtensibleBase { ... } class ExactlyExtensible extends ExtensibleBase { ... } Constructors Like for classes without inheritance, constructors for a derived class are ordinary functions that return an instance of the derived class. Generally constructor functions should return the constructed value without copying, as in proposal #257: Initialization of memory and variables . This means either creating the object in the return statement itself , or in a returned var declaration . As before, instances can be created using by casting a struct value into the class type, this time with a .base member to initialize the members of the immediate base type. class MyDerivedType extends MyBaseType { fn Create() -> MyDerivedType { return {.base = MyBaseType.Create(), .derived_field = ...}; } } There are two cases that aren't well supported with this pattern: Users cannot create a value of an abstract class, which is necessary when it has private fields or otherwise requires initialization. Users may want to reduce the chance of mistakes from calling a method on a partially constructed object. Of particular concern is calling a virtual method prior to forming the derived class and so it uses the base class implementation. While expected to be relatively rarely needed, we will address both of these concerns with a specialized type just used during construction of base classes, called the partial facet type for the class. Partial facet The partial facet for a base class type like MyBaseType is written partial MyBaseType . Only methods that take the partial facet type may be called on the partial facet type, so methods have to opt in to being called on an object that isn't fully constructed. No virtual methods may take the partial facet type, so there is no way to transitively call a virtual method on an object that isn't fully constructed. partial MyBaseClass and MyBaseClass have the same fields in the same order with the same data layout. The only difference is that partial MyBaseClass doesn't use (look into) its hidden vptr slot. To reliably catch any bugs where virtual function calls occur in this state, both fast and hardened release builds will initialize the hidden vptr slot to a null pointer. Debug builds will initialize it to an alternate vtable whose functions will abort the program with a clear diagnostic. Since partial MyBaseClass has the same data layout but only uses a subset, there is a subtyping relationship between these types. A MyBaseClass value is a partial MyBaseClass value, but not the other way around. So you can cast MyBaseClass* to partial MyBaseClass* , but the other direction is not safe. When MyBaseClass may be instantiated, there is a conversion from partial MyBaseClass to MyBaseClass . It changes the value by filling in the hidden vptr slot. If MyBaseClass is abstract, then attempting that conversion is an error. partial MyBaseClass is considered final, even if MyBaseClass is not. This is despite the fact that from a data layout perspective, partial MyDerivedClass will have partial MyBaseClass as a prefix if MyDerivedClass extends MyBaseClass . The type partial MyBaseClass specifically means \"exactly this and no more.\" This means we don't need to look at the hidden vptr slot, and we can instantiate it even if it doesn't have a virtual destructor . The keyword partial may only be applied to a base class. For final classes, there is no need for a second type. Usage The general pattern is that base classes can define constructors returning the partial facet type. base class MyBaseClass { fn Create() -> partial Self { return {.base_field_1 = ..., .base_field_2 = ...}; } // ... } Extensible classes can be instantiated even from a partial facet value: var mbc: MyBaseClass = MyBaseClass.Create(); The conversion from partial MyBaseClass to MyBaseClass only fills in the vptr value and can be done in place. After the conversion, all public methods may be called, including virtual methods. The partial facet is required for abstract classes, since otherwise they may not be instantiated. Constructor functions for abstract classes should be marked protected so they may only be accessed in derived classes. abstract class MyAbstractClass { protected fn Create() -> partial Self { return {.base_field_1 = ..., .base_field_2 = ...}; } // ... } // \u274c Error: can't instantiate abstract class var abc: MyAbstractClass = ...; If a base class wants to store a pointer to itself somewhere in the constructor function, there are two choices: An extensible class could use the plain type instead of the partial facet. base class MyBaseClass { fn Create() -> Self { returned var result: Self = {...}; StoreMyPointerSomewhere(&result); return var; } } The other choice is to explicitly cast the type of its address. This pointer should not be used to call any virtual method until the object is finished being constructed, since the vptr will be null. abstract class MyAbstractClass { protected fn Create() -> partial Self { returned var result: partial Self = {...}; // Careful! Pointer to object that isn't fully constructed! StoreMyPointerSomewhere(&result as Self*); return var; } } The constructor for a derived class may construct values from a partial facet of the class' immediate base type or the full type: abstract class MyAbstractClass { protected fn Create() -> partial Self { ... } } // Base class returns a partial type base class Derived extends MyAbstractClass { protected fn Create() -> partial Self { return {.base = MyAbstractClass.Create(), .derived_field = ...}; } ... } base class MyBaseClass { fn Create() -> Self { ... } } // Base class returns a full type base class ExtensibleDerived extends MyBaseClass { fn Create() -> Self { return {.base = MyBaseClass.Create(), .derived_field = ...}; } ... } And final classes will return a type that does not use the partial facet: class FinalDerived extends MiddleDerived { fn Create() -> Self { return {.base = MiddleDerived.Create(), .derived_field = ...}; } ... } Observe that the vptr is only assigned twice in release builds if you use partial facets: The first class value created, by the factory function creating the base of the class hierarchy, initialized the vptr field to nullptr. Every derived type transitively created from that value will leave it alone. Only when the value has its most-derived class and is converted from the partial facet type to its final type is the vptr field set to its final value. In the case that the base class can be instantiated, tooling could optionally recommend that functions returning Self that are used to initialize a derived class be changed to return partial Self instead. However, the consequences of returning Self instead of partial Self when the value will be used to initialize a derived class are fairly minor: The vptr field will be assigned more than necessary. The types won't protect against calling methods on a value while it is being constructed, much like the situation in C++ currently. Assignment with inheritance Since the assignment operator method should not be virtual, it is only safe to implement it for final types. However, following the maxim that Carbon should \"focus on encouraging appropriate usage of features rather than restricting misuse\" , we allow users to also implement assignment on extensible classes, even though it can lead to slicing . Destructors Every non-abstract type is destructible , meaning has a defined destructor function called when the lifetime of a value of that type ends, such as when a variable goes out of scope. The destructor for a class may be customized using the destructor keyword: class MyClass { destructor [me: Self] { ... } } or: class MyClass { // Can modify `me` in the body. destructor [addr me: Self*] { ... } } If a class has no destructor declaration, it gets the default destructor, which is equivalent to destructor [me: Self] { } . The destructor for a class is run before the destructors of its data members. The data members are destroyed in reverse order of declaration. Derived classes are destroyed before their base classes, so the order of operations is: derived class' destructor runs, the data members of the derived class are destroyed, in reverse order of declaration, the immediate base class' destructor runs, the data members of the immediate base class are destroyed, in reverse order of declaration, and so on. Destructors may be declared in class scope and then defined out-of-line: class MyClass { destructor [addr me: Self*]; } destructor MyClass [addr me: Self*] { ... } It is illegal to delete an instance of a derived class through a pointer to one of its base classes unless it has a virtual destructor . An abstract or base class' destructor may be declared virtual using the virtual introducer, in which case any derived class destructor declaration must be impl : base class MyBaseClass { virtual destructor [addr me: Self*] { ... } } class MyDerivedClass extends MyBaseClass { impl destructor [addr me: Self*] { ... } } The properties of a type, whether type is abstract, base, or final, and whether the destructor is virtual or non-virtual, determines which type-of-types it satisfies. Non-abstract classes are Concrete . This means you can create local and member variables of this type. Concrete types have destructors that are called when the local variable goes out of scope or the containing object of the member variable is destroyed. Final classes and classes with a virtual destructor are Deletable . These may be safely deleted through a pointer. Classes that are Concrete , Deletable , or both are Destructible . These are types that may be deleted through a pointer, but it might not be safe. The concerning situation is when you have a pointer to a base class without a virtual destructor. It is unsafe to delete that pointer when it is actually pointing to a derived class. Note: The names Deletable and Destructible are placeholders since they do not conform to the decision on question-for-leads issue #1058: \"How should interfaces for core functionality be named?\" . Class Destructor Concrete Deletable Destructible abstract non-virtual no no no abstract virtual no yes yes base non-virtual yes no yes base virtual yes yes yes final any yes yes yes The compiler automatically determines which of these type-of-types a given type satisfies. It is illegal to directly implement Concrete , Deletable , or Destructible directly. For more about these constraints, see \"destructor constraints\" in the detailed generics design . A pointer to Deletable types may be passed to the Delete method of the Allocator interface . To deallocate a pointer to a base class without a virtual destructor, which may only be done when it is not actually pointing to a value with a derived type, call the UnsafeDelete method instead. Note that you may not call UnsafeDelete on abstract types without virtual destructors, it requires Destructible . interface Allocator { // ... fn Delete[T:! Deletable, addr me: Self*](p: T*); fn UnsafeDelete[T:! Destructible, addr me: Self*](p: T*); } To pass a pointer to a base class without a virtual destructor to a generic function expecting a Deletable type, use the UnsafeAllowDelete type adapter . adapter UnsafeAllowDelete(T:! Concrete) extends T { impl as Deletable {} } // Example usage: fn RequiresDeletable[T:! Deletable](p: T*); var x: MyExtensible; RequiresDeletable(&x as UnsafeAllowDelete(MyExtensible)*); If a virtual method is transitively called from inside a destructor, the implementation from the current class is used, not any overrides from derived classes. It will abort the execution of the program if that method is abstract and not implemented in the current class. Future work: Allow or require destructors to be declared as taking partial Self in order to prove no use of virtual methods. Types satisfy the TrivialDestructor type-of-type if: the class declaration does not define a destructor or the class defines the destructor with an empty body { } , all data members implement TrivialDestructor , and all base classes implement TrivialDestructor . For example, a struct type implements TrivialDestructor if all its members do. TrivialDestructor implies that their destructor does nothing, which may be used to generate optimized specializations. There is no provision for handling failure in a destructor. All operations that could potentially fail must be performed before the destructor is called. Unhandled failure during a destructor call will abort the program. Future work: Allow or require destructors to be declared as taking [var me: Self] . Alternatives considered: Types implement destructor interface Prevent virtual function calls in destructors Allow functions to act as destructors Allow private destructors Allow multiple conditional destructors Don't distinguish safe and unsafe delete operations Don't allow unsafe delete Allow final destructors Access control By default, all members of a class are fully publicly accessible. Access can be restricted by adding a keyword, called an access modifier , prior to the declaration. Access modifiers are how Carbon supports encapsulation . The access modifier is written before any virtual override keyword . Rationale: Carbon makes members public by default for a few reasons: The readability of public members is the most important, since we expect most readers to be concerned with the public API of a type. The members that are most commonly private are the data fields, which have relatively less complicated definitions that suffer less from the extra annotation. Additionally, there is precedent for this approach in modern object-oriented languages such as Kotlin and Python , both of which are well regarded for their usability. Keywords controlling visibility are attached to individual declarations instead of C++'s approach of labels controlling the visibility for all following declarations to reduce context sensitivity . This matches Rust , Swift , Java , C# , Kotlin , and D . References: Proposal #561: Basic classes included the decision that members default to publicly accessible originally asked in issue #665 . Private access As in C++, private means only accessible to members of the class and any friends . class Point { fn Distance[me: Self]() -> f32; // These are only accessible to members of `Point`. private var x: f32; private var y: f32; } A private virtual or private abstract method may be implemented in derived classes, even though it may not be called. This allows derived classes to customize the behavior of a function called by a method of the base class, while still preventing the derived class from calling it. This matches the behavior of C++ and is more orthogonal. Future work: private will give the member internal linkage unless it needs to be external because it is used in an inline method or template. We may in the future add a way to specify internal linkage explicitly . Open questions: Using private to mean \"restricted to this class\" matches C++. Other languages support restricting to different scopes: Swift supports \"restrict to this module\" and \"restrict to this file\". Rust supports \"restrict to this module and any children of this module\", as well as \"restrict to this crate\", \"restrict to parent module\", and \"restrict to a specific ancestor module\". Comparison to other languages: C++, Rust, and Swift all make class members private by default. C++ offers the struct keyword that makes members public by default. Protected access Protected members may only be accessed by members of this class, members of derived classes, and any friends . base class MyBaseClass { protected fn HelperClassFunction(x: i32) -> i32; protected fn HelperMethod[me: Self](x: i32) -> i32; protected var data: i32; } class MyDerivedClass extends MyBaseClass { fn UsesProtected[addr me: Self*]() { // Can access protected members in derived class var x: i32 = HelperClassFunction(3); me->data = me->HelperMethod(x); } } Friends Classes may have a friend declaration: class Buddy { ... } class Pal { private var x: i32; friend Buddy; } This declares Buddy to be a friend of Pal , which means that Buddy can access all members of this class, even the ones that are declared private or protected . The friend keyword is followed by the name of an existing function, type, or parameterized family of types. Unlike C++, it won't act as a forward declaration of that name. The name must be resolvable by the compiler, and so may not be a member of a template. Test friendship Future work: There should be a convenient way of allowing tests in the same library as the class definition to access private members of the class. Ideally this could be done without changing the class definition itself, since it doesn't affect the class' public API. Access control for construction A function may construct a class, by casting a struct value to the class type, if it has access to (write) all of its fields. Future work: There should be a way to limit which code can construct a class even when it only has public fields. This will be resolved in question-for-leads issue #803 . Operator overloading Developers may define how standard Carbon operators, such as + and / , apply to custom types by implementing the interface that corresponds to that operator for the types of the operands. See the \"operator overloading\" section of the generics design . The specific interface used for a given operator may be found in the expressions design . Future work This includes features that need to be designed, questions to answer, and a description of the provisional syntax in use until these decisions have been made. Struct literal shortcut We could allow you to write {x, y} as a short hand for {.x = x, .y = y} . Optional named parameters Structs are being considered as a possible mechanism for implementing optional named parameters. We have three main candidate approaches: allowing struct types to have field defaults, having dedicated support for destructuring struct values in pattern contexts, or having a dedicated optional named parameter syntax. Field defaults for struct types If struct types could have field defaults, you could write a function declaration with all of the optional parameters in an option struct: fn SortIntVector( v: Vector(i32)*, options: {.stable: Bool = false, .descending: Bool = false} = {}) { // Code using `options.stable` and `options.descending`. } // Uses defaults of `.stable` and `.descending` equal to `false`. SortIntVector(&v); SortIntVector(&v, {}); // Sets `.stable` option to `true`. SortIntVector(&v, {.stable = true}); // Sets `.descending` option to `true`. SortIntVector(&v, {.descending = true}); // Sets both `.stable` and `.descending` options to `true`. SortIntVector(&v, {.stable = true, .descending = true}); // Order can be different for arguments as well. SortIntVector(&v, {.descending = true, .stable = true}); Destructuring in pattern matching We might instead support destructuring struct patterns with defaults: fn SortIntVector( v: Vector(i32)*, {stable: Bool = false, descending: Bool = false}) { // Code using `stable` and `descending`. } This would allow the same syntax at the call site, but avoids some concerns with field defaults and allows some other use cases such as destructuring return values. Discussion We might support destructuring directly: var {key: String, value: i32} = ReturnKeyValue(); or by way of a mechanism that converts a struct into a tuple: var (key: String, value: i32) = ReturnKeyValue().extract(.key, .value); // or maybe: var (key: String, value: i32) = ReturnKeyValue()[(.key, .value)]; Similarly we might support optional named parameters directly instead of by way of struct types. Some discussion on this topic has occurred in: question-for-leads issue #505 on named parameters labeled params brainstorming docs 1 , 2 \"match\" in syntax choices doc Inheritance C++ abstract base classes interoperating with object-safe interfaces We want four things so that Carbon's object-safe interfaces may interoperate with C++ abstract base classes without data members, matching the interface as base class use case : Ability to convert an object-safe interface (a type-of-type) into an C++-compatible base class (a base type), maybe using AsBaseClass(MyInterface) . Ability to convert a C++ base class without data members (a base type) into an object-safe interface (a type-of-type), maybe using AsInterface(MyIBC) . Ability to convert a (thin) pointer to an abstract base class to a DynPtr of the corresponding interface. Ability to convert DynPtr(MyInterface) values to a proxy type that extends the corresponding base class AsBaseType(MyInterface) . Note that the proxy type extending AsBaseType(MyInterface) would be a different type than DynPtr(MyInterface) since the receiver input to the function members of the vtable for the former does not match those in the witness table for the latter. Overloaded methods We allow a derived class to define a class function with the same name as a class function in the base class. For example, we expect it to be pretty common to have a constructor function named Create at all levels of the type hierarchy. Beyond that, we may want some rules or restrictions about defining methods in a derived class with the same name as a base class method without overriding it. There are some opportunities to improve on and simplify the C++ story: We don't want to silently hide methods in the base class because of a method with the same name in a derived class. There are uses for this in C++, but it also causes problems and without multiple inheritance there isn't the same need in Carbon. Overload resolution should happen before virtual dispatch. For evolution purposes, you should be able to add private members to a base class that have the same name as member of a derived class without affecting overload resolution on instances of the derived class, in functions that aren't friends of the base class. References: This was discussed in the open discussion on 2021-07-12 . Interop with C++ inheritance This design directly supports Carbon classes inheriting from a single C++ class. class CarbonClass extends C++.CPlusPlusClass { fn Create() -> Self { return {.base = C++.CPlusPlusClass(...), .other_fields = ...}; } ... } To allow C++ classes to extend Carbon classes, there needs to be some way for C++ constructors to initialize their base class: There could be some way to export a Carbon class that identifies which factory functions may be used as constructors. We could explicitly call the Carbon factory function, as in: `` // Base` is a Carbon class which gets converted to a // C++ class for interop purposes: class Base { public: virtual ~Base() {} static auto Create() -> Base; }; // In C++ class Derived : public Base { public: virtual ~Derived() override {} // This isn't currently a case where C++ guarantees no copy, // and so it currently still requires a notional copy and // there appear to be implementation challenges with // removing them. This may require an extension to make work // reliably without an extraneous copy of the base subobject. Derived() : Base(Base::Create()) {} }; ``` However, this doesn't work in the case where Base can't be instantiated, or Base does not have a copy constructor, even though it shouldn't be called due to RVO. Virtual base classes TODO: Ask zygoloid to fill this in. Carbon won't support declaring virtual base classes, and the C++ interop use cases Carbon needs to support are limited. This will allow us to simplify the C++ interop by allowing Carbon to delegate initialization of virtual base classes to the C++ side. This requires that we enforce two rules: No multiple inheritance of C++ classes with virtual bases No C++ class extending a Carbon class that extends a C++ class with a virtual base Mixins We will need some way to declare mixins. This syntax will need a way to distinguish defining versus requiring member variables. Methods may additionally be given a default definition but may be overridden. Interface implementations may only be partially provided by a mixin. Mixin methods will need to be able to convert between pointers to the mixin type and the main type. Open questions include whether a mixin is its own type that is a member of the containing type, and whether mixins are templated on the containing type. Mixins also complicate how constructors work. Memory layout Carbon will need some way for users to specify the memory layout of class types beyond simple ordering of fields, such as controlling the packing and alignment for the whole type or individual members. We may allow members of a derived class like to put data members in the final padding of its base class prefix. Tail-padding reuse has both advantages and disadvantages, so we may have some way for a class to explicitly mark that its tail padding is available for use by a derived class, Advantages: Tail-padding reuse is sometimes a nice layout optimization (eg, in Clang we save 8 bytes per Expr by reusing tail padding). No class size regressions when migrating from C++. Special case of reusing the tail padding of a class that is empty other than its tail padding is very important, to the extent that we will likely need to support either zero-sized types or tail-padding reuse in order to have acceptable class layouts. Disadvantages: Cannot use memcpy(p, q, sizeof(Base)) to copy around base class subobjects if the destination is an in-lifetime, because they might overlap other objects' representations. Somewhat more complex model. We need some mechanism for disabling tail-padding reuse in \"standard layout\" types. We may also have to use narrowed loads for the last member of a base class to avoid accidentally creating a race condition. However, we can still use memcpy and memset to initialize a base class subobject, even if its tail padding might be reused, so long as we guarantee that no other object lives in the tail padding and is initialized before the base class. In C++, that happens only due to virtual base classes getting initialized early and laid out at the end of the object; if we disallow virtual base classes then we can guarantee that initialization order is address order, removing most of the downside of tail-padding reuse. No static variables At the moment, there is no proposal to support static member variables , in line with avoiding global variables more generally. Carbon may need some support in this area, though, for parity with and migration from C++. Computed properties Carbon might want to support members of a type that are accessed like a data member but return a computed value like a function. This has a number of implications: It would be a way of publicly exposing data members for encapsulated types , allowing for rules that otherwise forbid mixing public and private data members. It would provide a more graceful evolution path from a data class to an encapsulated type . It would give an option to start with a data class instead of writing all the boilerplate to create an encapsulated type preemptively to allow future evolution. It would let you take a variable away and put a property in its place with no other code changes. The number one use for this is so you can put a breakpoint in the property code, then later go back to public variable once you understand who was misbehaving. We should have some guidance for when to use a computed property instead of a function with no arguments. One possible criteria is when it is a pure function of the state of the object and executes in an amount of time similar to ordinary member access. However, there are likely to be differences between computed properties and other data members, such as the ability to take the address of them. We might want to support \"read only\" data members, that can be read through the public api but only modified with private access, for data members which may need to evolve into a computed property. There are also questions regarding how to support assigning or modifying computed properties, such as using += . Interfaces implemented for data classes We should define a way for defining implementations of interfaces for struct types. To satisfy coherence, these implementations would have to be defined in the library with the interface definition. The syntax might look like: interface ConstructWidgetFrom { fn Construct(Self) -> Widget; } external impl {.kind: WidgetKind, .size: i32} as ConstructWidgetFrom { ... } In addition, we should define a way for interfaces to define templated blanket implementations for data classes more generally. These implementations will typically subject to the criteria that all the data fields of the type must implement the interface. An example use case would be to say that a data class is serializable if all of its fields were. For this we will need a type-of-type for capturing that criteria, maybe something like DataFieldsImplement(MyInterface) . The templated implementation will need some way of iterating through the fields so it can perform operations fieldwise. This feature should also implement the interfaces for any tuples whose fields satisfy the criteria. It is an open question how to define implementations for binary operators. For example, if i32 is comparable to f64 , then {.x = 3, .y = 2.72} should be comparable to {.x = 3.14, .y = 2} . The trick is how to declare the criteria that \" T is comparable to U if they have the same field names in the same order, and for every field x , the type of T.x implements ComparableTo for the type of U.x .\" References #257: Initialization of memory and variables #561: Basic classes: use cases, struct literals, struct types, and future wor #722: Nominal classes and methods #777: Inheritance #981: Implicit conversions for aggregates #1154: Destructors","title":"Classes"},{"location":"design/classes/#classes","text":"","title":"Classes"},{"location":"design/classes/#table-of-contents","text":"Overview Use cases Data classes Encapsulated types Without inheritance With inheritance and subtyping Polymorphic types Interface as base class Non-polymorphic inheritance Interop with C++ multiple inheritance Mixins Background Members Data members have an order Struct types Literals Type expression Assignment and initialization Operations performed field-wise Nominal class types Forward declaration Self Construction Assignment Member functions Class functions Methods Name lookup in member function definitions Nominal data classes Member type Let Alias Inheritance Virtual methods Virtual override keywords Subtyping Constructors Partial facet Usage Assignment with inheritance Destructors Access control Private access Protected access Friends Test friendship Access control for construction Operator overloading Future work Struct literal shortcut Optional named parameters Field defaults for struct types Destructuring in pattern matching Discussion Inheritance C++ abstract base classes interoperating with object-safe interfaces Overloaded methods Interop with C++ inheritance Virtual base classes Mixins Memory layout No static variables Computed properties Interfaces implemented for data classes References","title":"Table of contents"},{"location":"design/classes/#overview","text":"A Carbon class is a user-defined record type . A class has members that are referenced by their names, in contrast to a Carbon tuple which defines a product type whose members are referenced positionally. Classes are the primary mechanism for users to extend the Carbon type system and are deeply rooted in C++ and its history (C and Simula). We call them classes rather than other terms as that is both familiar to existing programmers and accurately captures their essence: they define the types of objects with (optional) support for methods, encapsulation, and so on. Carbon supports both named, or \"nominal\", and unnamed, anonymous, or \"structural\", class types. Nominal class types are all distinct, but structural types are equal if they have the same sequence of member types and names. Structural class literals may be used to initialize or assign values to nominal class variables. A class type defines the interpretation of the bytes of a value of that type, including the size, data members, and layout. It defines the operations that may be performed on those values, including what methods may be called. A class type may directly have constant members. The type itself is a compile-time immutable constant value.","title":"Overview"},{"location":"design/classes/#use-cases","text":"The use cases for classes include both cases motivated by C++ interop, and cases that we expect to be included in idiomatic Carbon-only code. This design currently only attempts to address the \"data classes\" and \"encapsulated types\" use cases. Addressing the \"interface as base class\", \"interop with C++ multiple inheritance\" and \"mixin\" use cases is future work.","title":"Use cases"},{"location":"design/classes/#data-classes","text":"Data classes are types that consist of data fields that are publicly accessible and directly read and manipulated by client code. They have few if any methods, and generally are not involved in inheritance at all. Examples include: a key and value pair returned from a SortedMap or HashMap a 2D point that might be used in a rendering API Properties: Operations like copy, move, destroy, unformed, and so on are defined field-wise. Anonymous classes types and literals should match data class semantics. Expected in idiomatic Carbon-only code. Background: Kotlin has a dedicated concise syntax for defining data classes that avoids boilerplate. Python has a data class library , proposed in PEP 557 , that fills a similar role.","title":"Data classes"},{"location":"design/classes/#encapsulated-types","text":"There are several categories of types that support encapsulation . This is done by making their data fields private so access and modification of values are all done through methods defined on the type.","title":"Encapsulated types"},{"location":"design/classes/#without-inheritance","text":"The common case for encapsulated types are those that do not participate in inheritance. These types neither support being inherited from (they are \"final\" ) nor do they extend other types. Examples of this use case include: strings, containers, iterators types with invariants such as Date RAII types that are movable but not copyable like C++'s std::unique_ptr or a file handle non-movable types like Mutex We expect two kinds of methods on these types: public methods defining the API for accessing and manipulating values of the type, and private helper methods used as an implementation detail of the public methods. These types are expected in idiomatic Carbon-only code.","title":"Without inheritance"},{"location":"design/classes/#with-inheritance-and-subtyping","text":"The subtyping you get with inheritance is that you may assign the address of an object of a derived type to a pointer to its base type. For this to work, the compiler needs implementation strategies that allow operations performed through the pointer to the base type work independent of which derived type it actually points to. These strategies include: Arranging for the the data layout of derived types to start with the data layout of the base type as a prefix. Putting a pointer to a table of function pointers, a vtable , as the first data member of the object. This allows methods to be virtual and have a derived-type-specific implementation, an override , that is used even when invoking the method on a pointer to a base type. Non-virtual methods implemented on a base type should be applicable to all derived types. In general, derived types should not attempt to overload or override non-virtual names defined in the base type. Note that these subtyping implementation strategies generally rely on encapsulation, but encapsulation is not a strict requirement in all cases. This subtyping relationship also creates safety concerns, which Carbon should protect against. Slicing problems can arise when the source or target of an assignment is a dereferenced pointer to the base type. It is also incorrect to delete an object with a non-virtual destructor through a pointer to a base type.","title":"With inheritance and subtyping"},{"location":"design/classes/#polymorphic-types","text":"Carbon will fully support single-inheritance type hierarchies with polymorphic types. Polymorphic types support dynamic dispatch using a vtable , and data members, but only single inheritance. Individual methods opt in to using dynamic dispatch, so types will have a mix of \"virtual\" and non-virtual methods. Polymorphic types support traditional object-oriented single inheritance , a mix of subtyping and implementation and code reuse . We exclude complex multiple inheritance schemes, virtual inheritance, and so on from this use case. This is to avoid the complexity and overhead they bring, particularly since the use of these features in C++ is generally discouraged. The rule is that every type has at most one base type with data members for subtyping purposes. Carbon will support additional base types as long as they don't have data members or don't support subtyping . Background: The \"Nothing is Something\" talk by Sandi Metz and the Composition Over Inheritance Principle describe design patterns to use instead of multiple inheritance to support types that vary over multiple axes. In rare cases where the complex multiple inheritance schemes of C++ are truly needed, they can be effectively approximated using a combination of these simpler building blocks. Polymorphic types support a number of different kinds of methods: They will have virtual methods: Polymorphic types will typically include virtual destructors. The virtual methods types may have default implementations or be abstract (or pure virtual ). In the latter case, they must be implemented in any derived class that can be instantiated. Virtual methods may be protected or private , intended to be called by methods in the base type but implemented in the descendant. They may have non-virtual public or private helper methods, like encapsulated types without inheritance . These avoid the overhead of a virtual function call, and can be written when the base class has sufficient data members. They may have protected helper methods, typically non-virtual, provided by the base type to be called by the descendant. Note that there are two uses for protected methods: those implemented in the base and called in the descendant, and the other way around. \"The End Of Object Inheritance & The Beginning Of A New Modularity\" talk by Augie Fackler and Nathaniel Manista discusses design patterns that split up types to reduce the number of kinds of calls between base and derived types, and make sure calls only go in one direction. We expect polymorphic types in idiomatic Carbon-only code, at least for the medium term. Extending this design to support polymorphic types is future work.","title":"Polymorphic types"},{"location":"design/classes/#interface-as-base-class","text":"We distinguish the specific case of polymorphic base classes that have no data members: From an implementation perspective, the lack of data members removes most of the problems with supporting multiple inheritance. They are about decoupling two pieces of code instead of collaborating. As a use case, they are used primarily for subtyping and much less implementation reuse than other polymorphic types. This case overlaps with the interface concept introduced for Carbon generics . Removing support for data fields greatly simplifies supporting multiple inheritance. For example, it removes the need for a mechanism to figure out the offset of those data fields in the object. Similarly we don't need C++'s virtual inheritance to avoid duplicating those fields. Some complexities still remain, such as pointers changing values when casting to a secondary parent type, but these seem manageable given the benefits of supporting this useful case of multiple inheritance. While an interface base class is generally for providing an API that allows decoupling two pieces of code, a polymorphic type is a collaboration between a base and derived type to provide some functionality. This is a bit like the difference between a library and a framework, where you might use many of the former but only one of the latter. Interface base classes are primarily used for subtyping. The extent of implementation reuse is generally limited by the lack of data members, and the decoupling role they play is usually about defining an API as a set of public pure-virtual methods. Compared to other polymorphic types, they more rarely have methods with implementations (virtual or not), or have methods with restricted access. The main use case is when there is a method that is implemented in terms of pure-virtual methods. Those pure-virtual methods may be marked as protected to ensure they are only called through the non-abstract API, but can still be implemented in descendants. While it is typical for this case to be associated with single-level inheritance hierarchies, there are some cases where there is an interface at the root of a type hierarchy and polymorphic types as interior branches of the tree. The case of generic interfaces extending or requiring other interface would also be modeled by deeper inheritance hierarchies. An interface as base class needs to either have a virtual destructor or forbid deallocation. There is significant overlap between interface base classes and Carbon interfaces . Both represent APIs as a collection of method names and signatures to implement. The subset of interfaces that support dynamic dispatch are called object-safe , following Rust : They don't have a Self in the signature of a method in a contravariant position like a parameter. They don't have free associated types or other associated items used in a method signature. The restrictions on object-safe interfaces match the restrictions on base class methods. The main difference is the representation in memory. A type extending a base class with virtual methods includes a pointer to the table of methods in the object value itself, while a type implementing an interface would store the pointer alongside the pointer to the value in a DynPtr(MyInterface) . Of course, the interface option also allows the method table to be passed at compile time. Note: This presumes that we include some concept of final methods in interfaces to match non-virtual functions in base classes. We expect idiomatic Carbon-only code to generally use Carbon interfaces instead of interface base classes. We may still support interface base classes long term if we determine that the ability to put the pointer to the method implementations in the object value is important for users, particularly with a single parent as in the polymorphic type case . Extending this design to support interface base classes is future work. Background: C++ abstract base classes that don't have data members and Java interfaces model this case.","title":"Interface as base class"},{"location":"design/classes/#non-polymorphic-inheritance","text":"While it is not common, there are cases where C++ code uses inheritance without dynamic dispatch or a vtable . Instead, methods are never overridden, and derived types only add data and methods. There are some cases where this is done in C++ but would be done differently in Carbon: For implementation reuse without subtyping, Carbon code should use mixins or composition. Carbon won't support private inheritance. Carbon will allow data members to have size zero, so the empty-base optimization is unnecessary. For cases where the derived type does not add any data members, in Carbon you can potentially use adapter types instead of inheritance. However, there are still some cases where non-virtual inheritance makes sense. One is a parameterized type where a prefix of the data is the same independent of the parameter. An example of this is containers with a small-buffer optimization , as described in the talk CppCon 2016: Chandler Carruth \"High Performance Code 201: Hybrid Data Structures\" . By moving the data and methods that don't depend on the buffer size to a base class, we reduce the instantiation overhead for monomorphization. The base type is also useful for reducing instantiation for consumers of the container, as long as they only need to access methods defined in the base. Another case for non-virtual inheritance is for different node types within a data structure that have some data members in common. This is done in LLVM's map, red-black tree , and list data structure types. In a linked list, the base type might have the next and previous pointers, which is enough for a sentinel node, and there would also be a derived type with the actual data member. The base type can define operations like \"splice\" that only operate on the pointers not the data, and this is in fact enforced by the type system. Only the derived node type needs to be parameterized by the element type, saving on instantiation costs as before. Many of the concerns around non-polymorphic inheritance are the same as for the non-virtual methods of polymorphic types . Assignment and destruction are examples of operations that need particular care to be sure they are only done on values of the correct type, rather than through a subtyping relationship. This means having some extrinsic way of knowing when it is safe to downcast before performing one of those operations, or performing them on pointers that were never upcast to the base type.","title":"Non-polymorphic inheritance"},{"location":"design/classes/#interop-with-c-multiple-inheritance","text":"While Carbon won't support all the C++ forms of multiple inheritance, Carbon code will still need to interoperate with C++ code that does. Of particular concern are the std::iostream family of types. Most uses of those types are the input and output variations or could be migrated to use those variations, not the harder bidirectional cases. Much of the complexity of this interoperation could be alleviated by adopting the restriction that Carbon code can't directly access the fields of a virtual base class. In the cases where such access is needed, the workaround is to access them through C++ functions. We do not expect idiomatic Carbon-only code to use multiple inheritance. Extending this design to support interoperating with C++ types using multiple inheritance is future work.","title":"Interop with C++ multiple inheritance"},{"location":"design/classes/#mixins","text":"A mixin is a declaration of data, methods, and interface implementations that can be added to another type, called the \"main type\". The methods of a mixin may also use data, methods, and interface implementations provided by the main type. Mixins are designed around implementation reuse rather than subtyping, and so don't need to use a vtable. A mixin might be an implementation detail of a data class , or encapsulated type . A mixin might partially implement an interface as base class . Examples: intrusive linked list , intrusive reference count In both of these examples, the mixin needs the ability to convert between a pointer to the mixin's data (like a \"next\" pointer or reference count) and a pointer to the containing object with the main type. Mixins are expected in idiomatic Carbon-only code. Extending this design to support mixins is future work. Background: Mixins are typically implemented using the curiously recurring template pattern in C++, but other languages support them directly. In Dart, the mixin defines an interface that the destination type ends up implementing, which restores a form of subtyping. See Dart: What are mixins? . Swift is considering a proposal to add mixin support .","title":"Mixins"},{"location":"design/classes/#background","text":"See how other languages tackle this problem: Swift has two different concepts: classes support inheritance and use reference counting while structs have value semantics may have constructor functions called \"initializers\" and destructors called \"deinitializers\" supports properties , including computed & lazy properties methods are const by default unless marked mutating supports extensions has per-field access control Rust has no support for inheritance has no special constructor functions, instead has literal syntax has some convenience syntax for common cases: variable and field names matching , updating a subset of fields can have unnamed fields supports structs with size 0 Zig explicitly mark structs as packed to manually control layout has a struct literal syntax, including for anonymous structs no special constructor functions supports fields with undefined values supports structs with size 0 supports generics by way of memoized compile time functions accepting and returning types supports default field values has no properties or operator overloading -- Zig does not like hidden control flow","title":"Background"},{"location":"design/classes/#members","text":"The members of a class are named, and are accessed with the . notation. For example: var p: Point2D = ...; // Data member access p.x = 1; p.y = 2; // Method call Print(p.DistanceFromOrigin()); Tuples are used for cases where accessing the members positionally is more appropriate.","title":"Members"},{"location":"design/classes/#data-members-have-an-order","text":"The data members of a class, or fields , have an order that matches the order they are declared in. This determines the order of those fields in memory, and the order that the fields are destroyed when a value goes out of scope or is deallocated.","title":"Data members have an order"},{"location":"design/classes/#struct-types","text":"Structural data classes , or struct types , are convenient for defining data classes in an ad-hoc manner. They would commonly be used: as the return type of a function that returns multiple values and wants those values to have names so a tuple is inappropriate as an initializer for other class variables or values as a type parameter to a container Note that struct types are examples of data class types and are still classes. The \"nominal data classes\" section describes another way to define a data class type. Also note that there is no struct keyword, \"struct\" is just convenient shorthand terminology for a structural data class.","title":"Struct types"},{"location":"design/classes/#literals","text":"Structural data class literals , or struct literals , are written using this syntax: var kvpair: auto = {.key = \"the\", .value = 27}; This produces a struct value with two fields: The first field is named \" key \" and has the value \"the\" . The type of the field is set to the type of the value, and so is String . The second field is named \" value \" and has the value 27 . The type of the field is set to the type of the value, and so is i32 . Note: A comma , may optionally be included after the last field: var kvpair: auto = {.key = \"the\", .value = 27,}; Open question: To keep the literal syntax from being ambiguous with compound statements, Carbon will adopt some combination of: looking ahead after a { to see if it is followed by .name ; not allowing a struct literal at the beginning of a statement; only allowing { to introduce a compound statement in contexts introduced by a keyword where they are required, like requiring { ... } around the cases of an if...else statement.","title":"Literals"},{"location":"design/classes/#type-expression","text":"The type of kvpair in the last example would be represented by this expression: {.key: String, .value: i32} This syntax is intended to parallel the literal syntax, and so uses commas ( , ) to separate fields instead of a semicolon ( ; ) terminator. This choice also reflects the expected use inline in function signature declarations. Struct types may only have data members, so the type declaration is just a list of field names and types. The result of a struct type expression is an immutable compile-time type value. Note: Like with struct literal expressions, a comma , may optionally be included after the last field: {.key: String, .value: i32,} Also note that {} represents both the empty struct literal and its type.","title":"Type expression"},{"location":"design/classes/#assignment-and-initialization","text":"When initializing or assigning a variable with a data class such as a struct type to a struct value on the right hand side, the order of the fields does not have to match, just the names. var different_order: {.x: i32, .y: i32} = {.y = 2, .x = 3}; Assert(different_order.x == 3); Assert(different_order.y == 2); Initialization and assignment occur field-by-field. The order of fields is determined from the target on the left side of the = . This rule matches what we expect for classes with encapsulation more generally. Open question: What operations and in what order happen for assignment and initialization? Is assignment just destruction followed by initialization? Is that destruction completed for the whole object before initializing, or is it interleaved field-by-field? When initializing to a literal value, is a temporary containing the literal value constructed first or are the fields initialized directly? The latter approach supports types that can't be moved or copied, such as mutex. Perhaps some operations are not ordered with respect to each other?","title":"Assignment and initialization"},{"location":"design/classes/#operations-performed-field-wise","text":"Generally speaking, the operations that are available on a data class value, such as a value with a struct type, are dependent on those operations being available for all the types of the fields. For example, two values of the same data class type may be compared for equality or inequality if equality is supported for every member of the type: var p: auto = {.x = 2, .y = 3}; Assert(p == {.x = 2, .y = 3}); Assert(p != {.x = 2, .y = 4}); Assert({.x = 2, .y = 4} != {.x = 5, .y = 3}); Equality and inequality comparisons are also allowed between different data class types when: At least one is a struct type. They have the same set of field names, though the order may be different. Equality comparison is defined between the pairs of member types with the same field names. For example, since comparison between i32 and u32 is defined , equality comparison between values of types {.x: i32, .y: i32} and {.y: u32, .x: u32} is as well. Equality and inequality comparisons compare fields using the field order of the left-hand operand and stop once the outcome of the comparison is determined. However, the comparison order and short-circuiting are generally expected to affect only the performance characteristics of the comparison and not its meaning. Ordering comparisons, such as < and <= , use the order of the fields to do a lexicographical comparison . The argument types must have a matching order of the field names. Otherwise, the restrictions on ordering comparisons between different data class types are analogous to equality comparisons: At least one is a struct type. Ordering comparison is defined between the pairs of member types with the same field names. Implicit conversion from a struct type to a data class type is allowed when the set of field names is the same and implicit conversion is defined between the pairs of member types with the same field names. So calling a function effectively performs an assignment from each of the caller's arguments to the function's parameters, and will be valid when those assignments are all valid. A data class has an unformed state if all its members do. Treatment of unformed state follows proposal #257 . Destruction is performed field-wise in reverse order. Extending user-defined operations on the fields to an operation on an entire data class is future work . References: The rules for assignment, comparison, and implicit conversion for argument passing were decided in question-for-leads issue #710 .","title":"Operations performed field-wise"},{"location":"design/classes/#nominal-class-types","text":"The declarations for nominal class types will have: an optional abstract or base prefix class introducer the name of the class an optional extends followed by the name of the immediate base class { , an open curly brace a sequence of declarations } , a close curly brace Declarations should generally match declarations that can be declared in other contexts, for example variable declarations with var will define instance variables : class TextLabel { var x: i32; var y: i32; var text: String = \"default\"; } The main difference here is that \"default\" is a default instead of an initializer, and will be ignored if another value is supplied for that field when constructing a value. Defaults must be constants whose value can be determined at compile time.","title":"Nominal class types"},{"location":"design/classes/#forward-declaration","text":"To support circular references between class types, we allow forward declaration of types. Forward declarations end with semicolon ; after the name of the class, instead of any extends clause and the block of declarations in curly braces { ... } . A type that is forward declared is considered incomplete until the end of a definition with the same name. // Forward declaration of `GraphNode`. class GraphNode; class GraphEdge { var head: GraphNode*; var tail: GraphNode*; } class GraphNode { var edges: Vector(GraphEdge*); } // `GraphNode` is first complete here. Open question: What is specifically allowed and forbidden with an incomplete type has not yet been decided.","title":"Forward declaration"},{"location":"design/classes/#self","text":"A class definition may provisionally include references to its own name in limited ways. These limitations arise from the type not being complete until the end of its definition is reached. class IntListNode { var data: i32; var next: IntListNode*; } An equivalent definition of IntListNode , since Self is an alias for the current type, is: class IntListNode { var data: i32; var next: Self*; } Self refers to the innermost type declaration: class IntList { class IntListNode { var data: i32; var next: Self*; } var first: IntListNode*; }","title":"Self"},{"location":"design/classes/#construction","text":"Any function with access to all the data fields of a class can construct one by converting a struct value to the class type: var tl1: TextLabel = {.x = 1, .y = 2}; var tl2: auto = {.x = 1, .y = 2} as TextLabel; Assert(tl1.x == tl2.x); fn ReturnsATextLabel() -> TextLabel { return {.x = 1, .y = 2}; } var tl3: TextLabel = ReturnsATextLabel(); fn AcceptsATextLabel(tl: TextLabel) -> i32 { return tl.x + tl.y; } Assert(AcceptsATextLabel({.x = 2, .y = 4}) == 6); Note that a nominal class, unlike a struct type , can define default values for fields, and so may be initialized with a struct value that omits some or all of those fields.","title":"Construction"},{"location":"design/classes/#assignment","text":"Assignment to a struct value is also allowed in a function with access to all the data fields of a class. Assignment always overwrites all of the field members. var tl: TextLabel = {.x = 1, .y = 2}; Assert(tl.text == \"default\"); // \u2705 Allowed: assigns all fields tl = {.x = 3, .y = 4, .text = \"new\"}; // \u2705 Allowed: This statement is evaluated in two steps: // 1. {.x = 5, .y = 6} is converted into a new TextLabel value, // using default for field `text`. // 2. tl is assigned to a TextLabel, which has values for all // fields. tl = {.x = 5, .y = 6}; Assert(tl.text == \"default\"); Open question: This behavior might be surprising because there is an ambiguity about whether to use the default value or the previous value for a field. We could require all fields to be specified when assigning, and only use field defaults when initializing a new value. // \u274c Forbidden: should tl.text == \"default\" or \"new\"? tl = {.x = 5, .y = 6};","title":"Assignment"},{"location":"design/classes/#member-functions","text":"Member functions can either be class functions or methods. Class functions are members of the type, while methods can only be called on instances.","title":"Member functions"},{"location":"design/classes/#class-functions","text":"A class function is like a C++ static member function or method , and is declared like a function at file scope. The declaration can include a definition of the function body, or that definition can be provided out of line after the class definition is finished. A common use is for constructor functions. class Point { fn Origin() -> Self { return {.x = 0, .y = 0}; } fn CreateCentered() -> Self; var x: i32; var y: i32; } fn Point.CreateCentered() -> Self { return {.x = ScreenWidth() / 2, .y = ScreenHeight() / 2}; } Class functions are members of the type, and may be accessed as using dot . member access either the type or any instance. var p1: Point = Point.Origin(); var p2: Point = p1.CreateCentered();","title":"Class functions"},{"location":"design/classes/#methods","text":"Method declarations are distinguished from class function declarations by having a me parameter in square brackets [ ... ] before the explicit parameter list in parens ( ... ) . There is no implicit member access in methods, so inside the method body members are accessed through the me parameter. Methods may be written lexically inline or after the class declaration. class Circle { fn Diameter[me: Self]() -> f32 { return me.radius * 2; } fn Expand[addr me: Self*](distance: f32); var center: Point; var radius: f32; } fn Circle.Expand[addr me: Self*](distance: f32) { me->radius += distance; } var c: Circle = {.center = Point.Origin(), .radius = 1.5 }; Assert(Math.Abs(c.Diameter() - 3.0) < 0.001); c.Expand(0.5); Assert(Math.Abs(c.Diameter() - 4.0) < 0.001); Methods are called using using the dot . member syntax, c.Diameter() and c.Expand( ... ) . Diameter computes and returns the diameter of the circle without modifying the Circle instance. This is signified using [me: Self] in the method declaration. c.Expand( ... ) does modify the value of c . This is signified using [addr me: Self*] in the method declaration. The pattern ' addr patt ' means \"first take the address of the argument, which must be an l-value , and then match pattern patt against it\". If the method declaration also includes deduced generic parameters , the me parameter must be in the same list in square brackets [ ... ] . The me parameter may appear in any position in that list, as long as it appears after any names needed to describe its type.","title":"Methods"},{"location":"design/classes/#name-lookup-in-member-function-definitions","text":"When defining a member function lexically inline, we delay type checking of the function body until the definition of the current type is complete. This means that name lookup for members of objects is also delayed. That means that you can reference me.F() in a lexically inline method definition even before the declaration of F in that class definition. However, other names still need to be declared before they are used. This includes unqualified names, names within namespaces, and names for members of types . class Point { fn Distance[me: Self]() -> f32 { // \u2705 Allowed: `x` and `y` are names for members of an object, // and so lookup is delayed until `type_of(me) == Self` is complete. return Math.Sqrt(me.x * me.x + me.y * me.y); } fn CreatePolarInvalid(r: f32, theta: f32) -> Point { // \u274c Forbidden: unqualified name used before declaration. return Create(r * Math.Cos(theta), r * Math.Sin(theta)); } fn CreatePolarValid1(r: f32, theta: f32) -> Point { // \u274c Forbidden: `Create` is not yet declared. return Point.Create(r * Math.Cos(theta), r * Math.Sin(theta)); } fn CreatePolarValid2(r: f32, theta: f32) -> Point { // \u274c Forbidden: `Create` is not yet declared. return Self.Create(r * Math.Cos(theta), r * Math.Sin(theta)); } fn Create(x: f32, y: f32) -> Point { // \u2705 Allowed: checking that conversion of `{.x: f32, .y: f32}` // to `Point` is delayed until `Point` is complete. return {.x = x, .y = y}; } fn CreateXEqualsY(xy: f32) -> Point { // \u2705 Allowed: `Create` is declared earlier. return Create(xy, xy); } fn CreateXAxis(x: f32) -> Point; fn Angle[me: Self]() -> f32; var x: f32; var y: f32; } fn Point.CreateXAxis(x: f32) -> Point; // \u2705 Allowed: `Point` type is complete. // Members of `Point` like `Create` are in scope. return Create(x, 0); } fn Point.Angle[me: Self]() -> f32 { // \u2705 Allowed: `Point` type is complete. // Function is checked immediately. return Math.ATan2(me.y, me.x); } Note: The details of name lookup are still being decided in issue #472: Open question: Calling functions defined later in the same file .","title":"Name lookup in member function definitions"},{"location":"design/classes/#nominal-data-classes","text":"We will mark data classes with an impl as Data {} line. class TextLabel { var x: i32; var y: i32; var text: String; // This line makes `TextLabel` a data class, which defines // a number of operations field-wise. impl as Data {} } The fields of data classes must all be public. That line will add field-wise implementations and operations of all interfaces that a struct with the same fields would get by default . The word Data here refers to an empty interface in the Carbon prologue. That interface would then be part of our strategy for defining how other interfaces are implemented for data classes . References: Rationale for this approach is given in proposal #722 .","title":"Nominal data classes"},{"location":"design/classes/#member-type","text":"Additional types may be defined in the scope of a class definition. class StringCounts { class Node { var key: String; var count: i32; } var counts: Vector(Node); } The inner type is a member of the type, and is given the name StringCounts.Node . This case is called a member class since the type is a class, but other kinds of type declarations, like choice types, are allowed.","title":"Member type"},{"location":"design/classes/#let","text":"Other type constants can be defined using a let declaration: class MyClass { let Pi:! f32 = 3.141592653589793; let IndexType:! Type = i32; } The :! indicates that this is defining a compile-time constant, and so does not affect the storage of instances of that class.","title":"Let"},{"location":"design/classes/#alias","text":"You may declare aliases of the names of class members. This is to allow them to be renamed in multiple steps or support alternate names. class StringPair { var key: String; var value: String; alias first = key; alias second = value; } var sp1: StringPair = {.key = \"K\", .value = \"1\"}; var sp2: StringPair = {.first = \"K\", .second = \"2\"}; Assert(sp1.first == sp2.key); Assert(&sp1.first == &sp1.key); Future work: This needs to be connected to the broader design of aliases, once that lands.","title":"Alias"},{"location":"design/classes/#inheritance","text":"Carbon supports inheritance using a class hierarchy , on an opt-in basis. Classes by default are final , which means they may not be extended. To declare a class as allowing extension, use either the base class or abstract class introducer: base class MyBaseClass { ... } A base class may be extended to get a derived class : base class MiddleDerived extends MyBaseClass { ... } class FinalDerived extends MiddleDerived { ... } // \u274c Forbidden: class Illegal extends FinalDerived { ... } An abstract class or abstract base class is a base class that may not be instantiated. abstract class MyAbstractClass { ... } // \u274c Forbidden: var a: MyAbstractClass = ...; Future work: For now, the Carbon design only supports single inheritance. In the future, Carbon will support multiple inheritance with limitations on all base classes except the one listed first. Terminology: We say MiddleDerived and FinalDerived are derived classes , transitively extending or derived from MyBaseClass . Similarly FinalDerived is derived from or extends MiddleDerived . MiddleDerived is FinalDerived 's immediate base class , and both MiddleDerived and MyBaseClass are base classes of FinalDerived . Base classes that are not abstract are called extensible classes . A derived class has all the members of the class it extends, including data members and methods, though it may not be able to access them if they were declared private .","title":"Inheritance"},{"location":"design/classes/#virtual-methods","text":"A base class may define virtual methods . These are methods whose implementation may be overridden in a derived class. Only methods defined in the scope of the class definition may be virtual, not any defined in external interface impls . Interface methods may be implemented using virtual methods when the impl is internal , and calls to those methods by way of the interface will do virtual dispatch just like a direct call to the method does. Class functions may not be declared virtual.","title":"Virtual methods"},{"location":"design/classes/#virtual-override-keywords","text":"A method is declared as virtual by using a virtual override keyword in its declaration before fn . base class MyBaseClass { virtual fn Overridable[me: Self]() -> i32 { return 7; } } This matches C++, and makes it relatively easy for authors of derived classes to find the functions that can be overridden. If no keyword is specified, the default for methods is that they are non-virtual . This means: they can't override methods in bases of this class; they can't be overridden in derived classes; and they have an implementation in the current class, and that implementation must work for all derived classes. There are three virtual override keywords: virtual - This marks a method as not present in bases of this class and having an implementation in this class. That implementation may be overridden in derived classes. abstract - This marks a method that must be overridden in a derived class since it has no implementation in this class. This is short for \"abstract virtual\" but is called \"pure virtual\" in C++ . Only abstract classes may have unimplemented abstract methods. impl - This marks a method that overrides a method marked virtual or abstract in the base class with an implementation specific to -- and defined within -- this class. The method is still virtual and may be overridden again in subsequent derived classes if this is a base class. See method overriding in Wikipedia . Requiring a keyword when overriding allows the compiler to diagnose when the derived class accidentally uses the wrong signature or spelling and so doesn't match the base class. We intentionally use the same keyword here as for implementing interfaces, to emphasize that they are similar operations. Keyword on method in C Allowed in abstract class C Allowed in base class C Allowed in final class C in B where C extends B in D where D extends C virtual \u2705 \u2705 \u274c not present abstract impl not mentioned abstract \u2705 \u274c \u274c not present virtual abstract impl abstract impl may not be mentioned if D is not final impl \u2705 \u2705 \u2705 virtual abstract impl abstract impl","title":"Virtual override keywords"},{"location":"design/classes/#subtyping","text":"A pointer to a base class, like MyBaseClass* is actually considered to be a pointer to that type or any derived class, like MiddleDerived or FinalDerived . This means that a FinalDerived* value may be implicitly cast to type MiddleDerived* or MyBaseClass* . This is accomplished by making the data layout of a type extending MyBaseClass have MyBaseClass as a prefix. In addition, the first class in the inheritance chain with a virtual method will include a virtual pointer, or vptr , pointing to a virtual method table , or vtable . Any calls to virtual methods will perform dynamic dispatch by calling the method using the function pointer in the vtable, to get the overridden implementation from the most derived class that implements the method. Since a final class may not be extended, the compiler can bypass the vtable and use static dispatch . In general, you can use a combination of an abstract base class and a final class instead of an extensible class if you need to distinguish between exactly a type and possibly a subtype. base class Extensible { ... } // Can be replaced by: abstract class ExtensibleBase { ... } class ExactlyExtensible extends ExtensibleBase { ... }","title":"Subtyping"},{"location":"design/classes/#constructors","text":"Like for classes without inheritance, constructors for a derived class are ordinary functions that return an instance of the derived class. Generally constructor functions should return the constructed value without copying, as in proposal #257: Initialization of memory and variables . This means either creating the object in the return statement itself , or in a returned var declaration . As before, instances can be created using by casting a struct value into the class type, this time with a .base member to initialize the members of the immediate base type. class MyDerivedType extends MyBaseType { fn Create() -> MyDerivedType { return {.base = MyBaseType.Create(), .derived_field = ...}; } } There are two cases that aren't well supported with this pattern: Users cannot create a value of an abstract class, which is necessary when it has private fields or otherwise requires initialization. Users may want to reduce the chance of mistakes from calling a method on a partially constructed object. Of particular concern is calling a virtual method prior to forming the derived class and so it uses the base class implementation. While expected to be relatively rarely needed, we will address both of these concerns with a specialized type just used during construction of base classes, called the partial facet type for the class.","title":"Constructors"},{"location":"design/classes/#partial-facet","text":"The partial facet for a base class type like MyBaseType is written partial MyBaseType . Only methods that take the partial facet type may be called on the partial facet type, so methods have to opt in to being called on an object that isn't fully constructed. No virtual methods may take the partial facet type, so there is no way to transitively call a virtual method on an object that isn't fully constructed. partial MyBaseClass and MyBaseClass have the same fields in the same order with the same data layout. The only difference is that partial MyBaseClass doesn't use (look into) its hidden vptr slot. To reliably catch any bugs where virtual function calls occur in this state, both fast and hardened release builds will initialize the hidden vptr slot to a null pointer. Debug builds will initialize it to an alternate vtable whose functions will abort the program with a clear diagnostic. Since partial MyBaseClass has the same data layout but only uses a subset, there is a subtyping relationship between these types. A MyBaseClass value is a partial MyBaseClass value, but not the other way around. So you can cast MyBaseClass* to partial MyBaseClass* , but the other direction is not safe. When MyBaseClass may be instantiated, there is a conversion from partial MyBaseClass to MyBaseClass . It changes the value by filling in the hidden vptr slot. If MyBaseClass is abstract, then attempting that conversion is an error. partial MyBaseClass is considered final, even if MyBaseClass is not. This is despite the fact that from a data layout perspective, partial MyDerivedClass will have partial MyBaseClass as a prefix if MyDerivedClass extends MyBaseClass . The type partial MyBaseClass specifically means \"exactly this and no more.\" This means we don't need to look at the hidden vptr slot, and we can instantiate it even if it doesn't have a virtual destructor . The keyword partial may only be applied to a base class. For final classes, there is no need for a second type.","title":"Partial facet"},{"location":"design/classes/#usage","text":"The general pattern is that base classes can define constructors returning the partial facet type. base class MyBaseClass { fn Create() -> partial Self { return {.base_field_1 = ..., .base_field_2 = ...}; } // ... } Extensible classes can be instantiated even from a partial facet value: var mbc: MyBaseClass = MyBaseClass.Create(); The conversion from partial MyBaseClass to MyBaseClass only fills in the vptr value and can be done in place. After the conversion, all public methods may be called, including virtual methods. The partial facet is required for abstract classes, since otherwise they may not be instantiated. Constructor functions for abstract classes should be marked protected so they may only be accessed in derived classes. abstract class MyAbstractClass { protected fn Create() -> partial Self { return {.base_field_1 = ..., .base_field_2 = ...}; } // ... } // \u274c Error: can't instantiate abstract class var abc: MyAbstractClass = ...; If a base class wants to store a pointer to itself somewhere in the constructor function, there are two choices: An extensible class could use the plain type instead of the partial facet. base class MyBaseClass { fn Create() -> Self { returned var result: Self = {...}; StoreMyPointerSomewhere(&result); return var; } } The other choice is to explicitly cast the type of its address. This pointer should not be used to call any virtual method until the object is finished being constructed, since the vptr will be null. abstract class MyAbstractClass { protected fn Create() -> partial Self { returned var result: partial Self = {...}; // Careful! Pointer to object that isn't fully constructed! StoreMyPointerSomewhere(&result as Self*); return var; } } The constructor for a derived class may construct values from a partial facet of the class' immediate base type or the full type: abstract class MyAbstractClass { protected fn Create() -> partial Self { ... } } // Base class returns a partial type base class Derived extends MyAbstractClass { protected fn Create() -> partial Self { return {.base = MyAbstractClass.Create(), .derived_field = ...}; } ... } base class MyBaseClass { fn Create() -> Self { ... } } // Base class returns a full type base class ExtensibleDerived extends MyBaseClass { fn Create() -> Self { return {.base = MyBaseClass.Create(), .derived_field = ...}; } ... } And final classes will return a type that does not use the partial facet: class FinalDerived extends MiddleDerived { fn Create() -> Self { return {.base = MiddleDerived.Create(), .derived_field = ...}; } ... } Observe that the vptr is only assigned twice in release builds if you use partial facets: The first class value created, by the factory function creating the base of the class hierarchy, initialized the vptr field to nullptr. Every derived type transitively created from that value will leave it alone. Only when the value has its most-derived class and is converted from the partial facet type to its final type is the vptr field set to its final value. In the case that the base class can be instantiated, tooling could optionally recommend that functions returning Self that are used to initialize a derived class be changed to return partial Self instead. However, the consequences of returning Self instead of partial Self when the value will be used to initialize a derived class are fairly minor: The vptr field will be assigned more than necessary. The types won't protect against calling methods on a value while it is being constructed, much like the situation in C++ currently.","title":"Usage"},{"location":"design/classes/#assignment-with-inheritance","text":"Since the assignment operator method should not be virtual, it is only safe to implement it for final types. However, following the maxim that Carbon should \"focus on encouraging appropriate usage of features rather than restricting misuse\" , we allow users to also implement assignment on extensible classes, even though it can lead to slicing .","title":"Assignment with inheritance"},{"location":"design/classes/#destructors","text":"Every non-abstract type is destructible , meaning has a defined destructor function called when the lifetime of a value of that type ends, such as when a variable goes out of scope. The destructor for a class may be customized using the destructor keyword: class MyClass { destructor [me: Self] { ... } } or: class MyClass { // Can modify `me` in the body. destructor [addr me: Self*] { ... } } If a class has no destructor declaration, it gets the default destructor, which is equivalent to destructor [me: Self] { } . The destructor for a class is run before the destructors of its data members. The data members are destroyed in reverse order of declaration. Derived classes are destroyed before their base classes, so the order of operations is: derived class' destructor runs, the data members of the derived class are destroyed, in reverse order of declaration, the immediate base class' destructor runs, the data members of the immediate base class are destroyed, in reverse order of declaration, and so on. Destructors may be declared in class scope and then defined out-of-line: class MyClass { destructor [addr me: Self*]; } destructor MyClass [addr me: Self*] { ... } It is illegal to delete an instance of a derived class through a pointer to one of its base classes unless it has a virtual destructor . An abstract or base class' destructor may be declared virtual using the virtual introducer, in which case any derived class destructor declaration must be impl : base class MyBaseClass { virtual destructor [addr me: Self*] { ... } } class MyDerivedClass extends MyBaseClass { impl destructor [addr me: Self*] { ... } } The properties of a type, whether type is abstract, base, or final, and whether the destructor is virtual or non-virtual, determines which type-of-types it satisfies. Non-abstract classes are Concrete . This means you can create local and member variables of this type. Concrete types have destructors that are called when the local variable goes out of scope or the containing object of the member variable is destroyed. Final classes and classes with a virtual destructor are Deletable . These may be safely deleted through a pointer. Classes that are Concrete , Deletable , or both are Destructible . These are types that may be deleted through a pointer, but it might not be safe. The concerning situation is when you have a pointer to a base class without a virtual destructor. It is unsafe to delete that pointer when it is actually pointing to a derived class. Note: The names Deletable and Destructible are placeholders since they do not conform to the decision on question-for-leads issue #1058: \"How should interfaces for core functionality be named?\" . Class Destructor Concrete Deletable Destructible abstract non-virtual no no no abstract virtual no yes yes base non-virtual yes no yes base virtual yes yes yes final any yes yes yes The compiler automatically determines which of these type-of-types a given type satisfies. It is illegal to directly implement Concrete , Deletable , or Destructible directly. For more about these constraints, see \"destructor constraints\" in the detailed generics design . A pointer to Deletable types may be passed to the Delete method of the Allocator interface . To deallocate a pointer to a base class without a virtual destructor, which may only be done when it is not actually pointing to a value with a derived type, call the UnsafeDelete method instead. Note that you may not call UnsafeDelete on abstract types without virtual destructors, it requires Destructible . interface Allocator { // ... fn Delete[T:! Deletable, addr me: Self*](p: T*); fn UnsafeDelete[T:! Destructible, addr me: Self*](p: T*); } To pass a pointer to a base class without a virtual destructor to a generic function expecting a Deletable type, use the UnsafeAllowDelete type adapter . adapter UnsafeAllowDelete(T:! Concrete) extends T { impl as Deletable {} } // Example usage: fn RequiresDeletable[T:! Deletable](p: T*); var x: MyExtensible; RequiresDeletable(&x as UnsafeAllowDelete(MyExtensible)*); If a virtual method is transitively called from inside a destructor, the implementation from the current class is used, not any overrides from derived classes. It will abort the execution of the program if that method is abstract and not implemented in the current class. Future work: Allow or require destructors to be declared as taking partial Self in order to prove no use of virtual methods. Types satisfy the TrivialDestructor type-of-type if: the class declaration does not define a destructor or the class defines the destructor with an empty body { } , all data members implement TrivialDestructor , and all base classes implement TrivialDestructor . For example, a struct type implements TrivialDestructor if all its members do. TrivialDestructor implies that their destructor does nothing, which may be used to generate optimized specializations. There is no provision for handling failure in a destructor. All operations that could potentially fail must be performed before the destructor is called. Unhandled failure during a destructor call will abort the program. Future work: Allow or require destructors to be declared as taking [var me: Self] . Alternatives considered: Types implement destructor interface Prevent virtual function calls in destructors Allow functions to act as destructors Allow private destructors Allow multiple conditional destructors Don't distinguish safe and unsafe delete operations Don't allow unsafe delete Allow final destructors","title":"Destructors"},{"location":"design/classes/#access-control","text":"By default, all members of a class are fully publicly accessible. Access can be restricted by adding a keyword, called an access modifier , prior to the declaration. Access modifiers are how Carbon supports encapsulation . The access modifier is written before any virtual override keyword . Rationale: Carbon makes members public by default for a few reasons: The readability of public members is the most important, since we expect most readers to be concerned with the public API of a type. The members that are most commonly private are the data fields, which have relatively less complicated definitions that suffer less from the extra annotation. Additionally, there is precedent for this approach in modern object-oriented languages such as Kotlin and Python , both of which are well regarded for their usability. Keywords controlling visibility are attached to individual declarations instead of C++'s approach of labels controlling the visibility for all following declarations to reduce context sensitivity . This matches Rust , Swift , Java , C# , Kotlin , and D . References: Proposal #561: Basic classes included the decision that members default to publicly accessible originally asked in issue #665 .","title":"Access control"},{"location":"design/classes/#private-access","text":"As in C++, private means only accessible to members of the class and any friends . class Point { fn Distance[me: Self]() -> f32; // These are only accessible to members of `Point`. private var x: f32; private var y: f32; } A private virtual or private abstract method may be implemented in derived classes, even though it may not be called. This allows derived classes to customize the behavior of a function called by a method of the base class, while still preventing the derived class from calling it. This matches the behavior of C++ and is more orthogonal. Future work: private will give the member internal linkage unless it needs to be external because it is used in an inline method or template. We may in the future add a way to specify internal linkage explicitly . Open questions: Using private to mean \"restricted to this class\" matches C++. Other languages support restricting to different scopes: Swift supports \"restrict to this module\" and \"restrict to this file\". Rust supports \"restrict to this module and any children of this module\", as well as \"restrict to this crate\", \"restrict to parent module\", and \"restrict to a specific ancestor module\". Comparison to other languages: C++, Rust, and Swift all make class members private by default. C++ offers the struct keyword that makes members public by default.","title":"Private access"},{"location":"design/classes/#protected-access","text":"Protected members may only be accessed by members of this class, members of derived classes, and any friends . base class MyBaseClass { protected fn HelperClassFunction(x: i32) -> i32; protected fn HelperMethod[me: Self](x: i32) -> i32; protected var data: i32; } class MyDerivedClass extends MyBaseClass { fn UsesProtected[addr me: Self*]() { // Can access protected members in derived class var x: i32 = HelperClassFunction(3); me->data = me->HelperMethod(x); } }","title":"Protected access"},{"location":"design/classes/#friends","text":"Classes may have a friend declaration: class Buddy { ... } class Pal { private var x: i32; friend Buddy; } This declares Buddy to be a friend of Pal , which means that Buddy can access all members of this class, even the ones that are declared private or protected . The friend keyword is followed by the name of an existing function, type, or parameterized family of types. Unlike C++, it won't act as a forward declaration of that name. The name must be resolvable by the compiler, and so may not be a member of a template.","title":"Friends"},{"location":"design/classes/#test-friendship","text":"Future work: There should be a convenient way of allowing tests in the same library as the class definition to access private members of the class. Ideally this could be done without changing the class definition itself, since it doesn't affect the class' public API.","title":"Test friendship"},{"location":"design/classes/#access-control-for-construction","text":"A function may construct a class, by casting a struct value to the class type, if it has access to (write) all of its fields. Future work: There should be a way to limit which code can construct a class even when it only has public fields. This will be resolved in question-for-leads issue #803 .","title":"Access control for construction"},{"location":"design/classes/#operator-overloading","text":"Developers may define how standard Carbon operators, such as + and / , apply to custom types by implementing the interface that corresponds to that operator for the types of the operands. See the \"operator overloading\" section of the generics design . The specific interface used for a given operator may be found in the expressions design .","title":"Operator overloading"},{"location":"design/classes/#future-work","text":"This includes features that need to be designed, questions to answer, and a description of the provisional syntax in use until these decisions have been made.","title":"Future work"},{"location":"design/classes/#struct-literal-shortcut","text":"We could allow you to write {x, y} as a short hand for {.x = x, .y = y} .","title":"Struct literal shortcut"},{"location":"design/classes/#optional-named-parameters","text":"Structs are being considered as a possible mechanism for implementing optional named parameters. We have three main candidate approaches: allowing struct types to have field defaults, having dedicated support for destructuring struct values in pattern contexts, or having a dedicated optional named parameter syntax.","title":"Optional named parameters"},{"location":"design/classes/#field-defaults-for-struct-types","text":"If struct types could have field defaults, you could write a function declaration with all of the optional parameters in an option struct: fn SortIntVector( v: Vector(i32)*, options: {.stable: Bool = false, .descending: Bool = false} = {}) { // Code using `options.stable` and `options.descending`. } // Uses defaults of `.stable` and `.descending` equal to `false`. SortIntVector(&v); SortIntVector(&v, {}); // Sets `.stable` option to `true`. SortIntVector(&v, {.stable = true}); // Sets `.descending` option to `true`. SortIntVector(&v, {.descending = true}); // Sets both `.stable` and `.descending` options to `true`. SortIntVector(&v, {.stable = true, .descending = true}); // Order can be different for arguments as well. SortIntVector(&v, {.descending = true, .stable = true});","title":"Field defaults for struct types"},{"location":"design/classes/#destructuring-in-pattern-matching","text":"We might instead support destructuring struct patterns with defaults: fn SortIntVector( v: Vector(i32)*, {stable: Bool = false, descending: Bool = false}) { // Code using `stable` and `descending`. } This would allow the same syntax at the call site, but avoids some concerns with field defaults and allows some other use cases such as destructuring return values.","title":"Destructuring in pattern matching"},{"location":"design/classes/#discussion","text":"We might support destructuring directly: var {key: String, value: i32} = ReturnKeyValue(); or by way of a mechanism that converts a struct into a tuple: var (key: String, value: i32) = ReturnKeyValue().extract(.key, .value); // or maybe: var (key: String, value: i32) = ReturnKeyValue()[(.key, .value)]; Similarly we might support optional named parameters directly instead of by way of struct types. Some discussion on this topic has occurred in: question-for-leads issue #505 on named parameters labeled params brainstorming docs 1 , 2 \"match\" in syntax choices doc","title":"Discussion"},{"location":"design/classes/#inheritance_1","text":"","title":"Inheritance"},{"location":"design/classes/#c-abstract-base-classes-interoperating-with-object-safe-interfaces","text":"We want four things so that Carbon's object-safe interfaces may interoperate with C++ abstract base classes without data members, matching the interface as base class use case : Ability to convert an object-safe interface (a type-of-type) into an C++-compatible base class (a base type), maybe using AsBaseClass(MyInterface) . Ability to convert a C++ base class without data members (a base type) into an object-safe interface (a type-of-type), maybe using AsInterface(MyIBC) . Ability to convert a (thin) pointer to an abstract base class to a DynPtr of the corresponding interface. Ability to convert DynPtr(MyInterface) values to a proxy type that extends the corresponding base class AsBaseType(MyInterface) . Note that the proxy type extending AsBaseType(MyInterface) would be a different type than DynPtr(MyInterface) since the receiver input to the function members of the vtable for the former does not match those in the witness table for the latter.","title":"C++ abstract base classes interoperating with object-safe interfaces"},{"location":"design/classes/#overloaded-methods","text":"We allow a derived class to define a class function with the same name as a class function in the base class. For example, we expect it to be pretty common to have a constructor function named Create at all levels of the type hierarchy. Beyond that, we may want some rules or restrictions about defining methods in a derived class with the same name as a base class method without overriding it. There are some opportunities to improve on and simplify the C++ story: We don't want to silently hide methods in the base class because of a method with the same name in a derived class. There are uses for this in C++, but it also causes problems and without multiple inheritance there isn't the same need in Carbon. Overload resolution should happen before virtual dispatch. For evolution purposes, you should be able to add private members to a base class that have the same name as member of a derived class without affecting overload resolution on instances of the derived class, in functions that aren't friends of the base class. References: This was discussed in the open discussion on 2021-07-12 .","title":"Overloaded methods"},{"location":"design/classes/#interop-with-c-inheritance","text":"This design directly supports Carbon classes inheriting from a single C++ class. class CarbonClass extends C++.CPlusPlusClass { fn Create() -> Self { return {.base = C++.CPlusPlusClass(...), .other_fields = ...}; } ... } To allow C++ classes to extend Carbon classes, there needs to be some way for C++ constructors to initialize their base class: There could be some way to export a Carbon class that identifies which factory functions may be used as constructors. We could explicitly call the Carbon factory function, as in: `` // Base` is a Carbon class which gets converted to a // C++ class for interop purposes: class Base { public: virtual ~Base() {} static auto Create() -> Base; }; // In C++ class Derived : public Base { public: virtual ~Derived() override {} // This isn't currently a case where C++ guarantees no copy, // and so it currently still requires a notional copy and // there appear to be implementation challenges with // removing them. This may require an extension to make work // reliably without an extraneous copy of the base subobject. Derived() : Base(Base::Create()) {} }; ``` However, this doesn't work in the case where Base can't be instantiated, or Base does not have a copy constructor, even though it shouldn't be called due to RVO.","title":"Interop with C++ inheritance"},{"location":"design/classes/#virtual-base-classes","text":"TODO: Ask zygoloid to fill this in. Carbon won't support declaring virtual base classes, and the C++ interop use cases Carbon needs to support are limited. This will allow us to simplify the C++ interop by allowing Carbon to delegate initialization of virtual base classes to the C++ side. This requires that we enforce two rules: No multiple inheritance of C++ classes with virtual bases No C++ class extending a Carbon class that extends a C++ class with a virtual base","title":"Virtual base classes"},{"location":"design/classes/#mixins_1","text":"We will need some way to declare mixins. This syntax will need a way to distinguish defining versus requiring member variables. Methods may additionally be given a default definition but may be overridden. Interface implementations may only be partially provided by a mixin. Mixin methods will need to be able to convert between pointers to the mixin type and the main type. Open questions include whether a mixin is its own type that is a member of the containing type, and whether mixins are templated on the containing type. Mixins also complicate how constructors work.","title":"Mixins"},{"location":"design/classes/#memory-layout","text":"Carbon will need some way for users to specify the memory layout of class types beyond simple ordering of fields, such as controlling the packing and alignment for the whole type or individual members. We may allow members of a derived class like to put data members in the final padding of its base class prefix. Tail-padding reuse has both advantages and disadvantages, so we may have some way for a class to explicitly mark that its tail padding is available for use by a derived class, Advantages: Tail-padding reuse is sometimes a nice layout optimization (eg, in Clang we save 8 bytes per Expr by reusing tail padding). No class size regressions when migrating from C++. Special case of reusing the tail padding of a class that is empty other than its tail padding is very important, to the extent that we will likely need to support either zero-sized types or tail-padding reuse in order to have acceptable class layouts. Disadvantages: Cannot use memcpy(p, q, sizeof(Base)) to copy around base class subobjects if the destination is an in-lifetime, because they might overlap other objects' representations. Somewhat more complex model. We need some mechanism for disabling tail-padding reuse in \"standard layout\" types. We may also have to use narrowed loads for the last member of a base class to avoid accidentally creating a race condition. However, we can still use memcpy and memset to initialize a base class subobject, even if its tail padding might be reused, so long as we guarantee that no other object lives in the tail padding and is initialized before the base class. In C++, that happens only due to virtual base classes getting initialized early and laid out at the end of the object; if we disallow virtual base classes then we can guarantee that initialization order is address order, removing most of the downside of tail-padding reuse.","title":"Memory layout"},{"location":"design/classes/#no-static-variables","text":"At the moment, there is no proposal to support static member variables , in line with avoiding global variables more generally. Carbon may need some support in this area, though, for parity with and migration from C++.","title":"No static variables"},{"location":"design/classes/#computed-properties","text":"Carbon might want to support members of a type that are accessed like a data member but return a computed value like a function. This has a number of implications: It would be a way of publicly exposing data members for encapsulated types , allowing for rules that otherwise forbid mixing public and private data members. It would provide a more graceful evolution path from a data class to an encapsulated type . It would give an option to start with a data class instead of writing all the boilerplate to create an encapsulated type preemptively to allow future evolution. It would let you take a variable away and put a property in its place with no other code changes. The number one use for this is so you can put a breakpoint in the property code, then later go back to public variable once you understand who was misbehaving. We should have some guidance for when to use a computed property instead of a function with no arguments. One possible criteria is when it is a pure function of the state of the object and executes in an amount of time similar to ordinary member access. However, there are likely to be differences between computed properties and other data members, such as the ability to take the address of them. We might want to support \"read only\" data members, that can be read through the public api but only modified with private access, for data members which may need to evolve into a computed property. There are also questions regarding how to support assigning or modifying computed properties, such as using += .","title":"Computed properties"},{"location":"design/classes/#interfaces-implemented-for-data-classes","text":"We should define a way for defining implementations of interfaces for struct types. To satisfy coherence, these implementations would have to be defined in the library with the interface definition. The syntax might look like: interface ConstructWidgetFrom { fn Construct(Self) -> Widget; } external impl {.kind: WidgetKind, .size: i32} as ConstructWidgetFrom { ... } In addition, we should define a way for interfaces to define templated blanket implementations for data classes more generally. These implementations will typically subject to the criteria that all the data fields of the type must implement the interface. An example use case would be to say that a data class is serializable if all of its fields were. For this we will need a type-of-type for capturing that criteria, maybe something like DataFieldsImplement(MyInterface) . The templated implementation will need some way of iterating through the fields so it can perform operations fieldwise. This feature should also implement the interfaces for any tuples whose fields satisfy the criteria. It is an open question how to define implementations for binary operators. For example, if i32 is comparable to f64 , then {.x = 3, .y = 2.72} should be comparable to {.x = 3.14, .y = 2} . The trick is how to declare the criteria that \" T is comparable to U if they have the same field names in the same order, and for every field x , the type of T.x implements ComparableTo for the type of U.x .\"","title":"Interfaces implemented for data classes"},{"location":"design/classes/#references","text":"#257: Initialization of memory and variables #561: Basic classes: use cases, struct literals, struct types, and future wor #722: Nominal classes and methods #777: Inheritance #981: Implicit conversions for aggregates #1154: Destructors","title":"References"},{"location":"design/functions/","text":"Functions Table of contents Overview Function definitions Return clause return statements Function declarations Function calls Functions in other features Alternatives considered References Overview Functions are the core building block for applications. Carbon's basic function syntax is: parameter : identifier : expression parameter-list : [ parameter , parameter , ... ] return-clause : [ -> < expression | auto > ] signature : fn identifier ( parameter-list ) return-clause function-definition : signature { statements } function-declaration : signature ; function-call : identifier ( [ expression , expression , ... ] ) A function with only a signature and no body is a function declaration, or forward declaration. When the body is a present, it's a function definition. The body introduces nested scopes which may contain local variable declarations. Function definitions A basic function definition may look like: fn Add(a: i64, b: i64) -> i64 { return a + b; } This declares a function called Add which accepts two i64 parameters, the first called a and the second called b , and returns an i64 result. It returns the result of adding the two arguments. C++ might declare the same thing: std::int64_t Add(std::int64_t a, std::int64_t b) { return a + b; } // Or with trailing return type syntax: auto Add(std::int64_t a, std::int64_t b) -> std::int64_t { return a + b; } Return clause The return clause of a function specifies the return type using one of three possible syntaxes: -> followed by an expression , such as i64 , directly states the return type. This expression will be evaluated at compile-time, so must be valid in that context. For example, fn ToString(val: i64) -> String; has a return type of String . -> followed by the auto keyword indicates that type inference should be used to determine the return type. For example, fn Echo(val: i64) -> auto { return val; } will have a return type of i64 through type inference. Declarations must have a known return type, so auto is not valid. The function must have precisely one return statement. That return statement's expression will then be used for type inference. Omission indicates that the return type is the empty tuple, () . For example, fn Sleep(seconds: i64); is similar to fn Sleep(seconds: i64) -> (); . () is similar to a void return type in C++. return statements The return statement is essential to function control flow. It ends the flow of the function and returns execution to the caller. When the return clause is omitted, the return statement has no expression argument, and function control flow implicitly ends after the last statement in the function's body as if return; were present. When the return clause is provided, including when it is -> () , the return statement must have an expression that is convertible to the return type, and a return statement must be used to end control flow of the function. Function declarations Functions may be declared separate from the definition by providing only a signature, with no body. This provides an API which may be called. For example: // Declaration: fn Add(a: i64, b: i64) -> i64; // Definition: fn Add(a: i64, b: i64) -> i64 { return a + b; } The corresponding definition may be provided later in the same file or, when the declaration is in an api file of a library , in the impl file of the same library. The signature of a function declaration must match the corresponding definition. This includes the return clause ; even though an omitted return type has equivalent behavior to -> () , the presence or omission must match. Function calls Function calls use a function's identifier to pass multiple expression arguments corresponding to the function signature's parameters. For example: fn Add(a: i64, b: i64) -> i64 { return a + b; } fn Run() { Add(1, 2); } Here, Add(1, 2) is a function call expression. Add refers to the function definition's identifier. The parenthesized arguments 1 and 2 are passed to the a and b parameters of Add . Functions in other features Other designs build upon basic function syntax to add advanced features: Generic functions adds support for deduced parameters and generic type parameters. Class member functions adds support for methods and class functions. Alternatives considered Function keyword Only allow auto return types if parameters are generic Provide alternate function syntax for concise return type inference Allow separate declaration and definition References Proposal #438: Add statement syntax for function declarations Proposal #826: Function return type inference","title":"Functions"},{"location":"design/functions/#functions","text":"","title":"Functions"},{"location":"design/functions/#table-of-contents","text":"Overview Function definitions Return clause return statements Function declarations Function calls Functions in other features Alternatives considered References","title":"Table of contents"},{"location":"design/functions/#overview","text":"Functions are the core building block for applications. Carbon's basic function syntax is: parameter : identifier : expression parameter-list : [ parameter , parameter , ... ] return-clause : [ -> < expression | auto > ] signature : fn identifier ( parameter-list ) return-clause function-definition : signature { statements } function-declaration : signature ; function-call : identifier ( [ expression , expression , ... ] ) A function with only a signature and no body is a function declaration, or forward declaration. When the body is a present, it's a function definition. The body introduces nested scopes which may contain local variable declarations.","title":"Overview"},{"location":"design/functions/#function-definitions","text":"A basic function definition may look like: fn Add(a: i64, b: i64) -> i64 { return a + b; } This declares a function called Add which accepts two i64 parameters, the first called a and the second called b , and returns an i64 result. It returns the result of adding the two arguments. C++ might declare the same thing: std::int64_t Add(std::int64_t a, std::int64_t b) { return a + b; } // Or with trailing return type syntax: auto Add(std::int64_t a, std::int64_t b) -> std::int64_t { return a + b; }","title":"Function definitions"},{"location":"design/functions/#return-clause","text":"The return clause of a function specifies the return type using one of three possible syntaxes: -> followed by an expression , such as i64 , directly states the return type. This expression will be evaluated at compile-time, so must be valid in that context. For example, fn ToString(val: i64) -> String; has a return type of String . -> followed by the auto keyword indicates that type inference should be used to determine the return type. For example, fn Echo(val: i64) -> auto { return val; } will have a return type of i64 through type inference. Declarations must have a known return type, so auto is not valid. The function must have precisely one return statement. That return statement's expression will then be used for type inference. Omission indicates that the return type is the empty tuple, () . For example, fn Sleep(seconds: i64); is similar to fn Sleep(seconds: i64) -> (); . () is similar to a void return type in C++.","title":"Return clause"},{"location":"design/functions/#return-statements","text":"The return statement is essential to function control flow. It ends the flow of the function and returns execution to the caller. When the return clause is omitted, the return statement has no expression argument, and function control flow implicitly ends after the last statement in the function's body as if return; were present. When the return clause is provided, including when it is -> () , the return statement must have an expression that is convertible to the return type, and a return statement must be used to end control flow of the function.","title":"return statements"},{"location":"design/functions/#function-declarations","text":"Functions may be declared separate from the definition by providing only a signature, with no body. This provides an API which may be called. For example: // Declaration: fn Add(a: i64, b: i64) -> i64; // Definition: fn Add(a: i64, b: i64) -> i64 { return a + b; } The corresponding definition may be provided later in the same file or, when the declaration is in an api file of a library , in the impl file of the same library. The signature of a function declaration must match the corresponding definition. This includes the return clause ; even though an omitted return type has equivalent behavior to -> () , the presence or omission must match.","title":"Function declarations"},{"location":"design/functions/#function-calls","text":"Function calls use a function's identifier to pass multiple expression arguments corresponding to the function signature's parameters. For example: fn Add(a: i64, b: i64) -> i64 { return a + b; } fn Run() { Add(1, 2); } Here, Add(1, 2) is a function call expression. Add refers to the function definition's identifier. The parenthesized arguments 1 and 2 are passed to the a and b parameters of Add .","title":"Function calls"},{"location":"design/functions/#functions-in-other-features","text":"Other designs build upon basic function syntax to add advanced features: Generic functions adds support for deduced parameters and generic type parameters. Class member functions adds support for methods and class functions.","title":"Functions in other features"},{"location":"design/functions/#alternatives-considered","text":"Function keyword Only allow auto return types if parameters are generic Provide alternate function syntax for concise return type inference Allow separate declaration and definition","title":"Alternatives considered"},{"location":"design/functions/#references","text":"Proposal #438: Add statement syntax for function declarations Proposal #826: Function return type inference","title":"References"},{"location":"design/metaprogramming/","text":"Metaprogramming Table of contents TODO Overview TODO This is a skeletal design, added to support the overview . It should not be treated as accepted by the core team; rather, it is a placeholder until we have more time to examine this detail. Please feel welcome to rewrite and update as appropriate. See proposal PR 89 for context -- that proposal may replace this. Overview Carbon provides metaprogramming facilities that look similar to regular Carbon code. These are structured, and do not offer inclusion or arbitrary preprocessing of source text such as C/C++ does.","title":"Metaprogramming"},{"location":"design/metaprogramming/#metaprogramming","text":"","title":"Metaprogramming"},{"location":"design/metaprogramming/#table-of-contents","text":"TODO Overview","title":"Table of contents"},{"location":"design/metaprogramming/#todo","text":"This is a skeletal design, added to support the overview . It should not be treated as accepted by the core team; rather, it is a placeholder until we have more time to examine this detail. Please feel welcome to rewrite and update as appropriate. See proposal PR 89 for context -- that proposal may replace this.","title":"TODO"},{"location":"design/metaprogramming/#overview","text":"Carbon provides metaprogramming facilities that look similar to regular Carbon code. These are structured, and do not offer inclusion or arbitrary preprocessing of source text such as C/C++ does.","title":"Overview"},{"location":"design/name_lookup/","text":"Name lookup Table of contents TODO Overview Unqualified name lookup Alternatives Name lookup for common, standard types Open questions Shadowing TODO This is a skeletal design, added to support the overview . It should not be treated as accepted by the core team; rather, it is a placeholder until we have more time to examine this detail. Please feel welcome to rewrite and update as appropriate. Overview Names are always introduced into some scope which defines where they can be referenced. Many of these scopes are themselves named. Carbon has a special facility for introducing a dedicated named scope just like C++, but we traverse nested names in a uniform way with . -separated names: namespace Foo { namespace Bar { alias ??? MyInt = Int; } } fn F(x: Foo.Bar.MyInt); Carbon packages are also namespaces so to get to an imported name from the Abseil package you would write Abseil.Foo . The \"top-level\" file scope is that of the Carbon package containing the file, meaning that there is no \"global\" scope. Dedicated namespaces can be reopened within a package, but there is no way to reopen a package without being a library and file within that package. Note that libraries (unlike packages) do not introduce a scope, they share the scope of their package. This is based on the observation that in practice, a fairly coarse scoping tends to work best, with some degree of global registry to establish a unique package name. Unqualified name lookup Unqualified name lookup in Carbon will always find a file-local result, other than the implicit \"prelude\" of importing and aliasing the fundamentals of the standard library. There will be an explicit mention of the name in the file that declares the name in the current or enclosing scope, which must also precede the reference. Alternatives This implies that other names within your own package but not declared within the file must be found by way of the package name. It isn't clear if this is the desirable end state. We need to consider alternatives where names from the same library or any library in the same package are made immediately visible within the package scope for unqualified name lookup. Name lookup for common, standard types The Carbon standard library is in the Carbon package. A very small subset of this standard library is provided implicitly in every file's scope. This is called the \"prelude\" package. Names in the prelude package will be available without scoping names. For example, Bool will be the commonly used name in code, even though the underlying type may be Carbon::Bool . Also, no import will be necessary to use Bool . Open questions Shadowing We can probably disallow the use of shadowed unqualified names, but the actual design for such needs to be thought through.","title":"Name lookup"},{"location":"design/name_lookup/#name-lookup","text":"","title":"Name lookup"},{"location":"design/name_lookup/#table-of-contents","text":"TODO Overview Unqualified name lookup Alternatives Name lookup for common, standard types Open questions Shadowing","title":"Table of contents"},{"location":"design/name_lookup/#todo","text":"This is a skeletal design, added to support the overview . It should not be treated as accepted by the core team; rather, it is a placeholder until we have more time to examine this detail. Please feel welcome to rewrite and update as appropriate.","title":"TODO"},{"location":"design/name_lookup/#overview","text":"Names are always introduced into some scope which defines where they can be referenced. Many of these scopes are themselves named. Carbon has a special facility for introducing a dedicated named scope just like C++, but we traverse nested names in a uniform way with . -separated names: namespace Foo { namespace Bar { alias ??? MyInt = Int; } } fn F(x: Foo.Bar.MyInt); Carbon packages are also namespaces so to get to an imported name from the Abseil package you would write Abseil.Foo . The \"top-level\" file scope is that of the Carbon package containing the file, meaning that there is no \"global\" scope. Dedicated namespaces can be reopened within a package, but there is no way to reopen a package without being a library and file within that package. Note that libraries (unlike packages) do not introduce a scope, they share the scope of their package. This is based on the observation that in practice, a fairly coarse scoping tends to work best, with some degree of global registry to establish a unique package name.","title":"Overview"},{"location":"design/name_lookup/#unqualified-name-lookup","text":"Unqualified name lookup in Carbon will always find a file-local result, other than the implicit \"prelude\" of importing and aliasing the fundamentals of the standard library. There will be an explicit mention of the name in the file that declares the name in the current or enclosing scope, which must also precede the reference.","title":"Unqualified name lookup"},{"location":"design/name_lookup/#alternatives","text":"This implies that other names within your own package but not declared within the file must be found by way of the package name. It isn't clear if this is the desirable end state. We need to consider alternatives where names from the same library or any library in the same package are made immediately visible within the package scope for unqualified name lookup.","title":"Alternatives"},{"location":"design/name_lookup/#name-lookup-for-common-standard-types","text":"The Carbon standard library is in the Carbon package. A very small subset of this standard library is provided implicitly in every file's scope. This is called the \"prelude\" package. Names in the prelude package will be available without scoping names. For example, Bool will be the commonly used name in code, even though the underlying type may be Carbon::Bool . Also, no import will be necessary to use Bool .","title":"Name lookup for common, standard types"},{"location":"design/name_lookup/#open-questions","text":"","title":"Open questions"},{"location":"design/name_lookup/#shadowing","text":"We can probably disallow the use of shadowed unqualified names, but the actual design for such needs to be thought through.","title":"Shadowing"},{"location":"design/naming_conventions/","text":"Naming conventions Table of contents Overview Details Constants Carbon-provided item naming Alternatives considered References Overview Our naming conventions are: For idiomatic Carbon code: UpperCamelCase will be used when the named entity cannot have a dynamically varying value. For example, functions, namespaces, or compile-time constant values. lower_snake_case will be used when the named entity's value won't be known until runtime, such as for variables. For Carbon-provided features: Keywords and type literals will use lower_snake_case . Other code will use the guidelines for idiomatic Carbon code. In other words: Item Convention Explanation Packages UpperCamelCase Used for compile-time lookup. Types UpperCamelCase Resolved at compile-time. Functions UpperCamelCase Resolved at compile-time. Methods UpperCamelCase Methods, including virtual methods, are equivalent to functions. Generic parameters UpperCamelCase May vary based on inputs, but are ultimately resolved at compile-time. Compile-time constants UpperCamelCase Resolved at compile-time. See constants for more remarks. Variables lower_snake_case May be reassigned and thus require runtime information. Member variables lower_snake_case Behave like variables. Keywords lower_snake_case Special, and developers can be expected to be comfortable with this casing cross-language. Type literals lower_snake_case Equivalent to keywords. Boolean type and literals lower_snake_case Equivalent to keywords. Other Carbon types UpperCamelCase Behave like normal types. Self and Base UpperCamelCase These are similar to type members on a class. We only use UpperCamelCase and lower_snake_case in naming conventions in order to minimize the variation in rules. Details Constants Consider the following code: package Example; let CompileTimeConstant: i32 = 7; fn RuntimeFunction(runtime_constant: i32); In this example, CompileTimeConstant has a singular value ( 7 ) which is known at compile-time. As such, it uses UpperCamelCase . On the other hand, runtime_constant may be constant within the function body, but it is assigned at runtime when RuntimeFunction is called. Its value is only known in a given runtime invocation of RuntimeFunction . As such, it uses lower_snake_case . Carbon-provided item naming Carbon-provided items are split into a few categories: Keywords; for example, for , fn , and var . Type literals; for example, i<digits> , u<digits> , and f<digits> . Boolean type and literals; for example, bool , true , and false . The separate categorization of booleans should not be taken as a rule that only booleans would use lowercase; it's just the only example right now. Self and Base . Other Carbon types; for example, Int , UInt , and String . Note that while other Carbon types currently use UpperCamelCase , that should not be inferred to mean that future Carbon types will do the same. The leads will make decisions on future naming. Alternatives considered Other naming conventions Other conventions for naming Carbon types References Proposal #861: Naming conventions","title":"Naming conventions"},{"location":"design/naming_conventions/#naming-conventions","text":"","title":"Naming conventions"},{"location":"design/naming_conventions/#table-of-contents","text":"Overview Details Constants Carbon-provided item naming Alternatives considered References","title":"Table of contents"},{"location":"design/naming_conventions/#overview","text":"Our naming conventions are: For idiomatic Carbon code: UpperCamelCase will be used when the named entity cannot have a dynamically varying value. For example, functions, namespaces, or compile-time constant values. lower_snake_case will be used when the named entity's value won't be known until runtime, such as for variables. For Carbon-provided features: Keywords and type literals will use lower_snake_case . Other code will use the guidelines for idiomatic Carbon code. In other words: Item Convention Explanation Packages UpperCamelCase Used for compile-time lookup. Types UpperCamelCase Resolved at compile-time. Functions UpperCamelCase Resolved at compile-time. Methods UpperCamelCase Methods, including virtual methods, are equivalent to functions. Generic parameters UpperCamelCase May vary based on inputs, but are ultimately resolved at compile-time. Compile-time constants UpperCamelCase Resolved at compile-time. See constants for more remarks. Variables lower_snake_case May be reassigned and thus require runtime information. Member variables lower_snake_case Behave like variables. Keywords lower_snake_case Special, and developers can be expected to be comfortable with this casing cross-language. Type literals lower_snake_case Equivalent to keywords. Boolean type and literals lower_snake_case Equivalent to keywords. Other Carbon types UpperCamelCase Behave like normal types. Self and Base UpperCamelCase These are similar to type members on a class. We only use UpperCamelCase and lower_snake_case in naming conventions in order to minimize the variation in rules.","title":"Overview"},{"location":"design/naming_conventions/#details","text":"","title":"Details"},{"location":"design/naming_conventions/#constants","text":"Consider the following code: package Example; let CompileTimeConstant: i32 = 7; fn RuntimeFunction(runtime_constant: i32); In this example, CompileTimeConstant has a singular value ( 7 ) which is known at compile-time. As such, it uses UpperCamelCase . On the other hand, runtime_constant may be constant within the function body, but it is assigned at runtime when RuntimeFunction is called. Its value is only known in a given runtime invocation of RuntimeFunction . As such, it uses lower_snake_case .","title":"Constants"},{"location":"design/naming_conventions/#carbon-provided-item-naming","text":"Carbon-provided items are split into a few categories: Keywords; for example, for , fn , and var . Type literals; for example, i<digits> , u<digits> , and f<digits> . Boolean type and literals; for example, bool , true , and false . The separate categorization of booleans should not be taken as a rule that only booleans would use lowercase; it's just the only example right now. Self and Base . Other Carbon types; for example, Int , UInt , and String . Note that while other Carbon types currently use UpperCamelCase , that should not be inferred to mean that future Carbon types will do the same. The leads will make decisions on future naming.","title":"Carbon-provided item naming"},{"location":"design/naming_conventions/#alternatives-considered","text":"Other naming conventions Other conventions for naming Carbon types","title":"Alternatives considered"},{"location":"design/naming_conventions/#references","text":"Proposal #861: Naming conventions","title":"References"},{"location":"design/pattern_matching/","text":"Pattern matching Table of contents TODO Overview Pattern match control flow Pattern matching in local variables Open questions Slice or array nested value pattern matching Generic/template pattern matching Pattern matching as function overload resolution TODO This is a skeletal design, added to support the overview . It should not be treated as accepted by the core team; rather, it is a placeholder until we have more time to examine this detail. Please feel welcome to rewrite and update as appropriate. Overview The most prominent mechanism to manipulate and work with types in Carbon is pattern matching. This may seem like a deviation from C++, but in fact this is largely about building a clear, coherent model for a fundamental part of C++: overload resolution. Pattern match control flow The most powerful form and easiest to explain form of pattern matching is a dedicated control flow construct that subsumes the switch of C and C++ into something much more powerful, match . This is not a novel construct, and is widely used in existing languages (Swift and Rust among others) and is currently under active investigation for C++. Carbon's match can be used as follows: fn Bar() -> (i32, (f32, f32)); fn Foo() -> f32 { match (Bar()) { case (42, (x: f32, y: f32)) => { return x - y; } case (p: i32, (x: f32, _: f32)) if (p < 13) => { return p * x; } case (p: i32, _: auto) if (p > 3) => { return p * Pi; } default => { return Pi; } } } There is a lot going on here. First, let's break down the core structure of a match statement. It accepts a value that will be inspected, here the result of the call to Bar() . It then will find the first case that matches this value, and execute that block. If none match, then it executes the default block. Each case contains a pattern. The first part is a value pattern ( (p: i32, _: auto) for example) optionally followed by an if and boolean predicate. The value pattern has to match, and then the predicate has to evaluate to true for the overall pattern to match. Value patterns can be composed of the following: An expression ( 42 for example), whose value must be equal to match. An identifier to bind the value to, followed by a colon ( : ) and a type ( i32 for example). An underscore ( _ ) may be used instead of the identifier to discard the value once matched. A tuple destructuring pattern containing a tuple of value patterns ( (x: f32, y: f32) ) which match against tuples and tuple-like values by recursively matching on their elements. An unwrapping pattern containing a nested value pattern which matches against a variant or variant-like value by unwrapping it. In order to match a value, whatever is specified in the pattern must match. Using auto for a type will always match, making _: auto the wildcard pattern. Pattern matching in local variables Value patterns may be used when declaring local variables to conveniently destructure them and do other type manipulations. However, the patterns must match at compile time, so they can't use an if clause. fn Bar() -> (i32, (f32, f32)); fn Foo() -> i32 { var (p: i32, _: auto) = Bar(); return p; } This extracts the first value from the result of calling Bar() and binds it to a local variable named p which is then returned. Open questions Slice or array nested value pattern matching An open question is how to effectively fit a \"slice\" or \"array\" pattern into nested value pattern matching, or whether we shouldn't do so. Generic/template pattern matching An open question is going beyond a simple \"type\" to things that support generics and/or templates. Pattern matching as function overload resolution Need to flesh out specific details of how overload selection leverages the pattern matching machinery, what (if any) restrictions are imposed, etc.","title":"Pattern matching"},{"location":"design/pattern_matching/#pattern-matching","text":"","title":"Pattern matching"},{"location":"design/pattern_matching/#table-of-contents","text":"TODO Overview Pattern match control flow Pattern matching in local variables Open questions Slice or array nested value pattern matching Generic/template pattern matching Pattern matching as function overload resolution","title":"Table of contents"},{"location":"design/pattern_matching/#todo","text":"This is a skeletal design, added to support the overview . It should not be treated as accepted by the core team; rather, it is a placeholder until we have more time to examine this detail. Please feel welcome to rewrite and update as appropriate.","title":"TODO"},{"location":"design/pattern_matching/#overview","text":"The most prominent mechanism to manipulate and work with types in Carbon is pattern matching. This may seem like a deviation from C++, but in fact this is largely about building a clear, coherent model for a fundamental part of C++: overload resolution.","title":"Overview"},{"location":"design/pattern_matching/#pattern-match-control-flow","text":"The most powerful form and easiest to explain form of pattern matching is a dedicated control flow construct that subsumes the switch of C and C++ into something much more powerful, match . This is not a novel construct, and is widely used in existing languages (Swift and Rust among others) and is currently under active investigation for C++. Carbon's match can be used as follows: fn Bar() -> (i32, (f32, f32)); fn Foo() -> f32 { match (Bar()) { case (42, (x: f32, y: f32)) => { return x - y; } case (p: i32, (x: f32, _: f32)) if (p < 13) => { return p * x; } case (p: i32, _: auto) if (p > 3) => { return p * Pi; } default => { return Pi; } } } There is a lot going on here. First, let's break down the core structure of a match statement. It accepts a value that will be inspected, here the result of the call to Bar() . It then will find the first case that matches this value, and execute that block. If none match, then it executes the default block. Each case contains a pattern. The first part is a value pattern ( (p: i32, _: auto) for example) optionally followed by an if and boolean predicate. The value pattern has to match, and then the predicate has to evaluate to true for the overall pattern to match. Value patterns can be composed of the following: An expression ( 42 for example), whose value must be equal to match. An identifier to bind the value to, followed by a colon ( : ) and a type ( i32 for example). An underscore ( _ ) may be used instead of the identifier to discard the value once matched. A tuple destructuring pattern containing a tuple of value patterns ( (x: f32, y: f32) ) which match against tuples and tuple-like values by recursively matching on their elements. An unwrapping pattern containing a nested value pattern which matches against a variant or variant-like value by unwrapping it. In order to match a value, whatever is specified in the pattern must match. Using auto for a type will always match, making _: auto the wildcard pattern.","title":"Pattern match control flow"},{"location":"design/pattern_matching/#pattern-matching-in-local-variables","text":"Value patterns may be used when declaring local variables to conveniently destructure them and do other type manipulations. However, the patterns must match at compile time, so they can't use an if clause. fn Bar() -> (i32, (f32, f32)); fn Foo() -> i32 { var (p: i32, _: auto) = Bar(); return p; } This extracts the first value from the result of calling Bar() and binds it to a local variable named p which is then returned.","title":"Pattern matching in local variables"},{"location":"design/pattern_matching/#open-questions","text":"","title":"Open questions"},{"location":"design/pattern_matching/#slice-or-array-nested-value-pattern-matching","text":"An open question is how to effectively fit a \"slice\" or \"array\" pattern into nested value pattern matching, or whether we shouldn't do so.","title":"Slice or array nested value pattern matching"},{"location":"design/pattern_matching/#generictemplate-pattern-matching","text":"An open question is going beyond a simple \"type\" to things that support generics and/or templates.","title":"Generic/template pattern matching"},{"location":"design/pattern_matching/#pattern-matching-as-function-overload-resolution","text":"Need to flesh out specific details of how overload selection leverages the pattern matching machinery, what (if any) restrictions are imposed, etc.","title":"Pattern matching as function overload resolution"},{"location":"design/primitive_types/","text":"Primitive types Table of contents TODO Overview Integers Floats BFloat16 Open questions Primitive types as code vs built-in String view vs owning string Syntax for wrapping operations Non-power-of-two sizes TODO This is a skeletal design, added to support the overview . It should not be treated as accepted by the core team; rather, it is a placeholder until we have more time to examine this detail. Please feel welcome to rewrite and update as appropriate. Overview These types are fundamental to the language as they aren't either formed from or modifying other types. They also have semantics that are defined from first principles rather than in terms of other operations. These will be made available through the prelude package . Bool - a boolean type with two possible values: True and False . Int and UInt - signed and unsigned 64-bit integer types. Standard sizes are available, both signed and unsigned, including Int8 , Int16 , Int32 , Int128 , and Int256 . Overflow in either direction is an error. Float64 - a floating point type with semantics based on IEEE-754. Standard sizes are available, including Float16 , Float32 , and Float128 . BFloat16 is also provided. String - a byte sequence treated as containing UTF-8 encoded text. StringView - a read-only reference to a byte sequence treated as containing UTF-8 encoded text. Integers Integer types can be either signed or unsigned, much like in C++. Signed integers are represented using 2's complement and notionally modeled as unbounded natural numbers. Overflow in either direction is an error. That includes unsigned integers, differing from C++. The default size for both is 64-bits: Int and UInt . Specific sizes are also available, for example: Int8 , Int16 , Int32 , Int128 , UInt256 . Arbitrary powers of two above 8 are supported for both (although perhaps we'll want to avoid huge values for implementation simplicity). Floats Floating point types are based on the binary floating point formats provided by IEEE-754. Float16 , Float32 , Float64 and Float128 correspond exactly to those sized IEEE-754 formats, and have the semantics defined by IEEE-754. BFloat16 Carbon also supports the BFloat16 format, a 16-bit truncation of a \"binary32\" IEEE-754 format floating point number. Open questions Primitive types as code vs built-in There are open questions about the extent to which these types should be defined in Carbon code rather than special. Clearly they can't be directly implemented w/o help, but it might still be useful to force the programmer-observed interface to reside in code. However, this can cause difficulty with avoiding the need to import things gratuitously. String view vs owning string The right model of a string view versus an owning string is still very much unsettled. Syntax for wrapping operations Open question around allowing special syntax for wrapping operations (even on signed types) and/or requiring such syntax for wrapping operations on unsigned types. Non-power-of-two sizes Supporting non-power-of-two sizes is likely needed to have a clean model for bitfields, but requires more details to be worked out around memory access.","title":"Primitive types"},{"location":"design/primitive_types/#primitive-types","text":"","title":"Primitive types"},{"location":"design/primitive_types/#table-of-contents","text":"TODO Overview Integers Floats BFloat16 Open questions Primitive types as code vs built-in String view vs owning string Syntax for wrapping operations Non-power-of-two sizes","title":"Table of contents"},{"location":"design/primitive_types/#todo","text":"This is a skeletal design, added to support the overview . It should not be treated as accepted by the core team; rather, it is a placeholder until we have more time to examine this detail. Please feel welcome to rewrite and update as appropriate.","title":"TODO"},{"location":"design/primitive_types/#overview","text":"These types are fundamental to the language as they aren't either formed from or modifying other types. They also have semantics that are defined from first principles rather than in terms of other operations. These will be made available through the prelude package . Bool - a boolean type with two possible values: True and False . Int and UInt - signed and unsigned 64-bit integer types. Standard sizes are available, both signed and unsigned, including Int8 , Int16 , Int32 , Int128 , and Int256 . Overflow in either direction is an error. Float64 - a floating point type with semantics based on IEEE-754. Standard sizes are available, including Float16 , Float32 , and Float128 . BFloat16 is also provided. String - a byte sequence treated as containing UTF-8 encoded text. StringView - a read-only reference to a byte sequence treated as containing UTF-8 encoded text.","title":"Overview"},{"location":"design/primitive_types/#integers","text":"Integer types can be either signed or unsigned, much like in C++. Signed integers are represented using 2's complement and notionally modeled as unbounded natural numbers. Overflow in either direction is an error. That includes unsigned integers, differing from C++. The default size for both is 64-bits: Int and UInt . Specific sizes are also available, for example: Int8 , Int16 , Int32 , Int128 , UInt256 . Arbitrary powers of two above 8 are supported for both (although perhaps we'll want to avoid huge values for implementation simplicity).","title":"Integers"},{"location":"design/primitive_types/#floats","text":"Floating point types are based on the binary floating point formats provided by IEEE-754. Float16 , Float32 , Float64 and Float128 correspond exactly to those sized IEEE-754 formats, and have the semantics defined by IEEE-754.","title":"Floats"},{"location":"design/primitive_types/#bfloat16","text":"Carbon also supports the BFloat16 format, a 16-bit truncation of a \"binary32\" IEEE-754 format floating point number.","title":"BFloat16"},{"location":"design/primitive_types/#open-questions","text":"","title":"Open questions"},{"location":"design/primitive_types/#primitive-types-as-code-vs-built-in","text":"There are open questions about the extent to which these types should be defined in Carbon code rather than special. Clearly they can't be directly implemented w/o help, but it might still be useful to force the programmer-observed interface to reside in code. However, this can cause difficulty with avoiding the need to import things gratuitously.","title":"Primitive types as code vs built-in"},{"location":"design/primitive_types/#string-view-vs-owning-string","text":"The right model of a string view versus an owning string is still very much unsettled.","title":"String view vs owning string"},{"location":"design/primitive_types/#syntax-for-wrapping-operations","text":"Open question around allowing special syntax for wrapping operations (even on signed types) and/or requiring such syntax for wrapping operations on unsigned types.","title":"Syntax for wrapping operations"},{"location":"design/primitive_types/#non-power-of-two-sizes","text":"Supporting non-power-of-two sizes is likely needed to have a clean model for bitfields, but requires more details to be worked out around memory access.","title":"Non-power-of-two sizes"},{"location":"design/templates/","text":"Templates Table of contents TODO Overview Types with template parameters Functions with template parameters Overloading Constraining templates with interfaces TODO This is a skeletal design, added to support the overview . It should not be treated as accepted by the core team; rather, it is a placeholder until we have more time to examine this detail. Please feel welcome to rewrite and update as appropriate. Overview Carbon templates follow the same fundamental paradigm as C++ templates: they are instantiated, resulting in late type checking, duck typing, and lazy binding. They both enable interoperability between Carbon and C++ and address some (hopefully limited) use cases where the type checking rigor imposed by generics isn't helpful. Types with template parameters When parameterizing a user-defined type, the parameters can be marked as template parameters. The resulting type-function will instantiate the parameterized definition with the provided arguments to produce a complete type when used. Note that only the parameters marked as having this template behavior are subject to full instantiation -- other parameters will be type checked and bound early to the extent possible. For example: class Stack(template T:! Type) { var storage: Array(T); fn Push[addr me: Self*](value: T); fn Pop[addr me: Self*]() -> T; } This both defines a parameterized type ( Stack ) and uses one ( Array ). Within the definition of the type, the template type parameter T can be used in all of the places a normal type would be used, and it will only by type checked on instantiation. Functions with template parameters Both deduced and explicit function parameters in Carbon can be marked as template parameters. When called, the arguments to these parameters trigger instantiation of the function definition, fully type checking and resolving that definition after substituting in the provided (or computed if deduced) arguments. The runtime call then passes the remaining arguments to the resulting complete definition. fn Convert[template T:! Type](source: T, template U:! Type) -> U { var converted: U = source; return converted; } fn Foo(i: i32) -> f32 { // Instantiates with the `T` deduced argument set to `i32` and the `U` // explicit argument set to `f32`, then calls with the runtime value `i`. return Convert(i, f32); } Here we deduce one type parameter and explicitly pass another. It is not possible to explicitly pass a deduced type parameter, instead the call site should cast or convert the argument to control the deduction. The explicit type is passed after a runtime parameter. While this makes that type unavailable to the declaration of that runtime parameter, it still is a template parameter and available to use as a type even within the remaining parts of the function declaration. Overloading An important feature of templates in C++ is the ability to customize how they end up specialized for specific types. Because template parameters (whether as type parameters or function parameters) are pattern matched, we expect to leverage pattern matching techniques to provide \"better match\" definitions that are selected analogously to specializations in C++ templates. When expressed through pattern matching, this may enable things beyond just template parameter specialization, but that is an area that we want to explore cautiously. Constraining templates with interfaces Because we consider only specific parameters to be templated and they could be individually migrated to a constrained interface using the generics system , constraining templates themselves may be less critical. Instead, we expect parameterized types and functions may use a mixture of generic parameters and templated parameters based on where they are constrained. However, if there are still use cases, we would like to explore applying the interface constraints of the generics system directly to template parameters rather than create a new constraint system.","title":"Templates"},{"location":"design/templates/#templates","text":"","title":"Templates"},{"location":"design/templates/#table-of-contents","text":"TODO Overview Types with template parameters Functions with template parameters Overloading Constraining templates with interfaces","title":"Table of contents"},{"location":"design/templates/#todo","text":"This is a skeletal design, added to support the overview . It should not be treated as accepted by the core team; rather, it is a placeholder until we have more time to examine this detail. Please feel welcome to rewrite and update as appropriate.","title":"TODO"},{"location":"design/templates/#overview","text":"Carbon templates follow the same fundamental paradigm as C++ templates: they are instantiated, resulting in late type checking, duck typing, and lazy binding. They both enable interoperability between Carbon and C++ and address some (hopefully limited) use cases where the type checking rigor imposed by generics isn't helpful.","title":"Overview"},{"location":"design/templates/#types-with-template-parameters","text":"When parameterizing a user-defined type, the parameters can be marked as template parameters. The resulting type-function will instantiate the parameterized definition with the provided arguments to produce a complete type when used. Note that only the parameters marked as having this template behavior are subject to full instantiation -- other parameters will be type checked and bound early to the extent possible. For example: class Stack(template T:! Type) { var storage: Array(T); fn Push[addr me: Self*](value: T); fn Pop[addr me: Self*]() -> T; } This both defines a parameterized type ( Stack ) and uses one ( Array ). Within the definition of the type, the template type parameter T can be used in all of the places a normal type would be used, and it will only by type checked on instantiation.","title":"Types with template parameters"},{"location":"design/templates/#functions-with-template-parameters","text":"Both deduced and explicit function parameters in Carbon can be marked as template parameters. When called, the arguments to these parameters trigger instantiation of the function definition, fully type checking and resolving that definition after substituting in the provided (or computed if deduced) arguments. The runtime call then passes the remaining arguments to the resulting complete definition. fn Convert[template T:! Type](source: T, template U:! Type) -> U { var converted: U = source; return converted; } fn Foo(i: i32) -> f32 { // Instantiates with the `T` deduced argument set to `i32` and the `U` // explicit argument set to `f32`, then calls with the runtime value `i`. return Convert(i, f32); } Here we deduce one type parameter and explicitly pass another. It is not possible to explicitly pass a deduced type parameter, instead the call site should cast or convert the argument to control the deduction. The explicit type is passed after a runtime parameter. While this makes that type unavailable to the declaration of that runtime parameter, it still is a template parameter and available to use as a type even within the remaining parts of the function declaration.","title":"Functions with template parameters"},{"location":"design/templates/#overloading","text":"An important feature of templates in C++ is the ability to customize how they end up specialized for specific types. Because template parameters (whether as type parameters or function parameters) are pattern matched, we expect to leverage pattern matching techniques to provide \"better match\" definitions that are selected analogously to specializations in C++ templates. When expressed through pattern matching, this may enable things beyond just template parameter specialization, but that is an area that we want to explore cautiously.","title":"Overloading"},{"location":"design/templates/#constraining-templates-with-interfaces","text":"Because we consider only specific parameters to be templated and they could be individually migrated to a constrained interface using the generics system , constraining templates themselves may be less critical. Instead, we expect parameterized types and functions may use a mixture of generic parameters and templated parameters based on where they are constrained. However, if there are still use cases, we would like to explore applying the interface constraints of the generics system directly to template parameters rather than create a new constraint system.","title":"Constraining templates with interfaces"},{"location":"design/tuples/","text":"Tuples Table of contents TODO Overview Empty tuples Indices as compile-time constants Operations performed field-wise Open questions Slicing ranges Single-value tuples Function pattern match Type vs tuple of types TODO This is a skeletal design, added to support the overview . It should not be treated as accepted by the core team; rather, it is a placeholder until we have more time to examine this detail. Please feel welcome to rewrite and update as appropriate. Overview The primary composite type involves simple aggregation of other types as a tuple (called a \"product type\" in formal type theory): fn DoubleBoth(x: i32, y: i32) -> (i32, i32) { return (2 * x, 2 * y); } This function returns a tuple of two integers represented by the type (i32, i32) . The expression to return it uses a special tuple syntax to build a tuple within an expression: (<expression>, <expression>) . This is actually the same syntax in both cases. The return type is a tuple expression, and the first and second elements are expressions referring to the i32 type. The only difference is the type of these expressions. Both are tuples, but one is a tuple of types. Element access uses subscript syntax: fn Bar(x: i32, y: i32) -> i32 { var t: (i32, i32) = (x, y); return t[0] + t[1]; } Tuples also support multiple indices and slicing to restructure tuple elements: fn Baz(x: i32, y: i32, z: i32) -> (i32, i32) { var t1: (i32, i32, i32) = (x, y, z); var t2: (i32, i32, i32) = t1[(2, 1, 0)]; return t2[0 .. 2]; } This code first reverses the tuple, and then extracts a slice using a half-open range of indices. Empty tuples () is the empty tuple. This is used in other parts of the design, such as functions . Indices as compile-time constants In the example t1[(2, 1, 0)] , we will likely want to restrict these indices to compile-time constants. Without that, run-time indexing would need to suddenly switch to a variant-style return type to handle heterogeneous tuples. This would both be surprising and complex for little or no value. Operations performed field-wise Like some other aggregate data types like struct types , there are some operations are defined for tuples field-wise: initialization assignment equality and inequality comparison ordered comparison implicit conversion for argument passing destruction For binary operations, the two tuples must have the same number of components and the operation must be defined for the corresponding component types of the two tuples. References: The rules for assignment, comparison, and implicit conversion for argument passing were decided in question-for-leads issue #710 . Open questions Slicing ranges The intent of 0 .. 2 is to be syntax for forming a sequence of indices based on the half-open range [0, 2). There are a bunch of questions we'll need to answer here: Is this valid anywhere? Only some places? What is the sequence? If it is a tuple of indices, maybe that solves the above issue, and unlike function call indexing with multiple indices is different from indexing with a tuple of indexes. Do we need syntax for a closed range ( ... perhaps, unclear if that ends up aligned or in conflict with other likely uses of ... in pattern matching)? All of these syntaxes are also very close to 0.2 , is that similarity of syntax OK? Do we want to require the .. to be surrounded by whitespace to minimize that collision? Single-value tuples This remains an area of active investigation. There are serious problems with all approaches here. Without the collapse of one-tuples to scalars we need to distinguish between a parenthesized expression ( (42) ) and a one tuple (in Python or Rust, (42,) ), and if we distinguish them then we cannot model a function call as simply a function name followed by a tuple of arguments; one of f(0) and f(0,) becomes a special case. With the collapse, we either break genericity by forbidding (42)[0] from working, or it isn't clear what it means to access a nested tuple's first element from a parenthesized expression: ((1, 2))[0] . Function pattern match There are some interesting corner cases we need to expand on to fully and more precisely talk about the exact semantic model of function calls and their pattern match here, especially to handle variadic patterns and forwarding of tuples as arguments. We are hoping for a purely type system answer here without needing templates to be directly involved outside the type system as happens in C++ variadics. Type vs tuple of types Is (i32, i32) a type, a tuple of types, or is there even a difference between the two? Is different syntax needed for these cases?","title":"Tuples"},{"location":"design/tuples/#tuples","text":"","title":"Tuples"},{"location":"design/tuples/#table-of-contents","text":"TODO Overview Empty tuples Indices as compile-time constants Operations performed field-wise Open questions Slicing ranges Single-value tuples Function pattern match Type vs tuple of types","title":"Table of contents"},{"location":"design/tuples/#todo","text":"This is a skeletal design, added to support the overview . It should not be treated as accepted by the core team; rather, it is a placeholder until we have more time to examine this detail. Please feel welcome to rewrite and update as appropriate.","title":"TODO"},{"location":"design/tuples/#overview","text":"The primary composite type involves simple aggregation of other types as a tuple (called a \"product type\" in formal type theory): fn DoubleBoth(x: i32, y: i32) -> (i32, i32) { return (2 * x, 2 * y); } This function returns a tuple of two integers represented by the type (i32, i32) . The expression to return it uses a special tuple syntax to build a tuple within an expression: (<expression>, <expression>) . This is actually the same syntax in both cases. The return type is a tuple expression, and the first and second elements are expressions referring to the i32 type. The only difference is the type of these expressions. Both are tuples, but one is a tuple of types. Element access uses subscript syntax: fn Bar(x: i32, y: i32) -> i32 { var t: (i32, i32) = (x, y); return t[0] + t[1]; } Tuples also support multiple indices and slicing to restructure tuple elements: fn Baz(x: i32, y: i32, z: i32) -> (i32, i32) { var t1: (i32, i32, i32) = (x, y, z); var t2: (i32, i32, i32) = t1[(2, 1, 0)]; return t2[0 .. 2]; } This code first reverses the tuple, and then extracts a slice using a half-open range of indices.","title":"Overview"},{"location":"design/tuples/#empty-tuples","text":"() is the empty tuple. This is used in other parts of the design, such as functions .","title":"Empty tuples"},{"location":"design/tuples/#indices-as-compile-time-constants","text":"In the example t1[(2, 1, 0)] , we will likely want to restrict these indices to compile-time constants. Without that, run-time indexing would need to suddenly switch to a variant-style return type to handle heterogeneous tuples. This would both be surprising and complex for little or no value.","title":"Indices as compile-time constants"},{"location":"design/tuples/#operations-performed-field-wise","text":"Like some other aggregate data types like struct types , there are some operations are defined for tuples field-wise: initialization assignment equality and inequality comparison ordered comparison implicit conversion for argument passing destruction For binary operations, the two tuples must have the same number of components and the operation must be defined for the corresponding component types of the two tuples. References: The rules for assignment, comparison, and implicit conversion for argument passing were decided in question-for-leads issue #710 .","title":"Operations performed field-wise"},{"location":"design/tuples/#open-questions","text":"","title":"Open questions"},{"location":"design/tuples/#slicing-ranges","text":"The intent of 0 .. 2 is to be syntax for forming a sequence of indices based on the half-open range [0, 2). There are a bunch of questions we'll need to answer here: Is this valid anywhere? Only some places? What is the sequence? If it is a tuple of indices, maybe that solves the above issue, and unlike function call indexing with multiple indices is different from indexing with a tuple of indexes. Do we need syntax for a closed range ( ... perhaps, unclear if that ends up aligned or in conflict with other likely uses of ... in pattern matching)? All of these syntaxes are also very close to 0.2 , is that similarity of syntax OK? Do we want to require the .. to be surrounded by whitespace to minimize that collision?","title":"Slicing ranges"},{"location":"design/tuples/#single-value-tuples","text":"This remains an area of active investigation. There are serious problems with all approaches here. Without the collapse of one-tuples to scalars we need to distinguish between a parenthesized expression ( (42) ) and a one tuple (in Python or Rust, (42,) ), and if we distinguish them then we cannot model a function call as simply a function name followed by a tuple of arguments; one of f(0) and f(0,) becomes a special case. With the collapse, we either break genericity by forbidding (42)[0] from working, or it isn't clear what it means to access a nested tuple's first element from a parenthesized expression: ((1, 2))[0] .","title":"Single-value tuples"},{"location":"design/tuples/#function-pattern-match","text":"There are some interesting corner cases we need to expand on to fully and more precisely talk about the exact semantic model of function calls and their pattern match here, especially to handle variadic patterns and forwarding of tuples as arguments. We are hoping for a purely type system answer here without needing templates to be directly involved outside the type system as happens in C++ variadics.","title":"Function pattern match"},{"location":"design/tuples/#type-vs-tuple-of-types","text":"Is (i32, i32) a type, a tuple of types, or is there even a difference between the two? Is different syntax needed for these cases?","title":"Type vs tuple of types"},{"location":"design/type_inference/","text":"Type inference Table of contents Overview Open questions Inferring a variable type from literals Alternatives considered References Overview Type inference occurs in Carbon when the auto keyword is used. This may occur in variable declarations or function declarations . At present, type inference is very simple: given the expression which generates the value to be used for type inference, the inferred type is the precise type of that expression. For example, the inferred type for auto in fn Foo(x: i64) -> auto { return x; } is i64 . Type inference is currently supported for function return types and declared variable types . Open questions Inferring a variable type from literals Using the type on the right side for var y: auto = 1 currently results in a constant IntLiteral(1) value, whereas most languages would suggest a variable integer type, such as i64 . Carbon might also make it an error. Although type inference currently only addresses auto for variables and function return types, this is something that will be considered as part of type inference in general, because it also affects generics, templates, lambdas, and return types. Alternatives considered Use _ instead of auto References Proposal #851: auto keyword for vars","title":"Type inference"},{"location":"design/type_inference/#type-inference","text":"","title":"Type inference"},{"location":"design/type_inference/#table-of-contents","text":"Overview Open questions Inferring a variable type from literals Alternatives considered References","title":"Table of contents"},{"location":"design/type_inference/#overview","text":"Type inference occurs in Carbon when the auto keyword is used. This may occur in variable declarations or function declarations . At present, type inference is very simple: given the expression which generates the value to be used for type inference, the inferred type is the precise type of that expression. For example, the inferred type for auto in fn Foo(x: i64) -> auto { return x; } is i64 . Type inference is currently supported for function return types and declared variable types .","title":"Overview"},{"location":"design/type_inference/#open-questions","text":"","title":"Open questions"},{"location":"design/type_inference/#inferring-a-variable-type-from-literals","text":"Using the type on the right side for var y: auto = 1 currently results in a constant IntLiteral(1) value, whereas most languages would suggest a variable integer type, such as i64 . Carbon might also make it an error. Although type inference currently only addresses auto for variables and function return types, this is something that will be considered as part of type inference in general, because it also affects generics, templates, lambdas, and return types.","title":"Inferring a variable type from literals"},{"location":"design/type_inference/#alternatives-considered","text":"Use _ instead of auto","title":"Alternatives considered"},{"location":"design/type_inference/#references","text":"Proposal #851: auto keyword for vars","title":"References"},{"location":"design/variables/","text":"Variables Table of contents Overview Notes Global variables Alternatives considered References Overview Carbon's local variable syntax is: var identifier : < expression | auto > [ = value ] ; Blocks introduce nested scopes and can contain local variable declarations that work similarly to function parameters. For example: fn Foo() { var x: i32 = 42; } This introduces a local variable named x into the block's scope. It has the type Int and is initialized with the value 42 . These variable declarations (and function declarations) have a lot more power than what we're covering just yet, but this gives you the basic idea. If auto is used in place of the type, type inference is used to automatically determine the variable's type. While there can be global constants, there are no global variables. Notes TODO: Constant syntax is an ongoing discussion. Global variables We are exploring several different ideas for how to design less bug-prone patterns to replace the important use cases programmers still have for global variables. We may be unable to fully address them, at least for migrated code, and be forced to add some limited form of global variables back. We may also discover that their convenience outweighs any improvements afforded. Alternatives considered No var introducer keyword Name of the var statement introducer Colon between type and identifier Type elision Type ordering Elide the type instead of using auto References Proposal #339: var statement Proposal #618: var ordering Proposal #851: auto keyword for vars","title":"Variables"},{"location":"design/variables/#variables","text":"","title":"Variables"},{"location":"design/variables/#table-of-contents","text":"Overview Notes Global variables Alternatives considered References","title":"Table of contents"},{"location":"design/variables/#overview","text":"Carbon's local variable syntax is: var identifier : < expression | auto > [ = value ] ; Blocks introduce nested scopes and can contain local variable declarations that work similarly to function parameters. For example: fn Foo() { var x: i32 = 42; } This introduces a local variable named x into the block's scope. It has the type Int and is initialized with the value 42 . These variable declarations (and function declarations) have a lot more power than what we're covering just yet, but this gives you the basic idea. If auto is used in place of the type, type inference is used to automatically determine the variable's type. While there can be global constants, there are no global variables.","title":"Overview"},{"location":"design/variables/#notes","text":"TODO: Constant syntax is an ongoing discussion.","title":"Notes"},{"location":"design/variables/#global-variables","text":"We are exploring several different ideas for how to design less bug-prone patterns to replace the important use cases programmers still have for global variables. We may be unable to fully address them, at least for migrated code, and be forced to add some limited form of global variables back. We may also discover that their convenience outweighs any improvements afforded.","title":"Global variables"},{"location":"design/variables/#alternatives-considered","text":"No var introducer keyword Name of the var statement introducer Colon between type and identifier Type elision Type ordering Elide the type instead of using auto","title":"Alternatives considered"},{"location":"design/variables/#references","text":"Proposal #339: var statement Proposal #618: var ordering Proposal #851: auto keyword for vars","title":"References"},{"location":"design/code_and_name_organization/","text":"Code and name organization Table of contents Goals and philosophy Overview Sizing packages and libraries Imports Details Source file introduction Name paths package directives Packages Shorthand notation for libraries in packages Package name conflicts Libraries Exporting entities from an API file Granularity of libraries Exporting namespaces Imports Imports from the current package Namespaces Re-declaring imported namespaces Aliasing Caveats Package and library name conflicts Potential refactorings Update imports Between api and impl files Other refactorings Preference for few child namespaces Redundant markers Open questions Different file extensions Imports from other languages Imports from URLs Test file type Alternatives considered References Goals and philosophy Important Carbon goals for code and name organization are: Language tools and ecosystem Tooling support is important for Carbon, including the possibility of a package manager. Developer tooling, including both IDEs and refactoring tools, are expected to exist and be well-supported. Software and language evolution : We should support libraries adding new structs, functions or other identifiers without those new identifiers being able to shadow or break existing users that already have identifiers with conflicting names. We should make it easy to refactor code, including moving code between files. This includes refactoring both by humans and by developer tooling. Fast and scalable development : It should be easy for developer tooling to parse code, without needing to parse imports for context. Structure should be provided for large projects to opt into features which will help maintain scaling of their codebase, while not adding burdens to small projects that don't need it. Overview Carbon source files have a .carbon extension, such as geometry.carbon . These files are the basic unit of compilation. Each file begins with a declaration of which package [ define ] it belongs in. The package is the unit of distribution . The package name is a single identifier, such as Geometry . An example API file in the Geometry package would start with: package Geometry api; A tiny package may consist of a single library with a single file, and not use any further features of the package keyword. It is often useful to use separate files for the API and its implementation. This may help organize code as a library grows, or to let the build system distinguish between the dependencies of the API itself and its underlying implementation. Implementation files allow for code to be extracted out from the API file, while only being callable from other files within the library, including both API and implementation files. Implementation files are marked by both naming the file to use an extension of .impl.carbon and instead start with: package Geometry impl; However, as a package adds more files, it will probably want to separate out into multiple libraries [ define ] . A library is the basic unit of dependency . Separating code into multiple libraries can speed up the overall build while also making it clear which code is being reused. For example, an API file adding the library Shapes to the Geometry package, or Geometry//Shapes in shorthand , would start with: package Geometry library \"Shapes\" api; As code becomes more complex, and users pull in more code, it may also be helpful to add namespaces [ define ] to give related entities consistently structured names. A namespace affects the name path [ define ] used when calling code. For example, with no namespace, if a Geometry package defines Circle then the name path will be Geometry.Circle . However, it can be named Geometry.TwoDimensional.Circle with a namespace ; for example: package Geometry library \"Shapes\" api; namespace TwoDimensional; struct TwoDimensional.Circle { ... }; This scaling of packages into libraries and namespaces is how Carbon supports both small and large codebases. Sizing packages and libraries A different way to think of the sizing of packages and libraries is: A package is a GitHub repository. Small and medium projects that fit in a single repository will typically have a single package. For example, a medium-sized project like Abseil could still use a single Abseil package. Large projects will have multiple packages. For example, Mozilla may have multiple packages for Firefox and other efforts. A library is a few files that provide an interface and implementation, and should remain small. Small projects will have a single library when it's easy to maintain all code in a few files. Medium and large projects will have multiple libraries. For example, Boost Geometry's Distance interface and implementation might be its own library within Boost , with dependencies on other libraries in Boost and potentially other packages from Boost. Library names could be named after the feature, such as library \"Algorithms\" , or include part of the path to reduce the chance of name collisions, such as library \"Geometry/Algorithms\" . Packages may choose to expose libraries that expose unions of interfaces from other libraries within the package. However, doing so would also provide the transitive closure of build-time dependencies, and is likely to be discouraged in many cases. Imports The import keyword supports reusing code from other files and libraries. For example, to use Geometry.Circle from the Geometry//Shapes library: import Geometry library \"Shapes\"; fn Area(circle: Geometry.Circle) { ... }; The library keyword is optional for import , and its use should parallel that of library on the package of the code being imported. Details Source file introduction Every source file will consist of, in order: One package directive. A section of zero or more import directives. Source file body, with other code. Comments and blank lines may be intermingled with these sections. Metaprogramming code may also be intermingled, so long as the outputted code is consistent with the enforced ordering. Other types of code must be in the source file body. Name paths Name paths are defined above as sequences of identifiers separated by dots. This syntax may be loosely expressed as a regular expression: IDENTIFIER(\\.IDENTIFIER)* Name conflicts are addressed by name lookup . package directives Packages The package directive's syntax may be loosely expressed as a regular expression: package IDENTIFIER (library STRING)? (api|impl); For example: package Geometry library \"Objects/FourSides\" api; Breaking this apart: The identifier passed to the package keyword, Geometry , is the package name and will prefix both library and namespace paths. The package keyword also declares a package entity matching the package name. A package entity is almost identical to a namespace entity, except with some package/import-specific handling. In other words, if the file declares struct Line , that may be used from within the file as both Line directly and Geometry.TwoDimensional.Line using the Geometry package entity created by the package keyword. When the optional library keyword is specified, sets the name of the library within the package. In this example, the Geometry//Objects/FourSides library will be used. The use of the api keyword indicates this is an API files as described under libraries . If it instead had impl , this would be an implementation file. Because every file must have exactly one package directive, there are a couple important and deliberate side-effects: Every file will be in precisely one library. A library still exists even when there is no explicit library argument, such as package Geometry api; . This could be considered equivalent to package Geometry library \"\" api; , although we should not allow that specific syntax as error-prone. Every entity in Carbon will be in a namespace, even if its namespace path consists of only the package name. There is no \"global\" namespace. Every entity in a file will be defined within the namespace described in the package directive. Entities within a file may be defined in child namespaces . Files contributing to the Geometry//Objects/FourSides library must all start with package Geometry library \"Objects/FourSides\" , but will differ on api / impl types. Shorthand notation for libraries in packages Library names may also be referred to as PACKAGE//LIBRARY as shorthand in text. PACKAGE//default will refer to the name of the library used when no library argument is specified, although PACKAGE may also be used in situations where it is unambiguous that it still refers to the default library. It's recommended that libraries use a single / for separators where desired, in order to distinguish between the // of the package and / separating library segments. For example, Geometry//Objects/FourSides uses a single / to separate the Object/FourSides library name. Package name conflicts Because the package also declares a namespace entity with the same name, conflicts with the package name are possible. We do not support packages providing entities with the same name as the package. For example, this is a conflict for DateTime : package DateTime api; struct DateTime { ... }; This declaration is important for implementation files , which implicitly import the library's API, because it keeps the package name as an explicit entity in source files. Note that imported name conflicts are handled differently. Libraries Every Carbon library consists of one or more files. Each Carbon library has a primary file that defines its API, and may optionally contain additional files that are implementation. An API file's package directive will have api . For example, package Geometry library \"Shapes\" api; API filenames must have the .carbon extension. They must not have a .impl.carbon extension. API file paths will correspond to the library name. The precise form of this correspondence is undetermined, but should be expected to be similar to a \"Math/Algebra\" library being in a \"Math/Algebra.carbon\" file path. The package will not be used when considering the file path. An implementation file's package directive will have impl . For example, package Geometry library \"Shapes\" impl; . Implementation filenames must have the .impl.carbon extension. Implementation file paths need not correspond to the library name. Implementation files implicitly import the library's API. Implementation files cannot import each other. There is no facility for file or non- api imports. The difference between API and implementation will act as a form of access control. API files must compile independently of implementation, only importing from APIs from other libraries. API files are also visible to all files and libraries for import. Implementation files only see API files for import, not other implementation files. When any file imports a library's API, it should be expected that the transitive closure of imported files from the primary API file will be a compilation dependency. The size of that transitive closure affects compilation time, so libraries with complex implementations should endeavor to minimize their API imports. Libraries also serve as a critical unit of compilation. Dependencies between libraries must be clearly marked, and the resulting dependency graph will allow for separate compilation. Exporting entities from an API file Entities in the API file are part of the library's public API by default. They may be marked as private to indicate they should only be visible to other parts of the library. package Geometry library \"Shapes\" api; // Circle is an API, and will be available to other libraries as Geometry.Circle. struct Circle { ... } // CircleHelper is private, and so will not be available to other libraries. private fn CircleHelper(circle: Circle) { ... } // Only entities in namespaces should be marked as an API, not the namespace // itself. namespace Operations; // Operations.GetCircumference is an API, and will be available to // other libraries as Geometry.Operations.GetCircumference. fn Operations.GetCircumference(circle: Circle) { ... } This means that an API file can contain all implementation code for a library. However, separate implementation files are still desirable for a few reasons: It will be easier for readers to quickly scan an API-only file for API documentation. Reducing the amount of code in an API file can speed up compilation, especially if fewer imports are needed. This can result in transitive compilation performance improvements for files using the library. From a code maintenance perspective, having smaller files can make a library more maintainable. Entities in the impl file should never have visibility keywords. If they are forward declared in the api file, they use the declaration's visibility; if they are only present in the impl file, they are implicitly private . Granularity of libraries The compilation graph of Carbon will generally consist of api files depending on each other, and impl files depending only on api files. Compiling a given file requires compiling the transitive closure of api files first. Parallelization of compilation is then limited by how large that transitive closure is, in terms of total volume of code rather than quantity. This also affects build cache invalidation. In order to maximize opportunities to improve compilation performance, we will encourage granular libraries. Conceptually, we want libraries to be very small, possibly containing only a single class. The choice of only allowing a single api file per library should help encourage developers to write small libraries. Exporting namespaces Any entity may be marked with api except for namespace and package entities. That is, api namespace Sha256; is invalid code. Instead, namespaces are implicitly exported based on the name paths of other entities marked as api . For example, given this code: package Checksums library \"Sha\" api; namespaces Sha256; fn Sha256.HexDigest(data: Bytes) -> String { ... } Calling code may look like: package Caller api; import Checksums library \"Sha\"; fn Process(data: Bytes) { ... var digest: String = Checksums.Sha256.HexDigest(data); ... } In this example, the Sha256 namespace is exported as part of the API implicitly. Imports import directives supports reusing code from other files and libraries. The import directive's syntax may be loosely expressed as a regular expression: import IDENTIFIER (library NAME_PATH)?; An import declares a package entity named after the imported package, and makes API entities from the imported library available through it. The full name path is a concatenation of the names of the package entity, any namespace entities applied, and the final entity addressed. Child namespaces or entities may be aliased if desired. For example, given a library: package Math api; namespace Trigonometry; fn Trigonometry.Sin(...); Calling code would import it and use it like: package Geometry api; import Math; fn DoSomething() { ... Math.Trigonometry.Sin(...); ... } Repeat imports from the same package reuse the same package entity. For example, this produces only one Math package entity: import Math; import Math library \"Trigonometry\"; NOTE: A library must never import itself. Any impl files in a library automatically import the api , so a self-import should never be required. Imports from the current package Entities defined in the current file may be used without mentioning the package prefix. However, other symbols from the package must be imported and accessed through the package namespace just like symbols from any other package. For example: package Geometry api; // This is required even though it's still in the Geometry package. import Geometry library \"Shapes\"; // Circle must be referenced using the Geometry namespace of the import. fn GetArea(c: Geometry.Circle) { ... } Namespaces Namespaces offer named paths for entities. Namespaces may be nested. Multiple libraries may contribute to the same namespace. In practice, packages may have namespaces such as Testing containing entities that benefit from an isolated space but are present in many libraries. The namespace keyword's syntax may loosely be expressed as a regular expression: namespace NAME_PATH; The namespace keyword declares a namespace entity. The namespace is applied to other entities by including it as a prefix when declaring a name. For example: package Time; namespace Timezones.Internal; struct Timezones.Internal.RawData { ... } fn ParseData(data: Timezones.Internal.RawData); A namespace declaration adds the first identifier in the name path as a name in the file's namespace. In the above example, after declaring namespace Timezones.Internal; , Timezones is available as an identifier and Internal is reached through Timezones . Re-declaring imported namespaces Namespaces may exist on imported package entities, in addition to being declared in the current file. However, even if the namespace already exists in an imported library from the current package, the namespace must still be declared locally in order to add symbols to it. For example, if the Geometry//Shapes/ThreeSides library provides the Geometry.Shapes namespace, this code is still valid: package Geometry library \"Shapes/FourSides\" api; import Geometry library \"Shapes/ThreeSides\"; // This does not conflict with the existence of `Geometry.Shapes` from // `Geometry//Shapes/ThreeSides`, even though the name path is identical. namespace Shapes; // This requires the above 'namespace Shapes' declaration. It cannot use // `Geometry.Shapes` from `Geometry//Shapes/ThreeSides`. struct Shapes.Square { ... }; Aliasing Carbon's alias keyword will support aliasing namespaces. For example, this would be valid code: namespace Timezones.Internal; alias TI = Timezones.internal; struct TI.RawData { ... } fn ParseData(data: TI.RawData); Caveats Package and library name conflicts Library name conflicts should not occur, because it's expected that a given package is maintained by a single organization. It's the responsibility of that organization to maintain unique library names within their package. A package name conflict occurs when two different packages use the same name, such as two packages named Stats . Versus libraries, package name conflicts are more likely because two organizations may independently choose identical names. We will encourage a unique package naming scheme, such as maintaining a name server for open source packages. Conflicts can also be addressed by renaming one of the packages, either at the source, or as a local modification. We do need to address the case of package names conflicting with other entity names. It's possible that a pre-existing entity will conflict with a new import, and that renaming the entity is infeasible to rename due to existing callers. Alternately, the entity may be using an idiomatic name that it would contradict naming conventions to rename. In either case, this conflict may exist in a single file without otherwise affecting users of the API. This will be addressed by name lookup . Potential refactorings These are potential refactorings that we consider important to make it easy to automate. Update imports Imports will frequently need to be updated as part of refactorings. When code is deleted, it should be possible to parse the remaining code, parse the imports, and determine which entities in imports are referred to. Unused imports can then be removed. When code is moved, it's similar to deletion in the originating file. For the destination file, the moved code should be parsed to determine which entities it referred to from the originating file's imports, and these will need to be included in the destination file: either reused if already present, or added. When new code is added, existing imports can be checked to see if they provide the symbol in question. There may also be heuristics which can be implemented to check build dependencies for where imports should be added from, such as a database of possible entities and their libraries. However, adding references may require manually adding imports. Between api and impl files Move an implementation of an API from an api file to an impl file, while leaving a declaration behind. This should be a local change that will not affect any calling code. Inlining will be affected because the implementation won't be visible to callers. Update imports . Split an api and impl file. This is a repeated operation of individual API moves, as noted above. Move an implementation of an API from an impl file to an api file. This should be a local change that will not affect any calling code. Inlining will be affected because the implementation becomes visible to callers. Update imports . Combine an api and impl file. This is a repeated operation of individual API moves, as noted above. Remove the api label from a declaration. Search for library-external callers, and fix them first. Add the api label to a declaration. This should be a local change that will not affect any calling code. Move a non- api -labeled declaration from an api file to an impl file. The declaration must be moved to the same file as the implementation of the declaration. The declaration can only be used by the impl file that now contains it. Search for other callers within the library, and fix them first. Update imports . Move a non- api -labeled declaration from an impl file to an api file. This should be a local change that will not affect any calling code. Update imports . Move a declaration and implementation from one impl file to another. Search for any callers within the source impl file, and either move them too, or fix them first. Update imports . Other refactorings Rename a package. The imports of all calling files must be updated accordingly. All call sites must be changed, as the package name changes. Update imports . Move an api -labeled declaration and implementation between different packages. The imports of all calling files must be updated accordingly. All call sites must be changed, as the package name changes. Update imports . Move an api -labeled declaration and implementation between libraries in the same package. The imports of all calling files must be updated accordingly. As long as the namespaces remain the same, no call sites will need to be changed. Update imports . Rename a library. This is equivalent to a repeated operation of moving an api -labeled declaration and implementation between libraries in the same package. Move a declaration and implementation from one namespace to another. Ensure the new namespace is declared for the declaration and implementation. Update the namespace used by call sites. The imports of all calling files may remain the same. Rename a namespace. This is equivalent to a repeated operation of moving a declaration and implementation from one namespace to another. Rename a file, or move a file between directories. Build configuration will need to be updated. This additionally requires the steps to rename a library, because library names must correspond to the renamed paths. Preference for few child namespaces We expect that most code should use a package and library, but avoid specifying namespaces beneath the package. The package name itself should typically be sufficient distinction for names. Child namespaces create longer names, which engineers will dislike typing. Based on experience, we expect to start seeing aliasing even at name lengths around six characters long. With longer names, we should expect more aliasing, which in turn will reduce code readability because more types will have local names. We believe it's feasible for even large projects to collapse namespaces down to a top level, avoiding internal tiers of namespaces. We understand that child namespaces are sometimes helpful, and will robustly support them for that. However, we will model code organization to encourage fewer namespaces. Redundant markers We use a few possibly redundant markers for packages and libraries: The package keyword requires one of api and impl , rather than excluding either or both. The filename repeats the api versus impl choice. The import keyword requires the full library. These choices are made to assist human readability and tooling: Being explicit about imports creates the opportunity to generate build dependencies from files, rather than having them maintained separately. Being explicit about api versus impl makes it easier for both humans and tooling to determine what to expect. Repeating the type in the filename makes it possible to check the type without reading file content. Repeating the type in the file content makes non-file-system-based builds possible. Open questions These open questions are expected to be revisited by future proposals. Different file extensions Currently, we're using .carbon and .impl.carbon . In the future, we may want to change the extension, particularly because Carbon may be renamed. There are several other possible extensions / commands that we've considered in coming to the current extension: .carbon : This is an obvious and unsurprising choice, but also quite long for a file extension. .6c : This sounds a little like 'sexy' when read aloud. .c6 : This seems a weird incorrect ordering of the atomic number and has a bad, if obscure, Internet slang association. .cb or .cbn : These collide with several acronyms and may not be especially memorable as referring to Carbon. .crb : This has a bad Internet slang association. Imports from other languages Currently, we do not support cross-language imports. In the future, we will likely want to support imports from other languages, particularly for C++ interoperability. To fit into the proposed import syntax, we are provisionally using a special Cpp package to import headers from C++ code, as in: import Cpp library \"<map>\"; import Cpp library \"myproject/myclass.h\"; fn MyCarbonCall(x: Cpp.std.map(Cpp.MyProject.MyClass)); Imports from URLs Currently, we don't support any kind of package management with imports. In the future, we may want to support tagging imports with a URL that identifies the repository where that package can be found. This can be used to help drive package management tooling and to support providing a non-name identity for a package that is used to enable handling conflicted package names. Although we're not designing this right now, it could fit into the proposed syntax. For example: import Carbon library \"Utilities\" url(\"https://github.com/carbon-language/carbon-libraries\"); Test file type Similar to api and impl , we may eventually want a type like test . This should be part of a larger testing plan. Alternatives considered Packages Name paths for package names Referring to the package as package Remove the library keyword from package and import Rename package concept No association between the file system path and library/namespace Libraries Allow exporting namespaces Allow importing implementation files from within the same library Alternative library separators and shorthand Single-word libraries Collapse API and implementation file concepts Automatically generating the API separation Collapse file and library concepts Collapse the library concept into packages Collapse the package concept into libraries Default api to private Default impl to public Different file type labels Function-like syntax Inlining from implementation files Library-private access controls Make keywords either optional or required in separate definitions Managing API versus implementation in libraries Multiple API files Name paths as library names Imports Block imports Block imports of libraries of a single package Broader imports, either all names or arbitrary code Direct name imports Optional package names Namespaces File-level namespaces Scoped namespaces References Proposal #107: Code and name organization","title":"Code and name organization"},{"location":"design/code_and_name_organization/#code-and-name-organization","text":"","title":"Code and name organization"},{"location":"design/code_and_name_organization/#table-of-contents","text":"Goals and philosophy Overview Sizing packages and libraries Imports Details Source file introduction Name paths package directives Packages Shorthand notation for libraries in packages Package name conflicts Libraries Exporting entities from an API file Granularity of libraries Exporting namespaces Imports Imports from the current package Namespaces Re-declaring imported namespaces Aliasing Caveats Package and library name conflicts Potential refactorings Update imports Between api and impl files Other refactorings Preference for few child namespaces Redundant markers Open questions Different file extensions Imports from other languages Imports from URLs Test file type Alternatives considered References","title":"Table of contents"},{"location":"design/code_and_name_organization/#goals-and-philosophy","text":"Important Carbon goals for code and name organization are: Language tools and ecosystem Tooling support is important for Carbon, including the possibility of a package manager. Developer tooling, including both IDEs and refactoring tools, are expected to exist and be well-supported. Software and language evolution : We should support libraries adding new structs, functions or other identifiers without those new identifiers being able to shadow or break existing users that already have identifiers with conflicting names. We should make it easy to refactor code, including moving code between files. This includes refactoring both by humans and by developer tooling. Fast and scalable development : It should be easy for developer tooling to parse code, without needing to parse imports for context. Structure should be provided for large projects to opt into features which will help maintain scaling of their codebase, while not adding burdens to small projects that don't need it.","title":"Goals and philosophy"},{"location":"design/code_and_name_organization/#overview","text":"Carbon source files have a .carbon extension, such as geometry.carbon . These files are the basic unit of compilation. Each file begins with a declaration of which package [ define ] it belongs in. The package is the unit of distribution . The package name is a single identifier, such as Geometry . An example API file in the Geometry package would start with: package Geometry api; A tiny package may consist of a single library with a single file, and not use any further features of the package keyword. It is often useful to use separate files for the API and its implementation. This may help organize code as a library grows, or to let the build system distinguish between the dependencies of the API itself and its underlying implementation. Implementation files allow for code to be extracted out from the API file, while only being callable from other files within the library, including both API and implementation files. Implementation files are marked by both naming the file to use an extension of .impl.carbon and instead start with: package Geometry impl; However, as a package adds more files, it will probably want to separate out into multiple libraries [ define ] . A library is the basic unit of dependency . Separating code into multiple libraries can speed up the overall build while also making it clear which code is being reused. For example, an API file adding the library Shapes to the Geometry package, or Geometry//Shapes in shorthand , would start with: package Geometry library \"Shapes\" api; As code becomes more complex, and users pull in more code, it may also be helpful to add namespaces [ define ] to give related entities consistently structured names. A namespace affects the name path [ define ] used when calling code. For example, with no namespace, if a Geometry package defines Circle then the name path will be Geometry.Circle . However, it can be named Geometry.TwoDimensional.Circle with a namespace ; for example: package Geometry library \"Shapes\" api; namespace TwoDimensional; struct TwoDimensional.Circle { ... }; This scaling of packages into libraries and namespaces is how Carbon supports both small and large codebases.","title":"Overview"},{"location":"design/code_and_name_organization/#sizing-packages-and-libraries","text":"A different way to think of the sizing of packages and libraries is: A package is a GitHub repository. Small and medium projects that fit in a single repository will typically have a single package. For example, a medium-sized project like Abseil could still use a single Abseil package. Large projects will have multiple packages. For example, Mozilla may have multiple packages for Firefox and other efforts. A library is a few files that provide an interface and implementation, and should remain small. Small projects will have a single library when it's easy to maintain all code in a few files. Medium and large projects will have multiple libraries. For example, Boost Geometry's Distance interface and implementation might be its own library within Boost , with dependencies on other libraries in Boost and potentially other packages from Boost. Library names could be named after the feature, such as library \"Algorithms\" , or include part of the path to reduce the chance of name collisions, such as library \"Geometry/Algorithms\" . Packages may choose to expose libraries that expose unions of interfaces from other libraries within the package. However, doing so would also provide the transitive closure of build-time dependencies, and is likely to be discouraged in many cases.","title":"Sizing packages and libraries"},{"location":"design/code_and_name_organization/#imports","text":"The import keyword supports reusing code from other files and libraries. For example, to use Geometry.Circle from the Geometry//Shapes library: import Geometry library \"Shapes\"; fn Area(circle: Geometry.Circle) { ... }; The library keyword is optional for import , and its use should parallel that of library on the package of the code being imported.","title":"Imports"},{"location":"design/code_and_name_organization/#details","text":"","title":"Details"},{"location":"design/code_and_name_organization/#source-file-introduction","text":"Every source file will consist of, in order: One package directive. A section of zero or more import directives. Source file body, with other code. Comments and blank lines may be intermingled with these sections. Metaprogramming code may also be intermingled, so long as the outputted code is consistent with the enforced ordering. Other types of code must be in the source file body.","title":"Source file introduction"},{"location":"design/code_and_name_organization/#name-paths","text":"Name paths are defined above as sequences of identifiers separated by dots. This syntax may be loosely expressed as a regular expression: IDENTIFIER(\\.IDENTIFIER)* Name conflicts are addressed by name lookup .","title":"Name paths"},{"location":"design/code_and_name_organization/#package-directives","text":"","title":"package directives"},{"location":"design/code_and_name_organization/#packages","text":"The package directive's syntax may be loosely expressed as a regular expression: package IDENTIFIER (library STRING)? (api|impl); For example: package Geometry library \"Objects/FourSides\" api; Breaking this apart: The identifier passed to the package keyword, Geometry , is the package name and will prefix both library and namespace paths. The package keyword also declares a package entity matching the package name. A package entity is almost identical to a namespace entity, except with some package/import-specific handling. In other words, if the file declares struct Line , that may be used from within the file as both Line directly and Geometry.TwoDimensional.Line using the Geometry package entity created by the package keyword. When the optional library keyword is specified, sets the name of the library within the package. In this example, the Geometry//Objects/FourSides library will be used. The use of the api keyword indicates this is an API files as described under libraries . If it instead had impl , this would be an implementation file. Because every file must have exactly one package directive, there are a couple important and deliberate side-effects: Every file will be in precisely one library. A library still exists even when there is no explicit library argument, such as package Geometry api; . This could be considered equivalent to package Geometry library \"\" api; , although we should not allow that specific syntax as error-prone. Every entity in Carbon will be in a namespace, even if its namespace path consists of only the package name. There is no \"global\" namespace. Every entity in a file will be defined within the namespace described in the package directive. Entities within a file may be defined in child namespaces . Files contributing to the Geometry//Objects/FourSides library must all start with package Geometry library \"Objects/FourSides\" , but will differ on api / impl types.","title":"Packages"},{"location":"design/code_and_name_organization/#shorthand-notation-for-libraries-in-packages","text":"Library names may also be referred to as PACKAGE//LIBRARY as shorthand in text. PACKAGE//default will refer to the name of the library used when no library argument is specified, although PACKAGE may also be used in situations where it is unambiguous that it still refers to the default library. It's recommended that libraries use a single / for separators where desired, in order to distinguish between the // of the package and / separating library segments. For example, Geometry//Objects/FourSides uses a single / to separate the Object/FourSides library name.","title":"Shorthand notation for libraries in packages"},{"location":"design/code_and_name_organization/#package-name-conflicts","text":"Because the package also declares a namespace entity with the same name, conflicts with the package name are possible. We do not support packages providing entities with the same name as the package. For example, this is a conflict for DateTime : package DateTime api; struct DateTime { ... }; This declaration is important for implementation files , which implicitly import the library's API, because it keeps the package name as an explicit entity in source files. Note that imported name conflicts are handled differently.","title":"Package name conflicts"},{"location":"design/code_and_name_organization/#libraries","text":"Every Carbon library consists of one or more files. Each Carbon library has a primary file that defines its API, and may optionally contain additional files that are implementation. An API file's package directive will have api . For example, package Geometry library \"Shapes\" api; API filenames must have the .carbon extension. They must not have a .impl.carbon extension. API file paths will correspond to the library name. The precise form of this correspondence is undetermined, but should be expected to be similar to a \"Math/Algebra\" library being in a \"Math/Algebra.carbon\" file path. The package will not be used when considering the file path. An implementation file's package directive will have impl . For example, package Geometry library \"Shapes\" impl; . Implementation filenames must have the .impl.carbon extension. Implementation file paths need not correspond to the library name. Implementation files implicitly import the library's API. Implementation files cannot import each other. There is no facility for file or non- api imports. The difference between API and implementation will act as a form of access control. API files must compile independently of implementation, only importing from APIs from other libraries. API files are also visible to all files and libraries for import. Implementation files only see API files for import, not other implementation files. When any file imports a library's API, it should be expected that the transitive closure of imported files from the primary API file will be a compilation dependency. The size of that transitive closure affects compilation time, so libraries with complex implementations should endeavor to minimize their API imports. Libraries also serve as a critical unit of compilation. Dependencies between libraries must be clearly marked, and the resulting dependency graph will allow for separate compilation.","title":"Libraries"},{"location":"design/code_and_name_organization/#exporting-entities-from-an-api-file","text":"Entities in the API file are part of the library's public API by default. They may be marked as private to indicate they should only be visible to other parts of the library. package Geometry library \"Shapes\" api; // Circle is an API, and will be available to other libraries as Geometry.Circle. struct Circle { ... } // CircleHelper is private, and so will not be available to other libraries. private fn CircleHelper(circle: Circle) { ... } // Only entities in namespaces should be marked as an API, not the namespace // itself. namespace Operations; // Operations.GetCircumference is an API, and will be available to // other libraries as Geometry.Operations.GetCircumference. fn Operations.GetCircumference(circle: Circle) { ... } This means that an API file can contain all implementation code for a library. However, separate implementation files are still desirable for a few reasons: It will be easier for readers to quickly scan an API-only file for API documentation. Reducing the amount of code in an API file can speed up compilation, especially if fewer imports are needed. This can result in transitive compilation performance improvements for files using the library. From a code maintenance perspective, having smaller files can make a library more maintainable. Entities in the impl file should never have visibility keywords. If they are forward declared in the api file, they use the declaration's visibility; if they are only present in the impl file, they are implicitly private .","title":"Exporting entities from an API file"},{"location":"design/code_and_name_organization/#granularity-of-libraries","text":"The compilation graph of Carbon will generally consist of api files depending on each other, and impl files depending only on api files. Compiling a given file requires compiling the transitive closure of api files first. Parallelization of compilation is then limited by how large that transitive closure is, in terms of total volume of code rather than quantity. This also affects build cache invalidation. In order to maximize opportunities to improve compilation performance, we will encourage granular libraries. Conceptually, we want libraries to be very small, possibly containing only a single class. The choice of only allowing a single api file per library should help encourage developers to write small libraries.","title":"Granularity of libraries"},{"location":"design/code_and_name_organization/#exporting-namespaces","text":"Any entity may be marked with api except for namespace and package entities. That is, api namespace Sha256; is invalid code. Instead, namespaces are implicitly exported based on the name paths of other entities marked as api . For example, given this code: package Checksums library \"Sha\" api; namespaces Sha256; fn Sha256.HexDigest(data: Bytes) -> String { ... } Calling code may look like: package Caller api; import Checksums library \"Sha\"; fn Process(data: Bytes) { ... var digest: String = Checksums.Sha256.HexDigest(data); ... } In this example, the Sha256 namespace is exported as part of the API implicitly.","title":"Exporting namespaces"},{"location":"design/code_and_name_organization/#imports_1","text":"import directives supports reusing code from other files and libraries. The import directive's syntax may be loosely expressed as a regular expression: import IDENTIFIER (library NAME_PATH)?; An import declares a package entity named after the imported package, and makes API entities from the imported library available through it. The full name path is a concatenation of the names of the package entity, any namespace entities applied, and the final entity addressed. Child namespaces or entities may be aliased if desired. For example, given a library: package Math api; namespace Trigonometry; fn Trigonometry.Sin(...); Calling code would import it and use it like: package Geometry api; import Math; fn DoSomething() { ... Math.Trigonometry.Sin(...); ... } Repeat imports from the same package reuse the same package entity. For example, this produces only one Math package entity: import Math; import Math library \"Trigonometry\"; NOTE: A library must never import itself. Any impl files in a library automatically import the api , so a self-import should never be required.","title":"Imports"},{"location":"design/code_and_name_organization/#imports-from-the-current-package","text":"Entities defined in the current file may be used without mentioning the package prefix. However, other symbols from the package must be imported and accessed through the package namespace just like symbols from any other package. For example: package Geometry api; // This is required even though it's still in the Geometry package. import Geometry library \"Shapes\"; // Circle must be referenced using the Geometry namespace of the import. fn GetArea(c: Geometry.Circle) { ... }","title":"Imports from the current package"},{"location":"design/code_and_name_organization/#namespaces","text":"Namespaces offer named paths for entities. Namespaces may be nested. Multiple libraries may contribute to the same namespace. In practice, packages may have namespaces such as Testing containing entities that benefit from an isolated space but are present in many libraries. The namespace keyword's syntax may loosely be expressed as a regular expression: namespace NAME_PATH; The namespace keyword declares a namespace entity. The namespace is applied to other entities by including it as a prefix when declaring a name. For example: package Time; namespace Timezones.Internal; struct Timezones.Internal.RawData { ... } fn ParseData(data: Timezones.Internal.RawData); A namespace declaration adds the first identifier in the name path as a name in the file's namespace. In the above example, after declaring namespace Timezones.Internal; , Timezones is available as an identifier and Internal is reached through Timezones .","title":"Namespaces"},{"location":"design/code_and_name_organization/#re-declaring-imported-namespaces","text":"Namespaces may exist on imported package entities, in addition to being declared in the current file. However, even if the namespace already exists in an imported library from the current package, the namespace must still be declared locally in order to add symbols to it. For example, if the Geometry//Shapes/ThreeSides library provides the Geometry.Shapes namespace, this code is still valid: package Geometry library \"Shapes/FourSides\" api; import Geometry library \"Shapes/ThreeSides\"; // This does not conflict with the existence of `Geometry.Shapes` from // `Geometry//Shapes/ThreeSides`, even though the name path is identical. namespace Shapes; // This requires the above 'namespace Shapes' declaration. It cannot use // `Geometry.Shapes` from `Geometry//Shapes/ThreeSides`. struct Shapes.Square { ... };","title":"Re-declaring imported namespaces"},{"location":"design/code_and_name_organization/#aliasing","text":"Carbon's alias keyword will support aliasing namespaces. For example, this would be valid code: namespace Timezones.Internal; alias TI = Timezones.internal; struct TI.RawData { ... } fn ParseData(data: TI.RawData);","title":"Aliasing"},{"location":"design/code_and_name_organization/#caveats","text":"","title":"Caveats"},{"location":"design/code_and_name_organization/#package-and-library-name-conflicts","text":"Library name conflicts should not occur, because it's expected that a given package is maintained by a single organization. It's the responsibility of that organization to maintain unique library names within their package. A package name conflict occurs when two different packages use the same name, such as two packages named Stats . Versus libraries, package name conflicts are more likely because two organizations may independently choose identical names. We will encourage a unique package naming scheme, such as maintaining a name server for open source packages. Conflicts can also be addressed by renaming one of the packages, either at the source, or as a local modification. We do need to address the case of package names conflicting with other entity names. It's possible that a pre-existing entity will conflict with a new import, and that renaming the entity is infeasible to rename due to existing callers. Alternately, the entity may be using an idiomatic name that it would contradict naming conventions to rename. In either case, this conflict may exist in a single file without otherwise affecting users of the API. This will be addressed by name lookup .","title":"Package and library name conflicts"},{"location":"design/code_and_name_organization/#potential-refactorings","text":"These are potential refactorings that we consider important to make it easy to automate.","title":"Potential refactorings"},{"location":"design/code_and_name_organization/#update-imports","text":"Imports will frequently need to be updated as part of refactorings. When code is deleted, it should be possible to parse the remaining code, parse the imports, and determine which entities in imports are referred to. Unused imports can then be removed. When code is moved, it's similar to deletion in the originating file. For the destination file, the moved code should be parsed to determine which entities it referred to from the originating file's imports, and these will need to be included in the destination file: either reused if already present, or added. When new code is added, existing imports can be checked to see if they provide the symbol in question. There may also be heuristics which can be implemented to check build dependencies for where imports should be added from, such as a database of possible entities and their libraries. However, adding references may require manually adding imports.","title":"Update imports"},{"location":"design/code_and_name_organization/#between-api-and-impl-files","text":"Move an implementation of an API from an api file to an impl file, while leaving a declaration behind. This should be a local change that will not affect any calling code. Inlining will be affected because the implementation won't be visible to callers. Update imports . Split an api and impl file. This is a repeated operation of individual API moves, as noted above. Move an implementation of an API from an impl file to an api file. This should be a local change that will not affect any calling code. Inlining will be affected because the implementation becomes visible to callers. Update imports . Combine an api and impl file. This is a repeated operation of individual API moves, as noted above. Remove the api label from a declaration. Search for library-external callers, and fix them first. Add the api label to a declaration. This should be a local change that will not affect any calling code. Move a non- api -labeled declaration from an api file to an impl file. The declaration must be moved to the same file as the implementation of the declaration. The declaration can only be used by the impl file that now contains it. Search for other callers within the library, and fix them first. Update imports . Move a non- api -labeled declaration from an impl file to an api file. This should be a local change that will not affect any calling code. Update imports . Move a declaration and implementation from one impl file to another. Search for any callers within the source impl file, and either move them too, or fix them first. Update imports .","title":"Between api and impl files"},{"location":"design/code_and_name_organization/#other-refactorings","text":"Rename a package. The imports of all calling files must be updated accordingly. All call sites must be changed, as the package name changes. Update imports . Move an api -labeled declaration and implementation between different packages. The imports of all calling files must be updated accordingly. All call sites must be changed, as the package name changes. Update imports . Move an api -labeled declaration and implementation between libraries in the same package. The imports of all calling files must be updated accordingly. As long as the namespaces remain the same, no call sites will need to be changed. Update imports . Rename a library. This is equivalent to a repeated operation of moving an api -labeled declaration and implementation between libraries in the same package. Move a declaration and implementation from one namespace to another. Ensure the new namespace is declared for the declaration and implementation. Update the namespace used by call sites. The imports of all calling files may remain the same. Rename a namespace. This is equivalent to a repeated operation of moving a declaration and implementation from one namespace to another. Rename a file, or move a file between directories. Build configuration will need to be updated. This additionally requires the steps to rename a library, because library names must correspond to the renamed paths.","title":"Other refactorings"},{"location":"design/code_and_name_organization/#preference-for-few-child-namespaces","text":"We expect that most code should use a package and library, but avoid specifying namespaces beneath the package. The package name itself should typically be sufficient distinction for names. Child namespaces create longer names, which engineers will dislike typing. Based on experience, we expect to start seeing aliasing even at name lengths around six characters long. With longer names, we should expect more aliasing, which in turn will reduce code readability because more types will have local names. We believe it's feasible for even large projects to collapse namespaces down to a top level, avoiding internal tiers of namespaces. We understand that child namespaces are sometimes helpful, and will robustly support them for that. However, we will model code organization to encourage fewer namespaces.","title":"Preference for few child namespaces"},{"location":"design/code_and_name_organization/#redundant-markers","text":"We use a few possibly redundant markers for packages and libraries: The package keyword requires one of api and impl , rather than excluding either or both. The filename repeats the api versus impl choice. The import keyword requires the full library. These choices are made to assist human readability and tooling: Being explicit about imports creates the opportunity to generate build dependencies from files, rather than having them maintained separately. Being explicit about api versus impl makes it easier for both humans and tooling to determine what to expect. Repeating the type in the filename makes it possible to check the type without reading file content. Repeating the type in the file content makes non-file-system-based builds possible.","title":"Redundant markers"},{"location":"design/code_and_name_organization/#open-questions","text":"These open questions are expected to be revisited by future proposals.","title":"Open questions"},{"location":"design/code_and_name_organization/#different-file-extensions","text":"Currently, we're using .carbon and .impl.carbon . In the future, we may want to change the extension, particularly because Carbon may be renamed. There are several other possible extensions / commands that we've considered in coming to the current extension: .carbon : This is an obvious and unsurprising choice, but also quite long for a file extension. .6c : This sounds a little like 'sexy' when read aloud. .c6 : This seems a weird incorrect ordering of the atomic number and has a bad, if obscure, Internet slang association. .cb or .cbn : These collide with several acronyms and may not be especially memorable as referring to Carbon. .crb : This has a bad Internet slang association.","title":"Different file extensions"},{"location":"design/code_and_name_organization/#imports-from-other-languages","text":"Currently, we do not support cross-language imports. In the future, we will likely want to support imports from other languages, particularly for C++ interoperability. To fit into the proposed import syntax, we are provisionally using a special Cpp package to import headers from C++ code, as in: import Cpp library \"<map>\"; import Cpp library \"myproject/myclass.h\"; fn MyCarbonCall(x: Cpp.std.map(Cpp.MyProject.MyClass));","title":"Imports from other languages"},{"location":"design/code_and_name_organization/#imports-from-urls","text":"Currently, we don't support any kind of package management with imports. In the future, we may want to support tagging imports with a URL that identifies the repository where that package can be found. This can be used to help drive package management tooling and to support providing a non-name identity for a package that is used to enable handling conflicted package names. Although we're not designing this right now, it could fit into the proposed syntax. For example: import Carbon library \"Utilities\" url(\"https://github.com/carbon-language/carbon-libraries\");","title":"Imports from URLs"},{"location":"design/code_and_name_organization/#test-file-type","text":"Similar to api and impl , we may eventually want a type like test . This should be part of a larger testing plan.","title":"Test file type"},{"location":"design/code_and_name_organization/#alternatives-considered","text":"Packages Name paths for package names Referring to the package as package Remove the library keyword from package and import Rename package concept No association between the file system path and library/namespace Libraries Allow exporting namespaces Allow importing implementation files from within the same library Alternative library separators and shorthand Single-word libraries Collapse API and implementation file concepts Automatically generating the API separation Collapse file and library concepts Collapse the library concept into packages Collapse the package concept into libraries Default api to private Default impl to public Different file type labels Function-like syntax Inlining from implementation files Library-private access controls Make keywords either optional or required in separate definitions Managing API versus implementation in libraries Multiple API files Name paths as library names Imports Block imports Block imports of libraries of a single package Broader imports, either all names or arbitrary code Direct name imports Optional package names Namespaces File-level namespaces Scoped namespaces","title":"Alternatives considered"},{"location":"design/code_and_name_organization/#references","text":"Proposal #107: Code and name organization","title":"References"},{"location":"design/code_and_name_organization/source_files/","text":"Source files Table of contents Overview Encoding Alternatives considered References Overview A Carbon source file is a sequence of Unicode code points in Unicode Normalization Form C (\"NFC\"), and represents a portion of the complete text of a program. Program text can come from a variety of sources, such as an interactive programming environment (a so-called \"Read-Evaluate-Print-Loop\" or REPL), a database, a memory buffer of an IDE, or a command-line argument. The canonical representation for Carbon programs is in files stored as a sequence of bytes in a file system on disk. Such files have a .carbon extension. Encoding The on-disk representation of a Carbon source file is encoded in UTF-8. Such files may begin with an optional UTF-8 BOM, that is, the byte sequence EF 16 ,BB 16 ,BF 16 . This prefix, if present, is ignored. No Unicode normalization is performed when reading an on-disk representation of a Carbon source file, so the byte representation is required to be normalized in Normalization Form C. The Carbon source formatting tool will convert source files to NFC as necessary. Alternatives considered Character encoding Byte order marks Normalization forms References Proposal #142: Unicode source files Unicode is a universal character encoding, maintained by the Unicode Consortium . It is the canonical encoding used for textual information interchange across all modern technology. Carbon is based on Unicode 13.0, which is currently the latest version of the Unicode standard. Newer versions will be considered for adoption as they are released. Unicode Standard Annex #15: Unicode Normalization Forms Wikipedia Unicode equivalence page: Normal forms","title":"Source files"},{"location":"design/code_and_name_organization/source_files/#source-files","text":"","title":"Source files"},{"location":"design/code_and_name_organization/source_files/#table-of-contents","text":"Overview Encoding Alternatives considered References","title":"Table of contents"},{"location":"design/code_and_name_organization/source_files/#overview","text":"A Carbon source file is a sequence of Unicode code points in Unicode Normalization Form C (\"NFC\"), and represents a portion of the complete text of a program. Program text can come from a variety of sources, such as an interactive programming environment (a so-called \"Read-Evaluate-Print-Loop\" or REPL), a database, a memory buffer of an IDE, or a command-line argument. The canonical representation for Carbon programs is in files stored as a sequence of bytes in a file system on disk. Such files have a .carbon extension.","title":"Overview"},{"location":"design/code_and_name_organization/source_files/#encoding","text":"The on-disk representation of a Carbon source file is encoded in UTF-8. Such files may begin with an optional UTF-8 BOM, that is, the byte sequence EF 16 ,BB 16 ,BF 16 . This prefix, if present, is ignored. No Unicode normalization is performed when reading an on-disk representation of a Carbon source file, so the byte representation is required to be normalized in Normalization Form C. The Carbon source formatting tool will convert source files to NFC as necessary.","title":"Encoding"},{"location":"design/code_and_name_organization/source_files/#alternatives-considered","text":"Character encoding Byte order marks Normalization forms","title":"Alternatives considered"},{"location":"design/code_and_name_organization/source_files/#references","text":"Proposal #142: Unicode source files Unicode is a universal character encoding, maintained by the Unicode Consortium . It is the canonical encoding used for textual information interchange across all modern technology. Carbon is based on Unicode 13.0, which is currently the latest version of the Unicode standard. Newer versions will be considered for adoption as they are released. Unicode Standard Annex #15: Unicode Normalization Forms Wikipedia Unicode equivalence page: Normal forms","title":"References"},{"location":"design/control_flow/","text":"Control flow Overview Blocks of statements are generally executed linearly. However, statements are the primary place where this flow of execution can be controlled. Carbon's flow control statements are: if and else provides conditional execution of statements. Loops: while executes the loop body for as long as the loop expression returns True . for iterates over an object, such as elements in an array. break exits loops. continue goes to the next iteration of a loop. return ends the flow of execution within a function, returning it to the caller.","title":"Control flow"},{"location":"design/control_flow/#control-flow","text":"","title":"Control flow"},{"location":"design/control_flow/#overview","text":"Blocks of statements are generally executed linearly. However, statements are the primary place where this flow of execution can be controlled. Carbon's flow control statements are: if and else provides conditional execution of statements. Loops: while executes the loop body for as long as the loop expression returns True . for iterates over an object, such as elements in an array. break exits loops. continue goes to the next iteration of a loop. return ends the flow of execution within a function, returning it to the caller.","title":"Overview"},{"location":"design/control_flow/conditionals/","text":"Control flow Table of contents Overview Alternatives considered References Overview if and else provide conditional execution of statements. Syntax is: if ( boolean expression ) { statements } [ else if ( boolean expression ) { statements } ] ... [ else { statements } ] Only one group of statements will execute: When the first if 's boolean expression evaluates to true, its associated statements will execute. When earlier boolean expressions evaluate to false and an else if 's boolean expression evaluates to true, its associated statements will execute. ... else if ... is equivalent to ... else { if ... } , but without visible nesting of braces. When all boolean expressions evaluate to false, the else 's associated statements will execute. When a boolean expression evaluates to true, no later boolean expressions will evaluate. Note that else if may be repeated. For example: if (fruit.IsYellow()) { Print(\"Banana!\"); } else if (fruit.IsOrange()) { Print(\"Orange!\"); } else if (fruit.IsGreen()) { Print(\"Apple!\"); } else { Print(\"Vegetable!\"); } fruit.Eat(); This code will: Evaluate fruit.IsYellow() : When True , print Banana! and resume execution at fruit.Eat() . When False , evaluate fruit.IsOrange() : When True , print Orange! and resume execution at fruit.Eat() . When False , evaluate fruit.IsGreen() : When True , print Orange! and resume execution at fruit.Eat() . When False , print Vegetable! and resume execution at fruit.Eat() . Alternatives considered Optional braces Optional parentheses elif References Proposal #285: if and else Proposal #623: Require braces","title":"Control flow"},{"location":"design/control_flow/conditionals/#control-flow","text":"","title":"Control flow"},{"location":"design/control_flow/conditionals/#table-of-contents","text":"Overview Alternatives considered References","title":"Table of contents"},{"location":"design/control_flow/conditionals/#overview","text":"if and else provide conditional execution of statements. Syntax is: if ( boolean expression ) { statements } [ else if ( boolean expression ) { statements } ] ... [ else { statements } ] Only one group of statements will execute: When the first if 's boolean expression evaluates to true, its associated statements will execute. When earlier boolean expressions evaluate to false and an else if 's boolean expression evaluates to true, its associated statements will execute. ... else if ... is equivalent to ... else { if ... } , but without visible nesting of braces. When all boolean expressions evaluate to false, the else 's associated statements will execute. When a boolean expression evaluates to true, no later boolean expressions will evaluate. Note that else if may be repeated. For example: if (fruit.IsYellow()) { Print(\"Banana!\"); } else if (fruit.IsOrange()) { Print(\"Orange!\"); } else if (fruit.IsGreen()) { Print(\"Apple!\"); } else { Print(\"Vegetable!\"); } fruit.Eat(); This code will: Evaluate fruit.IsYellow() : When True , print Banana! and resume execution at fruit.Eat() . When False , evaluate fruit.IsOrange() : When True , print Orange! and resume execution at fruit.Eat() . When False , evaluate fruit.IsGreen() : When True , print Orange! and resume execution at fruit.Eat() . When False , print Vegetable! and resume execution at fruit.Eat() .","title":"Overview"},{"location":"design/control_flow/conditionals/#alternatives-considered","text":"Optional braces Optional parentheses elif","title":"Alternatives considered"},{"location":"design/control_flow/conditionals/#references","text":"Proposal #285: if and else Proposal #623: Require braces","title":"References"},{"location":"design/control_flow/loops/","text":"Loops Table of contents Overview Details while for break continue Alternatives considered References Overview Carbon provides loops using the while and for statements. Within a loop, the break and continue statements can be used for flow control. Details while while statements loop for as long as the passed expression returns True . Syntax is: while ( boolean expression ) { statements } For example, this prints 0 , 1 , 2 , then Done! : var x: Int = 0; while (x < 3) { Print(x); ++x; } Print(\"Done!\"); for for statements support range-based looping, typically over containers. Syntax is: for ( var declaration in expression ) { statements } For example, this prints all names in names : for (var name: String in names) { Print(name); } PrintNames() prints each String in the names List in iteration order. break The break statement immediately ends a while or for loop. Execution will resume at the end of the loop's scope. Syntax is: break; For example, this processes steps until a manual step is hit (if no manual step is hit, all steps are processed): for (var step: Step in steps) { if (step.IsManual()) { Print(\"Reached manual step!\"); break; } step.Process(); } continue The continue statement immediately goes to the next loop of a while or for . In a while , execution continues with the while expression. Syntax is: continue; For example, this prints all non-empty lines of a file, using continue to skip empty lines: var f: File = OpenFile(path); while (!f.EOF()) { var line: String = f.ReadLine(); if (line.IsEmpty()) { continue; } Print(line); } Alternatives considered Non-C++ syntax Initializing variables in the while for : Include semisemi for loops Multi-variable bindings : versus in Optional braces Optional parentheses References Proposal #340: while Proposal #353: for Proposal #618: var ordering Proposal #623: Require braces","title":"Loops"},{"location":"design/control_flow/loops/#loops","text":"","title":"Loops"},{"location":"design/control_flow/loops/#table-of-contents","text":"Overview Details while for break continue Alternatives considered References","title":"Table of contents"},{"location":"design/control_flow/loops/#overview","text":"Carbon provides loops using the while and for statements. Within a loop, the break and continue statements can be used for flow control.","title":"Overview"},{"location":"design/control_flow/loops/#details","text":"","title":"Details"},{"location":"design/control_flow/loops/#while","text":"while statements loop for as long as the passed expression returns True . Syntax is: while ( boolean expression ) { statements } For example, this prints 0 , 1 , 2 , then Done! : var x: Int = 0; while (x < 3) { Print(x); ++x; } Print(\"Done!\");","title":"while"},{"location":"design/control_flow/loops/#for","text":"for statements support range-based looping, typically over containers. Syntax is: for ( var declaration in expression ) { statements } For example, this prints all names in names : for (var name: String in names) { Print(name); } PrintNames() prints each String in the names List in iteration order.","title":"for"},{"location":"design/control_flow/loops/#break","text":"The break statement immediately ends a while or for loop. Execution will resume at the end of the loop's scope. Syntax is: break; For example, this processes steps until a manual step is hit (if no manual step is hit, all steps are processed): for (var step: Step in steps) { if (step.IsManual()) { Print(\"Reached manual step!\"); break; } step.Process(); }","title":"break"},{"location":"design/control_flow/loops/#continue","text":"The continue statement immediately goes to the next loop of a while or for . In a while , execution continues with the while expression. Syntax is: continue; For example, this prints all non-empty lines of a file, using continue to skip empty lines: var f: File = OpenFile(path); while (!f.EOF()) { var line: String = f.ReadLine(); if (line.IsEmpty()) { continue; } Print(line); }","title":"continue"},{"location":"design/control_flow/loops/#alternatives-considered","text":"Non-C++ syntax Initializing variables in the while for : Include semisemi for loops Multi-variable bindings : versus in Optional braces Optional parentheses","title":"Alternatives considered"},{"location":"design/control_flow/loops/#references","text":"Proposal #340: while Proposal #353: for Proposal #618: var ordering Proposal #623: Require braces","title":"References"},{"location":"design/control_flow/return/","text":"return Table of contents Overview Returning empty tuples returned var return and initialization Alternatives considered References Overview The return statement ends the flow of execution within a function , returning execution to the caller. Its syntax is: return [ expression ] ; If the function returns a value to the caller, that value is provided by an expression in the return statement. For example: fn Sum(a: i32, b: i32) -> i32 { return a + b; } When a return type is specified, a function must always return before control flow can reach the end of the function body. In other words, fn DoNothing() -> i32 {} would be invalid because execution will reach the end of the function body without returning a value. Returning empty tuples Returning an empty tuple () is special, and similar to C++'s void returns. When a function has no specified return type, its return type is implicitly () . return must not have an expression argument in this case. It also has an implicit return; at the end of the function body. For example: // No return type is specified, so this returns `()` implicitly. fn MaybeDraw(should_draw: bool) { if (!should_draw) { // No expression is passed to `return`. return; } ActuallyDraw(); // There is an implicit `return;` here. } When -> () is specified in the function signature, the return expression is required. Omitting -> () is encouraged, but specifying it is supported for generalized code structures, including templates . In order to be consistent with other explicitly specified return types, return; is invalid in this case. For example: // `-> ()` defines an explicit return value. fn MaybeDraw(should_draw: bool) -> () { if (!should_draw) { // As a consequence, a return value must be passed. return (); } ActuallyDraw(); // The return value must again be explicit. return (); } returned var Variables may be declared with a returned statement. Its syntax is: returned var statement When a variable is marked as returned , it must be the only returned value in-scope. If a returned var is returned, the specific syntax return var must be used. Returning expressions is not allowed while a returned var is in scope. For example: fn MakeCircle(radius: i32) -> Circle { returned var c: Circle; c.radius = radius; // `return c` would be invalid because `returned` is in use. return var; } If control flow exits the scope of a returned variable in any way other than return var , the returned var 's lifetime ends as normal. When this occurs, return may again be used with expressions. For example: fn MakePointInArea(area: Area, preferred_x: i32, preferred_y: i32) -> Point { if (preferred_x >= 0 && preferred_y >= 0) { returned var p: Point = { .x = preferred_x, .y = preferred_y }; if (area.Contains(p)) { return var; } // p's lifetime ends here when `return var` is not reached. } return area.RandomPoint(); } return and initialization Consider the following common initialization code: fn CreateMyObject() -> MyType { return <expression>; } var x: MyType = CreateMyObject(); The <expression> in the return statement of CreateMyObject initializes the variable x here. There is no copy or similar. It is equivalent to: var x: MyType = <expression>; This applies recursively, similar to C++'s guaranteed copy elision. In the case where additional statements should be run between constructing the return value and returning, the use of returned var allows for improved efficiency because the returned var can directly use the address of var declared by the caller. For example, here the returned var vector in CreateVector uses the storage of my_vector for initialization, avoiding a copy: fn CreateVector(x: i32, y: i32) -> Vector { returned var vector: Vector; vector.x = x; vector.y = y; return var; } var my_vector: Vector = CreateVector(1, 2); As a consequence, returned var is encouraged because it makes it easier to avoid copies. TODO: Have some discussion of RVO and NRVO as they are found in C++ here, and the fact that Carbon provides the essential part of these as first-class features and therefore they are never \"optimizations\" or done implicitly or optionally. Alternatives considered Implicit or expression returns Named return variable in place of a return type Retain the C++ rule Fully divorce functions and procedures References Proposal #257: Initialization of memory and variables Proposal #415: Syntax: return Proposal #538: return with no argument","title":"`return`"},{"location":"design/control_flow/return/#return","text":"","title":"return"},{"location":"design/control_flow/return/#table-of-contents","text":"Overview Returning empty tuples returned var return and initialization Alternatives considered References","title":"Table of contents"},{"location":"design/control_flow/return/#overview","text":"The return statement ends the flow of execution within a function , returning execution to the caller. Its syntax is: return [ expression ] ; If the function returns a value to the caller, that value is provided by an expression in the return statement. For example: fn Sum(a: i32, b: i32) -> i32 { return a + b; } When a return type is specified, a function must always return before control flow can reach the end of the function body. In other words, fn DoNothing() -> i32 {} would be invalid because execution will reach the end of the function body without returning a value.","title":"Overview"},{"location":"design/control_flow/return/#returning-empty-tuples","text":"Returning an empty tuple () is special, and similar to C++'s void returns. When a function has no specified return type, its return type is implicitly () . return must not have an expression argument in this case. It also has an implicit return; at the end of the function body. For example: // No return type is specified, so this returns `()` implicitly. fn MaybeDraw(should_draw: bool) { if (!should_draw) { // No expression is passed to `return`. return; } ActuallyDraw(); // There is an implicit `return;` here. } When -> () is specified in the function signature, the return expression is required. Omitting -> () is encouraged, but specifying it is supported for generalized code structures, including templates . In order to be consistent with other explicitly specified return types, return; is invalid in this case. For example: // `-> ()` defines an explicit return value. fn MaybeDraw(should_draw: bool) -> () { if (!should_draw) { // As a consequence, a return value must be passed. return (); } ActuallyDraw(); // The return value must again be explicit. return (); }","title":"Returning empty tuples"},{"location":"design/control_flow/return/#returned-var","text":"Variables may be declared with a returned statement. Its syntax is: returned var statement When a variable is marked as returned , it must be the only returned value in-scope. If a returned var is returned, the specific syntax return var must be used. Returning expressions is not allowed while a returned var is in scope. For example: fn MakeCircle(radius: i32) -> Circle { returned var c: Circle; c.radius = radius; // `return c` would be invalid because `returned` is in use. return var; } If control flow exits the scope of a returned variable in any way other than return var , the returned var 's lifetime ends as normal. When this occurs, return may again be used with expressions. For example: fn MakePointInArea(area: Area, preferred_x: i32, preferred_y: i32) -> Point { if (preferred_x >= 0 && preferred_y >= 0) { returned var p: Point = { .x = preferred_x, .y = preferred_y }; if (area.Contains(p)) { return var; } // p's lifetime ends here when `return var` is not reached. } return area.RandomPoint(); }","title":"returned var"},{"location":"design/control_flow/return/#return-and-initialization","text":"Consider the following common initialization code: fn CreateMyObject() -> MyType { return <expression>; } var x: MyType = CreateMyObject(); The <expression> in the return statement of CreateMyObject initializes the variable x here. There is no copy or similar. It is equivalent to: var x: MyType = <expression>; This applies recursively, similar to C++'s guaranteed copy elision. In the case where additional statements should be run between constructing the return value and returning, the use of returned var allows for improved efficiency because the returned var can directly use the address of var declared by the caller. For example, here the returned var vector in CreateVector uses the storage of my_vector for initialization, avoiding a copy: fn CreateVector(x: i32, y: i32) -> Vector { returned var vector: Vector; vector.x = x; vector.y = y; return var; } var my_vector: Vector = CreateVector(1, 2); As a consequence, returned var is encouraged because it makes it easier to avoid copies. TODO: Have some discussion of RVO and NRVO as they are found in C++ here, and the fact that Carbon provides the essential part of these as first-class features and therefore they are never \"optimizations\" or done implicitly or optionally.","title":"return and initialization"},{"location":"design/control_flow/return/#alternatives-considered","text":"Implicit or expression returns Named return variable in place of a return type Retain the C++ rule Fully divorce functions and procedures","title":"Alternatives considered"},{"location":"design/control_flow/return/#references","text":"Proposal #257: Initialization of memory and variables Proposal #415: Syntax: return Proposal #538: return with no argument","title":"References"},{"location":"design/expressions/","text":"Expressions Table of contents Overview Precedence Names Unqualified names Qualified names and member access Operators Conversions and casts if expressions Alternatives considered References Overview Expressions are the portions of Carbon syntax that produce values. Because types in Carbon are values, this includes anywhere that a type is specified. fn Foo(a: i32*) -> i32 { return *a; } Here, the parameter type i32* , the return type i32 , and the operand *a of the return statement are all expressions. Precedence Expressions are interpreted based on a partial precedence ordering . Expression components which lack a relative ordering must be disambiguated by the developer, for example by adding parentheses; otherwise, the expression will be invalid due to ambiguity. Precedence orderings will only be added when it's reasonable to expect most developers to understand the precedence without parentheses. The precedence diagram is defined thusly: %%{init: {'themeVariables': {'fontFamily': 'monospace'}}}%% graph BT parens[\"(...)\"] braces[\"{...}\"] click braces \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/classes.md#literals\" unqualifiedName[\"x\"] click unqualifiedName \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/README.md#unqualified-names\" memberAccess>\"x.y<br> x.(...)\"] click memberAccess \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/member_access.md\" negation[\"-x\"] click negation \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/arithmetic.md\" complement[\"^x\"] click complement \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/bitwise.md\" unary((\" \")) as[\"x as T\"] click as \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/implicit_conversions.md\" multiplication>\"x * y<br> x / y\"] click multiplication \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/arithmetic.md\" addition>\"x + y<br> x - y\"] click addition \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/arithmetic.md\" modulo[\"x % y\"] click modulo \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/arithmetic.md\" bitwise_and>\"x & y\"] bitwise_or>\"x | y\"] bitwise_xor>\"x ^ y\"] click bitwise_and \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/bitwise.md\" click bitwise_or \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/bitwise.md\" click bitwise_xor \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/bitwise.md\" shift[\"x << y<br> x >> y\"] click shift \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/bitwise.md\" comparison[\"x == y<br> x != y<br> x < y<br> x <= y<br> x > y<br> x >= y\"] click comparison \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/comparison_operators.md\" not[\"not x\"] click not \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/logical_operators.md\" logicalOperand((\" \")) and>\"x and y\"] click and \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/logical_operators.md\" or>\"x or y\"] click or \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/logical_operators.md\" logicalExpression((\" \")) if>\"if x then y else z\"] click if \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/if.md\" expressionEnd[\"x;\"] memberAccess --> parens & braces & unqualifiedName negation --> memberAccess complement --> memberAccess unary --> negation & complement %% Use a longer arrow here to put `not` next to `and` and `or`. not -----> memberAccess multiplication & modulo & as & bitwise_and & bitwise_or & bitwise_xor & shift --> unary addition --> multiplication comparison --> modulo & addition & as & bitwise_and & bitwise_or & bitwise_xor & shift logicalOperand --> comparison & not and & or --> logicalOperand logicalExpression --> and & or if & expressionEnd --> logicalExpression The diagram's attributes are: Each non-empty node represents a precedence group. Empty circles are used to simplify the graph, and do not represent a precedence group. When an expression is composed from different precedence groups, the interpretation is determined by the precedence edges: A precedence edge A --> B means that A is lower precedence than B, so A can contain B without parentheses. For example, or --> not means that not x or y is treated as (not x) or y . Precedence edges are transitive. For example, or --> == --> as means that or is lower precedence than as . When an expression is composed from a single precedence group, the interpretation is determined by the associativity of the precedence group: mermaid graph TD non[\"Non-associative\"] left>\"Left associative\"] For example, + and - are left-associative and in the same precedence group, so a + b + c - d is treated as ((a + b) + c) - d . Names Unqualified names An unqualified name is a word that is not a keyword and is not preceded by a period ( . ). TODO: Name lookup rules for unqualified names. Qualified names and member access A qualified name is a word that appears immediately after a period. Qualified names appear in the following contexts: Designators : . word Simple member access expressions : expression . word var x: auto = {.hello = 1, .world = 2}; ^^^^^ ^^^^^ qualified name ^^^^^^ ^^^^^^ designator x.hello = x.world; ^^^^^ ^^^^^ qualified name ^^^^^^^ ^^^^^^^ member access expression Qualified names refer to members of an entity determined by the context in which the expression appears. For a member access, the entity is named by the expression preceding the period. In a struct literal, the entity is the struct type. For example: package Foo api; namespace N; fn N.F() {} fn G() { // Same as `(Foo.N).F()`. // `Foo.N` names namespace `N` in package `Foo`. // `(Foo.N).F` names function `F` in namespace `N`. Foo.N.F(); } // `.n` refers to the member `n` of `{.n: i32}`. fn H(a: {.n: i32}) -> i32 { // `a.n` is resolved to the member `{.n: i32}.n`, // and names the corresponding subobject of `a`. return a.n; } fn J() { // `.n` refers to the member `n of `{.n: i32}`. H({.n = 5 as i32}); } Member access expressions associate left-to-right. If the member name is more complex than a single word , a compound member access expression can be used, with parentheses around the member name: expression . ( expression ) interface I { fn F[me: Self](); } class X {} external impl X as I { fn F[me: Self]() {} } // `x.I.F()` would mean `(x.I).F()`. fn Q(x: X) { x.(I.F)(); } Operators Most expressions are modeled as operators: Category Operator Syntax Function Arithmetic - (unary) -x The negation of x . Bitwise ^ (unary) ^x The bitwise complement of x . Arithmetic + x + y The sum of x and y . Arithmetic - (binary) x - y The difference of x and y . Arithmetic * x * y The product of x and y . Arithmetic / x / y x divided by y , or the quotient thereof. Arithmetic % x % y x modulo y . Bitwise & x & y The bitwise AND of x and y . Bitwise \\| x \\| y The bitwise OR of x and y . Bitwise ^ (binary) x ^ y The bitwise XOR of x and y . Bitwise << x << y x bit-shifted left y places. Bitwise >> x >> y x bit-shifted right y places. Conversion as x as T Converts the value x to the type T . Comparison == x == y Equality: true if x is equal to y . Comparison != x != y Inequality: true if x is not equal to y . Comparison < x < y Less than: true if x is less than y . Comparison <= x <= y Less than or equal: true if x is less than or equal to y . Comparison > x > y Greater than: true if x is greater than to y . Comparison >= x >= y Greater than or equal: true if x is greater than or equal to y . Logical and x and y A short-circuiting logical AND: true if both operands are true . Logical or x or y A short-circuiting logical OR: true if either operand is true . Logical not not x Logical NOT: true if the operand is false . Conversions and casts When an expression appears in a context in which an expression of a specific type is expected, implicit conversions are applied to convert the expression to the target type. Expressions can also be converted to a specific type using an as expression . fn Bar(n: i32); fn Baz(n: i64) { // OK, same as Bar(n as i32) Bar(n); } if expressions An if expression chooses between two expressions. fn Run(args: Span(StringView)) { var file: StringView = if args.size() > 1 then args[1] else \"/dev/stdin\"; } if expressions are analogous to ?: ternary expressions in C and C++. Alternatives considered Other expression documents will list more alternatives; this lists alternatives not noted elsewhere. Total order Different precedence for different operands Require less than a partial order References Other expression documents will list more references; this lists references not noted elsewhere. Proposal #555: Operator precedence .","title":"Expressions"},{"location":"design/expressions/#expressions","text":"","title":"Expressions"},{"location":"design/expressions/#table-of-contents","text":"Overview Precedence Names Unqualified names Qualified names and member access Operators Conversions and casts if expressions Alternatives considered References","title":"Table of contents"},{"location":"design/expressions/#overview","text":"Expressions are the portions of Carbon syntax that produce values. Because types in Carbon are values, this includes anywhere that a type is specified. fn Foo(a: i32*) -> i32 { return *a; } Here, the parameter type i32* , the return type i32 , and the operand *a of the return statement are all expressions.","title":"Overview"},{"location":"design/expressions/#precedence","text":"Expressions are interpreted based on a partial precedence ordering . Expression components which lack a relative ordering must be disambiguated by the developer, for example by adding parentheses; otherwise, the expression will be invalid due to ambiguity. Precedence orderings will only be added when it's reasonable to expect most developers to understand the precedence without parentheses. The precedence diagram is defined thusly: %%{init: {'themeVariables': {'fontFamily': 'monospace'}}}%% graph BT parens[\"(...)\"] braces[\"{...}\"] click braces \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/classes.md#literals\" unqualifiedName[\"x\"] click unqualifiedName \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/README.md#unqualified-names\" memberAccess>\"x.y<br> x.(...)\"] click memberAccess \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/member_access.md\" negation[\"-x\"] click negation \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/arithmetic.md\" complement[\"^x\"] click complement \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/bitwise.md\" unary((\" \")) as[\"x as T\"] click as \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/implicit_conversions.md\" multiplication>\"x * y<br> x / y\"] click multiplication \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/arithmetic.md\" addition>\"x + y<br> x - y\"] click addition \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/arithmetic.md\" modulo[\"x % y\"] click modulo \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/arithmetic.md\" bitwise_and>\"x & y\"] bitwise_or>\"x | y\"] bitwise_xor>\"x ^ y\"] click bitwise_and \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/bitwise.md\" click bitwise_or \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/bitwise.md\" click bitwise_xor \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/bitwise.md\" shift[\"x << y<br> x >> y\"] click shift \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/bitwise.md\" comparison[\"x == y<br> x != y<br> x < y<br> x <= y<br> x > y<br> x >= y\"] click comparison \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/comparison_operators.md\" not[\"not x\"] click not \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/logical_operators.md\" logicalOperand((\" \")) and>\"x and y\"] click and \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/logical_operators.md\" or>\"x or y\"] click or \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/logical_operators.md\" logicalExpression((\" \")) if>\"if x then y else z\"] click if \"https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/expressions/if.md\" expressionEnd[\"x;\"] memberAccess --> parens & braces & unqualifiedName negation --> memberAccess complement --> memberAccess unary --> negation & complement %% Use a longer arrow here to put `not` next to `and` and `or`. not -----> memberAccess multiplication & modulo & as & bitwise_and & bitwise_or & bitwise_xor & shift --> unary addition --> multiplication comparison --> modulo & addition & as & bitwise_and & bitwise_or & bitwise_xor & shift logicalOperand --> comparison & not and & or --> logicalOperand logicalExpression --> and & or if & expressionEnd --> logicalExpression The diagram's attributes are: Each non-empty node represents a precedence group. Empty circles are used to simplify the graph, and do not represent a precedence group. When an expression is composed from different precedence groups, the interpretation is determined by the precedence edges: A precedence edge A --> B means that A is lower precedence than B, so A can contain B without parentheses. For example, or --> not means that not x or y is treated as (not x) or y . Precedence edges are transitive. For example, or --> == --> as means that or is lower precedence than as . When an expression is composed from a single precedence group, the interpretation is determined by the associativity of the precedence group: mermaid graph TD non[\"Non-associative\"] left>\"Left associative\"] For example, + and - are left-associative and in the same precedence group, so a + b + c - d is treated as ((a + b) + c) - d .","title":"Precedence"},{"location":"design/expressions/#names","text":"","title":"Names"},{"location":"design/expressions/#unqualified-names","text":"An unqualified name is a word that is not a keyword and is not preceded by a period ( . ). TODO: Name lookup rules for unqualified names.","title":"Unqualified names"},{"location":"design/expressions/#qualified-names-and-member-access","text":"A qualified name is a word that appears immediately after a period. Qualified names appear in the following contexts: Designators : . word Simple member access expressions : expression . word var x: auto = {.hello = 1, .world = 2}; ^^^^^ ^^^^^ qualified name ^^^^^^ ^^^^^^ designator x.hello = x.world; ^^^^^ ^^^^^ qualified name ^^^^^^^ ^^^^^^^ member access expression Qualified names refer to members of an entity determined by the context in which the expression appears. For a member access, the entity is named by the expression preceding the period. In a struct literal, the entity is the struct type. For example: package Foo api; namespace N; fn N.F() {} fn G() { // Same as `(Foo.N).F()`. // `Foo.N` names namespace `N` in package `Foo`. // `(Foo.N).F` names function `F` in namespace `N`. Foo.N.F(); } // `.n` refers to the member `n` of `{.n: i32}`. fn H(a: {.n: i32}) -> i32 { // `a.n` is resolved to the member `{.n: i32}.n`, // and names the corresponding subobject of `a`. return a.n; } fn J() { // `.n` refers to the member `n of `{.n: i32}`. H({.n = 5 as i32}); } Member access expressions associate left-to-right. If the member name is more complex than a single word , a compound member access expression can be used, with parentheses around the member name: expression . ( expression ) interface I { fn F[me: Self](); } class X {} external impl X as I { fn F[me: Self]() {} } // `x.I.F()` would mean `(x.I).F()`. fn Q(x: X) { x.(I.F)(); }","title":"Qualified names and member access"},{"location":"design/expressions/#operators","text":"Most expressions are modeled as operators: Category Operator Syntax Function Arithmetic - (unary) -x The negation of x . Bitwise ^ (unary) ^x The bitwise complement of x . Arithmetic + x + y The sum of x and y . Arithmetic - (binary) x - y The difference of x and y . Arithmetic * x * y The product of x and y . Arithmetic / x / y x divided by y , or the quotient thereof. Arithmetic % x % y x modulo y . Bitwise & x & y The bitwise AND of x and y . Bitwise \\| x \\| y The bitwise OR of x and y . Bitwise ^ (binary) x ^ y The bitwise XOR of x and y . Bitwise << x << y x bit-shifted left y places. Bitwise >> x >> y x bit-shifted right y places. Conversion as x as T Converts the value x to the type T . Comparison == x == y Equality: true if x is equal to y . Comparison != x != y Inequality: true if x is not equal to y . Comparison < x < y Less than: true if x is less than y . Comparison <= x <= y Less than or equal: true if x is less than or equal to y . Comparison > x > y Greater than: true if x is greater than to y . Comparison >= x >= y Greater than or equal: true if x is greater than or equal to y . Logical and x and y A short-circuiting logical AND: true if both operands are true . Logical or x or y A short-circuiting logical OR: true if either operand is true . Logical not not x Logical NOT: true if the operand is false .","title":"Operators"},{"location":"design/expressions/#conversions-and-casts","text":"When an expression appears in a context in which an expression of a specific type is expected, implicit conversions are applied to convert the expression to the target type. Expressions can also be converted to a specific type using an as expression . fn Bar(n: i32); fn Baz(n: i64) { // OK, same as Bar(n as i32) Bar(n); }","title":"Conversions and casts"},{"location":"design/expressions/#if-expressions","text":"An if expression chooses between two expressions. fn Run(args: Span(StringView)) { var file: StringView = if args.size() > 1 then args[1] else \"/dev/stdin\"; } if expressions are analogous to ?: ternary expressions in C and C++.","title":"if expressions"},{"location":"design/expressions/#alternatives-considered","text":"Other expression documents will list more alternatives; this lists alternatives not noted elsewhere. Total order Different precedence for different operands Require less than a partial order","title":"Alternatives considered"},{"location":"design/expressions/#references","text":"Other expression documents will list more references; this lists references not noted elsewhere. Proposal #555: Operator precedence .","title":"References"},{"location":"design/expressions/arithmetic/","text":"Arithmetic Table of contents Overview Precedence and associativity Built-in types Integer types Overflow and other error conditions Floating-point types Strings Extensibility Alternatives considered References Overview Carbon provides a conventional set of arithmetic operators: var a: i32 = 5; var b: i32 = 3; // -5 var negation: i32 = -a; // 8 var sum: i32 = a + b; // 2 var difference: i32 = a - b; // 15 var product: i32 = a * b; // 1 var quotient: i32 = a / b; // 2 var remainder: i32 = a % b; These operators have predefined meanings for some of Carbon's built-in types . User-defined types can define the meaning of these operations by implementing an interface provided as part of the Carbon standard library. Precedence and associativity %%{init: {'themeVariables': {'fontFamily': 'monospace'}}}%% graph BT negation[\"-x\"] multiplication>\"x * y<br> x / y\"] addition>\"x + y<br> x - y\"] modulo[\"x % y\"] multiplication & modulo --> negation addition --> multiplication Instructions for reading this diagram. Binary + and - can be freely mixed, and are left-associative. // -2, same as `((1 - 2) + 3) - 4`. var n: i32 = 1 - 2 + 3 - 4; Binary * and / can be freely mixed, and are left-associative. // 0.375, same as `((1.0 / 2.0) * 3.0) / 4.0`. var m: f32 = 1.0 / 2.0 * 3.0 / 4.0; Unary - has higher precedence than binary * , / , and % . Binary * and / have higher precedence than binary + and - . // 5, same as `(-1) + ((-2) * (-3))`. var x: i32 = -1 + -2 * -3; // Error, parentheses required: no precedence order between `+` and `%`. var y: i32 = 2 + 3 % 5; Built-in types For binary operators, if the operands have different built-in types, they are converted as follows: If the types are uN and uM , or they are iN and iM , the operands are converted to the larger type. If one type is iN and the other type is uM , and M < N , the uM operand is converted to iN . If one type is fN and the other type is iM or uM , and there is an implicit conversion from the integer type to fN , then the integer operand is converted to fN . More broadly, if one operand is of built-in type and the other operand can be implicitly converted to that type, then it is, unless that behavior is overridden . A built-in arithmetic operation is performed if, after the above conversion step, the operands have the same built-in type. The result type is that type. The result type is never wider than the operands, and the conversions applied to the operands are always lossless, so arithmetic between a wider unsigned integer type and a narrower signed integer is not defined. Although the conversions are always lossless, the arithmetic may still overflow . Integer types Signed and unsigned integer types support all the arithmetic operators. Signed integer arithmetic produces the usual mathematical result. Unsigned integer arithmetic in uN wraps around modulo 2 N . Division truncates towards zero. The result of the % operator is defined by the equation a % b == a - a / b * b . Overflow and other error conditions Integer arithmetic is subject to two classes of problems for which an operation has no representable result: Overflow, where the resulting value is too large to be represented in the type, or, for % , when the implied multiplication overflows. Division by zero. Unsigned integer arithmetic cannot overflow, but division by zero can still occur. Note: All arithmetic operators can overflow for signed integer types. For example, given a value v: iN that is the least possible value for its type, -v , v + v , v - 1 , v * 2 , v / -1 , and v % -1 all result in overflow. Signed integer overflow and signed or unsigned integer division by zero are programming errors: In a development build, they will be caught immediately when they happen at runtime. In a performance build, the optimizer can assume that such conditions don't occur. As a consequence, if they do, the behavior of the program is not defined. In a hardened build, overflow and division by zero do not result in undefined behavior. On overflow and division by zero, either the program will be aborted, or the arithmetic will evaluate to a mathematically incorrect result, such as a two's complement result or zero. The program might not in all cases be aborted immediately -- for example, multiple overflow checks might be combined into one -- but no control flow or memory access that depends on the value will be performed. TODO: Unify the description of these programming errors with those of bit-shift domain errors, document the behavior in a common place and link to it from here. TODO: In a hardened build, should we prefer to trap on overflow, give a two's complement result, or produce zero? Using zero may defeat some classes of exploit, but comes at a code size and performance cost. Floating-point types Floating-point types support all the arithmetic operators other than % . Floating-point types in Carbon have IEEE 754 semantics, use the round-to-nearest rounding mode, and do not set any floating-point exception state. Because floating-point arithmetic follows IEEE 754 rules: overflow results in \u00b1\u221e, and division by zero results in either \u00b1\u221e or, for 0.0 / 0.0, a quiet NaN. Strings TODO: Decide whether strings are built-in types, and whether they support + for concatenation. See #457 . Extensibility Arithmetic operators can be provided for user-defined types by implementing the following family of interfaces: // Unary `-`. interface Negate { let Result:! Type = Self; fn Op[me: Self]() -> Result; } // Binary `+`. interface AddWith(U:! Type) { let Result:! Type = Self; fn Op[me: Self](other: U) -> Result; } constraint Add { extends AddWith(Self) where .Result = Self; } // Binary `-`. interface SubWith(U:! Type) { let Result:! Type = Self; fn Op[me: Self](other: U) -> Result; } constraint Sub { extends SubWith(Self) where .Result = Self; } // Binary `*`. interface MulWith(U:! Type) { let Result:! Type = Self; fn Op[me: Self](other: U) -> Result; } constraint Mul { extends MulWith(Self) where .Result = Self; } // Binary `/`. interface DivWith(U:! Type) { let Result:! Type = Self; fn Op[me: Self](other: U) -> Result; } constraint Div { extends DivWith(Self) where .Result = Self; } // Binary `%`. interface ModWith(U:! Type) { let Result:! Type = Self; fn Op[me: Self](other: U) -> Result; } constraint Mod { extends ModWith(Self) where .Result = Self; } Given x: T and y: U : The expression -x is rewritten to x.(Negate.Op)() . The expression x + y is rewritten to x.(AddWith(U).Op)(y) . The expression x - y is rewritten to x.(SubWith(U).Op)(y) . The expression x * y is rewritten to x.(MulWith(U).Op)(y) . The expression x / y is rewritten to x.(DivWith(U).Op)(y) . The expression x % y is rewritten to x.(ModWith(U).Op)(y) . Implementations of these interfaces are provided for built-in types as necessary to give the semantics described above. Alternatives considered Use a sufficiently wide result type to avoid overflow Guarantee that the program never proceeds with an incorrect value after overflow Guarantee that all integer arithmetic is two's complement Treat overflow as an error but don't optimize on it Don't let Unsigned arithmetic wrap Provide separate wrapping types Do not provide an ordering or division for uN Give unary - lower precedence Include a unary plus operator Floating-point modulo operator Provide different division operators Use different division and modulo semantics Use different precedence groups for division and multiplication Use the same precedence group for modulo and multiplication Use a different spelling for modulo References Proposal #1083: Arithmetic Proposal #1178: Rework operator interfaces","title":"Arithmetic"},{"location":"design/expressions/arithmetic/#arithmetic","text":"","title":"Arithmetic"},{"location":"design/expressions/arithmetic/#table-of-contents","text":"Overview Precedence and associativity Built-in types Integer types Overflow and other error conditions Floating-point types Strings Extensibility Alternatives considered References","title":"Table of contents"},{"location":"design/expressions/arithmetic/#overview","text":"Carbon provides a conventional set of arithmetic operators: var a: i32 = 5; var b: i32 = 3; // -5 var negation: i32 = -a; // 8 var sum: i32 = a + b; // 2 var difference: i32 = a - b; // 15 var product: i32 = a * b; // 1 var quotient: i32 = a / b; // 2 var remainder: i32 = a % b; These operators have predefined meanings for some of Carbon's built-in types . User-defined types can define the meaning of these operations by implementing an interface provided as part of the Carbon standard library.","title":"Overview"},{"location":"design/expressions/arithmetic/#precedence-and-associativity","text":"%%{init: {'themeVariables': {'fontFamily': 'monospace'}}}%% graph BT negation[\"-x\"] multiplication>\"x * y<br> x / y\"] addition>\"x + y<br> x - y\"] modulo[\"x % y\"] multiplication & modulo --> negation addition --> multiplication Instructions for reading this diagram. Binary + and - can be freely mixed, and are left-associative. // -2, same as `((1 - 2) + 3) - 4`. var n: i32 = 1 - 2 + 3 - 4; Binary * and / can be freely mixed, and are left-associative. // 0.375, same as `((1.0 / 2.0) * 3.0) / 4.0`. var m: f32 = 1.0 / 2.0 * 3.0 / 4.0; Unary - has higher precedence than binary * , / , and % . Binary * and / have higher precedence than binary + and - . // 5, same as `(-1) + ((-2) * (-3))`. var x: i32 = -1 + -2 * -3; // Error, parentheses required: no precedence order between `+` and `%`. var y: i32 = 2 + 3 % 5;","title":"Precedence and associativity"},{"location":"design/expressions/arithmetic/#built-in-types","text":"For binary operators, if the operands have different built-in types, they are converted as follows: If the types are uN and uM , or they are iN and iM , the operands are converted to the larger type. If one type is iN and the other type is uM , and M < N , the uM operand is converted to iN . If one type is fN and the other type is iM or uM , and there is an implicit conversion from the integer type to fN , then the integer operand is converted to fN . More broadly, if one operand is of built-in type and the other operand can be implicitly converted to that type, then it is, unless that behavior is overridden . A built-in arithmetic operation is performed if, after the above conversion step, the operands have the same built-in type. The result type is that type. The result type is never wider than the operands, and the conversions applied to the operands are always lossless, so arithmetic between a wider unsigned integer type and a narrower signed integer is not defined. Although the conversions are always lossless, the arithmetic may still overflow .","title":"Built-in types"},{"location":"design/expressions/arithmetic/#integer-types","text":"Signed and unsigned integer types support all the arithmetic operators. Signed integer arithmetic produces the usual mathematical result. Unsigned integer arithmetic in uN wraps around modulo 2 N . Division truncates towards zero. The result of the % operator is defined by the equation a % b == a - a / b * b .","title":"Integer types"},{"location":"design/expressions/arithmetic/#overflow-and-other-error-conditions","text":"Integer arithmetic is subject to two classes of problems for which an operation has no representable result: Overflow, where the resulting value is too large to be represented in the type, or, for % , when the implied multiplication overflows. Division by zero. Unsigned integer arithmetic cannot overflow, but division by zero can still occur. Note: All arithmetic operators can overflow for signed integer types. For example, given a value v: iN that is the least possible value for its type, -v , v + v , v - 1 , v * 2 , v / -1 , and v % -1 all result in overflow. Signed integer overflow and signed or unsigned integer division by zero are programming errors: In a development build, they will be caught immediately when they happen at runtime. In a performance build, the optimizer can assume that such conditions don't occur. As a consequence, if they do, the behavior of the program is not defined. In a hardened build, overflow and division by zero do not result in undefined behavior. On overflow and division by zero, either the program will be aborted, or the arithmetic will evaluate to a mathematically incorrect result, such as a two's complement result or zero. The program might not in all cases be aborted immediately -- for example, multiple overflow checks might be combined into one -- but no control flow or memory access that depends on the value will be performed. TODO: Unify the description of these programming errors with those of bit-shift domain errors, document the behavior in a common place and link to it from here. TODO: In a hardened build, should we prefer to trap on overflow, give a two's complement result, or produce zero? Using zero may defeat some classes of exploit, but comes at a code size and performance cost.","title":"Overflow and other error conditions"},{"location":"design/expressions/arithmetic/#floating-point-types","text":"Floating-point types support all the arithmetic operators other than % . Floating-point types in Carbon have IEEE 754 semantics, use the round-to-nearest rounding mode, and do not set any floating-point exception state. Because floating-point arithmetic follows IEEE 754 rules: overflow results in \u00b1\u221e, and division by zero results in either \u00b1\u221e or, for 0.0 / 0.0, a quiet NaN.","title":"Floating-point types"},{"location":"design/expressions/arithmetic/#strings","text":"TODO: Decide whether strings are built-in types, and whether they support + for concatenation. See #457 .","title":"Strings"},{"location":"design/expressions/arithmetic/#extensibility","text":"Arithmetic operators can be provided for user-defined types by implementing the following family of interfaces: // Unary `-`. interface Negate { let Result:! Type = Self; fn Op[me: Self]() -> Result; } // Binary `+`. interface AddWith(U:! Type) { let Result:! Type = Self; fn Op[me: Self](other: U) -> Result; } constraint Add { extends AddWith(Self) where .Result = Self; } // Binary `-`. interface SubWith(U:! Type) { let Result:! Type = Self; fn Op[me: Self](other: U) -> Result; } constraint Sub { extends SubWith(Self) where .Result = Self; } // Binary `*`. interface MulWith(U:! Type) { let Result:! Type = Self; fn Op[me: Self](other: U) -> Result; } constraint Mul { extends MulWith(Self) where .Result = Self; } // Binary `/`. interface DivWith(U:! Type) { let Result:! Type = Self; fn Op[me: Self](other: U) -> Result; } constraint Div { extends DivWith(Self) where .Result = Self; } // Binary `%`. interface ModWith(U:! Type) { let Result:! Type = Self; fn Op[me: Self](other: U) -> Result; } constraint Mod { extends ModWith(Self) where .Result = Self; } Given x: T and y: U : The expression -x is rewritten to x.(Negate.Op)() . The expression x + y is rewritten to x.(AddWith(U).Op)(y) . The expression x - y is rewritten to x.(SubWith(U).Op)(y) . The expression x * y is rewritten to x.(MulWith(U).Op)(y) . The expression x / y is rewritten to x.(DivWith(U).Op)(y) . The expression x % y is rewritten to x.(ModWith(U).Op)(y) . Implementations of these interfaces are provided for built-in types as necessary to give the semantics described above.","title":"Extensibility"},{"location":"design/expressions/arithmetic/#alternatives-considered","text":"Use a sufficiently wide result type to avoid overflow Guarantee that the program never proceeds with an incorrect value after overflow Guarantee that all integer arithmetic is two's complement Treat overflow as an error but don't optimize on it Don't let Unsigned arithmetic wrap Provide separate wrapping types Do not provide an ordering or division for uN Give unary - lower precedence Include a unary plus operator Floating-point modulo operator Provide different division operators Use different division and modulo semantics Use different precedence groups for division and multiplication Use the same precedence group for modulo and multiplication Use a different spelling for modulo","title":"Alternatives considered"},{"location":"design/expressions/arithmetic/#references","text":"Proposal #1083: Arithmetic Proposal #1178: Rework operator interfaces","title":"References"},{"location":"design/expressions/as_expressions/","text":"as expressions Table of contents Overview Precedence and associativity Built-in types Data types Compatible types Extensibility Alternatives considered References Overview An expression of one type can be explicitly cast to another type by using an as expression: var n: i32 = Get(); var f: f32 = n as f32; An as expression can be used to perform any implicit conversion, either when the context does not imply a destination type or when it is valuable to a reader of the code to make the conversion explicit. In addition, as expressions can perform safe conversions that nonetheless should not be performed implicitly, such as lossy conversions or conversions that lose capabilities or change the way a type would be interpreted. As guidelines, an as conversion should be permitted when: The conversion is complete : it produces a well-defined output value for each input value. The conversion is unsurprising : the resulting value is the expected value in the destination type. For example: A conversion from fM to iN is not complete, because it is not defined for input values that are out of the range of the destination type, such as infinities or, if N is too small, large finite values. A conversion from iM to iN , where N < M , is either not complete or not unsurprising, because there is more than one possible expected behavior for an input value that is not within the destination type, and those behaviors are not substantially the same -- we could perform two's complement wrapping, saturate, or produce undefined behavior analogous to arithmetic overflow. A conversion from iM to fN can be unsurprising, because even though there may be a choice of which way to round, the possible values are substantially the same. It is possible for user-defined types to extend the set of valid explicit casts that can be performed by as . Such extensions are expected to follow these guidelines. Precedence and associativity as expressions are non-associative. var b: bool = true; // OK var n: i32 = (b as i1) as i32; var m: auto = b as (bool as Hashable); // Error, ambiguous var m: auto = b as T as U; Note: b as (bool as Hashable) is valid but not useful, because the second operand of as is implicitly converted to type Type . This expression therefore has the same interpretation as b as bool . TODO: We should consider making as expressions left-associative now that facet types have been removed from the language. The as operator has lower precedence than operators that visually bind tightly: prefix symbolic operators dereference ( *a ) negation ( -a ) complement ( ~a ) postfix symbolic operators pointer type formation ( T* ), function call ( a(...) ), array indexing ( a[...] ), and member access ( a.m ). The as operator has higher precedence than assignment and comparison. It is unordered with respect to binary arithmetic, bitwise operators, and unary not . // OK var x: i32* as Eq; // OK, `x as (U*)` not `(x as U)*`. var y: auto = x as U*; var a: i32; var b: i32; // OK, `(a as i64) < ((*x) as i64)`. if (a as i64 < *x as i64) {} // Ambiguous: `(a + b) as i64` or `a + (b as i64)`? var c: i32 = a + b as i64; // Ambiguous: `(a as i64) + b` or `a as (i64 + b)`? var d: i32 = a as i64 + b; // OK, `(-a) as f64`, not `-(a as f64)`. // Unfortunately, the former is undefined if `a` is `i32.MinValue()`; // the latter is not. var u: f64 = -a as f64; // OK, `i32 as (GetType())`, not `(i32 as GetType)()`. var e: i32 as GetType(); Built-in types Data types In addition to the implicit conversions , the following numeric conversions are supported by as : iN , uN , or fN -> fM , for any N and M . Values that cannot be exactly represented are suitably rounded to one of the two nearest representable values. Very large finite values may be rounded to an infinity. NaN values are converted to NaN values. bool -> iN or uN . false converts to 0 and true converts to 1 (or to -1 for i1 ). Conversions from numeric types to bool are not supported with as ; instead of using as bool , such conversions can be performed with != 0 . Lossy conversions between iN or uN and iM or uM are not supported with as , and similarly conversions from fN to iM are not supported. Future work: Add mechanisms to perform these conversions. Compatible types The following conversion is supported by as : T -> U if T is compatible with U . Future work: We may need a mechanism to restrict which conversions between adapters are permitted and which code can perform them. Some of the conversions permitted by this rule may only be allowed in certain contexts. Extensibility Explicit casts can be defined for user-defined types such as classes by implementing the As interface: interface As(Dest:! Type) { fn Convert[me: Self]() -> Dest; } The expression x as U is rewritten to x.(As(U).Convert)() . Note: This rewrite causes the expression U to be implicitly converted to type Type . The program is invalid if this conversion is not possible. Alternatives considered Allow as to perform some unsafe conversions Allow as to perform two's complement truncation as only performs implicit conversions Integer to bool conversions Bool to integer conversions References Implicit conversions in C++ Proposal #845: as expressions .","title":"`as` expressions"},{"location":"design/expressions/as_expressions/#as-expressions","text":"","title":"as expressions"},{"location":"design/expressions/as_expressions/#table-of-contents","text":"Overview Precedence and associativity Built-in types Data types Compatible types Extensibility Alternatives considered References","title":"Table of contents"},{"location":"design/expressions/as_expressions/#overview","text":"An expression of one type can be explicitly cast to another type by using an as expression: var n: i32 = Get(); var f: f32 = n as f32; An as expression can be used to perform any implicit conversion, either when the context does not imply a destination type or when it is valuable to a reader of the code to make the conversion explicit. In addition, as expressions can perform safe conversions that nonetheless should not be performed implicitly, such as lossy conversions or conversions that lose capabilities or change the way a type would be interpreted. As guidelines, an as conversion should be permitted when: The conversion is complete : it produces a well-defined output value for each input value. The conversion is unsurprising : the resulting value is the expected value in the destination type. For example: A conversion from fM to iN is not complete, because it is not defined for input values that are out of the range of the destination type, such as infinities or, if N is too small, large finite values. A conversion from iM to iN , where N < M , is either not complete or not unsurprising, because there is more than one possible expected behavior for an input value that is not within the destination type, and those behaviors are not substantially the same -- we could perform two's complement wrapping, saturate, or produce undefined behavior analogous to arithmetic overflow. A conversion from iM to fN can be unsurprising, because even though there may be a choice of which way to round, the possible values are substantially the same. It is possible for user-defined types to extend the set of valid explicit casts that can be performed by as . Such extensions are expected to follow these guidelines.","title":"Overview"},{"location":"design/expressions/as_expressions/#precedence-and-associativity","text":"as expressions are non-associative. var b: bool = true; // OK var n: i32 = (b as i1) as i32; var m: auto = b as (bool as Hashable); // Error, ambiguous var m: auto = b as T as U; Note: b as (bool as Hashable) is valid but not useful, because the second operand of as is implicitly converted to type Type . This expression therefore has the same interpretation as b as bool . TODO: We should consider making as expressions left-associative now that facet types have been removed from the language. The as operator has lower precedence than operators that visually bind tightly: prefix symbolic operators dereference ( *a ) negation ( -a ) complement ( ~a ) postfix symbolic operators pointer type formation ( T* ), function call ( a(...) ), array indexing ( a[...] ), and member access ( a.m ). The as operator has higher precedence than assignment and comparison. It is unordered with respect to binary arithmetic, bitwise operators, and unary not . // OK var x: i32* as Eq; // OK, `x as (U*)` not `(x as U)*`. var y: auto = x as U*; var a: i32; var b: i32; // OK, `(a as i64) < ((*x) as i64)`. if (a as i64 < *x as i64) {} // Ambiguous: `(a + b) as i64` or `a + (b as i64)`? var c: i32 = a + b as i64; // Ambiguous: `(a as i64) + b` or `a as (i64 + b)`? var d: i32 = a as i64 + b; // OK, `(-a) as f64`, not `-(a as f64)`. // Unfortunately, the former is undefined if `a` is `i32.MinValue()`; // the latter is not. var u: f64 = -a as f64; // OK, `i32 as (GetType())`, not `(i32 as GetType)()`. var e: i32 as GetType();","title":"Precedence and associativity"},{"location":"design/expressions/as_expressions/#built-in-types","text":"","title":"Built-in types"},{"location":"design/expressions/as_expressions/#data-types","text":"In addition to the implicit conversions , the following numeric conversions are supported by as : iN , uN , or fN -> fM , for any N and M . Values that cannot be exactly represented are suitably rounded to one of the two nearest representable values. Very large finite values may be rounded to an infinity. NaN values are converted to NaN values. bool -> iN or uN . false converts to 0 and true converts to 1 (or to -1 for i1 ). Conversions from numeric types to bool are not supported with as ; instead of using as bool , such conversions can be performed with != 0 . Lossy conversions between iN or uN and iM or uM are not supported with as , and similarly conversions from fN to iM are not supported. Future work: Add mechanisms to perform these conversions.","title":"Data types"},{"location":"design/expressions/as_expressions/#compatible-types","text":"The following conversion is supported by as : T -> U if T is compatible with U . Future work: We may need a mechanism to restrict which conversions between adapters are permitted and which code can perform them. Some of the conversions permitted by this rule may only be allowed in certain contexts.","title":"Compatible types"},{"location":"design/expressions/as_expressions/#extensibility","text":"Explicit casts can be defined for user-defined types such as classes by implementing the As interface: interface As(Dest:! Type) { fn Convert[me: Self]() -> Dest; } The expression x as U is rewritten to x.(As(U).Convert)() . Note: This rewrite causes the expression U to be implicitly converted to type Type . The program is invalid if this conversion is not possible.","title":"Extensibility"},{"location":"design/expressions/as_expressions/#alternatives-considered","text":"Allow as to perform some unsafe conversions Allow as to perform two's complement truncation as only performs implicit conversions Integer to bool conversions Bool to integer conversions","title":"Alternatives considered"},{"location":"design/expressions/as_expressions/#references","text":"Implicit conversions in C++ Proposal #845: as expressions .","title":"References"},{"location":"design/expressions/bitwise/","text":"Bitwise and shift operators Table of contents Overview Precedence and associativity Integer types Integer constants Extensibility Alternatives considered References Overview Carbon provides a conventional set of operators for operating on bits: var a: u8 = 5; var b: u8 = 3; var c: i8 = -5; // 250 var complement: u8 = ^a; // 1 var bitwise_and: u8 = a & b; // 7 var bitwise_or: u8 = a | b; // 6 var bitwise_xor: u8 = a ^ b; // 40 var left_shift: u8 = a << b; // 2 var logical_right_shift: u8 = a >> 1; // -3 var arithmetic_right_shift: i8 = c >> 1; These operators have predefined meanings for Carbon's integer types. User-defined types can define the meaning of these operations by implementing an interface provided as part of the Carbon standard library. Precedence and associativity %%{init: {'themeVariables': {'fontFamily': 'monospace'}}}%% graph BT complement[\"^x\"] bitwise_and>\"x & y\"] bitwise_or>\"x | y\"] bitwise_xor>\"x ^ y\"] shift[\"x << y<br> x >> y\"] bitwise_and & bitwise_or & bitwise_xor & shift --> complement Instructions for reading this diagram. Parentheses are required when mixing different bitwise and bit-shift operators. Binary & , | , and ^ are left-associative. The bit-shift operators << and >> are non-associative. // \u2705 Same as (1 | 2) | 4, evaluates to 7. var a: i32 = 1 | 2 | 4; // \u274c Error, parentheses are required to distinguish between // (3 | 5) & 6, which evaluates to 6, and // 3 | (5 & 6), which evaluates to 7. var b: i32 = 3 | 5 & 6; // \u274c Error, parentheses are required to distinguish between // (1 << 2) << 3, which evaluates to 4 << 3 == 32, and // 1 << (2 << 3), which evaluates to 1 << 16 == 65536. var c: i32 = 1 << 2 << 3; // \u274c Error, can't repeat the `^` operator. Use `^(^4)` or simply `4`. var d: i32 = ^^4; Integer types Bitwise and bit-shift operators are supported for Carbon's built-in integer types, and, unless that behavior is overridden , for types that can be implicitly converted to integer types, as follows: For binary bitwise operators, if one operand has an integer type and the other operand can be implicitly converted to that type, then it is. If both operands are of integer type, this results in the following conversions: If the types are uN and uM , or they are iN and iM , the operands are converted to the larger type. If one type is iN and the other type is uM , and M < N , the uM operand is converted to iN . A built-in binary bitwise & , | , or ^ operation is performed if, after the above conversion step, the operands have the same integer type. The result type is that type, and the result value is produced by applying the relevant operation -- AND, OR, or XOR -- to each pair of corresponding bits in the input, including the sign bit for a signed integer type. A built-in complement operation is performed if the operand can be implicitly converted to an integer type. The result type is that type, and the result value is produced by flipping all bits in the input, including the sign bit for a signed integer type. ^a is equivalent to a ^ x , where x is the all-one-bits value of the same type as a . A built-in bit-shift operation is performed if both operands are, or can be implicitly converted to, integer types. The result type is the converted type of the first operand. The result value is produced by shifting the first operand left for << or right for >> a number of positions equal to the second operand. Vacant positions are filled with 0 bits, except for a right shift where the first operand is of a signed type and has a negative value, in which case they are filled with 1 bits. For the purposes of bit-shifts, bits are ordered by significance, with the most significant bit being the leftmost bit and the least significant bit being the rightmost bit. As a consequence, in the absence of overflow a left shift is equivalent to a multiplication by a power of 2 and a right shift is equivalent to a division by a power of two, rounding downwards. The second operand of a bit-shift is required to be between zero (inclusive) and the bit-width of the first operand (exclusive); it is a programming error if the second operand is not within that range. In a development build, they will be caught immediately when they happen at runtime. In a performance build, the optimizer may assume that this programming error does not occur. In a hardened build, the result will have well the defined behavior of either aborting the program or performing a shift of an unspecified number of bits, which if wider than the first operand will result in 0 or -1 . In the case where the program is aborted, the program might not in all cases be aborted immediately -- for example, multiple checks might be combined into one -- but no control flow or memory access that depends on the value will be performed. TODO: Unify the description of these programming errors with those of arithmetic overflow, document the behavior in a common place and link to it from here. Integer constants These operations can also be applied to a pair of integer constants, or to an integer constant and a value of integer type, as follows: If any binary bitwise or bit-shift operator is applied to two integer constants, or the unary ^ operator is applied to an integer constant, the result is an integer constant. Integer constants are treated as having infinitely many high-order bits, where all but finitely many of those bits are sign bits. For example, -1 comprises infinitely many 1 bits. Note that there is no difference between an arithmetic and a logical right shift on an integer constant, because every bit always has a higher-order bit to shift from. It is easy to produce extremely large numbers by left-shifting an integer constant. For example, the binary representation of 1 << (1 << 1000) is thought to be substantially larger than the total entropy in the observable universe. In practice, Carbon implementations will set a much lower limit on the largest integer constant that they support. If a binary bitwise & , | , or ^ operation is applied to an integer constant and a value of an integer type to which the constant can be implicitly converted, the operand that is an integer constant is implicitly converted to the integer type and the computation is performed as described above . If the second operand of a bit-shift operator is an integer constant and the first operand is not, and the second operand is between 0 (inclusive) and the bit-width of the first operand (exclusive), the integer constant is converted to an integer type that can hold its value and the computation is performed as described above. Other operations involving integer constants are invalid. For example, a bitwise & between a u8 and an integer constant 500 is invalid because 500 doesn't fit into u8 , and 1 << n is invalid if n is an integer variable because we don't know what type to use to compute the result. Note that the unary ^ operator applied to a non-negative integer constant results in a negative integer constant, and the binary ^ operator gives a negative result if exactly one of the input operands was negative. For example, ^0 == -1 evaluates to true . Extensibility Bitwise and shift operators can be provided for user-defined types by implementing the following family of interfaces: // Unary `^`. interface BitComplement { let Result:! Type = Self; fn Op[me: Self]() -> Result; } // Binary `&`. interface BitAndWith(U:! Type) { let Result:! Type = Self; fn Op[me: Self](other: U) -> Result; } constraint BitAnd { extends BitAndWith(Self) where .Result = Self; } // Binary `|`. interface BitOrWith(U:! Type) { let Result:! Type = Self; fn Op[me: Self](other: U) -> Result; } constraint BitOr { extends BitOrWith(Self) where .Result = Self; } // Binary `^`. interface BitXorWith(U:! Type) { let Result:! Type = Self; fn Op[me: Self](other: U) -> Result; } constraint BitXor { extends BitXorWith(Self) where .Result = Self; } // Binary `<<`. interface LeftShiftWith(U:! Type) { let Result:! Type = Self; fn Op[me: Self](other: U) -> Result; } constraint LeftShift { extends LeftShiftWith(Self) where .Result = Self; } // Binary `>>`. interface RightShiftWith(U:! Type) { let Result:! Type = Self; fn Op[me: Self](other: U) -> Result; } constraint RightShift { extends RightShiftWith(Self) where .Result = Self; } Given x: T and y: U : The expression ^x is rewritten to x.(BitComplement.Op)() . The expression x & y is rewritten to x.(BitAndWith(U).Op)(y) . The expression x | y is rewritten to x.(BitOrWith(U).Op)(y) . The expression x ^ y is rewritten to x.(BitXorWith(U).Op)(y) . The expression x << y is rewritten to x.(LeftShiftWith(U).Op)(y) . The expression x >> y is rewritten to x.(RightShiftWith(U).Op)(y) . Implementations of these interfaces are provided for built-in types as necessary to give the semantics described above. Alternatives considered Use different symbols for bitwise operators Provide different operators for arithmetic and logical shifts Provide rotate operators Guarantee the behavior of large shifts Support shifting a constant by a variable References Proposal #1191: bitwise and shift operators .","title":"Bitwise and shift operators"},{"location":"design/expressions/bitwise/#bitwise-and-shift-operators","text":"","title":"Bitwise and shift operators"},{"location":"design/expressions/bitwise/#table-of-contents","text":"Overview Precedence and associativity Integer types Integer constants Extensibility Alternatives considered References","title":"Table of contents"},{"location":"design/expressions/bitwise/#overview","text":"Carbon provides a conventional set of operators for operating on bits: var a: u8 = 5; var b: u8 = 3; var c: i8 = -5; // 250 var complement: u8 = ^a; // 1 var bitwise_and: u8 = a & b; // 7 var bitwise_or: u8 = a | b; // 6 var bitwise_xor: u8 = a ^ b; // 40 var left_shift: u8 = a << b; // 2 var logical_right_shift: u8 = a >> 1; // -3 var arithmetic_right_shift: i8 = c >> 1; These operators have predefined meanings for Carbon's integer types. User-defined types can define the meaning of these operations by implementing an interface provided as part of the Carbon standard library.","title":"Overview"},{"location":"design/expressions/bitwise/#precedence-and-associativity","text":"%%{init: {'themeVariables': {'fontFamily': 'monospace'}}}%% graph BT complement[\"^x\"] bitwise_and>\"x & y\"] bitwise_or>\"x | y\"] bitwise_xor>\"x ^ y\"] shift[\"x << y<br> x >> y\"] bitwise_and & bitwise_or & bitwise_xor & shift --> complement Instructions for reading this diagram. Parentheses are required when mixing different bitwise and bit-shift operators. Binary & , | , and ^ are left-associative. The bit-shift operators << and >> are non-associative. // \u2705 Same as (1 | 2) | 4, evaluates to 7. var a: i32 = 1 | 2 | 4; // \u274c Error, parentheses are required to distinguish between // (3 | 5) & 6, which evaluates to 6, and // 3 | (5 & 6), which evaluates to 7. var b: i32 = 3 | 5 & 6; // \u274c Error, parentheses are required to distinguish between // (1 << 2) << 3, which evaluates to 4 << 3 == 32, and // 1 << (2 << 3), which evaluates to 1 << 16 == 65536. var c: i32 = 1 << 2 << 3; // \u274c Error, can't repeat the `^` operator. Use `^(^4)` or simply `4`. var d: i32 = ^^4;","title":"Precedence and associativity"},{"location":"design/expressions/bitwise/#integer-types","text":"Bitwise and bit-shift operators are supported for Carbon's built-in integer types, and, unless that behavior is overridden , for types that can be implicitly converted to integer types, as follows: For binary bitwise operators, if one operand has an integer type and the other operand can be implicitly converted to that type, then it is. If both operands are of integer type, this results in the following conversions: If the types are uN and uM , or they are iN and iM , the operands are converted to the larger type. If one type is iN and the other type is uM , and M < N , the uM operand is converted to iN . A built-in binary bitwise & , | , or ^ operation is performed if, after the above conversion step, the operands have the same integer type. The result type is that type, and the result value is produced by applying the relevant operation -- AND, OR, or XOR -- to each pair of corresponding bits in the input, including the sign bit for a signed integer type. A built-in complement operation is performed if the operand can be implicitly converted to an integer type. The result type is that type, and the result value is produced by flipping all bits in the input, including the sign bit for a signed integer type. ^a is equivalent to a ^ x , where x is the all-one-bits value of the same type as a . A built-in bit-shift operation is performed if both operands are, or can be implicitly converted to, integer types. The result type is the converted type of the first operand. The result value is produced by shifting the first operand left for << or right for >> a number of positions equal to the second operand. Vacant positions are filled with 0 bits, except for a right shift where the first operand is of a signed type and has a negative value, in which case they are filled with 1 bits. For the purposes of bit-shifts, bits are ordered by significance, with the most significant bit being the leftmost bit and the least significant bit being the rightmost bit. As a consequence, in the absence of overflow a left shift is equivalent to a multiplication by a power of 2 and a right shift is equivalent to a division by a power of two, rounding downwards. The second operand of a bit-shift is required to be between zero (inclusive) and the bit-width of the first operand (exclusive); it is a programming error if the second operand is not within that range. In a development build, they will be caught immediately when they happen at runtime. In a performance build, the optimizer may assume that this programming error does not occur. In a hardened build, the result will have well the defined behavior of either aborting the program or performing a shift of an unspecified number of bits, which if wider than the first operand will result in 0 or -1 . In the case where the program is aborted, the program might not in all cases be aborted immediately -- for example, multiple checks might be combined into one -- but no control flow or memory access that depends on the value will be performed. TODO: Unify the description of these programming errors with those of arithmetic overflow, document the behavior in a common place and link to it from here.","title":"Integer types"},{"location":"design/expressions/bitwise/#integer-constants","text":"These operations can also be applied to a pair of integer constants, or to an integer constant and a value of integer type, as follows: If any binary bitwise or bit-shift operator is applied to two integer constants, or the unary ^ operator is applied to an integer constant, the result is an integer constant. Integer constants are treated as having infinitely many high-order bits, where all but finitely many of those bits are sign bits. For example, -1 comprises infinitely many 1 bits. Note that there is no difference between an arithmetic and a logical right shift on an integer constant, because every bit always has a higher-order bit to shift from. It is easy to produce extremely large numbers by left-shifting an integer constant. For example, the binary representation of 1 << (1 << 1000) is thought to be substantially larger than the total entropy in the observable universe. In practice, Carbon implementations will set a much lower limit on the largest integer constant that they support. If a binary bitwise & , | , or ^ operation is applied to an integer constant and a value of an integer type to which the constant can be implicitly converted, the operand that is an integer constant is implicitly converted to the integer type and the computation is performed as described above . If the second operand of a bit-shift operator is an integer constant and the first operand is not, and the second operand is between 0 (inclusive) and the bit-width of the first operand (exclusive), the integer constant is converted to an integer type that can hold its value and the computation is performed as described above. Other operations involving integer constants are invalid. For example, a bitwise & between a u8 and an integer constant 500 is invalid because 500 doesn't fit into u8 , and 1 << n is invalid if n is an integer variable because we don't know what type to use to compute the result. Note that the unary ^ operator applied to a non-negative integer constant results in a negative integer constant, and the binary ^ operator gives a negative result if exactly one of the input operands was negative. For example, ^0 == -1 evaluates to true .","title":"Integer constants"},{"location":"design/expressions/bitwise/#extensibility","text":"Bitwise and shift operators can be provided for user-defined types by implementing the following family of interfaces: // Unary `^`. interface BitComplement { let Result:! Type = Self; fn Op[me: Self]() -> Result; } // Binary `&`. interface BitAndWith(U:! Type) { let Result:! Type = Self; fn Op[me: Self](other: U) -> Result; } constraint BitAnd { extends BitAndWith(Self) where .Result = Self; } // Binary `|`. interface BitOrWith(U:! Type) { let Result:! Type = Self; fn Op[me: Self](other: U) -> Result; } constraint BitOr { extends BitOrWith(Self) where .Result = Self; } // Binary `^`. interface BitXorWith(U:! Type) { let Result:! Type = Self; fn Op[me: Self](other: U) -> Result; } constraint BitXor { extends BitXorWith(Self) where .Result = Self; } // Binary `<<`. interface LeftShiftWith(U:! Type) { let Result:! Type = Self; fn Op[me: Self](other: U) -> Result; } constraint LeftShift { extends LeftShiftWith(Self) where .Result = Self; } // Binary `>>`. interface RightShiftWith(U:! Type) { let Result:! Type = Self; fn Op[me: Self](other: U) -> Result; } constraint RightShift { extends RightShiftWith(Self) where .Result = Self; } Given x: T and y: U : The expression ^x is rewritten to x.(BitComplement.Op)() . The expression x & y is rewritten to x.(BitAndWith(U).Op)(y) . The expression x | y is rewritten to x.(BitOrWith(U).Op)(y) . The expression x ^ y is rewritten to x.(BitXorWith(U).Op)(y) . The expression x << y is rewritten to x.(LeftShiftWith(U).Op)(y) . The expression x >> y is rewritten to x.(RightShiftWith(U).Op)(y) . Implementations of these interfaces are provided for built-in types as necessary to give the semantics described above.","title":"Extensibility"},{"location":"design/expressions/bitwise/#alternatives-considered","text":"Use different symbols for bitwise operators Provide different operators for arithmetic and logical shifts Provide rotate operators Guarantee the behavior of large shifts Support shifting a constant by a variable","title":"Alternatives considered"},{"location":"design/expressions/bitwise/#references","text":"Proposal #1191: bitwise and shift operators .","title":"References"},{"location":"design/expressions/comparison_operators/","text":"Comparison operators Table of contents Overview Details Precedence Associativity Built-in comparisons and implicit conversions Consistency with implicit conversions Comparisons with constants Extensibility Equality Ordering Compatibility of equality and ordering Custom result types Default implementations for basic types Open questions Alternatives considered References Overview Carbon provides equality and relational comparison operators, each with a standard mathematical meaning: Category Operator Example Mathematical meaning Description Equality == x == y = Equality or equal to Equality != x != y \u2260 Inequality or not equal to Relational < x < y < Less than Relational <= x <= y \u2264 Less than or equal to Relational > x > y > Less than Relational >= x >= y \u2265 Less than or equal to Comparison operators all return a bool ; they evaluate to true when the indicated comparison is true. All comparison operators are infix binary operators. These operators have predefined meanings for some of Carbon's built-in types , as well as for simple \"data\" types like structs and tuples. User-defined types can define the meaning of these operations by implementing an interface provided as part of the Carbon standard library. Details Precedence The comparison operators are all at the same precedence level. This level is lower than operators used to compute (non- bool ) values, higher than the logical operators and and or , and incomparable with the precedence of not . For example: // \u2705 Valid: precedence provides order of evaluation. if (n + m * 3 < n * n and 3 < m and m < 6) { ... } // The above is equivalent to: if (((n + (m * 3)) < (n * n)) and ((3 < m) and (m < 6))) { ... } // \u274c Invalid due to ambiguity: `(not a) == b` or `not (a == b)`? if (not a == b) { ... } // \u274c Invalid due to precedence: write `a == (not b)`. if (a == not b) { ... } // \u274c Invalid due to precedence: write `not (f < 5.0)`. if (not f < 5.0) { .... } Associativity The comparison operators are non-associative. For example: // \u274c Invalid: write `3 < m and m < 6`. if (3 < m < 6) { ... } // \u274c Invalid: write `a == b and b == c`. if (a == b == c) { ... } // \u274c Invalid: write `(m > 1) == (n > 1)`. if (m > 1 == n > 1) { ... } Built-in comparisons and implicit conversions Built-in comparisons are permitted in three cases: When both operands are of standard Carbon integer types ( Int(n) or Unsigned(n) ). When both operands are of standard Carbon floating-point types ( Float(n) ). When one operand is of floating-point type and the other is of integer type, if all values of the integer type can be exactly represented in the floating-point type. In each case, the result is the mathematically-correct answer. This applies even when comparing Int(n) with Unsigned(m) . For example: // \u2705 Valid: Fits case #1. The value of `compared` is `true` because `a` is less // than `b`, even though the result of a wrapping `i32` or `u32` comparison // would be `false`. fn Compare(a: i32, b: u32) -> bool { return a < b; } let compared: bool = Compare(-1, 4_000_000_000); // \u274c Invalid: Doesn't fit case #3 because `i64` values in general are not // exactly representable in the type `f32`. let float: f32 = 1.0e18; let integer: i64 = 1_000_000_000_000_000_000; let eq: bool = float == integer; Comparisons involving integer and floating-point constants are not covered by these rules and are discussed separately . Consistency with implicit conversions We support the following implicit conversions : From Int(n) to Int(m) if m > n . From Unsigned(n) to Int(m) or Unsigned(m) if m > n . From Float(n) to Float(m) if m > n . From Int(n) to Float(m) if Float(m) can represent all values of Int(n) . These rules can be summarized as: a type T can be converted to U if every value of type T is a value of type U . Implicit conversions are also supported from certain kinds of integer and floating-point constants to Int(n) and Float(n) types, if the constant can be represented in the type. All built-in comparisons can be viewed as performing implicit conversions on at most one of the operands in order to reach a suitable pair of identical or very similar types, and then performing a comparison on those types. The target types for these implicit conversions are, for each suitable value n : Int(n) versus Int(n) Unsigned(n) versus Unsigned(n) Int(n) versus Unsigned(n) Unsigned(n) versus Int(n) Float(n) versus Float(n) There will in general be multiple combinations of implicit conversions that will lead to one of the above forms, but we will arrive at the same result regardless of which is selected, because all comparisons are mathematically correct and all implicit conversions are lossless. Implementations are expected to do whatever is most efficient: for example, for u16 < i32 it is likely that the best choice would be to promote the u16 to i32 , not u32 . Because we only ever convert at most one operand, we never use an intermediate type that is larger than both input types. For example, both i32 and f32 can be implicitly converted to f64 , but we do not permit comparisons between i32 and f32 even though we could perform those comparisons in f64 . If such comparisons were permitted, the results could be surprising: // `i32` can exactly represent this value. var integer: i32 = 2_000_000_001; // This value is within the representable range for `f32`, but will be rounded // to 2_000_000_000.0 due to the limited precision of `f32`. var float: f32 = 2_000_000_001.0; // \u274c Invalid: `f32` cannot exactly represent all values of `i32`. if (integer == float) { ... } // \u2705 Valid: An explicit cast to `f64` on either side makes the code valid, but // will compare unequal because `float` was rounded to 2_000_000_000.0 // but `integer` will convert to exactly 2_000_000_001.0. if (integer == float as f64) { ... } if (integer as f64 == float) { ... } The two kinds of mixed-type comparison may be less efficient than the other kinds due to the slightly wider domain. Note that this approach diverges from C++, which would convert both operands to a common type first, sometimes performing a lossy conversion potentially giving an incorrect result, sometimes converting both operands, and sometimes using a wider type than either of the operand types. Comparisons with constants We permit the following comparisons involving constants: A constant can be compared with a value of any type to which it can be implicitly converted. Any two constants can be compared, even if there is no type that can represent both. As described in implicit conversions , integer constants can be implicitly converted to any integer or floating-point type that can represent their value, and floating-point constants can be implicitly converted to any floating-point type that can represent their value. Note that this disallows comparisons between, for example, i32 and an integer literal that cannot be represented in i32 . Such comparisons would always be tautological. This decision should be revisited if it proves problematic in practice, for example in templated code where the literal is sometimes in range. Extensibility User-defined types can extend the behavior of the comparison operators by implementing interfaces. In this section, various properties are specified that such implementations \"should\" satisfy. These properties are not enforced in general, but the standard library might detect violations of some of them in some circumstances. These properties may be assumed by generic code, resulting in unexpected behavior if they are violated. Equality Comparison operators can be provided for user-defined types by implementing the EqWith and OrderedWith interfaces. The EqWith interface is used to define the semantics of the == and != operators for a given pair of types: interface EqWith(U:! Type) { fn Equal[me: Self](u: U) -> bool; default fn NotEqual[me: Self](u: U) -> bool { return not (me == u); } } constraint Eq { extends EqWith(Self); } Given x: T and y: U : The expression x == y calls x.(EqWith(U).Equal)(y) . The expression x != y calls x.(EqWith(U).NotEqual)(y) . class Path { private var drive: String; private var path: String; private fn CanonicalPath[me: Self]() -> String; external impl as Eq { fn Equal[me: Self](other: Self) -> bool { return (me.drive, me.CanonicalPath()) == (other.drive, other.CanonicalPath()); } } } The EqWith overload is selected without considering possible implicit conversions. To permit implicit conversions in the operands of an == overload, the like operator can be used: class MyInt { var value: i32; fn Value[me: Self]() -> i32 { return me.value; } } external impl i32 as ImplicitAs(MyInt); external impl like MyInt as EqWith(like MyInt) { fn Equal[me: Self](other: Self) -> bool { return me.Value() == other.Value(); } } fn CompareBothWays(a: MyInt, b: i32, c: MyInt) -> bool { // OK, calls above implementation three times. return a == a and a != b and b == c; } The behavior of NotEqual can be overridden separately from the behavior of Equal to support cases like floating-point NaN values, where two values can compare neither equal nor not-equal, and thus both functions would return false . However, an implementation of EqWith should not allow both Equal and NotEqual to return true for the same pair of values. Additionally, these operations should have no observable side-effects. external impl like MyFloat as EqWith(like MyFloat) { fn Equal[me: MyFloat](other: MyFloat) -> bool { if (me.IsNaN() or other.IsNaN()) { return false; } return me.Representation() == other.Representation(); } fn NotEqual[me: MyFloat](other: MyFloat) -> bool { if (me.IsNaN() or other.IsNaN()) { return false; } return me.Representation() != other.Representation(); } } Heterogeneous comparisons must be defined both ways around: external impl like MyInt as EqWith(like MyFloat); external impl like MyFloat as EqWith(like MyInt); TODO: Add an adapter to the standard library to make it easy to define the reverse comparison. Ordering The OrderedWith interface is used to define the semantics of the < , <= , > , and >= operators for a given pair of types. choice Ordering { Less, Equivalent, Greater, Incomparable } interface OrderedWith(U:! Type) { fn Compare[me: Self](u: U) -> Ordering; default fn Less[me: Self](u: U) -> bool { return me.Compare(u) == Ordering.Less; } default fn LessOrEquivalent[me: Self](u: U) -> bool { let c: Ordering = me.Compare(u); return c == Ordering.Less or c == Ordering.Equivalent; } default fn Greater[me: Self](u: U) -> bool { return me.Compare(u) == Ordering.Greater; } default fn GreaterOrEquivalent[me: Self](u: U) -> bool { let c: Ordering = me.Compare(u); return c == Ordering.Greater or c == Ordering.Equivalent; } } constraint Ordered { extends OrderedWith(Self); } // Ordering.Less < Ordering.Equivalent < Ordering.Greater. // Ordering.Incomparable is incomparable with all three. external impl Ordering as Ordered; TODO: Revise the above when we have a concrete design for enumerated types. Given x: T and y: U : The expression x < y calls x.(OrderedWith(U).Less)(y) . The expression x <= y calls x.(OrderedWith(U).LessOrEquivalent)(y) . The expression x > y calls x.(OrderedWith(U).Greater)(y) . The expression x >= y calls x.(OrderedWith(U).GreaterOrEquivalent)(y) . For example: class MyWidget { var width: i32; var height: i32; fn Size[me: Self]() -> i32 { return me.width * me.height; } // Widgets are normally ordered by size. external impl as Ordered { fn Compare[me: Self](other: Self) -> Ordering { return me.Size().(Ordered.Compare)(other.Size()); } } } fn F(a: MyWidget, b: MyWidget) -> bool { return a <= b; } As for EqWith , the like operator can be used to permit implicit conversions when invoking a comparison, and heterogeneous comparisons must be defined both ways around: fn ReverseOrdering(o: Ordering) -> Ordering { return Ordering.Equivalent.(Ordered.Compare)(o); } external impl like MyInt as OrderedWith(like MyFloat); external impl like MyFloat as OrderedWith(like MyInt) { fn Compare[me: Self](other: Self) -> Ordering { return Reverse(other.(OrderedWith(Self).Compare)(me)); } } The default implementations of Less , LessOrEquivalent , Greater , and GreaterOrEquivalent can be overridden if a more efficient version can be implemented. The behaviors of such overrides should follow those of the above default implementations, and the members of an OrderedWith implementation should have no observable side-effects. OrderedWith implementations should be transitive . That is, given V:! Type , U:! OrderedWith(V) , T:! OrderedWith(U) & OrderedWith(V) , a: T , b: U , c: V , then: If a <= b and b <= c then a <= c , and moreover if either a < b or b < c then a < c . If a >= b and b >= c then a >= c , and moreover if either a > b or b > c then a > c . If a and b are equivalent, then a.Compare(c) == b.Compare(c) . Similarly, if b and c are equivalent, then a.Compare(b) == a.Compare(c) . OrderedWith implementations should also be consistent under reversal . That is, given types T and U where T is OrderedWith(U) and U is OrderedWith(T) , and values a: T and b: U : If a.(OrderedWith.Compare)(b) is Ordering.Greater , then b.(OrderedWith.Compare)(a) is Ordering.Less , and the other way around. Otherwise, a.(OrderedWith.Compare)(b) returns the same value as b.(OrderedWith.Compare)(a) . There is no expectation that an Ordered implementation be a total order, a weak order, or a partial order, and in particular the implementation for floating-point types is none of these because NaN values do not compare less than or equivalent to themselves. TODO: The standard library should provide a way to specify that an ordering is a weak, partial, or total ordering, and a way to request such an ordering in a generic. Compatibility of equality and ordering There is no requirement that a pair of types that implements OrderedWith also implements EqWith . If a pair of types does implement both, however, the equality relation provided by x.(EqWith.Equal)(y) should be a refinement of the equivalence relation provided by x.(OrderedWith.Compare)(y) == Ordering.Equivalent . Custom result types TODO: Support a lower-level extensibility mechanism that allows a result type other than bool . Default implementations for basic types In addition to being defined for standard Carbon numeric types, equality and relational comparisons are also defined for all \"data\" types: Tuples Struct types Classes implementing an interface that identifies them as data classes Relational comparisons for these types provide a lexicographical ordering. In each case, the comparison is only available if it is supported by all element types. Because implicit conversions between data classes can reorder fields, the implementations for data classes do not permit implicit conversions on their arguments in general. Instead: Equality comparisons are permitted between any two data classes that have the same unordered set of field names, if each corresponding pair of fields has an EqWith implementation. Fields are compared in the order they appear in the left-hand operand. Relational comparisons are permitted between any two data classes that have the same ordered sequence of field names, if each corresponding pair of fields has an OrderedWith implementation. Fields are compared in order. Comparisons between tuples permit implicit conversions for either operand, but not both. Open questions The bool type should be treated as a choice type, and so should support equality comparisons and relational comparisons if and only if choice types do in general. That decision is left to a future proposal. Alternatives considered Alternative symbols Chained comparisons Convert operands like C++ Provide a three-way comparison operator Allow comparisons as the operand of not Rename OrderedWith to ComparableWith References Proposal #702: Comparison operators Proposal #1178: Rework operator interfaces Issue #710: Default comparison for data classes","title":"Comparison operators"},{"location":"design/expressions/comparison_operators/#comparison-operators","text":"","title":"Comparison operators"},{"location":"design/expressions/comparison_operators/#table-of-contents","text":"Overview Details Precedence Associativity Built-in comparisons and implicit conversions Consistency with implicit conversions Comparisons with constants Extensibility Equality Ordering Compatibility of equality and ordering Custom result types Default implementations for basic types Open questions Alternatives considered References","title":"Table of contents"},{"location":"design/expressions/comparison_operators/#overview","text":"Carbon provides equality and relational comparison operators, each with a standard mathematical meaning: Category Operator Example Mathematical meaning Description Equality == x == y = Equality or equal to Equality != x != y \u2260 Inequality or not equal to Relational < x < y < Less than Relational <= x <= y \u2264 Less than or equal to Relational > x > y > Less than Relational >= x >= y \u2265 Less than or equal to Comparison operators all return a bool ; they evaluate to true when the indicated comparison is true. All comparison operators are infix binary operators. These operators have predefined meanings for some of Carbon's built-in types , as well as for simple \"data\" types like structs and tuples. User-defined types can define the meaning of these operations by implementing an interface provided as part of the Carbon standard library.","title":"Overview"},{"location":"design/expressions/comparison_operators/#details","text":"","title":"Details"},{"location":"design/expressions/comparison_operators/#precedence","text":"The comparison operators are all at the same precedence level. This level is lower than operators used to compute (non- bool ) values, higher than the logical operators and and or , and incomparable with the precedence of not . For example: // \u2705 Valid: precedence provides order of evaluation. if (n + m * 3 < n * n and 3 < m and m < 6) { ... } // The above is equivalent to: if (((n + (m * 3)) < (n * n)) and ((3 < m) and (m < 6))) { ... } // \u274c Invalid due to ambiguity: `(not a) == b` or `not (a == b)`? if (not a == b) { ... } // \u274c Invalid due to precedence: write `a == (not b)`. if (a == not b) { ... } // \u274c Invalid due to precedence: write `not (f < 5.0)`. if (not f < 5.0) { .... }","title":"Precedence"},{"location":"design/expressions/comparison_operators/#associativity","text":"The comparison operators are non-associative. For example: // \u274c Invalid: write `3 < m and m < 6`. if (3 < m < 6) { ... } // \u274c Invalid: write `a == b and b == c`. if (a == b == c) { ... } // \u274c Invalid: write `(m > 1) == (n > 1)`. if (m > 1 == n > 1) { ... }","title":"Associativity"},{"location":"design/expressions/comparison_operators/#built-in-comparisons-and-implicit-conversions","text":"Built-in comparisons are permitted in three cases: When both operands are of standard Carbon integer types ( Int(n) or Unsigned(n) ). When both operands are of standard Carbon floating-point types ( Float(n) ). When one operand is of floating-point type and the other is of integer type, if all values of the integer type can be exactly represented in the floating-point type. In each case, the result is the mathematically-correct answer. This applies even when comparing Int(n) with Unsigned(m) . For example: // \u2705 Valid: Fits case #1. The value of `compared` is `true` because `a` is less // than `b`, even though the result of a wrapping `i32` or `u32` comparison // would be `false`. fn Compare(a: i32, b: u32) -> bool { return a < b; } let compared: bool = Compare(-1, 4_000_000_000); // \u274c Invalid: Doesn't fit case #3 because `i64` values in general are not // exactly representable in the type `f32`. let float: f32 = 1.0e18; let integer: i64 = 1_000_000_000_000_000_000; let eq: bool = float == integer; Comparisons involving integer and floating-point constants are not covered by these rules and are discussed separately .","title":"Built-in comparisons and implicit conversions"},{"location":"design/expressions/comparison_operators/#consistency-with-implicit-conversions","text":"We support the following implicit conversions : From Int(n) to Int(m) if m > n . From Unsigned(n) to Int(m) or Unsigned(m) if m > n . From Float(n) to Float(m) if m > n . From Int(n) to Float(m) if Float(m) can represent all values of Int(n) . These rules can be summarized as: a type T can be converted to U if every value of type T is a value of type U . Implicit conversions are also supported from certain kinds of integer and floating-point constants to Int(n) and Float(n) types, if the constant can be represented in the type. All built-in comparisons can be viewed as performing implicit conversions on at most one of the operands in order to reach a suitable pair of identical or very similar types, and then performing a comparison on those types. The target types for these implicit conversions are, for each suitable value n : Int(n) versus Int(n) Unsigned(n) versus Unsigned(n) Int(n) versus Unsigned(n) Unsigned(n) versus Int(n) Float(n) versus Float(n) There will in general be multiple combinations of implicit conversions that will lead to one of the above forms, but we will arrive at the same result regardless of which is selected, because all comparisons are mathematically correct and all implicit conversions are lossless. Implementations are expected to do whatever is most efficient: for example, for u16 < i32 it is likely that the best choice would be to promote the u16 to i32 , not u32 . Because we only ever convert at most one operand, we never use an intermediate type that is larger than both input types. For example, both i32 and f32 can be implicitly converted to f64 , but we do not permit comparisons between i32 and f32 even though we could perform those comparisons in f64 . If such comparisons were permitted, the results could be surprising: // `i32` can exactly represent this value. var integer: i32 = 2_000_000_001; // This value is within the representable range for `f32`, but will be rounded // to 2_000_000_000.0 due to the limited precision of `f32`. var float: f32 = 2_000_000_001.0; // \u274c Invalid: `f32` cannot exactly represent all values of `i32`. if (integer == float) { ... } // \u2705 Valid: An explicit cast to `f64` on either side makes the code valid, but // will compare unequal because `float` was rounded to 2_000_000_000.0 // but `integer` will convert to exactly 2_000_000_001.0. if (integer == float as f64) { ... } if (integer as f64 == float) { ... } The two kinds of mixed-type comparison may be less efficient than the other kinds due to the slightly wider domain. Note that this approach diverges from C++, which would convert both operands to a common type first, sometimes performing a lossy conversion potentially giving an incorrect result, sometimes converting both operands, and sometimes using a wider type than either of the operand types.","title":"Consistency with implicit conversions"},{"location":"design/expressions/comparison_operators/#comparisons-with-constants","text":"We permit the following comparisons involving constants: A constant can be compared with a value of any type to which it can be implicitly converted. Any two constants can be compared, even if there is no type that can represent both. As described in implicit conversions , integer constants can be implicitly converted to any integer or floating-point type that can represent their value, and floating-point constants can be implicitly converted to any floating-point type that can represent their value. Note that this disallows comparisons between, for example, i32 and an integer literal that cannot be represented in i32 . Such comparisons would always be tautological. This decision should be revisited if it proves problematic in practice, for example in templated code where the literal is sometimes in range.","title":"Comparisons with constants"},{"location":"design/expressions/comparison_operators/#extensibility","text":"User-defined types can extend the behavior of the comparison operators by implementing interfaces. In this section, various properties are specified that such implementations \"should\" satisfy. These properties are not enforced in general, but the standard library might detect violations of some of them in some circumstances. These properties may be assumed by generic code, resulting in unexpected behavior if they are violated.","title":"Extensibility"},{"location":"design/expressions/comparison_operators/#equality","text":"Comparison operators can be provided for user-defined types by implementing the EqWith and OrderedWith interfaces. The EqWith interface is used to define the semantics of the == and != operators for a given pair of types: interface EqWith(U:! Type) { fn Equal[me: Self](u: U) -> bool; default fn NotEqual[me: Self](u: U) -> bool { return not (me == u); } } constraint Eq { extends EqWith(Self); } Given x: T and y: U : The expression x == y calls x.(EqWith(U).Equal)(y) . The expression x != y calls x.(EqWith(U).NotEqual)(y) . class Path { private var drive: String; private var path: String; private fn CanonicalPath[me: Self]() -> String; external impl as Eq { fn Equal[me: Self](other: Self) -> bool { return (me.drive, me.CanonicalPath()) == (other.drive, other.CanonicalPath()); } } } The EqWith overload is selected without considering possible implicit conversions. To permit implicit conversions in the operands of an == overload, the like operator can be used: class MyInt { var value: i32; fn Value[me: Self]() -> i32 { return me.value; } } external impl i32 as ImplicitAs(MyInt); external impl like MyInt as EqWith(like MyInt) { fn Equal[me: Self](other: Self) -> bool { return me.Value() == other.Value(); } } fn CompareBothWays(a: MyInt, b: i32, c: MyInt) -> bool { // OK, calls above implementation three times. return a == a and a != b and b == c; } The behavior of NotEqual can be overridden separately from the behavior of Equal to support cases like floating-point NaN values, where two values can compare neither equal nor not-equal, and thus both functions would return false . However, an implementation of EqWith should not allow both Equal and NotEqual to return true for the same pair of values. Additionally, these operations should have no observable side-effects. external impl like MyFloat as EqWith(like MyFloat) { fn Equal[me: MyFloat](other: MyFloat) -> bool { if (me.IsNaN() or other.IsNaN()) { return false; } return me.Representation() == other.Representation(); } fn NotEqual[me: MyFloat](other: MyFloat) -> bool { if (me.IsNaN() or other.IsNaN()) { return false; } return me.Representation() != other.Representation(); } } Heterogeneous comparisons must be defined both ways around: external impl like MyInt as EqWith(like MyFloat); external impl like MyFloat as EqWith(like MyInt); TODO: Add an adapter to the standard library to make it easy to define the reverse comparison.","title":"Equality"},{"location":"design/expressions/comparison_operators/#ordering","text":"The OrderedWith interface is used to define the semantics of the < , <= , > , and >= operators for a given pair of types. choice Ordering { Less, Equivalent, Greater, Incomparable } interface OrderedWith(U:! Type) { fn Compare[me: Self](u: U) -> Ordering; default fn Less[me: Self](u: U) -> bool { return me.Compare(u) == Ordering.Less; } default fn LessOrEquivalent[me: Self](u: U) -> bool { let c: Ordering = me.Compare(u); return c == Ordering.Less or c == Ordering.Equivalent; } default fn Greater[me: Self](u: U) -> bool { return me.Compare(u) == Ordering.Greater; } default fn GreaterOrEquivalent[me: Self](u: U) -> bool { let c: Ordering = me.Compare(u); return c == Ordering.Greater or c == Ordering.Equivalent; } } constraint Ordered { extends OrderedWith(Self); } // Ordering.Less < Ordering.Equivalent < Ordering.Greater. // Ordering.Incomparable is incomparable with all three. external impl Ordering as Ordered; TODO: Revise the above when we have a concrete design for enumerated types. Given x: T and y: U : The expression x < y calls x.(OrderedWith(U).Less)(y) . The expression x <= y calls x.(OrderedWith(U).LessOrEquivalent)(y) . The expression x > y calls x.(OrderedWith(U).Greater)(y) . The expression x >= y calls x.(OrderedWith(U).GreaterOrEquivalent)(y) . For example: class MyWidget { var width: i32; var height: i32; fn Size[me: Self]() -> i32 { return me.width * me.height; } // Widgets are normally ordered by size. external impl as Ordered { fn Compare[me: Self](other: Self) -> Ordering { return me.Size().(Ordered.Compare)(other.Size()); } } } fn F(a: MyWidget, b: MyWidget) -> bool { return a <= b; } As for EqWith , the like operator can be used to permit implicit conversions when invoking a comparison, and heterogeneous comparisons must be defined both ways around: fn ReverseOrdering(o: Ordering) -> Ordering { return Ordering.Equivalent.(Ordered.Compare)(o); } external impl like MyInt as OrderedWith(like MyFloat); external impl like MyFloat as OrderedWith(like MyInt) { fn Compare[me: Self](other: Self) -> Ordering { return Reverse(other.(OrderedWith(Self).Compare)(me)); } } The default implementations of Less , LessOrEquivalent , Greater , and GreaterOrEquivalent can be overridden if a more efficient version can be implemented. The behaviors of such overrides should follow those of the above default implementations, and the members of an OrderedWith implementation should have no observable side-effects. OrderedWith implementations should be transitive . That is, given V:! Type , U:! OrderedWith(V) , T:! OrderedWith(U) & OrderedWith(V) , a: T , b: U , c: V , then: If a <= b and b <= c then a <= c , and moreover if either a < b or b < c then a < c . If a >= b and b >= c then a >= c , and moreover if either a > b or b > c then a > c . If a and b are equivalent, then a.Compare(c) == b.Compare(c) . Similarly, if b and c are equivalent, then a.Compare(b) == a.Compare(c) . OrderedWith implementations should also be consistent under reversal . That is, given types T and U where T is OrderedWith(U) and U is OrderedWith(T) , and values a: T and b: U : If a.(OrderedWith.Compare)(b) is Ordering.Greater , then b.(OrderedWith.Compare)(a) is Ordering.Less , and the other way around. Otherwise, a.(OrderedWith.Compare)(b) returns the same value as b.(OrderedWith.Compare)(a) . There is no expectation that an Ordered implementation be a total order, a weak order, or a partial order, and in particular the implementation for floating-point types is none of these because NaN values do not compare less than or equivalent to themselves. TODO: The standard library should provide a way to specify that an ordering is a weak, partial, or total ordering, and a way to request such an ordering in a generic.","title":"Ordering"},{"location":"design/expressions/comparison_operators/#compatibility-of-equality-and-ordering","text":"There is no requirement that a pair of types that implements OrderedWith also implements EqWith . If a pair of types does implement both, however, the equality relation provided by x.(EqWith.Equal)(y) should be a refinement of the equivalence relation provided by x.(OrderedWith.Compare)(y) == Ordering.Equivalent .","title":"Compatibility of equality and ordering"},{"location":"design/expressions/comparison_operators/#custom-result-types","text":"TODO: Support a lower-level extensibility mechanism that allows a result type other than bool .","title":"Custom result types"},{"location":"design/expressions/comparison_operators/#default-implementations-for-basic-types","text":"In addition to being defined for standard Carbon numeric types, equality and relational comparisons are also defined for all \"data\" types: Tuples Struct types Classes implementing an interface that identifies them as data classes Relational comparisons for these types provide a lexicographical ordering. In each case, the comparison is only available if it is supported by all element types. Because implicit conversions between data classes can reorder fields, the implementations for data classes do not permit implicit conversions on their arguments in general. Instead: Equality comparisons are permitted between any two data classes that have the same unordered set of field names, if each corresponding pair of fields has an EqWith implementation. Fields are compared in the order they appear in the left-hand operand. Relational comparisons are permitted between any two data classes that have the same ordered sequence of field names, if each corresponding pair of fields has an OrderedWith implementation. Fields are compared in order. Comparisons between tuples permit implicit conversions for either operand, but not both.","title":"Default implementations for basic types"},{"location":"design/expressions/comparison_operators/#open-questions","text":"The bool type should be treated as a choice type, and so should support equality comparisons and relational comparisons if and only if choice types do in general. That decision is left to a future proposal.","title":"Open questions"},{"location":"design/expressions/comparison_operators/#alternatives-considered","text":"Alternative symbols Chained comparisons Convert operands like C++ Provide a three-way comparison operator Allow comparisons as the operand of not Rename OrderedWith to ComparableWith","title":"Alternatives considered"},{"location":"design/expressions/comparison_operators/#references","text":"Proposal #702: Comparison operators Proposal #1178: Rework operator interfaces Issue #710: Default comparison for data classes","title":"References"},{"location":"design/expressions/if/","text":"if expressions Table of contents Overview Syntax Semantics Finding a common type Symmetry Same type Implicit conversions Alternatives considered References Overview An if expression is an expression of the form: if condition then value1 else value2 The condition is converted to a bool value in the same way as the condition of an if statement. Note: These conversions have not yet been decided. The value1 and value2 are implicitly converted to their common type , which is the type of the if expression. Syntax if expressions have very low precedence, and cannot appear as the operand of any operator, except as the right-hand operand in an assignment. They can appear in other context where an expression is permitted, such as within parentheses, as the operand of a return statement, as an initializer, or in a comma-separated list such as a function call. The value1 and value2 expressions are arbitrary expressions, and can themselves be if expressions. value2 extends as far to the right as possible. An if expression can be parenthesized if the intent is for value2 to end earlier. // OK, same as `if cond then (1 + 1) else (2 + (4 * 6))` var a: i32 = if cond then 1 + 1 else 2 + 4 * 6; // OK var b: i32 = (if cond then 1 + 1 else 2) + 4 * 6; An if keyword at the start of a statement is always interpreted as an if statement , never as an if expression, even if it is followed eventually by a then keyword. Semantics The converted condition is evaluated. If it evaluates to true , then the converted value1 is evaluated and its value is the result of the expression. Otherwise, the converted value2 is evaluated and its value is the result of the expression. Finding a common type The common type of two types T and U is (T as CommonType(U)).Result , where CommonType is the Carbon.CommonType constraint. CommonType is notionally defined as follows: constraint CommonType(U:! CommonTypeWith(Self)) { extend CommonTypeWith(U) where .Result == U.Result; } The actual definition is a bit more complex than this, as described in symmetry . The interface CommonTypeWith is used to customize the behavior of CommonType : interface CommonTypeWith(U:! Type) { let Result:! Type where Self is ImplicitAs(.Self) and U is ImplicitAs(.Self); } The implementation A as CommonTypeWith(B) specifies the type that A would like to result from unifying A and B as its Result . Note: It is required that both types implicitly convert to the common type. Some blanket impl s for CommonTypeWith are provided as part of the prelude. These are described in the following sections. Note: The same mechanism is expected to eventually be used to compute common types in other circumstances. Symmetry The common type of T and U should always be the same as the common type of U and T . This is enforced in two steps: A SymmetricCommonTypeWith interface implicitly provides a B as CommonTypeWith(A) implementation whenever one doesn't exist but an A as CommonTypeWith(B) implementation exists. CommonType is defined in terms of SymmetricCommonTypeWith , and requires that both A as SymmetricCommonTypeWith(B) and B as SymmetricCommonTypeWith(A) produce the same type. The interface SymmetricCommonTypeWith is an implementation detail of the CommonType constraint. It is defined and implemented as follows: interface SymmetricCommonTypeWith(U:! Type) { let Result:! Type where Self is ImplicitAs(.Self) and U is ImplicitAs(.Self); } match_first { impl [T:! Type, U:! CommonTypeWith(T)] T as SymmetricCommonTypeWith(U) { let Result:! Type = U.Result; } impl [U:! Type, T:! CommonTypeWith(U)] T as SymmetricCommonTypeWith(U) { let Result:! Type = T.Result; } } The SymmetricCommonTypeWith interface is not exported, so user-defined impl s can't be defined, and only the two blanket impl s above are used. The CommonType constraint is then defined as follows: constraint CommonType(U:! SymmetricCommonTypeWith(Self)) { extend SymmetricCommonTypeWith(U) where .Result == U.Result; } When computing the common type of T and U , if only one of the types provides a CommonTypeWith implementation, that determines the common type. If both types provide a CommonTypeWith implementation and their Result types are the same, that determines the common type. Otherwise, if both types provide implementations but their Result types differ, there is no common type, and the CommonType constraint is not met. For example, given: // Implementation #1 impl [T:! Type] MyX as CommonTypeWith(T) { let Result:! Type = MyX; } // Implementation #2 impl [T:! Type] MyY as CommonTypeWith(T) { let Result:! Type = MyY; } MyX as CommonTypeWith(MyY) will select #1, and MyY as CommonTypeWith(MyX) will select #2, but the constraints on MyX as CommonType(MyY) will not be met because result types differ. Same type If T is the same type as U , the result is that type: final impl [T:! Type] T as CommonTypeWith(T) { let Result:! Type = T; } Note: This rule is intended to be considered more specialized than the other rules in this document. Because this impl is declared final , T.(CommonType(T)).Result is always assumed to be T , even in contexts where T involves a generic parameter and so the result would normally be an unknown type whose type-of-type is Type . fn F[T:! Hashable](c: bool, x: T, y: T) -> HashCode { // OK, type of `if` expression is `T`. return (if c then x else y).Hash(); } Implicit conversions If T implicitly converts to U , the common type is U : impl [T:! Type, U:! ImplicitAs(T)] T as CommonTypeWith(U) { let Result:! Type = T; } Note: If an implicit conversion is possible in both directions, and no more specific implementation exists, the constraints on T as CommonType(U) will not be met because (T as CommonTypeWith(U)).Result and (U as CommonTypeWith(T)).Result will differ. In order to define a common type for such a case, CommonTypeWith implementations in both directions must be provided to override the blanket impl s in both directions: impl MyString as CommonTypeWith(YourString) { let Result:! Type = MyString; } impl YourString as CommonTypeWith(MyString) { let Result:! Type = MyString; } var my_string: MyString; var your_string: YourString; // The type of `also_my_string` is `MyString`. var also_my_string: auto = if cond then my_string else your_string; Alternatives considered Provide no conditional expression Use cond ? expr1 : expr2 , like in C and C++ syntax Use if (cond) expr1 else expr2 syntax Use if (cond) then expr1 else expr2 syntax Allow 1 + if cond then expr1 else expr2 Only require one impl to specify the common type if implicit conversions in both directions are possible Introduce special rules for lvalue conditionals References Proposal #911: Conditional expressions .","title":"`if` expressions"},{"location":"design/expressions/if/#if-expressions","text":"","title":"if expressions"},{"location":"design/expressions/if/#table-of-contents","text":"Overview Syntax Semantics Finding a common type Symmetry Same type Implicit conversions Alternatives considered References","title":"Table of contents"},{"location":"design/expressions/if/#overview","text":"An if expression is an expression of the form: if condition then value1 else value2 The condition is converted to a bool value in the same way as the condition of an if statement. Note: These conversions have not yet been decided. The value1 and value2 are implicitly converted to their common type , which is the type of the if expression.","title":"Overview"},{"location":"design/expressions/if/#syntax","text":"if expressions have very low precedence, and cannot appear as the operand of any operator, except as the right-hand operand in an assignment. They can appear in other context where an expression is permitted, such as within parentheses, as the operand of a return statement, as an initializer, or in a comma-separated list such as a function call. The value1 and value2 expressions are arbitrary expressions, and can themselves be if expressions. value2 extends as far to the right as possible. An if expression can be parenthesized if the intent is for value2 to end earlier. // OK, same as `if cond then (1 + 1) else (2 + (4 * 6))` var a: i32 = if cond then 1 + 1 else 2 + 4 * 6; // OK var b: i32 = (if cond then 1 + 1 else 2) + 4 * 6; An if keyword at the start of a statement is always interpreted as an if statement , never as an if expression, even if it is followed eventually by a then keyword.","title":"Syntax"},{"location":"design/expressions/if/#semantics","text":"The converted condition is evaluated. If it evaluates to true , then the converted value1 is evaluated and its value is the result of the expression. Otherwise, the converted value2 is evaluated and its value is the result of the expression.","title":"Semantics"},{"location":"design/expressions/if/#finding-a-common-type","text":"The common type of two types T and U is (T as CommonType(U)).Result , where CommonType is the Carbon.CommonType constraint. CommonType is notionally defined as follows: constraint CommonType(U:! CommonTypeWith(Self)) { extend CommonTypeWith(U) where .Result == U.Result; } The actual definition is a bit more complex than this, as described in symmetry . The interface CommonTypeWith is used to customize the behavior of CommonType : interface CommonTypeWith(U:! Type) { let Result:! Type where Self is ImplicitAs(.Self) and U is ImplicitAs(.Self); } The implementation A as CommonTypeWith(B) specifies the type that A would like to result from unifying A and B as its Result . Note: It is required that both types implicitly convert to the common type. Some blanket impl s for CommonTypeWith are provided as part of the prelude. These are described in the following sections. Note: The same mechanism is expected to eventually be used to compute common types in other circumstances.","title":"Finding a common type"},{"location":"design/expressions/if/#symmetry","text":"The common type of T and U should always be the same as the common type of U and T . This is enforced in two steps: A SymmetricCommonTypeWith interface implicitly provides a B as CommonTypeWith(A) implementation whenever one doesn't exist but an A as CommonTypeWith(B) implementation exists. CommonType is defined in terms of SymmetricCommonTypeWith , and requires that both A as SymmetricCommonTypeWith(B) and B as SymmetricCommonTypeWith(A) produce the same type. The interface SymmetricCommonTypeWith is an implementation detail of the CommonType constraint. It is defined and implemented as follows: interface SymmetricCommonTypeWith(U:! Type) { let Result:! Type where Self is ImplicitAs(.Self) and U is ImplicitAs(.Self); } match_first { impl [T:! Type, U:! CommonTypeWith(T)] T as SymmetricCommonTypeWith(U) { let Result:! Type = U.Result; } impl [U:! Type, T:! CommonTypeWith(U)] T as SymmetricCommonTypeWith(U) { let Result:! Type = T.Result; } } The SymmetricCommonTypeWith interface is not exported, so user-defined impl s can't be defined, and only the two blanket impl s above are used. The CommonType constraint is then defined as follows: constraint CommonType(U:! SymmetricCommonTypeWith(Self)) { extend SymmetricCommonTypeWith(U) where .Result == U.Result; } When computing the common type of T and U , if only one of the types provides a CommonTypeWith implementation, that determines the common type. If both types provide a CommonTypeWith implementation and their Result types are the same, that determines the common type. Otherwise, if both types provide implementations but their Result types differ, there is no common type, and the CommonType constraint is not met. For example, given: // Implementation #1 impl [T:! Type] MyX as CommonTypeWith(T) { let Result:! Type = MyX; } // Implementation #2 impl [T:! Type] MyY as CommonTypeWith(T) { let Result:! Type = MyY; } MyX as CommonTypeWith(MyY) will select #1, and MyY as CommonTypeWith(MyX) will select #2, but the constraints on MyX as CommonType(MyY) will not be met because result types differ.","title":"Symmetry"},{"location":"design/expressions/if/#same-type","text":"If T is the same type as U , the result is that type: final impl [T:! Type] T as CommonTypeWith(T) { let Result:! Type = T; } Note: This rule is intended to be considered more specialized than the other rules in this document. Because this impl is declared final , T.(CommonType(T)).Result is always assumed to be T , even in contexts where T involves a generic parameter and so the result would normally be an unknown type whose type-of-type is Type . fn F[T:! Hashable](c: bool, x: T, y: T) -> HashCode { // OK, type of `if` expression is `T`. return (if c then x else y).Hash(); }","title":"Same type"},{"location":"design/expressions/if/#implicit-conversions","text":"If T implicitly converts to U , the common type is U : impl [T:! Type, U:! ImplicitAs(T)] T as CommonTypeWith(U) { let Result:! Type = T; } Note: If an implicit conversion is possible in both directions, and no more specific implementation exists, the constraints on T as CommonType(U) will not be met because (T as CommonTypeWith(U)).Result and (U as CommonTypeWith(T)).Result will differ. In order to define a common type for such a case, CommonTypeWith implementations in both directions must be provided to override the blanket impl s in both directions: impl MyString as CommonTypeWith(YourString) { let Result:! Type = MyString; } impl YourString as CommonTypeWith(MyString) { let Result:! Type = MyString; } var my_string: MyString; var your_string: YourString; // The type of `also_my_string` is `MyString`. var also_my_string: auto = if cond then my_string else your_string;","title":"Implicit conversions"},{"location":"design/expressions/if/#alternatives-considered","text":"Provide no conditional expression Use cond ? expr1 : expr2 , like in C and C++ syntax Use if (cond) expr1 else expr2 syntax Use if (cond) then expr1 else expr2 syntax Allow 1 + if cond then expr1 else expr2 Only require one impl to specify the common type if implicit conversions in both directions are possible Introduce special rules for lvalue conditionals","title":"Alternatives considered"},{"location":"design/expressions/if/#references","text":"Proposal #911: Conditional expressions .","title":"References"},{"location":"design/expressions/implicit_conversions/","text":"Implicit conversions Table of contents Overview Properties of implicit conversions Lossless Semantics-preserving Examples Built-in types Data types Same type Pointer conversions Type-of-types Consistency with as Extensibility Alternatives considered References Overview When an expression appears in a context in which an expression of a specific type is expected, the expression is implicitly converted to that type if possible. For built-in types , implicit conversions are permitted when: The conversion is lossless : every possible value for the source expression converts to a distinct value in the target type. The conversion is semantics-preserving : corresponding values in the source and destination type have the same abstract meaning. These rules aim to ensure that implicit conversions are unsurprising: the value that is provided as the operand of an operation should match how that operation interprets the value, because the identity and abstract meaning of the value are preserved by any implicit conversions that are applied. It is possible for user-defined types to extend the set of valid implicit conversions. Such extensions are expected to also follow these rules. Properties of implicit conversions Lossless We expect implicit conversion to never lose information: if two values are distinguishable before the conversion, they should generally be distinguishable after the conversion. It should be possible to define a conversion in the opposite direction that restores the original value, but such a conversion is not expected to be provided in general, and might be computationally expensive. Because an implicit conversion is converting from a narrower type to a wider type, implicit conversions do not necessarily preserve static information about the source value. Semantics-preserving We expect implicit conversions to preserve the meaning of converted values. The assessment of this criterion will necessarily be subjective, because the meanings of values generally live in the mind of the programmer rather than in the program text. However, the semantic interpretation is expected to be consistent from one conversion to another, so we can provide a test: if multiple paths of implicit conversions from a type A to a type B exist, and the same value of type A would convert to different values of type B along different paths, then at least one of those conversions must not be semantics-preserving. A semantics-preserving conversion does not necessarily preserve the meaning of particular syntax when applied to the value. The same syntax may map to different operations in the new type. For example, division may mean different things in integer and floating-point types, and member access may find different members in a derived class pointer versus in a base class pointer. Examples Conversion from i32 to Vector(int) by forming a vector of N zeroes is lossless but not semantics-preserving. Conversion from i32 to f32 by rounding to the nearest representable value is semantics-preserving but not lossless. Conversion from String to StringView is lossless, because we can compute the String value from the StringView value, and semantics-preserving because the string value denoted is the same. Conversion in the other direction may or may not be semantics-preserving depending on whether we consider the address to be a salient part of a StringView 's value. Built-in types Data types The following implicit numeric conversions are available: iN or uN -> iM if M > N uN -> uM if M > N fN -> fM if M > N iN or uN -> fM if every value of type iN or uN can be represented in fM : i12 or u11 (or smaller) -> f16 i25 or u24 (or smaller) -> f32 i54 or u53 (or smaller) -> f64 i65 or u64 (or smaller) -> f80 (x86 only) i114 or u113 (or smaller) -> f128 (if available) i238 or u237 (or smaller) -> f256 (if available) In each case, the numerical value is the same before and after the conversion. An integer zero is translated into a floating-point positive zero. An integer constant can be implicitly converted to any type iM , uM , or fM in which that value can be exactly represented. A floating-point constant can be implicitly converted to any type fM in which that value is between the least representable finite value and the greatest representable finite value (inclusive), and converts to the nearest representable finite value, with ties broken by picking the value for which the mantissa is even. The above conversions are also precisely those that C++ considers non-narrowing, except: Carbon also permits integer to floating-point conversions in more cases. The most important of these is that Carbon permits i32 to be implicitly converted to f64 . Lossy conversions, such as from i32 to f32 , are not permitted. What Carbon considers to be an integer constant or floating-point constant may differ from what C++ considers to be a constant expression. Note: We have not yet decided what will qualify as a constant in this context, but it will include at least integer and floating-point literals, with optional enclosing parentheses. It is possible that such constants will have singleton types; see issue #508 . In addition to the above rules, a negative integer constant k can be implicitly converted to the type uN if the value k + 2 N can be exactly represented, and converts to that value. Note that this conversion violates the \"semantics-preserving\" test. For example, (-1 as u8) as i32 produces the value 255 whereas -1 as i32 produces the value -1 . However, this conversion is important in order to allow bitwise operations with masks, so we allow it: // We allow ^0 == -1 to convert to `u32` to represent an all-ones value. var a: u32 = ^0; // ^4 == -5 is negative, but we want to allow it to convert to u32 here. var b: u32 = a & ^4; Same type The following conversion is available for every type T : T -> T Pointer conversions The following pointer conversion is available: T* -> U* if T is a class derived from the class U . Even though we can convert Derived* to Base* , we cannot convert Derived** to Base** because that would allow storing a Derived2* into a Derived* : abstract class Base {} class Derived extends Base {} class Derived2 extends Base {} var d2: Derived2 = {}; var p: Derived*; var q: Derived2* = &d2; var r: Base** = &p; // Bad: would store q to p. *r = q; Type-of-types A type T with type-of-type TT1 can be implicitly converted to the type-of-type TT2 if T satisfies the requirements of TT2 . Consistency with as An implicit conversion of an expression E of type T to type U , when permitted, always has the same meaning as the explicit cast expression E as U . Moreover, because such an implicit conversion is expected to exactly preserve the value, (E as U) as T , if valid, should be expected to result in the same value as produced by E even if the as T cast cannot be performed as an implicit conversion. Extensibility Implicit conversions can be defined for user-defined types such as classes by implementing the ImplicitAs interface, which extends the As interface used to implement as expressions : interface ImplicitAs(Dest:! Type) { extends As(Dest); // Inherited from As(Dest): // fn Convert[me: Self]() -> Dest; } When attempting to implicitly convert an expression x to type U , the expression is rewritten to x.(ImplicitAs(U).Convert)() . Note that implicit conversions are not transitive. Even if an impl A as ImplicitAs(B) and an impl B as ImplicitAs(C) are both provided, an expression of type A cannot be implicitly converted to type C . Allowing transitivity would introduce the risk of ambiguity issues as code evolves and would in general require a search of a potentially unbounded set of intermediate types. Alternatives considered Provide lossy and non-semantics-preserving implicit conversions from C++ Provide no implicit conversions Provide no extensibility Apply implicit conversions transitively Do not allow negative constants to convert to unsigned types References Implicit conversions in C++ Proposal #820: Implicit conversions . Proposal #866: Allow ties in floating literals .","title":"Implicit conversions"},{"location":"design/expressions/implicit_conversions/#implicit-conversions","text":"","title":"Implicit conversions"},{"location":"design/expressions/implicit_conversions/#table-of-contents","text":"Overview Properties of implicit conversions Lossless Semantics-preserving Examples Built-in types Data types Same type Pointer conversions Type-of-types Consistency with as Extensibility Alternatives considered References","title":"Table of contents"},{"location":"design/expressions/implicit_conversions/#overview","text":"When an expression appears in a context in which an expression of a specific type is expected, the expression is implicitly converted to that type if possible. For built-in types , implicit conversions are permitted when: The conversion is lossless : every possible value for the source expression converts to a distinct value in the target type. The conversion is semantics-preserving : corresponding values in the source and destination type have the same abstract meaning. These rules aim to ensure that implicit conversions are unsurprising: the value that is provided as the operand of an operation should match how that operation interprets the value, because the identity and abstract meaning of the value are preserved by any implicit conversions that are applied. It is possible for user-defined types to extend the set of valid implicit conversions. Such extensions are expected to also follow these rules.","title":"Overview"},{"location":"design/expressions/implicit_conversions/#properties-of-implicit-conversions","text":"","title":"Properties of implicit conversions"},{"location":"design/expressions/implicit_conversions/#lossless","text":"We expect implicit conversion to never lose information: if two values are distinguishable before the conversion, they should generally be distinguishable after the conversion. It should be possible to define a conversion in the opposite direction that restores the original value, but such a conversion is not expected to be provided in general, and might be computationally expensive. Because an implicit conversion is converting from a narrower type to a wider type, implicit conversions do not necessarily preserve static information about the source value.","title":"Lossless"},{"location":"design/expressions/implicit_conversions/#semantics-preserving","text":"We expect implicit conversions to preserve the meaning of converted values. The assessment of this criterion will necessarily be subjective, because the meanings of values generally live in the mind of the programmer rather than in the program text. However, the semantic interpretation is expected to be consistent from one conversion to another, so we can provide a test: if multiple paths of implicit conversions from a type A to a type B exist, and the same value of type A would convert to different values of type B along different paths, then at least one of those conversions must not be semantics-preserving. A semantics-preserving conversion does not necessarily preserve the meaning of particular syntax when applied to the value. The same syntax may map to different operations in the new type. For example, division may mean different things in integer and floating-point types, and member access may find different members in a derived class pointer versus in a base class pointer.","title":"Semantics-preserving"},{"location":"design/expressions/implicit_conversions/#examples","text":"Conversion from i32 to Vector(int) by forming a vector of N zeroes is lossless but not semantics-preserving. Conversion from i32 to f32 by rounding to the nearest representable value is semantics-preserving but not lossless. Conversion from String to StringView is lossless, because we can compute the String value from the StringView value, and semantics-preserving because the string value denoted is the same. Conversion in the other direction may or may not be semantics-preserving depending on whether we consider the address to be a salient part of a StringView 's value.","title":"Examples"},{"location":"design/expressions/implicit_conversions/#built-in-types","text":"","title":"Built-in types"},{"location":"design/expressions/implicit_conversions/#data-types","text":"The following implicit numeric conversions are available: iN or uN -> iM if M > N uN -> uM if M > N fN -> fM if M > N iN or uN -> fM if every value of type iN or uN can be represented in fM : i12 or u11 (or smaller) -> f16 i25 or u24 (or smaller) -> f32 i54 or u53 (or smaller) -> f64 i65 or u64 (or smaller) -> f80 (x86 only) i114 or u113 (or smaller) -> f128 (if available) i238 or u237 (or smaller) -> f256 (if available) In each case, the numerical value is the same before and after the conversion. An integer zero is translated into a floating-point positive zero. An integer constant can be implicitly converted to any type iM , uM , or fM in which that value can be exactly represented. A floating-point constant can be implicitly converted to any type fM in which that value is between the least representable finite value and the greatest representable finite value (inclusive), and converts to the nearest representable finite value, with ties broken by picking the value for which the mantissa is even. The above conversions are also precisely those that C++ considers non-narrowing, except: Carbon also permits integer to floating-point conversions in more cases. The most important of these is that Carbon permits i32 to be implicitly converted to f64 . Lossy conversions, such as from i32 to f32 , are not permitted. What Carbon considers to be an integer constant or floating-point constant may differ from what C++ considers to be a constant expression. Note: We have not yet decided what will qualify as a constant in this context, but it will include at least integer and floating-point literals, with optional enclosing parentheses. It is possible that such constants will have singleton types; see issue #508 . In addition to the above rules, a negative integer constant k can be implicitly converted to the type uN if the value k + 2 N can be exactly represented, and converts to that value. Note that this conversion violates the \"semantics-preserving\" test. For example, (-1 as u8) as i32 produces the value 255 whereas -1 as i32 produces the value -1 . However, this conversion is important in order to allow bitwise operations with masks, so we allow it: // We allow ^0 == -1 to convert to `u32` to represent an all-ones value. var a: u32 = ^0; // ^4 == -5 is negative, but we want to allow it to convert to u32 here. var b: u32 = a & ^4;","title":"Data types"},{"location":"design/expressions/implicit_conversions/#same-type","text":"The following conversion is available for every type T : T -> T","title":"Same type"},{"location":"design/expressions/implicit_conversions/#pointer-conversions","text":"The following pointer conversion is available: T* -> U* if T is a class derived from the class U . Even though we can convert Derived* to Base* , we cannot convert Derived** to Base** because that would allow storing a Derived2* into a Derived* : abstract class Base {} class Derived extends Base {} class Derived2 extends Base {} var d2: Derived2 = {}; var p: Derived*; var q: Derived2* = &d2; var r: Base** = &p; // Bad: would store q to p. *r = q;","title":"Pointer conversions"},{"location":"design/expressions/implicit_conversions/#type-of-types","text":"A type T with type-of-type TT1 can be implicitly converted to the type-of-type TT2 if T satisfies the requirements of TT2 .","title":"Type-of-types"},{"location":"design/expressions/implicit_conversions/#consistency-with-as","text":"An implicit conversion of an expression E of type T to type U , when permitted, always has the same meaning as the explicit cast expression E as U . Moreover, because such an implicit conversion is expected to exactly preserve the value, (E as U) as T , if valid, should be expected to result in the same value as produced by E even if the as T cast cannot be performed as an implicit conversion.","title":"Consistency with as"},{"location":"design/expressions/implicit_conversions/#extensibility","text":"Implicit conversions can be defined for user-defined types such as classes by implementing the ImplicitAs interface, which extends the As interface used to implement as expressions : interface ImplicitAs(Dest:! Type) { extends As(Dest); // Inherited from As(Dest): // fn Convert[me: Self]() -> Dest; } When attempting to implicitly convert an expression x to type U , the expression is rewritten to x.(ImplicitAs(U).Convert)() . Note that implicit conversions are not transitive. Even if an impl A as ImplicitAs(B) and an impl B as ImplicitAs(C) are both provided, an expression of type A cannot be implicitly converted to type C . Allowing transitivity would introduce the risk of ambiguity issues as code evolves and would in general require a search of a potentially unbounded set of intermediate types.","title":"Extensibility"},{"location":"design/expressions/implicit_conversions/#alternatives-considered","text":"Provide lossy and non-semantics-preserving implicit conversions from C++ Provide no implicit conversions Provide no extensibility Apply implicit conversions transitively Do not allow negative constants to convert to unsigned types","title":"Alternatives considered"},{"location":"design/expressions/implicit_conversions/#references","text":"Implicit conversions in C++ Proposal #820: Implicit conversions . Proposal #866: Allow ties in floating literals .","title":"References"},{"location":"design/expressions/logical_operators/","text":"Logical operators Table of contents Overview Details Precedence Associativity Conversions Overloading Alternatives considered References Overview Carbon provides three operators to support logical operations on bool values: and provides a logical AND operation. x and y evaluates to true if both operands are true . or provides a logical OR operation. x or y evaluates to true if either operand is true . not provides a logical NOT operation. not x evaluates to true if the operand is false . and and or are infix binary operators, and use short-circuit evaluation . not is a prefix unary operator. Details Precedence and and or have very low precedence. When an expression appearing as the condition of an if uses these operators unparenthesized, they are always the lowest precedence operators in that expression. These operators permit any reasonable operator that might be used to form a bool value as a subexpression. In particular, comparison operators such as < and == have higher precedence than and and or . However, the precedence of and and or is not directly comparable with each other, so they cannot both be used directly in an expression without parentheses. not is higher precedence than and and or , but its precedence is incomparable with most other operators, including comparison operators. For example: // \u2705 Valid: `and` is lower precedence than the `<` or `==` operators. if (n + m == 3 and not n < m) { ... } // The above is equivalent to: if (((n + m) == 3) and (not (n < m))) { ... } // \u274c Invalid: `and` and `or` precedence is incomparable. if (cond1 and cond2 or cond3) { ... } // \u2705 Valid: Parentheses avoid the precedence check. if (cond1 and (cond2 or cond3)) { ... } // \u274c Invalid: `not` precedence is incomparable with `==`. if (not cond1 == cond2) { ... } // \u274c Invalid: `not` precedence is incomparable with `==`. if (cond1 == not cond2) { ... } // \u2705 Valid: Parentheses avoid the precedence check. if (cond1 == (not cond2)) { ... } Associativity and and or are left-associative. A not expression cannot be the operand of another not expression; not not b is an error without parentheses. // \u2705 Valid: `and` is left-associative, and precedence is fine. if (not a and not b and not c) { ... } // The above is equivalent to: if ((not a) and ((not b) and (not c))) { ... } // \u2705 Valid: Parentheses avoid the `not` associativity error. if (not (not a)) { ... } // \u274c Invalid: `not not` associativity requires parentheses. if (not not a) { ... } Conversions TODO: This should be addressed through a standard bool conversion design. The operand of and , or , or not is converted to a bool value in the same way as the condition of an if statement. In particular: If we decide that certain values, such as pointers or integers, should not be usable as the condition of an if without an explicit comparison against null or zero, then those values will also not be usable as the operand of and , or , or not without an explicit comparison. If an extension point is provided to determine how to branch on the truth of a value in an if (such as by supplying a conversion to a bool type), that extension point will also apply to and , or , and not . Overloading The logical operators and , or , and not are not overloadable. As noted above, any mechanism that allows types to customize how if treats them will also customize how and , or , and not treats them. Alternatives considered Use punctuation spelling for all three operators Precedence of AND versus OR Precedence of NOT Punctuation form of NOT Two forms of NOT Repeated NOT AND and OR produce the decisive value References Proposal #680: And, or, not . Proposal #702: Comparison operators .","title":"Logical operators"},{"location":"design/expressions/logical_operators/#logical-operators","text":"","title":"Logical operators"},{"location":"design/expressions/logical_operators/#table-of-contents","text":"Overview Details Precedence Associativity Conversions Overloading Alternatives considered References","title":"Table of contents"},{"location":"design/expressions/logical_operators/#overview","text":"Carbon provides three operators to support logical operations on bool values: and provides a logical AND operation. x and y evaluates to true if both operands are true . or provides a logical OR operation. x or y evaluates to true if either operand is true . not provides a logical NOT operation. not x evaluates to true if the operand is false . and and or are infix binary operators, and use short-circuit evaluation . not is a prefix unary operator.","title":"Overview"},{"location":"design/expressions/logical_operators/#details","text":"","title":"Details"},{"location":"design/expressions/logical_operators/#precedence","text":"and and or have very low precedence. When an expression appearing as the condition of an if uses these operators unparenthesized, they are always the lowest precedence operators in that expression. These operators permit any reasonable operator that might be used to form a bool value as a subexpression. In particular, comparison operators such as < and == have higher precedence than and and or . However, the precedence of and and or is not directly comparable with each other, so they cannot both be used directly in an expression without parentheses. not is higher precedence than and and or , but its precedence is incomparable with most other operators, including comparison operators. For example: // \u2705 Valid: `and` is lower precedence than the `<` or `==` operators. if (n + m == 3 and not n < m) { ... } // The above is equivalent to: if (((n + m) == 3) and (not (n < m))) { ... } // \u274c Invalid: `and` and `or` precedence is incomparable. if (cond1 and cond2 or cond3) { ... } // \u2705 Valid: Parentheses avoid the precedence check. if (cond1 and (cond2 or cond3)) { ... } // \u274c Invalid: `not` precedence is incomparable with `==`. if (not cond1 == cond2) { ... } // \u274c Invalid: `not` precedence is incomparable with `==`. if (cond1 == not cond2) { ... } // \u2705 Valid: Parentheses avoid the precedence check. if (cond1 == (not cond2)) { ... }","title":"Precedence"},{"location":"design/expressions/logical_operators/#associativity","text":"and and or are left-associative. A not expression cannot be the operand of another not expression; not not b is an error without parentheses. // \u2705 Valid: `and` is left-associative, and precedence is fine. if (not a and not b and not c) { ... } // The above is equivalent to: if ((not a) and ((not b) and (not c))) { ... } // \u2705 Valid: Parentheses avoid the `not` associativity error. if (not (not a)) { ... } // \u274c Invalid: `not not` associativity requires parentheses. if (not not a) { ... }","title":"Associativity"},{"location":"design/expressions/logical_operators/#conversions","text":"TODO: This should be addressed through a standard bool conversion design. The operand of and , or , or not is converted to a bool value in the same way as the condition of an if statement. In particular: If we decide that certain values, such as pointers or integers, should not be usable as the condition of an if without an explicit comparison against null or zero, then those values will also not be usable as the operand of and , or , or not without an explicit comparison. If an extension point is provided to determine how to branch on the truth of a value in an if (such as by supplying a conversion to a bool type), that extension point will also apply to and , or , and not .","title":"Conversions"},{"location":"design/expressions/logical_operators/#overloading","text":"The logical operators and , or , and not are not overloadable. As noted above, any mechanism that allows types to customize how if treats them will also customize how and , or , and not treats them.","title":"Overloading"},{"location":"design/expressions/logical_operators/#alternatives-considered","text":"Use punctuation spelling for all three operators Precedence of AND versus OR Precedence of NOT Punctuation form of NOT Two forms of NOT Repeated NOT AND and OR produce the decisive value","title":"Alternatives considered"},{"location":"design/expressions/logical_operators/#references","text":"Proposal #680: And, or, not . Proposal #702: Comparison operators .","title":"References"},{"location":"design/expressions/member_access/","text":"Qualified names and member access Table of contents Overview Member resolution Package and namespace members Lookup within values Templates and generics Lookup ambiguity impl lookup Instance binding Non-instance members Non-vacuous member access restriction Precedence and associativity Alternatives considered References Overview A qualified name is a word that is preceded by a period. The name is found within a contextually determined entity: In a member access expression, this is the entity preceding the period. For a designator in a struct literal, the name is introduced as a member of the struct type. A member access expression allows a member of a value, type, interface, namespace, and so on to be accessed by specifying a qualified name for the member. A member access expression is either a simple member access expression of the form: member-access-expression ::= expression . word or a compound member access of the form: member-access-expression ::= expression . ( expression ) Compound member accesses allow specifying a qualified member name. For example: package Widgets api; interface Widget { fn Grow[addr me: Self*](factor: f64); } class Cog { var size: i32; fn Make(size: i32) -> Self; impl as Widgets.Widget; } fn GrowSomeCogs() { var cog1: Cog = Cog.Make(1); var cog2: Cog = cog1.Make(2); let cog1_size: i32 = cog1.size; cog1.Grow(1.5); cog2.(Cog.Grow)(cog1_size as f64); cog1.(Widget.Grow)(1.1); cog2.(Widgets.Cog.(Widgets.Widget.Grow))(1.9); } A member access expression is processed using the following steps: First, the word or parenthesized expression to the right of the . is resolved to a specific member entity, called M in this document. Then, if necessary, impl lookup is performed to map from a member of an interface to a member of the relevant impl , potentially updating M . Then, if necessary, instance binding is performed to locate the member subobject corresponding to a field name or to build a bound method object, producing the result of the member access expression. If instance binding is not performed , the result is M . Member resolution The process of member resolution determines which member M a member access expression is referring to. Package and namespace members If the first operand is a package or namespace name, the expression must be a simple member access expression. The word must name a member of that package or namespace, and the result is the package or namespace member with that name. An expression that names a package or namespace can only be used as the first operand of a member access or as the target of an alias declaration. namespace MyNamespace; fn MyNamespace.MyFunction() {} // \u2705 OK, can alias a namespace. alias MyNS = MyNamespace; fn CallMyFunction() { MyNS.MyFunction(); } // \u274c Error: a namespace is not a value. let MyNS2:! auto = MyNamespace; fn CallMyFunction2() { // \u274c Error: cannot perform compound member access into a namespace. MyNamespace.(MyNamespace.MyFunction)(); } Lookup within values When the first operand is not a package or namespace name, there are three remaining cases we wish to support: The first operand is a value, and lookup should consider members of the value's type. The first operand is a type, and lookup should consider members of that type. For example, i32.Least should find the member constant Least of the type i32 . The first operand is a type-of-type, and lookup should consider members of that type-of-type. For example, Addable.Add should find the member function Add of the interface Addable . Because a type-of-type is a type, this is a special case of the previous bullet. Note that because a type is a value, and a type-of-type is a type, these cases are overlapping and not entirely separable. If any of the above lookups ever looks for members of a type parameter, it should consider members of the type-of-type, treating the type parameter as an archetype. Note: If lookup is performed into a type that involves a template parameter, the lookup will be performed both in the context of the template definition and in the context of the template instantiation, as described in templates and generics . For a simple member access, the word is looked up in the following types: If the first operand can be evaluated and evaluates to a type, that type. If the type of the first operand can be evaluated, that type. If the type of the first operand is a generic type parameter, and the type of that generic type parameter can be evaluated, that type-of-type. The results of these lookups are combined . For a compound member access, the second operand is evaluated as a constant to determine the member being accessed. The evaluation is required to succeed and to result in a member of a type or interface. For example: interface Printable { fn Print[me: Self](); } external impl i32 as Printable; class Point { var x: i32; var y: i32; // Internal impl injects the name `Print` into class `Point`. impl as Printable; } fn PrintPointTwice() { var p: Point = {.x = 0, .y = 0}; // \u2705 OK, `x` found in type of `p`, namely `Point`. p.x = 1; // \u2705 OK, `y` found in the type `Point`. p.(Point.y) = 1; // \u2705 OK, `Print` found in type of `p`, namely `Point`. p.Print(); // \u2705 OK, `Print` found in the type `Printable`. p.(Printable.Print)(); } fn GenericPrint[T:! Printable](a: T) { // \u2705 OK, type of `a` is the type parameter `T`; // `Print` found in the type of `T`, namely `Printable`. a.Print(); } fn CallGenericPrint(p: Point) { GenericPrint(p); } Templates and generics If the value or type of the first operand depends on a template or generic parameter, the lookup is performed from a context where the value of that parameter is unknown. Evaluation of an expression involving the parameter may still succeed, but will result in a symbolic value involving that parameter. class GenericWrapper(T:! Type) { var field: T; } fn F[T:! Type](x: GenericWrapper(T)) -> T { // \u2705 OK, finds `GenericWrapper(T).field`. return x.field; } class TemplateWrapper(template T:! Type) { var field: T; } fn G[template T:! Type](x: TemplateWrapper(T)) -> T { // \ud83e\udd37 Not yet decided. return x.field; } TODO: The behavior of G above is not yet fully decided. If class templates can be specialized, then we cannot know the members of TemplateWrapper(T) without knowing T , so this first lookup will find nothing. In any case, as described below, the lookup will be performed again when T is known. If the value or type depends on any template parameters, the lookup is redone from a context where the values of those parameters are known, but where the values of any generic parameters are still unknown. The lookup results from these two contexts are combined . Note: All lookups are done from a context where the values of any generic parameters that are in scope are unknown. Unlike for a template parameter, the actual value of a generic parameter never affects the result of member resolution. class Cowboy { fn Draw[me: Self](); } interface Renderable { fn Draw[me: Self](); } external impl Cowboy as Renderable { fn Draw[me: Self](); } fn DrawDirect(c: Cowboy) { c.Draw(); } fn DrawGeneric[T:! Renderable](c: T) { c.Draw(); } fn DrawTemplate[template T:! Renderable](c: T) { c.Draw(); } fn Draw(c: Cowboy) { // \u2705 Calls member of `Cowboy`. DrawDirect(c); // \u2705 Calls member of `impl Cowboy as Renderable`. DrawGeneric(c); // \u274c Error: ambiguous. DrawTemplate(c); } class RoundWidget { external impl as Renderable { fn Draw[me: Self](); } alias Draw = Renderable.Draw; } class SquareWidget { fn Draw[me: Self]() {} external impl as Renderable { alias Draw = Self.Draw; } } fn DrawWidget(r: RoundWidget, s: SquareWidget) { // \u2705 OK, lookup in type and lookup in type-of-type find the same entity. DrawTemplate(r); // \u2705 OK, lookup in type and lookup in type-of-type find the same entity. DrawTemplate(s); // \u2705 OK, found in type. r.Draw(); s.Draw(); } Lookup ambiguity Multiple lookups can be performed when resolving a member access expression. If more than one member is found, after performing impl lookup if necessary, the lookup is ambiguous, and the program is invalid. Similarly, if no members are found, the program is invalid. Otherwise, the result of combining the lookup results is the unique member that was found. impl lookup When the second operand of a member access expression resolves to a member of an interface I , and the first operand is a value other than a type-of-type, impl lookup is performed to map the member of the interface to the corresponding member of the relevant impl . The member of the impl replaces the member of the interface in all further processing of the member access expression. interface Addable { // #1 fn Add[me: Self](other: Self) -> Self; // #2 default fn Sum[Seq:! Iterable where .ValueType = Self](seq: Seq) -> Self { // ... } } class Integer { impl as Addable { // #3 fn Add[me: Self](other: Self) -> Self; // #4, generated from default implementation for #2. // fn Sum[...](...); } } fn SumIntegers(v: Vector(Integer)) -> Integer { // Member resolution resolves the name `Sum` to #2. // `impl` lookup then locates the `impl Integer as Addable`, // and determines that the member access refers to #4, // which is then called. return Integer.Sum(v); } fn AddTwoIntegers(a: Integer, b: Integer) -> Integer { // Member resolution resolves the name `Add` to #1. // `impl` lookup then locates the `impl Integer as Addable`, // and determines that the member access refers to #3. // Finally, instance binding will be performed as described later. // This can be written more verbosely and explicitly as any of: // - `return a.(Integer.Add)(b);` // - `return a.(Addable.Add)(b);` // - `return a.(Integer.(Addable.Add))(b);` return a.Add(b); } The type T that is expected to implement I depends on the first operand of the member access expression, V : If V can be evaluated and evaluates to a type, then T is V . carbon // `V` is `Integer`. `T` is `V`, which is `Integer`. // Alias refers to #2. alias AddIntegers = Integer.Add; Otherwise, T is the type of V . carbon let a: Integer = {}; // `V` is `a`. `T` is the type of `V`, which is `Integer`. // `a.Add` refers to #2. let twice_a: Integer = a.Add(a); The appropriate impl T as I implementation is located. The program is invalid if no such impl exists. When T or I depends on a generic parameter, a suitable constraint must be specified to ensure that such an impl will exist. When T or I depends on a template parameter, this check is deferred until the argument for the template parameter is known. M is replaced by the member of the impl that corresponds to M . interface I { // #1 default fn F[me: Self]() {} let N:! i32; } class C { impl as I where .N = 5 { // #2 fn F[me: C]() {} } } // `V` is `I` and `M` is `I.F`. Because `V` is a type-of-type, // `impl` lookup is not performed, and the alias binds to #1. alias A1 = I.F; // `V` is `C` and `M` is `I.F`. Because `V` is a type, `impl` // lookup is performed with `T` being `C`, and the alias binds to #2. alias A2 = C.F; let c: C = {}; // `V` is `c` and `M` is `I.N`. Because `V` is a non-type, `impl` // lookup is performed with `T` being the type of `c`, namely `C`, and // `M` becomes the associated constant from `impl C as I`. // The value of `Z` is 5. let Z: i32 = c.N; Instance binding may also apply if the member is an instance member. var c: C; // `V` is `c` and `M` is `I.F`. Because `V` is not a type, `T` is the // type of `c`, which is `C`. `impl` lookup is performed, and `M` is // replaced with #2. Then instance binding is performed. c.F(); Note: When an interface member is added to a class by an alias, impl lookup is not performed as part of handling the alias, but will happen when naming the interface member as a member of the class. interface Renderable { // #1 fn Draw[me: Self](); } class RoundWidget { external impl as Renderable { // #2 fn Draw[me: Self](); } // `Draw` names the member of the `Renderable` interface. alias Draw = Renderable.Draw; } class SquareWidget { // #3 fn Draw[me: Self]() {} external impl as Renderable { alias Draw = Self.Draw; } } fn DrawWidget(r: RoundWidget, s: SquareWidget) { // \u2705 OK: In the inner member access, the name `Draw` resolves to the // member `Draw` of `Renderable`, #1, which `impl` lookup replaces with // the member `Draw` of `impl RoundWidget as Renderable`, #2. // The outer member access then forms a bound member function that // calls #2 on `r`, as described in \"Instance binding\". r.(RoundWidget.Draw)(); // \u2705 OK: In the inner member access, the name `Draw` resolves to the // member `Draw` of `SquareWidget`, #3. // The outer member access then forms a bound member function that // calls #3 on `s`. s.(SquareWidget.Draw)(); // \u274c Error: In the inner member access, the name `Draw` resolves to the // member `Draw` of `SquareWidget`, #3. // The outer member access fails because we can't call // #3, `Draw[me: SquareWidget]()`, on a `RoundWidget` object `r`. r.(SquareWidget.Draw)(); // \u274c Error: In the inner member access, the name `Draw` resolves to the // member `Draw` of `Renderable`, #1, which `impl` lookup replaces with // the member `Draw` of `impl RoundWidget as Renderable`, #2. // The outer member access fails because we can't call // #2, `Draw[me: RoundWidget]()`, on a `SquareWidget` object `s`. s.(RoundWidget.Draw)(); } base class WidgetBase { // \u2705 OK, even though `WidgetBase` does not implement `Renderable`. alias Draw = Renderable.Draw; fn DrawAll[T:! Renderable](v: Vector(T)) { for (var w: T in v) { // \u2705 OK. Unqualified lookup for `Draw` finds alias `WidgetBase.Draw` // to `Renderable.Draw`, which does not perform `impl` lookup yet. // Then the compound member access expression performs `impl` lookup // into `impl T as Renderable`, since `T` is known to implement // `Renderable`. Finally, the member function is bound to `w` as // described in \"Instance binding\". w.(Draw)(); // \u274c Error: `Self.Draw` performs `impl` lookup, which fails // because `WidgetBase` does not implement `Renderable`. w.(Self.Draw)(); } } } class TriangleWidget extends WidgetBase { external impl as Renderable; } fn DrawTriangle(t: TriangleWidget) { // \u2705 OK: name `Draw` resolves to `Draw` member of `WidgetBase`, which // is `Renderable.Draw`. Then impl lookup replaces that with `Draw` // member of `impl TriangleWidget as Renderable`. t.Draw(); } Instance binding If member resolution and impl lookup produce a member M that is an instance member -- that is, a field or a method -- and the first operand V of . is a value other than a type, then instance binding is performed, as follows: For a field member in class C , V is required to be of type C or of a type derived from C . The result is the corresponding subobject within V . The result is an lvalue if V is an lvalue. carbon var dims: auto = {.width = 1, .height = 2}; // `dims.width` denotes the field `width` of the object `dims`. Print(dims.width); // `dims` is an lvalue, so `dims.height` is an lvalue. dims.height = 3; For a method, the result is a bound method , which is a value F such that a function call F(args) behaves the same as a call to M(args) with the me parameter initialized by a corresponding recipient argument: If the method declares its me parameter with addr , the recipient argument is &V . Otherwise, the recipient argument is V . ``carbon class Blob { fn Mutate[addr me: Self*](n: i32); } fn F(p: Blob*) { // \u2705 OK, forms bound method (( p).M) and calls it. // This calls Blob.Mutate with me initialized by &( p) // and n initialized by 5`. (*p).Mutate(5); // \u2705 OK, same as above. let bound_m: auto = (*p).Mutate; bound_m(5); } ``` Non-instance members If instance binding is not performed, the result is the member M determined by member resolution and impl lookup. Evaluating the member access expression evaluates V and discards the result. An expression that names an instance member, but for which instance binding is not performed, can only be used as the second operand of a compound member access or as the target of an alias declaration. class C { fn StaticMethod(); var field: i32; class Nested {} } fn CallStaticMethod(c: C) { // \u2705 OK, calls `C.StaticMethod`. C.StaticMethod(); // \u2705 OK, evaluates expression `c` then calls `C.StaticMethod`. c.StaticMethod(); // \u274c Error: name of instance member `C.field` can only be used in a // member access or alias. C.field = 1; // \u2705 OK, instance binding is performed by outer member access, // same as `c.field = 1;` c.(C.field) = 1; // \u2705 OK let T:! Type = C.Nested; // \u274c Error: value of `:!` binding is not constant because it // refers to local variable `c`. let U:! Type = c.Nested; } Non-vacuous member access restriction The first operand of a member access expression must be used in some way: a compound member access must result in impl lookup, instance binding, or both. In a simple member access, this always holds, because the first operand is always used for lookup. interface Printable { fn Print[me: Self](); } external impl i32 as Printable { fn Print[me: Self](); } fn MemberAccess(n: i32) { // \u2705 OK: `Printable.Print` is the interface member. // `i32.(Printable.Print)` is the corresponding member of the `impl`. // `n.(i32.(Printable.Print))` is a bound member function naming that member. n.(i32.(Printable.Print))(); // \u2705 Same as above, `n.(Printable.Print)` is effectively interpreted as // `n.(T.(Printable.Print))()`, where `T` is the type of `n`, // because `n` does not evaluate to a type. Performs impl lookup // and then instance binding. n.(Printable.Print)(); } // \u2705 OK, member `Print` of interface `Printable`. alias X1 = Printable.Print; // \u274c Error, compound access doesn't perform impl lookup or instance binding. alias X2 = Printable.(Printable.Print); // \u2705 OK, member `Print` of `impl i32 as Printable`. alias X3 = i32.(Printable.Print); // \u274c Error, compound access doesn't perform impl lookup or instance binding. alias X4 = i32.(i32.(Printable.Print)); Precedence and associativity Member access expressions associate left-to-right: class A { class B { fn F(); } } interface B { fn F(); } external impl A as B; fn Use(a: A) { // Calls member `F` of class `A.B`. (a.B).F(); // Calls member `F` of interface `B`, as implemented by type `A`. a.(B.F)(); // Same as `(a.B).F()`. a.B.F(); } Member access has lower precedence than primary expressions, and higher precedence than all other expression forms. // \u2705 OK, `*` has lower precedence than `.`. Same as `(A.B)*`. var p: A.B*; // \u2705 OK, `1 + (X.Y)` not `(1 + X).Y`. var n: i32 = 1 + X.Y; Alternatives considered Separate syntax for static versus dynamic access, such as :: versus . Use a different lookup rule for names in templates Meaning of Type.Interface References Proposal #989: member access expressions Question for leads: constrained template name lookup","title":"Qualified names and member access"},{"location":"design/expressions/member_access/#qualified-names-and-member-access","text":"","title":"Qualified names and member access"},{"location":"design/expressions/member_access/#table-of-contents","text":"Overview Member resolution Package and namespace members Lookup within values Templates and generics Lookup ambiguity impl lookup Instance binding Non-instance members Non-vacuous member access restriction Precedence and associativity Alternatives considered References","title":"Table of contents"},{"location":"design/expressions/member_access/#overview","text":"A qualified name is a word that is preceded by a period. The name is found within a contextually determined entity: In a member access expression, this is the entity preceding the period. For a designator in a struct literal, the name is introduced as a member of the struct type. A member access expression allows a member of a value, type, interface, namespace, and so on to be accessed by specifying a qualified name for the member. A member access expression is either a simple member access expression of the form: member-access-expression ::= expression . word or a compound member access of the form: member-access-expression ::= expression . ( expression ) Compound member accesses allow specifying a qualified member name. For example: package Widgets api; interface Widget { fn Grow[addr me: Self*](factor: f64); } class Cog { var size: i32; fn Make(size: i32) -> Self; impl as Widgets.Widget; } fn GrowSomeCogs() { var cog1: Cog = Cog.Make(1); var cog2: Cog = cog1.Make(2); let cog1_size: i32 = cog1.size; cog1.Grow(1.5); cog2.(Cog.Grow)(cog1_size as f64); cog1.(Widget.Grow)(1.1); cog2.(Widgets.Cog.(Widgets.Widget.Grow))(1.9); } A member access expression is processed using the following steps: First, the word or parenthesized expression to the right of the . is resolved to a specific member entity, called M in this document. Then, if necessary, impl lookup is performed to map from a member of an interface to a member of the relevant impl , potentially updating M . Then, if necessary, instance binding is performed to locate the member subobject corresponding to a field name or to build a bound method object, producing the result of the member access expression. If instance binding is not performed , the result is M .","title":"Overview"},{"location":"design/expressions/member_access/#member-resolution","text":"The process of member resolution determines which member M a member access expression is referring to.","title":"Member resolution"},{"location":"design/expressions/member_access/#package-and-namespace-members","text":"If the first operand is a package or namespace name, the expression must be a simple member access expression. The word must name a member of that package or namespace, and the result is the package or namespace member with that name. An expression that names a package or namespace can only be used as the first operand of a member access or as the target of an alias declaration. namespace MyNamespace; fn MyNamespace.MyFunction() {} // \u2705 OK, can alias a namespace. alias MyNS = MyNamespace; fn CallMyFunction() { MyNS.MyFunction(); } // \u274c Error: a namespace is not a value. let MyNS2:! auto = MyNamespace; fn CallMyFunction2() { // \u274c Error: cannot perform compound member access into a namespace. MyNamespace.(MyNamespace.MyFunction)(); }","title":"Package and namespace members"},{"location":"design/expressions/member_access/#lookup-within-values","text":"When the first operand is not a package or namespace name, there are three remaining cases we wish to support: The first operand is a value, and lookup should consider members of the value's type. The first operand is a type, and lookup should consider members of that type. For example, i32.Least should find the member constant Least of the type i32 . The first operand is a type-of-type, and lookup should consider members of that type-of-type. For example, Addable.Add should find the member function Add of the interface Addable . Because a type-of-type is a type, this is a special case of the previous bullet. Note that because a type is a value, and a type-of-type is a type, these cases are overlapping and not entirely separable. If any of the above lookups ever looks for members of a type parameter, it should consider members of the type-of-type, treating the type parameter as an archetype. Note: If lookup is performed into a type that involves a template parameter, the lookup will be performed both in the context of the template definition and in the context of the template instantiation, as described in templates and generics . For a simple member access, the word is looked up in the following types: If the first operand can be evaluated and evaluates to a type, that type. If the type of the first operand can be evaluated, that type. If the type of the first operand is a generic type parameter, and the type of that generic type parameter can be evaluated, that type-of-type. The results of these lookups are combined . For a compound member access, the second operand is evaluated as a constant to determine the member being accessed. The evaluation is required to succeed and to result in a member of a type or interface. For example: interface Printable { fn Print[me: Self](); } external impl i32 as Printable; class Point { var x: i32; var y: i32; // Internal impl injects the name `Print` into class `Point`. impl as Printable; } fn PrintPointTwice() { var p: Point = {.x = 0, .y = 0}; // \u2705 OK, `x` found in type of `p`, namely `Point`. p.x = 1; // \u2705 OK, `y` found in the type `Point`. p.(Point.y) = 1; // \u2705 OK, `Print` found in type of `p`, namely `Point`. p.Print(); // \u2705 OK, `Print` found in the type `Printable`. p.(Printable.Print)(); } fn GenericPrint[T:! Printable](a: T) { // \u2705 OK, type of `a` is the type parameter `T`; // `Print` found in the type of `T`, namely `Printable`. a.Print(); } fn CallGenericPrint(p: Point) { GenericPrint(p); }","title":"Lookup within values"},{"location":"design/expressions/member_access/#templates-and-generics","text":"If the value or type of the first operand depends on a template or generic parameter, the lookup is performed from a context where the value of that parameter is unknown. Evaluation of an expression involving the parameter may still succeed, but will result in a symbolic value involving that parameter. class GenericWrapper(T:! Type) { var field: T; } fn F[T:! Type](x: GenericWrapper(T)) -> T { // \u2705 OK, finds `GenericWrapper(T).field`. return x.field; } class TemplateWrapper(template T:! Type) { var field: T; } fn G[template T:! Type](x: TemplateWrapper(T)) -> T { // \ud83e\udd37 Not yet decided. return x.field; } TODO: The behavior of G above is not yet fully decided. If class templates can be specialized, then we cannot know the members of TemplateWrapper(T) without knowing T , so this first lookup will find nothing. In any case, as described below, the lookup will be performed again when T is known. If the value or type depends on any template parameters, the lookup is redone from a context where the values of those parameters are known, but where the values of any generic parameters are still unknown. The lookup results from these two contexts are combined . Note: All lookups are done from a context where the values of any generic parameters that are in scope are unknown. Unlike for a template parameter, the actual value of a generic parameter never affects the result of member resolution. class Cowboy { fn Draw[me: Self](); } interface Renderable { fn Draw[me: Self](); } external impl Cowboy as Renderable { fn Draw[me: Self](); } fn DrawDirect(c: Cowboy) { c.Draw(); } fn DrawGeneric[T:! Renderable](c: T) { c.Draw(); } fn DrawTemplate[template T:! Renderable](c: T) { c.Draw(); } fn Draw(c: Cowboy) { // \u2705 Calls member of `Cowboy`. DrawDirect(c); // \u2705 Calls member of `impl Cowboy as Renderable`. DrawGeneric(c); // \u274c Error: ambiguous. DrawTemplate(c); } class RoundWidget { external impl as Renderable { fn Draw[me: Self](); } alias Draw = Renderable.Draw; } class SquareWidget { fn Draw[me: Self]() {} external impl as Renderable { alias Draw = Self.Draw; } } fn DrawWidget(r: RoundWidget, s: SquareWidget) { // \u2705 OK, lookup in type and lookup in type-of-type find the same entity. DrawTemplate(r); // \u2705 OK, lookup in type and lookup in type-of-type find the same entity. DrawTemplate(s); // \u2705 OK, found in type. r.Draw(); s.Draw(); }","title":"Templates and generics"},{"location":"design/expressions/member_access/#lookup-ambiguity","text":"Multiple lookups can be performed when resolving a member access expression. If more than one member is found, after performing impl lookup if necessary, the lookup is ambiguous, and the program is invalid. Similarly, if no members are found, the program is invalid. Otherwise, the result of combining the lookup results is the unique member that was found.","title":"Lookup ambiguity"},{"location":"design/expressions/member_access/#impl-lookup","text":"When the second operand of a member access expression resolves to a member of an interface I , and the first operand is a value other than a type-of-type, impl lookup is performed to map the member of the interface to the corresponding member of the relevant impl . The member of the impl replaces the member of the interface in all further processing of the member access expression. interface Addable { // #1 fn Add[me: Self](other: Self) -> Self; // #2 default fn Sum[Seq:! Iterable where .ValueType = Self](seq: Seq) -> Self { // ... } } class Integer { impl as Addable { // #3 fn Add[me: Self](other: Self) -> Self; // #4, generated from default implementation for #2. // fn Sum[...](...); } } fn SumIntegers(v: Vector(Integer)) -> Integer { // Member resolution resolves the name `Sum` to #2. // `impl` lookup then locates the `impl Integer as Addable`, // and determines that the member access refers to #4, // which is then called. return Integer.Sum(v); } fn AddTwoIntegers(a: Integer, b: Integer) -> Integer { // Member resolution resolves the name `Add` to #1. // `impl` lookup then locates the `impl Integer as Addable`, // and determines that the member access refers to #3. // Finally, instance binding will be performed as described later. // This can be written more verbosely and explicitly as any of: // - `return a.(Integer.Add)(b);` // - `return a.(Addable.Add)(b);` // - `return a.(Integer.(Addable.Add))(b);` return a.Add(b); } The type T that is expected to implement I depends on the first operand of the member access expression, V : If V can be evaluated and evaluates to a type, then T is V . carbon // `V` is `Integer`. `T` is `V`, which is `Integer`. // Alias refers to #2. alias AddIntegers = Integer.Add; Otherwise, T is the type of V . carbon let a: Integer = {}; // `V` is `a`. `T` is the type of `V`, which is `Integer`. // `a.Add` refers to #2. let twice_a: Integer = a.Add(a); The appropriate impl T as I implementation is located. The program is invalid if no such impl exists. When T or I depends on a generic parameter, a suitable constraint must be specified to ensure that such an impl will exist. When T or I depends on a template parameter, this check is deferred until the argument for the template parameter is known. M is replaced by the member of the impl that corresponds to M . interface I { // #1 default fn F[me: Self]() {} let N:! i32; } class C { impl as I where .N = 5 { // #2 fn F[me: C]() {} } } // `V` is `I` and `M` is `I.F`. Because `V` is a type-of-type, // `impl` lookup is not performed, and the alias binds to #1. alias A1 = I.F; // `V` is `C` and `M` is `I.F`. Because `V` is a type, `impl` // lookup is performed with `T` being `C`, and the alias binds to #2. alias A2 = C.F; let c: C = {}; // `V` is `c` and `M` is `I.N`. Because `V` is a non-type, `impl` // lookup is performed with `T` being the type of `c`, namely `C`, and // `M` becomes the associated constant from `impl C as I`. // The value of `Z` is 5. let Z: i32 = c.N; Instance binding may also apply if the member is an instance member. var c: C; // `V` is `c` and `M` is `I.F`. Because `V` is not a type, `T` is the // type of `c`, which is `C`. `impl` lookup is performed, and `M` is // replaced with #2. Then instance binding is performed. c.F(); Note: When an interface member is added to a class by an alias, impl lookup is not performed as part of handling the alias, but will happen when naming the interface member as a member of the class. interface Renderable { // #1 fn Draw[me: Self](); } class RoundWidget { external impl as Renderable { // #2 fn Draw[me: Self](); } // `Draw` names the member of the `Renderable` interface. alias Draw = Renderable.Draw; } class SquareWidget { // #3 fn Draw[me: Self]() {} external impl as Renderable { alias Draw = Self.Draw; } } fn DrawWidget(r: RoundWidget, s: SquareWidget) { // \u2705 OK: In the inner member access, the name `Draw` resolves to the // member `Draw` of `Renderable`, #1, which `impl` lookup replaces with // the member `Draw` of `impl RoundWidget as Renderable`, #2. // The outer member access then forms a bound member function that // calls #2 on `r`, as described in \"Instance binding\". r.(RoundWidget.Draw)(); // \u2705 OK: In the inner member access, the name `Draw` resolves to the // member `Draw` of `SquareWidget`, #3. // The outer member access then forms a bound member function that // calls #3 on `s`. s.(SquareWidget.Draw)(); // \u274c Error: In the inner member access, the name `Draw` resolves to the // member `Draw` of `SquareWidget`, #3. // The outer member access fails because we can't call // #3, `Draw[me: SquareWidget]()`, on a `RoundWidget` object `r`. r.(SquareWidget.Draw)(); // \u274c Error: In the inner member access, the name `Draw` resolves to the // member `Draw` of `Renderable`, #1, which `impl` lookup replaces with // the member `Draw` of `impl RoundWidget as Renderable`, #2. // The outer member access fails because we can't call // #2, `Draw[me: RoundWidget]()`, on a `SquareWidget` object `s`. s.(RoundWidget.Draw)(); } base class WidgetBase { // \u2705 OK, even though `WidgetBase` does not implement `Renderable`. alias Draw = Renderable.Draw; fn DrawAll[T:! Renderable](v: Vector(T)) { for (var w: T in v) { // \u2705 OK. Unqualified lookup for `Draw` finds alias `WidgetBase.Draw` // to `Renderable.Draw`, which does not perform `impl` lookup yet. // Then the compound member access expression performs `impl` lookup // into `impl T as Renderable`, since `T` is known to implement // `Renderable`. Finally, the member function is bound to `w` as // described in \"Instance binding\". w.(Draw)(); // \u274c Error: `Self.Draw` performs `impl` lookup, which fails // because `WidgetBase` does not implement `Renderable`. w.(Self.Draw)(); } } } class TriangleWidget extends WidgetBase { external impl as Renderable; } fn DrawTriangle(t: TriangleWidget) { // \u2705 OK: name `Draw` resolves to `Draw` member of `WidgetBase`, which // is `Renderable.Draw`. Then impl lookup replaces that with `Draw` // member of `impl TriangleWidget as Renderable`. t.Draw(); }","title":"impl lookup"},{"location":"design/expressions/member_access/#instance-binding","text":"If member resolution and impl lookup produce a member M that is an instance member -- that is, a field or a method -- and the first operand V of . is a value other than a type, then instance binding is performed, as follows: For a field member in class C , V is required to be of type C or of a type derived from C . The result is the corresponding subobject within V . The result is an lvalue if V is an lvalue. carbon var dims: auto = {.width = 1, .height = 2}; // `dims.width` denotes the field `width` of the object `dims`. Print(dims.width); // `dims` is an lvalue, so `dims.height` is an lvalue. dims.height = 3; For a method, the result is a bound method , which is a value F such that a function call F(args) behaves the same as a call to M(args) with the me parameter initialized by a corresponding recipient argument: If the method declares its me parameter with addr , the recipient argument is &V . Otherwise, the recipient argument is V . ``carbon class Blob { fn Mutate[addr me: Self*](n: i32); } fn F(p: Blob*) { // \u2705 OK, forms bound method (( p).M) and calls it. // This calls Blob.Mutate with me initialized by &( p) // and n initialized by 5`. (*p).Mutate(5); // \u2705 OK, same as above. let bound_m: auto = (*p).Mutate; bound_m(5); } ```","title":"Instance binding"},{"location":"design/expressions/member_access/#non-instance-members","text":"If instance binding is not performed, the result is the member M determined by member resolution and impl lookup. Evaluating the member access expression evaluates V and discards the result. An expression that names an instance member, but for which instance binding is not performed, can only be used as the second operand of a compound member access or as the target of an alias declaration. class C { fn StaticMethod(); var field: i32; class Nested {} } fn CallStaticMethod(c: C) { // \u2705 OK, calls `C.StaticMethod`. C.StaticMethod(); // \u2705 OK, evaluates expression `c` then calls `C.StaticMethod`. c.StaticMethod(); // \u274c Error: name of instance member `C.field` can only be used in a // member access or alias. C.field = 1; // \u2705 OK, instance binding is performed by outer member access, // same as `c.field = 1;` c.(C.field) = 1; // \u2705 OK let T:! Type = C.Nested; // \u274c Error: value of `:!` binding is not constant because it // refers to local variable `c`. let U:! Type = c.Nested; }","title":"Non-instance members"},{"location":"design/expressions/member_access/#non-vacuous-member-access-restriction","text":"The first operand of a member access expression must be used in some way: a compound member access must result in impl lookup, instance binding, or both. In a simple member access, this always holds, because the first operand is always used for lookup. interface Printable { fn Print[me: Self](); } external impl i32 as Printable { fn Print[me: Self](); } fn MemberAccess(n: i32) { // \u2705 OK: `Printable.Print` is the interface member. // `i32.(Printable.Print)` is the corresponding member of the `impl`. // `n.(i32.(Printable.Print))` is a bound member function naming that member. n.(i32.(Printable.Print))(); // \u2705 Same as above, `n.(Printable.Print)` is effectively interpreted as // `n.(T.(Printable.Print))()`, where `T` is the type of `n`, // because `n` does not evaluate to a type. Performs impl lookup // and then instance binding. n.(Printable.Print)(); } // \u2705 OK, member `Print` of interface `Printable`. alias X1 = Printable.Print; // \u274c Error, compound access doesn't perform impl lookup or instance binding. alias X2 = Printable.(Printable.Print); // \u2705 OK, member `Print` of `impl i32 as Printable`. alias X3 = i32.(Printable.Print); // \u274c Error, compound access doesn't perform impl lookup or instance binding. alias X4 = i32.(i32.(Printable.Print));","title":"Non-vacuous member access restriction"},{"location":"design/expressions/member_access/#precedence-and-associativity","text":"Member access expressions associate left-to-right: class A { class B { fn F(); } } interface B { fn F(); } external impl A as B; fn Use(a: A) { // Calls member `F` of class `A.B`. (a.B).F(); // Calls member `F` of interface `B`, as implemented by type `A`. a.(B.F)(); // Same as `(a.B).F()`. a.B.F(); } Member access has lower precedence than primary expressions, and higher precedence than all other expression forms. // \u2705 OK, `*` has lower precedence than `.`. Same as `(A.B)*`. var p: A.B*; // \u2705 OK, `1 + (X.Y)` not `(1 + X).Y`. var n: i32 = 1 + X.Y;","title":"Precedence and associativity"},{"location":"design/expressions/member_access/#alternatives-considered","text":"Separate syntax for static versus dynamic access, such as :: versus . Use a different lookup rule for names in templates Meaning of Type.Interface","title":"Alternatives considered"},{"location":"design/expressions/member_access/#references","text":"Proposal #989: member access expressions Question for leads: constrained template name lookup","title":"References"},{"location":"design/generics/","text":"Generics This directory contains the collection of documents describing the generics feature of Carbon: Overview - A high-level description of the generics design, with pointers to other design documents that dive deeper into individual topics. Goals - The motivation and principles guiding the design direction. Terminology - A glossary establishing common terminology for describing the design. Detailed design - In depth description of how generic type parameters work. ~~Rejected alternatives~~ - not implemented yet","title":"Generics"},{"location":"design/generics/#generics","text":"This directory contains the collection of documents describing the generics feature of Carbon: Overview - A high-level description of the generics design, with pointers to other design documents that dive deeper into individual topics. Goals - The motivation and principles guiding the design direction. Terminology - A glossary establishing common terminology for describing the design. Detailed design - In depth description of how generic type parameters work. ~~Rejected alternatives~~ - not implemented yet","title":"Generics"},{"location":"design/generics/appendix-coherence/","text":"Carbon: alternatives to coherence This document explains the rationale for choosing to make implementation coherence a goal for Carbon , and the alternatives considered. Table of contents Approach taken: coherence The \"Hashtable Problem\" Rejected alternative: no orphan rule Rejected alternative: incoherence Incoherence means context sensitivity Rejected variation: dynamic implementation binding Rejected variation: manual conflict resolution Approach taken: coherence The main thing to understand is that coherence is a desirable property, but to get that property we need an orphan rule, and that rule has a cost. It in particular limits how much control users of a type have over how that type implements interfaces. There are a few main problematic use cases to consider: Selecting between multiple implementations of an interface for a type. For example selecting the implementation of the Comparable interface for a Song type to support \"by title\", \"by artist\", and \"by album\" orderings. Implementing an interface for a type when there is no relationship between the libraries defining the interface and the type. When the implementation of an interface for a type uses an associated type that can't be referenced from the file or files where the implementation is allowed to be defined. These last two cases are highlighted as concerns in Rust in Rust RFC #1856: orphan rules are stricter than we would like . Since Carbon is bundling interface implementations into types, for the convenience and expressiveness that provides, we satisfy those use cases by giving the user control over the type of a value. This means having facilities for defining new compatible types with different interface implementations, and casting between those types as needed. The \"Hashtable Problem\" The \"Hashtable problem\" is that the specific hash function used to compute the hash of keys in a hashtable must be the same when adding an entry, when looking it up, and other operations like resizing. So a hashtable type is dependent on both the key type, and the key type's implementation of the Hashable interface. If the key type can have more than one implementation of Hashable , there needs to be some mechanism for choosing a single one to be used consistently by the hashtable type, or the invariants of the type will be violated. Without the orphan rule to enforce coherence, we might have a situation like this: Package Container defines a HashSet type. package Container; struct HashSet(Key:! Hashable) { ... } A Song type is defined in package SongLib . Package SongHashArtistAndTitle defines an implementation of Hashable for SongLib.Song . package SongHashArtistAndTitle; import SongLib; impl SongLib.Song as Hashable { fn Hash[me: Self]() -> u64 { ... } } Package SongUtil uses the Hashable implementation from SongHashArtistAndTitle to define a function IsInHashSet . ``` package SongUtil; import SongLib; import SongHashArtistAndTitle; import Containers; fn IsInHashSet( s: SongLib.Song, h: Containers.HashSet(SongLib.Song)*) -> bool { return h->Contains(s); } ``` Package SongHashAppleMusicURL defines a different implementation of Hashable for SongLib.Song than package SongHashArtistAndTitle . package SongHashAppleMusicURL; import SongLib; impl SongLib.Song as Hashable { fn Hash[me: Self]() -> u64 { ... } } Finally, package Trouble imports SongHashAppleMusicURL , creates a hash set, and then calls the IsInHashSet function from package SongUtil . ``` package Trouble; import SongLib; import SongHashAppleMusicURL; import Containers; import SongUtil; fn SomethingWeirdHappens() { var unchained_melody: SongLib.Song = ...; var song_set: auto = Containers.HashSet(SongLib.Song).Create(); song_set.Add(unchained_melody); // Either this is a compile error or does something unexpected. if (SongUtil.IsInHashSet(unchained_melody, &song_set)) { Print(\"This is expected, but doesn't happen.\"); } else { Print(\"This is what happens even though it is unexpected.\"); } } ``` The issue is that in package Trouble , the song_set is created in a context where SongLib.Song has a Hashable implementation from SongHashAppleMusicURL , and stores unchained_melody under that hash value. When we go to look up the same song in SongUtil.IsInHashSet , it uses the hash function from SongHashArtistAndTitle which returns a different hash value for unchained_melody , and so reports the song is missing. Background: This post discusses the hashtable problem in the context of Haskell, and this 2011 Rust followup discusses how to detect problems at compile time. Rejected alternative: no orphan rule In Swift an implementation of an interface, or a \"protocol\" as it is called in Swift, can be provided in any module. As long as any module provides an implementation, that implementation is used globally throughout the program . In Swift, since some protocol implementations can come from the runtime environment provided by the operating system, multiple implementations for a protocol can arise as a runtime warning. When this happens, Swift picks one implementation arbitrarily. In Carbon, we could make this a build time error. However, there would be nothing preventing two independent libraries from providing conflicting implementations. Furthermore, the error would only be diagnosed at link time. Rejected alternative: incoherence Incoherence means context sensitivity The undesirable result of incoherence is that the interpretation of source code changes based on imports. In particular, imagine there is a function call that depends on a type implementing an interface, and two different implementations are defined in two different libraries. A call to that function will be treated differently depending on which of those two libraries are imported: If neither is imported, it is an error. If both are imported, it is ambiguous. If only one is imported, you get totally different code executed depending on which it is. Furthermore, this means that the behavior of a file can depend on an import even if nothing from that package is referenced explicitly. In general, Carbon is avoiding this sort of context sensitivity . This context sensitivity would make moving code between files when refactoring more difficult and less safe. Rejected variation: dynamic implementation binding One possible approach would be to bind interface implementations to a value at the point it was created. In the example above , the implementation of the Hashable interface for Song would be fixed for the song_set HashSet object based on which implementation was in scope in the body of the SomethingWeirdHappens function. This idea is discussed briefly in section 5.4 on separate compilation of WG21 proposal n1848 for implementing \"Indiana\" C++0x concepts ( 1 , and 2 ). This has some downsides: It is harder to reason about. The behavior of SongUtil.IsInHashSet depends on the dynamic behavior of the program. At the time of the call, we may have no idea where the HashSet argument was created. An object may be created far from a call that has a particular interface requirement, with no guarantee that the object was created with any implementation of the interface at all. This error would only be detected at runtime, not at type checking time. It requires more data space at runtime because we need to store a pointer to the witness table representing the implementation with the object, since it varies instead of being known statically. It is slower to execute from dynamic dispatch and the inability to inline. In some cases it may not be feasible to use dynamic dispatch. For example, if an interface method returns an associated type, we might not know the calling convention of the function without knowing some details about the type. As a result, this doesn't make sense as the default behavior for Carbon based on its goals . That being said, this could be a feature added later as opt-in behavior to either allow users to reduce code size or support use cases that require dynamic dispatch. Rejected variation: manual conflict resolution Carbon could alternatively provide some kind of manual disambiguation syntax to resolve problems where they arise. The problems with this approach have been considered in the context of Rust . A specific example of this approach is called scoped conformance , where the conflict resolution is based on limiting the visibility of implementations to particular scopes. This hasn't been implemented, but it has the drawbacks described above. Depending on the details of the implementation, either: there are incompatible values with types that have the same name, or it is difficult to reason about the program's behavior because it behaves like dynamic implementation binding (though perhaps with a monomorphization cost instead of a runtime cost).","title":"Carbon: alternatives to coherence"},{"location":"design/generics/appendix-coherence/#carbon-alternatives-to-coherence","text":"This document explains the rationale for choosing to make implementation coherence a goal for Carbon , and the alternatives considered.","title":"Carbon: alternatives to coherence"},{"location":"design/generics/appendix-coherence/#table-of-contents","text":"Approach taken: coherence The \"Hashtable Problem\" Rejected alternative: no orphan rule Rejected alternative: incoherence Incoherence means context sensitivity Rejected variation: dynamic implementation binding Rejected variation: manual conflict resolution","title":"Table of contents"},{"location":"design/generics/appendix-coherence/#approach-taken-coherence","text":"The main thing to understand is that coherence is a desirable property, but to get that property we need an orphan rule, and that rule has a cost. It in particular limits how much control users of a type have over how that type implements interfaces. There are a few main problematic use cases to consider: Selecting between multiple implementations of an interface for a type. For example selecting the implementation of the Comparable interface for a Song type to support \"by title\", \"by artist\", and \"by album\" orderings. Implementing an interface for a type when there is no relationship between the libraries defining the interface and the type. When the implementation of an interface for a type uses an associated type that can't be referenced from the file or files where the implementation is allowed to be defined. These last two cases are highlighted as concerns in Rust in Rust RFC #1856: orphan rules are stricter than we would like . Since Carbon is bundling interface implementations into types, for the convenience and expressiveness that provides, we satisfy those use cases by giving the user control over the type of a value. This means having facilities for defining new compatible types with different interface implementations, and casting between those types as needed.","title":"Approach taken: coherence"},{"location":"design/generics/appendix-coherence/#the-hashtable-problem","text":"The \"Hashtable problem\" is that the specific hash function used to compute the hash of keys in a hashtable must be the same when adding an entry, when looking it up, and other operations like resizing. So a hashtable type is dependent on both the key type, and the key type's implementation of the Hashable interface. If the key type can have more than one implementation of Hashable , there needs to be some mechanism for choosing a single one to be used consistently by the hashtable type, or the invariants of the type will be violated. Without the orphan rule to enforce coherence, we might have a situation like this: Package Container defines a HashSet type. package Container; struct HashSet(Key:! Hashable) { ... } A Song type is defined in package SongLib . Package SongHashArtistAndTitle defines an implementation of Hashable for SongLib.Song . package SongHashArtistAndTitle; import SongLib; impl SongLib.Song as Hashable { fn Hash[me: Self]() -> u64 { ... } } Package SongUtil uses the Hashable implementation from SongHashArtistAndTitle to define a function IsInHashSet . ``` package SongUtil; import SongLib; import SongHashArtistAndTitle; import Containers; fn IsInHashSet( s: SongLib.Song, h: Containers.HashSet(SongLib.Song)*) -> bool { return h->Contains(s); } ``` Package SongHashAppleMusicURL defines a different implementation of Hashable for SongLib.Song than package SongHashArtistAndTitle . package SongHashAppleMusicURL; import SongLib; impl SongLib.Song as Hashable { fn Hash[me: Self]() -> u64 { ... } } Finally, package Trouble imports SongHashAppleMusicURL , creates a hash set, and then calls the IsInHashSet function from package SongUtil . ``` package Trouble; import SongLib; import SongHashAppleMusicURL; import Containers; import SongUtil; fn SomethingWeirdHappens() { var unchained_melody: SongLib.Song = ...; var song_set: auto = Containers.HashSet(SongLib.Song).Create(); song_set.Add(unchained_melody); // Either this is a compile error or does something unexpected. if (SongUtil.IsInHashSet(unchained_melody, &song_set)) { Print(\"This is expected, but doesn't happen.\"); } else { Print(\"This is what happens even though it is unexpected.\"); } } ``` The issue is that in package Trouble , the song_set is created in a context where SongLib.Song has a Hashable implementation from SongHashAppleMusicURL , and stores unchained_melody under that hash value. When we go to look up the same song in SongUtil.IsInHashSet , it uses the hash function from SongHashArtistAndTitle which returns a different hash value for unchained_melody , and so reports the song is missing. Background: This post discusses the hashtable problem in the context of Haskell, and this 2011 Rust followup discusses how to detect problems at compile time.","title":"The \"Hashtable Problem\""},{"location":"design/generics/appendix-coherence/#rejected-alternative-no-orphan-rule","text":"In Swift an implementation of an interface, or a \"protocol\" as it is called in Swift, can be provided in any module. As long as any module provides an implementation, that implementation is used globally throughout the program . In Swift, since some protocol implementations can come from the runtime environment provided by the operating system, multiple implementations for a protocol can arise as a runtime warning. When this happens, Swift picks one implementation arbitrarily. In Carbon, we could make this a build time error. However, there would be nothing preventing two independent libraries from providing conflicting implementations. Furthermore, the error would only be diagnosed at link time.","title":"Rejected alternative: no orphan rule"},{"location":"design/generics/appendix-coherence/#rejected-alternative-incoherence","text":"","title":"Rejected alternative: incoherence"},{"location":"design/generics/appendix-coherence/#incoherence-means-context-sensitivity","text":"The undesirable result of incoherence is that the interpretation of source code changes based on imports. In particular, imagine there is a function call that depends on a type implementing an interface, and two different implementations are defined in two different libraries. A call to that function will be treated differently depending on which of those two libraries are imported: If neither is imported, it is an error. If both are imported, it is ambiguous. If only one is imported, you get totally different code executed depending on which it is. Furthermore, this means that the behavior of a file can depend on an import even if nothing from that package is referenced explicitly. In general, Carbon is avoiding this sort of context sensitivity . This context sensitivity would make moving code between files when refactoring more difficult and less safe.","title":"Incoherence means context sensitivity"},{"location":"design/generics/appendix-coherence/#rejected-variation-dynamic-implementation-binding","text":"One possible approach would be to bind interface implementations to a value at the point it was created. In the example above , the implementation of the Hashable interface for Song would be fixed for the song_set HashSet object based on which implementation was in scope in the body of the SomethingWeirdHappens function. This idea is discussed briefly in section 5.4 on separate compilation of WG21 proposal n1848 for implementing \"Indiana\" C++0x concepts ( 1 , and 2 ). This has some downsides: It is harder to reason about. The behavior of SongUtil.IsInHashSet depends on the dynamic behavior of the program. At the time of the call, we may have no idea where the HashSet argument was created. An object may be created far from a call that has a particular interface requirement, with no guarantee that the object was created with any implementation of the interface at all. This error would only be detected at runtime, not at type checking time. It requires more data space at runtime because we need to store a pointer to the witness table representing the implementation with the object, since it varies instead of being known statically. It is slower to execute from dynamic dispatch and the inability to inline. In some cases it may not be feasible to use dynamic dispatch. For example, if an interface method returns an associated type, we might not know the calling convention of the function without knowing some details about the type. As a result, this doesn't make sense as the default behavior for Carbon based on its goals . That being said, this could be a feature added later as opt-in behavior to either allow users to reduce code size or support use cases that require dynamic dispatch.","title":"Rejected variation: dynamic implementation binding"},{"location":"design/generics/appendix-coherence/#rejected-variation-manual-conflict-resolution","text":"Carbon could alternatively provide some kind of manual disambiguation syntax to resolve problems where they arise. The problems with this approach have been considered in the context of Rust . A specific example of this approach is called scoped conformance , where the conflict resolution is based on limiting the visibility of implementations to particular scopes. This hasn't been implemented, but it has the drawbacks described above. Depending on the details of the implementation, either: there are incompatible values with types that have the same name, or it is difficult to reason about the program's behavior because it behaves like dynamic implementation binding (though perhaps with a monomorphization cost instead of a runtime cost).","title":"Rejected variation: manual conflict resolution"},{"location":"design/generics/details/","text":"Generics: Details Table of contents Overview Interfaces Implementing interfaces Implementing multiple interfaces External impl Qualified member names and compound member access Access Generics Return type Implementation model Interfaces recap Type-of-types Named constraints Subtyping between type-of-types Combining interfaces by anding type-of-types Interface requiring other interfaces Interface extension extends and impl with named constraints Diamond dependency issue Use case: overload resolution Adapting types Adapter compatibility Extending adapter Use case: Using independent libraries together Use case: Defining an impl for use by other types Use case: Private impl Use case: Accessing external names Adapter with stricter invariants Associated constants Associated class functions Associated types Implementation model Parameterized interfaces Impl lookup Parameterized named constraints Where constraints Constraint use cases Set an associated constant to a specific value Same type constraints Set an associated type to a specific value Equal generic types Satisfying both type-of-types Type bound for associated type Type bounds on associated types in declarations Type bounds on associated types in interfaces Combining constraints Recursive constraints Parameterized type implements interface Another type implements parameterized interface Implied constraints Must be legal type argument constraints Referencing names in the interface being defined Manual type equality observe declarations Other constraints as type-of-types Is a derived class Type compatible with another type Same implementation restriction Example: Multiple implementations of the same interface Example: Creating an impl out of other impls Sized types and type-of-types Implementation model TypeId Destructor constraints Generic let Parameterized impls Impl for a parameterized type Conditional conformance Conditional methods Blanket impls Difference between blanket impls and named constraints Wildcard impls Combinations Lookup resolution and specialization Type structure of an impl declaration Orphan rule Overlap rule Prioritization rule Acyclic rule Termination rule final impls Libraries that can contain final impls Comparison to Rust Forward declarations and cyclic references Declaring interfaces and named constraints Declaring implementations Matching and agreeing Declaration examples Example of declaring interfaces with cyclic references Interfaces with parameters constrained by the same interface Interface members with definitions Interface defaults final members Interface requiring other interfaces revisited Requirements with where constraints Observing a type implements an interface Observing interface requirements Observing blanket impls Operator overloading Binary operators like operator for implicit conversions Parameterized types Specialization Future work Dynamic types Runtime type parameters Runtime type fields Abstract return types Evolution Testing Impls with state Generic associated types and higher-ranked types Generic associated types Higher-ranked types Field requirements Bridge for C++ customization points Variadic arguments Range constraints on generic integers References Overview This document goes into the details of the design of generic type parameters. Imagine we want to write a function parameterized by a type argument. Maybe our function is PrintToStdout and let's say we want to operate on values that have a type for which we have an implementation of the ConvertibleToString interface. The ConvertibleToString interface has a ToString method returning a string. To do this, we give the PrintToStdout function two parameters: one is the value to print, let's call that val , the other is the type of that value, let's call that T . The type of val is T , what is the type of T ? Well, since we want to let T be any type implementing the ConvertibleToString interface, we express that in the \"interfaces are type-of-types\" model by saying the type of T is ConvertibleToString . Since we can figure out T from the type of val , we don't need the caller to pass in T explicitly, so it can be a deduced parameter (also see deduced parameters in the Generics overview doc). Basically, the user passes in a value for val , and the type of val determines T . T still gets passed into the function though, and it plays an important role -- it defines the implementation of the interface. We can think of the interface as defining a struct type whose members are function pointers, and an implementation of an interface as a value of that struct with actual function pointer values. So an implementation is a table of function pointers (one per function defined in the interface) that gets passed into a function as the type argument. For more on this, see the implementation model section below. In addition to function pointer members, interfaces can include any constants that belong to a type. For example, the type's size (represented by an integer constant member of the type) could be a member of an interface and its implementation. There are a few cases why we would include another interface implementation as a member: associated types type parameters interface requirements The function expresses that the type argument is passed in statically , basically generating a separate function body for every different type passed in, by using the \"generic argument\" syntax :! , see the generics section below. The interface contains enough information to type and definition check the function body -- you can only call functions defined in the interface in the function body. Contrast this with making the type a template argument, where you could just use Type instead of an interface and it will work as long as the function is only called with types that allow the definition of the function to compile. The interface bound has other benefits: allows the compiler to deliver clearer error messages, documents expectations, and expresses that a type has certain semantics beyond what is captured in its member function names and signatures. The last piece of the puzzle is calling the function. For a value of type Song to be printed using the PrintToStdout function, Song needs to implement the ConvertibleToString interface. Interface implementations will usually be defined either with the type or with the interface. They may also be defined somewhere else as long as Carbon can be guaranteed to see the definition when needed. For more on this, see the implementing interfaces section below. Unless the implementation of ConvertibleToString for Song is defined as external , every member of ConvertibleToString is also a member of Song . This includes members of ConvertibleToString that are not explicitly named in the impl definition but have defaults. Whether the implementation is defined as internal or external , you may access the ToString function for a Song value s by a writing function call using a qualified member access expression , like s.(ConvertibleToString.ToString)() . If Song doesn't implement an interface or we would like to use a different implementation of that interface, we can define another type that also has the same data representation as Song that has whatever different interface implementations we want. However, Carbon won't implicitly convert to that other type, the user will have to explicitly cast to that type in order to select those alternate implementations. For more on this, see the adapting type section below. Interfaces An interface , defines an API that a given type can implement. For example, an interface capturing a linear-algebra vector API might have two methods: interface Vector { // Here `Self` means \"the type implementing this interface\". fn Add[me: Self](b: Self) -> Self; fn Scale[me: Self](v: f64) -> Self; } The syntax here is to match how the same members would be defined in a type . Each declaration in the interface defines an associated entity . In this example, Vector has two associated methods, Add and Scale . An interface defines a type-of-type, that is a type whose values are types. The values of an interface are any types implementing the interface, and so provide definitions for all the functions (and other members) declared in the interface. Implementing interfaces Carbon interfaces are \"nominal\" , which means that types explicitly describe how they implement interfaces. An \"impl\" defines how one interface is implemented for a type. Every associated entity is given a definition. Different types satisfying Vector can have different definitions for Add and Scale , so we say their definitions are associated with what type is implementing Vector . The impl defines what is associated with the type for that interface. Impls may be defined inline inside the type definition: class Point { var x: f64; var y: f64; impl as Vector { // In this scope, \"Self\" is an alias for \"Point\". fn Add[me: Self](b: Self) -> Self { return {.x = a.x + b.x, .y = a.y + b.y}; } fn Scale[me: Self](v: f64) -> Self { return {.x = a.x * v, .y = a.y * v}; } } } Interfaces that are implemented inline contribute to the type's API: var p1: Point = {.x = 1.0, .y = 2.0}; var p2: Point = {.x = 2.0, .y = 4.0}; Assert(p1.Scale(2.0) == p2); Assert(p1.Add(p1) == p2); Note: A type may implement any number of different interfaces, but may provide at most one implementation of any single interface. This makes the act of selecting an implementation of an interface for a type unambiguous throughout the whole program. Comparison with other languages: Rust defines implementations lexically outside of the class definition. This Carbon approach means that a type's API is described by declarations inside the class definition and doesn't change afterwards. References: This interface implementation syntax was accepted in proposal #553 . In particular, see the alternatives considered . Implementing multiple interfaces To implement more than one interface when defining a type, simply include an impl block per interface. class Point { var x: f64; var y: f64; impl as Vector { fn Add[me: Self](b: Self) -> Self { ... } fn Scale[me: Self](v: f64) -> Self { ... } } impl as Drawable { fn Draw[me: Self]() { ... } } } In this case, all the functions Add , Scale , and Draw end up a part of the API for Point . This means you can't implement two interfaces that have a name in common (unless you use an external impl for one or both, as described below). class GameBoard { impl as Drawable { fn Draw[me: Self]() { ... } } impl as EndOfGame { // \u274c Error: `GameBoard` has two methods named // `Draw` with the same signature. fn Draw[me: Self]() { ... } fn Winner[me: Self](player: i32) { ... } } } Open question: Should we have some syntax for the case where you want both names to be given the same implementation? It seems like that might be a common case, but we won't really know if this is an important case until we get more experience. class Player { var name: String; impl as Icon { fn Name[me: Self]() -> String { return me.name; } // ... } impl as GameUnit { // Possible syntax options for defining // `GameUnit.Name` as the same as `Icon.Name`: alias Name = Icon.Name; fn Name[me: Self]() -> String = Icon.Name; // ... } } External impl Interfaces may also be implemented for a type externally , by using the external impl construct. An external impl does not add the interface's methods to the type. class Point2 { var x: f64; var y: f64; external impl as Vector { // In this scope, `Self` is an alias for `Point2`. fn Add[me: Self](b: Self) -> Self { return {.x = a.x + b.x, .y = a.y + b.y}; } fn Scale[me: Self](v: f64) -> Self { return {.x = a.x * v, .y = a.y * v}; } } } var a: Point2 = {.x = 1.0, .y = 2.0}; // `a` does *not* have `Add` and `Scale` methods: // \u274c Error: a.Add(a.Scale(2.0)); An external impl may be defined out-of-line, by including the name of the existing type before as , which is otherwise optional: class Point3 { var x: f64; var y: f64; } external impl Point3 as Vector { // In this scope, `Self` is an alias for `Point3`. fn Add[me: Self](b: Self) -> Self { return {.x = a.x + b.x, .y = a.y + b.y}; } fn Scale[me: Self](v: f64) -> Self { return {.x = a.x * v, .y = a.y * v}; } } var a: Point3 = {.x = 1.0, .y = 2.0}; // `a` does *not* have `Add` and `Scale` methods: // \u274c Error: a.Add(a.Scale(2.0)); References: The external interface implementation syntax was decided in proposal #553 . In particular, see the alternatives considered . The external impl statement is allowed to be defined in a different library from Point3 , restricted by the coherence/orphan rules that ensure that the implementation of an interface can't change based on imports. In particular, the external impl statement is allowed in the library defining the interface ( Vector in this case) in addition to the library that defines the type ( Point3 here). This (at least partially) addresses the expression problem . Carbon requires impl s defined in a different library to be external so that the API of Point3 doesn't change based on what is imported. It would be particularly bad if two different libraries implemented interfaces with conflicting names that both affected the API of a single type. As a consequence of this restriction, you can find all the names of direct members (those available by simple member access ) of a type in the definition of that type. The only thing that may be in another library is an impl of an interface. You might also use external impl to implement an interface for a type to avoid cluttering the API of that type, for example to avoid a name collision. A syntax for reusing method implementations allows us to do this selectively when needed. In this case, the external impl may be declared lexically inside the class scope. class Point4a { var x: f64; var y: f64; fn Add[me: Self](b: Self) -> Self { return {.x = a.x + b.x, .y = a.y + b.y}; } external impl as Vector { alias Add = Point4a.Add; // Syntax TBD fn Scale[me: Self](v: f64) -> Self { return {.x = a.x * v, .y = a.y * v}; } } } // OR: class Point4b { var x: f64; var y: f64; external impl as Vector { fn Add[me: Self](b: Self) -> Self { return {.x = a.x + b.x, .y = a.y + b.y}; } fn Scale[me: Self](v: f64) -> Self { return {.x = a.x * v, .y = a.y * v}; } } alias Add = Vector.Add; // Syntax TBD } // OR: class Point4c { var x: f64; var y: f64; fn Add[me: Self](b: Self) -> Self { return {.x = a.x + b.x, .y = a.y + b.y}; } } external impl Point4c as Vector { alias Add = Point4c.Add; // Syntax TBD fn Scale[me: Self](v: f64) -> Self { return {.x = a.x * v, .y = a.y * v}; } } Being defined lexically inside the class means that implementation is available to other members defined in the class. For example, it would allow implementing another interface or method that requires this interface to be implemented. Open question: Do implementations need to be defined lexically inside the class to get access to private members, or is it sufficient to be defined in the same library as the class? Rejected alternative: We could allow types to have different APIs in different files based on explicit configuration in that file. For example, we could support a declaration that a given interface or a given method of an interface is \"in scope\" for a particular type in this file. With that declaration, the method could be called using simple member access . This avoids most concerns arising from name collisions between interfaces. It has a few downsides though: It increases variability between files, since the same type will have different APIs depending on these declarations. This makes it harder to copy-paste code between files. It makes reading code harder, since you have to search the file for these declarations that affect name lookup. Comparison with other languages: Both Rust and Swift support external implementation. Swift's syntax does this as an \"extension\" of the original type. In Rust, all implementations are external as in this example . Unlike Swift and Rust, we don't allow a type's API to be modified outside its definition. So in Carbon a type's API is consistent no matter what is imported, unlike Swift and Rust. Qualified member names and compound member access Given a value of type Point3 and an interface Vector implemented for that type, you can access the methods from that interface using a qualified member access expression whether or not the implementation is done externally with an external impl declaration. The qualified member access expression writes the member's qualified name in the parentheses of the compound member access syntax : var p1: Point3 = {.x = 1.0, .y = 2.0}; var p2: Point3 = {.x = 2.0, .y = 4.0}; Assert(p1.(Vector.Scale)(2.0) == p2); Assert(p1.(Vector.Add)(p1) == p2); Note that the name in the parens is looked up in the containing scope, not in the names of members of Point3 . So if there was another interface Drawable with method Draw defined in the Plot package also implemented for Point3 , as in: package Plot; import Points; interface Drawable { fn Draw[me: Self](); } external impl Points.Point3 as Drawable { ... } You could access Draw with a qualified name: import Plot; import Points; var p: Points.Point3 = {.x = 1.0, .y = 2.0}; p.(Plot.Drawable.Draw)(); Comparison with other languages: This is intended to be analogous to, in C++, adding ClassName:: in front of a member name to disambiguate, such as names defined in both a parent and child class . Access An impl must be visible to all code that can see both the type and the interface being implemented: If either the type or interface is private to a single file, then since the only way to define the impl is to use that private name, the impl must be defined private to that file as well. Otherwise, if the type or interface is private but declared in an API file, then the impl must be declared in the same file so the existence of that impl is visible to all files in that library. Otherwise, the impl must be defined in the public API file of the library, so it is visible in all places that might use it. No access control modifiers are allowed on impl declarations, an impl is always visible to the intersection of the visibility of all names used in the declaration of the impl . Generics Here is a function that can accept values of any type that has implemented the Vector interface: fn AddAndScaleGeneric[T:! Vector](a: T, b: T, s: f64) -> T { return a.Add(b).Scale(s); } var v: Point = AddAndScaleGeneric(a, w, 2.5); Here T is a type whose type is Vector . The :! syntax means that T is a generic parameter . That means it must be known to the caller, but we will only use the information present in the signature of the function to type check the body of AddAndScaleGeneric 's definition. In this case, we know that any value of type T implements the Vector interface and so has an Add and a Scale method. References: The :! syntax was accepted in proposal #676 . Names are looked up in the body of AddAndScaleGeneric for values of type T in Vector . This means that AddAndScaleGeneric is interpreted as equivalent to adding a Vector qualification to replace all simple member accesses of T : fn AddAndScaleGeneric[T:! Vector](a: T, b: T, s: Double) -> T { return a.(Vector.Add)(b).(Vector.Scale)(s); } With these qualifications, the function can be type-checked for any T implementing Vector . This type checking is equivalent to type checking the function with T set to an archetype of Vector . An archetype is a placeholder type considered to satisfy its constraint, which is Vector in this case, and no more. It acts as the most general type satisfying the interface. The effect of this is that an archetype of Vector acts like a supertype of any T implementing Vector . For name lookup purposes, an archetype is considered to have implemented its constraint internally . The only oddity is that the archetype may have different names for members than specific types T that implement interfaces from the constraint externally . This difference in names can also occur for supertypes in C++, for example members in a derived class can hide members in the base class with the same name, though it is not that common for it to come up in practice. The behavior of calling AddAndScaleGeneric with a value of a specific type like Point is to set T to Point after all the names have been qualified. // AddAndScaleGeneric with T = Point fn AddAndScaleForPoint(a: Point, b: Point, s: Double) -> Point { return a.(Vector.Add)(b).(Vector.Scale)(s); } This qualification gives a consistent interpretation to the body of the function even when the type supplied by the caller implements the interface externally , as Point2 does: // AddAndScaleGeneric with T = Point2 fn AddAndScaleForPoint2(a: Point2, b: Point2, s: Double) -> Point2 { // \u2705 This works even though `a.Add(b).Scale(s)` wouldn't. return a.(Vector.Add)(b).(Vector.Scale)(s); } Return type From the caller's perspective, the return type is the result of substituting the caller's values for the generic parameters into the return type expression. So AddAndScaleGeneric called with Point values returns a Point and called with Point2 values returns a Point2 . So looking up a member on the resulting value will look in Point or Point2 rather than Vector . This is part of realizing the goal that generic functions can be used in place of regular functions without changing the return type that callers see . In this example, AddAndScaleGeneric can be substituted for AddAndScaleForPoint and AddAndScaleForPoint2 without affecting the return types. This requires the return value to be converted to the type that the caller expects instead of the erased type used inside the generic function. A generic caller of a generic function performs the same substitution process to determine the return type, but the result may be generic. In this example of calling a generic from another generic, fn DoubleThreeTimes[U:! Vector](a: U) -> U { return AddAndScaleGeneric(a, a, 2.0).Scale(2.0); } the return type of AddAndScaleGeneric is found by substituting in the U from DoubleThreeTimes for the T from AddAndScaleGeneric in the return type expression of AddAndScaleGeneric . U is an archetype of Vector , and so implements Vector internally and therefore has a Scale method. If U had a more specific type, the return value would have the additional capabilities of U . For example, given a parameterized type GeneralPoint implementing Vector , and a function that takes a GeneralPoint and calls AddAndScaleGeneric with it: class GeneralPoint(C:! Numeric) { external impl as Vector { ... } fn Get[me: Self](i: i32) -> C; } fn CallWithGeneralPoint[C:! Numeric](p: GeneralPoint(C)) -> C { // `AddAndScaleGeneric` returns `T` and in these calls `T` is // deduced to be `GeneralPoint(C)`. // \u274c Illegal: AddAndScaleGeneric(p, p, 2.0).Scale(2.0); // `GeneralPoint(C)` implements `Vector` externally, and so // does not have a `Scale` method. // \u2705 Allowed: `GeneralPoint(C)` has a `Get` method AddAndScaleGeneric(p, p, 2.0).Get(0); // \u2705 Allowed: `GeneralPoint(C)` implements `Vector` // externally, and so has a `Vector.Scale` method. // `Vector.Scale` returns `Self` which is `GeneralPoint(C)` // again, and so has a `Get` method. return AddAndScaleGeneric(p, p, 2.0).(Vector.Scale)(2.0).Get(0); } The result of the call to AddAndScaleGeneric from CallWithGeneralPoint has type GeneralPoint(C) and so has a Get method and a Vector.Scale method. But, in contrast to how DoubleThreeTimes works, since Vector is implemented externally the return value in this case does not directly have a Scale method. Implementation model A possible model for generating code for a generic function is to use a witness table to represent how a type implements an interface: Interfaces are types of witness tables. Impls are witness table values. The compiler rewrites functions with an implicit type argument ( fn Foo[InterfaceName:! T](...) ) to have an actual argument with type determined by the interface, and supplied at the callsite using a value determined by the impl. For the example above, the Vector interface could be thought of defining a witness table type like: class Vector { // `Self` is the representation type, which is only // known at compile time. var Self:! Type; // `fnty` is **placeholder** syntax for a \"function type\", // so `Add` is a function that takes two `Self` parameters // and returns a value of type `Self`. var Add: fnty(a: Self, b: Self) -> Self; var Scale: fnty(a: Self, v: f64) -> Self; } The impl of Vector for Point would be a value of this type: var VectorForPoint: Vector = { .Self = Point, // `lambda` is **placeholder** syntax for defining a // function value. .Add = lambda(a: Point, b: Point) -> Point { return {.x = a.x + b.x, .y = a.y + b.y}; }, .Scale = lambda(a: Point, v: f64) -> Point { return {.x = a.x * v, .y = a.y * v}; }, }; Finally we can define a generic function and call it, like AddAndScaleGeneric from the \"Generics\" section by making the witness table an explicit argument to the function: fn AddAndScaleGeneric (t:! Vector, a: t.Self, b: t.Self, s: f64) -> t.Self { return t.Scale(t.Add(a, b), s); } // Point implements Vector. var v: Point = AddAndScaleGeneric(VectorForPoint, a, w, 2.5); The rule is that generic arguments (declared using :! ) are passed at compile time, so the actual value of the t argument here can be used to generate the code for AddAndScaleGeneric . So AddAndScaleGeneric is using a static-dispatch witness table . Note that this implementation strategy only works for impls that the caller knows the callee needs. Interfaces recap Interfaces have a name and a definition. The definition of an interface consists of a set of declarations. Each declaration defines a requirement for any impl that is in turn a capability that consumers of that impl can rely on. Typically those declarations also have names, useful for both saying how the impl satisfies the requirement and accessing the capability. Interfaces are \"nominal\" , which means their name is significant. So two interfaces with the same body definition but different names are different, just like two classes with the same definition but different names are considered different types. For example, lets say we define another interface, say LegoFish , with the same Add and Scale method signatures. Implementing Vector would not imply an implementation of LegoFish , because the impl definition explicitly refers to the name Vector . An interface's name may be used in a few different contexts: to define an impl for a type , as a namespace name in a qualified name , and as a type-of-type for a generic type parameter . While interfaces are examples of type-of-types, type-of-types are a more general concept, for which interfaces are a building block. Type-of-types A type-of-type consists of a set of requirements and a set of names. Requirements are typically a set of interfaces that a type must satisfy, though other kinds of requirements are added below. The names are aliases for qualified names in those interfaces. An interface is one particularly simple example of a type-of-type. For example, Vector as a type-of-type has a set of requirements consisting of the single interface Vector . Its set of names consists of Add and Scale which are aliases for the corresponding qualified names inside Vector as a namespace. The requirements determine which types are values of a given type-of-type. The set of names in a type-of-type determines the API of a generic type value and define the result of member access into the type-of-type. This general structure of type-of-types holds not just for interfaces, but others described in the rest of this document. Named constraints If the interfaces discussed above are the building blocks for type-of-types, generic named constraints describe how they may be composed together. Unlike interfaces which are nominal, the name of a named constraint is not a part of its value. Two different named constraints with the same definition are equivalent even if they have different names. This is because types don't explicitly specify which named constraints they implement, types automatically implement any named constraints they can satisfy. A named constraint definition can contain interface requirements using impl declarations and names using alias declarations. Note that this allows us to declare the aspects of a type-of-type directly. constraint VectorLegoFish { // Interface implementation requirements impl as Vector; impl as LegoFish; // Names alias Scale = Vector.Scale; alias VAdd = Vector.Add; alias LFAdd = LegoFish.Add; } We don't expect developers to directly define many named constraints, but other constructs we do expect them to use will be defined in terms of them. For example, we can define the Carbon builtin Type as: constraint Type { } That is, Type is the type-of-type with no requirements (so matches every type), and defines no names. fn Identity[T:! Type](x: T) -> T { // Can accept values of any type. But, since we know nothing about the // type, we don't know about any operations on `x` inside this function. return x; } var i: i32 = Identity(3); var s: String = Identity(\"string\"); Aside: We can define auto as syntactic sugar for (template _:! Type) . This definition allows you to use auto as the type for a local variable whose type can be statically determined by the compiler. It also allows you to use auto as the type of a function parameter, to mean \"accepts a value of any type, and this function will be instantiated separately for every different type.\" This is consistent with the use of auto in the C++20 Abbreviated function template feature . In general, the declarations in constraint definition match a subset of the declarations in an interface . Named constraints used with generics, as opposed to templates, should only include required interfaces and aliases to named members of those interfaces. To declare a named constraint that includes other declarations for use with template parameters, use the template keyword before constraint . Method, associated type, and associated function requirements may only be declared inside a template constraint . Note that a generic constraint ignores the names of members defined for a type, but a template constraint can depend on them. There is an analogy between declarations used in a constraint and in an interface definition. If an interface I has (non- alias ) declarations X , Y , and Z , like so: interface I { X; Y; Z; } Then a type implementing I would have impl as I with definitions for X , Y , and Z , as in: class ImplementsI { // ... impl as I { X { ... } Y { ... } Z { ... } } } But the corresponding constraint or template constraint , S : // or template constraint S { constraint S { X; Y; Z; } would match any type with definitions for X , Y , and Z directly: class ImplementsS { // ... X { ... } Y { ... } Z { ... } } TODO: Move the template constraint and auto content to the template design document, once it exists. Subtyping between type-of-types There is a subtyping relationship between type-of-types that allows calls of one generic function from another as long as it has a subset of the requirements. Given a generic type variable T with type-of-type I1 , it satisfies a type-of-type I2 as long as the requirements of I1 are a superset of the requirements of I2 . This means a value x of type T may be passed to functions requiring types to satisfy I2 , as in this example: interface Printable { fn Print[me: Self](); } interface Renderable { fn Draw[me: Self](); } constraint PrintAndRender { impl as Printable; impl as Renderable; } constraint JustPrint { impl as Printable; } fn PrintIt[T2:! JustPrint](x2: T2) { x2.(Printable.Print)(); } fn PrintDrawPrint[T1:! PrintAndRender](x1: T1) { // x1 implements `Printable` and `Renderable`. x1.(Printable.Print)(); x1.(Renderable.Draw)(); // Can call `PrintIt` since `T1` satisfies `JustPrint` since // it implements `Printable` (in addition to `Renderable`). PrintIt(x1); } Combining interfaces by anding type-of-types In order to support functions that require more than one interface to be implemented, we provide a combination operator on type-of-types, written & . This operator gives the type-of-type with the union of all the requirements and the union of the names minus any conflicts. interface Printable { fn Print[me: Self](); } interface Renderable { fn Center[me: Self]() -> (i32, i32); fn Draw[me: Self](); } // `Printable & Renderable` is syntactic sugar for this type-of-type: constraint { impl as Printable; impl as Renderable; alias Print = Printable.Print; alias Center = Renderable.Center; alias Draw = Renderable.Draw; } fn PrintThenDraw[T:! Printable & Renderable](x: T) { // Can use methods of `Printable` or `Renderable` on `x` here. x.Print(); // Same as `x.(Printable.Print)();`. x.Draw(); // Same as `x.(Renderable.Draw)();`. } class Sprite { // ... impl as Printable { fn Print[me: Self]() { ... } } impl as Renderable { fn Center[me: Self]() -> (i32, i32) { ... } fn Draw[me: Self]() { ... } } } var s: Sprite = ...; PrintThenDraw(s); Any conflicting names between the two types are replaced with a name that is an error to use. interface Renderable { fn Center[me: Self]() -> (i32, i32); fn Draw[me: Self](); } interface EndOfGame { fn Draw[me: Self](); fn Winner[me: Self](player: i32); } // `Renderable & EndOfGame` is syntactic sugar for this type-of-type: constraint { impl as Renderable; impl as EndOfGame; alias Center = Renderable.Center; // Open question: `forbidden`, `invalid`, or something else? forbidden Draw message \"Ambiguous, use either `(Renderable.Draw)` or `(EndOfGame.Draw)`.\"; alias Winner = EndOfGame.Winner; } Conflicts can be resolved at the call site using a qualified member access expression , or by defining a named constraint explicitly and renaming the methods: constraint RenderableAndEndOfGame { impl as Renderable; impl as EndOfGame; alias Center = Renderable.Center; alias RenderableDraw = Renderable.Draw; alias TieGame = EndOfGame.Draw; alias Winner = EndOfGame.Winner; } fn RenderTieGame[T:! RenderableAndEndOfGame](x: T) { // Calls Renderable.Draw() x.RenderableDraw(); // Calls EndOfGame.Draw() x.TieGame(); } Reserving the name when there is a conflict is part of resolving what happens when you combine more than two type-of-types. If x is forbidden in A , it is forbidden in A & B , whether or not B defines the name x . This makes & associative and commutative, and so it is well defined on sets of interfaces, or other type-of-types, independent of order. Note that we do not consider two type-of-types using the same name to mean the same thing to be a conflict. For example, combining a type-of-type with itself gives itself, MyTypeOfType & MyTypeOfType == MyTypeOfType . Also, given two interface extensions of a common base interface, the sum should not conflict on any names in the common base. Rejected alternative: Instead of using & as the combining operator, we considered using + , like Rust . See #531 for the discussion. Future work: We may want to define another operator on type-of-types for adding requirements to a type-of-type without affecting the names, and so avoid the possibility of name conflicts. Note this means the operation is not commutative. If we call this operator [&] , then A [&] B has the names of A and B [&] A has the names of B . // `Printable [&] Renderable` is syntactic sugar for this type-of-type: constraint { impl as Printable; impl as Renderable; alias Print = Printable.Print; } // `Renderable [&] EndOfGame` is syntactic sugar for this type-of-type: constraint { impl as Renderable; impl as EndOfGame; alias Center = Renderable.Center; alias Draw = Renderable.Draw; } Note that all three expressions A & B , A [&] B , and B [&] A have the same requirements, and so you would be able to switch a function declaration between them without affecting callers. Nothing in this design depends on the [&] operator, and having both & and [&] might be confusing for users, so it makes sense to postpone implementing [&] until we have a demonstrated need. The [&] operator seems most useful for adding requirements for interfaces used for operator overloading , where merely implementing the interface is enough to be able to use the operator to access the functionality. Alternatives considered: See Carbon: Access to interface methods . Comparison with other languages: This & operation on interfaces works very similarly to Rust's + operation, with the main difference being how you qualify names when there is a conflict . Interface requiring other interfaces Some interfaces will depend on other interfaces being implemented for the same type. For example, in C++, the Container concept requires all containers to also satisfy the requirements of DefaultConstructible , CopyConstructible , EqualityComparable , and Swappable . This is already a capability for type-of-types in general . For consistency we will use the same semantics and syntax as we do for named constraints : interface Equatable { fn Equals[me: Self](rhs: Self) -> bool; } interface Iterable { fn Advance[addr me: Self*]() -> bool; impl as Equatable; } def DoAdvanceAndEquals[T:! Iterable](x: T) { // `x` has type `T` that implements `Iterable`, and so has `Advance`. x.Advance(); // `Iterable` requires an implementation of `Equatable`, // so `T` also implements `Equatable`. x.(Equatable.Equals)(x); } class Iota { impl as Iterable { fn Advance[me: Self]() { ... } } impl as Equatable { fn Equals[me: Self](rhs: Self) -> bool { ... } } } var x: Iota; DoAdvanceAndEquals(x); Like with named constraints, an interface implementation requirement doesn't by itself add any names to the interface, but again those can be added with alias declarations: interface Hashable { fn Hash[me: Self]() -> u64; impl as Equatable; alias Equals = Equatable.Equals; } def DoHashAndEquals[T:! Hashable](x: T) { // Now both `Hash` and `Equals` are available directly: x.Hash(); x.Equals(x); } Comparison with other languages: This feature is called \"Supertraits\" in Rust . Note: The design for this feature is continued in a later section . Interface extension When implementing an interface, we should allow implementing the aliased names as well. In the case of Hashable above, this includes all the members of Equatable , obviating the need to implement Equatable itself: class Song { impl as Hashable { fn Hash[me: Self]() -> u64 { ... } fn Equals[me: Self](rhs: Self) -> bool { ... } } } var y: Song; DoHashAndEquals(y); This allows us to say that Hashable \"extends\" Equatable , with some benefits: This allows Equatable to be an implementation detail of Hashable . This allows types implementing Hashable to implement all of its API in one place. This reduces the boilerplate for types implementing Hashable . We expect this concept to be common enough to warrant dedicated syntax: interface Equatable { fn Equals[me: Self](rhs: Self) -> bool; } interface Hashable { extends Equatable; fn Hash[me: Self]() -> u64; } // is equivalent to the definition of Hashable from before: // interface Hashable { // impl as Equatable; // alias Equals = Equatable.Equals; // fn Hash[me: Self]() -> u64; // } No names in Hashable are allowed to conflict with names in Equatable (unless those names are marked as upcoming or deprecated as in evolution future work ). Hopefully this won't be a problem in practice, since interface extension is a very closely coupled relationship, but this may be something we will have to revisit in the future. Examples: The C++ Boost.Graph library graph concepts has many refining relationships between concepts. Carbon generics use case: graph library shows how those concepts might be translated into Carbon interfaces. The C++ concepts for containers, iterators, and concurrency include many requirement relationships. Swift protocols, such as Collection . To write an interface extending multiple interfaces, use multiple extends declarations. For example, the BinaryInteger protocol in Swift inherits from CustomStringConvertible , Hashable , Numeric , and Stridable . The SetAlgebra protocol extends Equatable and ExpressibleByArrayLiteral , which would be declared in Carbon: interface SetAlgebra { extends Equatable; extends ExpressibleByArrayLiteral; } Alternative considered: The extends declarations are in the body of the interface definition instead of the header so we can use associated types (defined below) also defined in the body in parameters or constraints of the interface being extended. // A type can implement `ConvertibleTo` many times, using // different values of `T`. interface ConvertibleTo(T:! Type) { ... } // A type can only implement `PreferredConversion` once. interface PreferredConversion { let AssociatedType:! Type; extends ConvertibleTo(AssociatedType); } extends and impl with named constraints The extends declaration makes sense with the same meaning inside a constraint definition, and so is also supported. interface Media { fn Play[me: Self](); } interface Job { fn Run[me: Self](); } constraint Combined { extends Media; extends Job; } This definition of Combined is equivalent to requiring both the Media and Job interfaces being implemented, and aliases their methods. // Equivalent constraint Combined { impl as Media; alias Play = Media.Play; impl as Job; alias Run = Job.Run; } Notice how Combined has aliases for all the methods in the interfaces it requires. That condition is sufficient to allow a type to impl the named constraint: class Song { impl as Combined { fn Play[me: Self]() { ... } fn Run[me: Self]() { ... } } } This is equivalent to implementing the required interfaces directly: class Song { impl as Media { fn Play[me: Self]() { ... } } impl as Job { fn Run[me: Self]() { ... } } } This is just like when you get an implementation of Equatable by implementing Hashable when Hashable extends Equatable . This provides a tool useful for evolution . Conversely, an interface can extend a constraint : interface MovieCodec { extends Combined; fn Load[addr me: Self*](filename: String); } This gives MovieCodec the same requirements and names as Combined , and so is equivalent to: interface MovieCodec { impl as Media; alias Play = Media.Play; impl as Job; alias Run = Job.Run; fn Load[addr me: Self*](filename: String); } Diamond dependency issue Consider this set of interfaces, simplified from this example generic graph library doc : interface Graph { fn Source[addr me: Self*](e: EdgeDescriptor) -> VertexDescriptor; fn Target[addr me: Self*](e: EdgeDescriptor) -> VertexDescriptor; } interface IncidenceGraph { extends Graph; fn OutEdges[addr me: Self*](u: VertexDescriptor) -> (EdgeIterator, EdgeIterator); } interface EdgeListGraph { extends Graph; fn Edges[addr me: Self*]() -> (EdgeIterator, EdgeIterator); } We need to specify what happens when a graph type implements both IncidenceGraph and EdgeListGraph , since both interfaces extend the Graph interface. class MyEdgeListIncidenceGraph { impl as IncidenceGraph { ... } impl as EdgeListGraph { ... } } The rule is that we need one definition of each method of Graph . Each method though could be defined in the impl block of IncidenceGraph , EdgeListGraph , or Graph . These would all be valid: IncidenceGraph implements all methods of Graph , EdgeListGraph implements none of them. class MyEdgeListIncidenceGraph { impl as IncidenceGraph { fn Source[me: Self](e: EdgeDescriptor) -> VertexDescriptor { ... } fn Target[me: Self](e: EdgeDescriptor) -> VertexDescriptor { ... } fn OutEdges[addr me: Self*](u: VertexDescriptor) -> (EdgeIterator, EdgeIterator) { ... } } impl as EdgeListGraph { fn Edges[addr me: Self*]() -> (EdgeIterator, EdgeIterator) { ... } } } IncidenceGraph and EdgeListGraph implement all methods of Graph between them, but with no overlap. class MyEdgeListIncidenceGraph { impl as IncidenceGraph { fn Source[me: Self](e: EdgeDescriptor) -> VertexDescriptor { ... } fn OutEdges[addr me: Self*](u: VertexDescriptor) -> (EdgeIterator, EdgeIterator) { ... } } impl as EdgeListGraph { fn Target[me: Self](e: EdgeDescriptor) -> VertexDescriptor { ... } fn Edges[addr me: Self*]() -> (EdgeIterator, EdgeIterator) { ... } } } Explicitly implementing Graph . class MyEdgeListIncidenceGraph { impl as Graph { fn Source[me: Self](e: EdgeDescriptor) -> VertexDescriptor { ... } fn Target[me: Self](e: EdgeDescriptor) -> VertexDescriptor { ... } } impl as IncidenceGraph { ... } impl as EdgeListGraph { ... } } Implementing Graph externally. class MyEdgeListIncidenceGraph { impl as IncidenceGraph { ... } impl as EdgeListGraph { ... } } external impl MyEdgeListIncidenceGraph as Graph { fn Source[me: Self](e: EdgeDescriptor) -> VertexDescriptor { ... } fn Target[me: Self](e: EdgeDescriptor) -> VertexDescriptor { ... } } This last point means that there are situations where we can only detect a missing method definition by the end of the file. This doesn't delay other aspects of semantic checking, which will just assume that these methods will eventually be provided. Open question: We could require that the external impl of the required interface be declared lexically in the class scope in this case. That would allow earlier detection of missing definitions. Use case: overload resolution Implementing an extended interface is an example of a more specific match for lookup resolution . For example, this could be used to provide different implementations of an algorithm depending on the capabilities of the iterator being passed in: interface ForwardIntIterator { fn Advance[addr me: Self*](); fn Get[me: Self]() -> i32; } interface BidirectionalIntIterator { extends ForwardIntIterator; fn Back[addr me: Self*](); } interface RandomAccessIntIterator { extends BidirectionalIntIterator; fn Skip[addr me: Self*](offset: i32); fn Difference[me: Self](rhs: Self) -> i32; } fn SearchInSortedList[IterT:! ForwardIntIterator] (begin: IterT, end: IterT, needle: i32) -> bool { ... // does linear search } // Will prefer the following overload when it matches // since it is more specific. fn SearchInSortedList[IterT:! RandomAccessIntIterator] (begin: IterT, end: IterT, needle: i32) -> bool { ... // does binary search } This would be an example of the more general rule that an interface A requiring an implementation of interface B means A is more specific than B . Adapting types Since interfaces may only be implemented for a type once, and we limit where implementations may be added to a type, there is a need to allow the user to switch the type of a value to access different interface implementations. Carbon therefore provides a way to create new types compatible with existing types with different APIs, in particular with different interface implementations, by adapting them: interface Printable { fn Print[me: Self](); } interface Comparable { fn Less[me: Self](rhs: Self) -> bool; } class Song { impl as Printable { fn Print[me: Self]() { ... } } } adapter SongByTitle for Song { impl as Comparable { fn Less[me: Self](rhs: Self) -> bool { ... } } } adapter FormattedSong for Song { impl as Printable { fn Print[me: Self]() { ... } } } adapter FormattedSongByTitle for Song { impl as Printable = FormattedSong; impl as Comparable = SongByTitle; } This allows developers to provide implementations of new interfaces (as in SongByTitle ), provide different implementations of the same interface (as in FormattedSong ), or mix and match implementations from other compatible types (as in FormattedSongByTitle ). The rules are: You can add any declaration that you could add to a class except for declarations that would change the representation of the type. This means you can add methods, functions, interface implementations, and aliases, but not fields, base classes, or virtual functions. The adapted type is compatible with the original type, and that relationship is an equivalence class, so all of Song , SongByTitle , FormattedSong , and FormattedSongByTitle end up compatible with each other. Since adapted types are compatible with the original type, you may explicitly cast between them, but there is no implicit conversion between these types. Inside an adapter, the Self type matches the adapter. Members of the original type may be accessed either by a cast: adapter SongByTitle for Song { impl as Comparable { fn Less[me: Self](rhs: Self) -> bool { return (me as Song).Title() < (rhs as Song).Title(); } } } or using a qualified member access expression: adapter SongByTitle for Song { impl as Comparable { fn Less[me: Self](rhs: Self) -> bool { return me.(Song.Title)() < rhs.(Song.Title)(); } } } Comparison with other languages: This matches the Rust idiom called \"newtype\", which is used to implement traits on types while avoiding coherence problems, see here and here . Rust's mechanism doesn't directly support reusing implementations, though some of that is provided by macros defined in libraries. Haskell has a newtype feature as well. Haskell's feature doesn't directly support reusing implementations either, but the most popular compiler provides it as an extension . Adapter compatibility Consider a type with a generic type parameter, like a hash map: interface Hashable { ... } class HashMap(KeyT:! Hashable, ValueT:! Type) { fn Find[me:Self](key: KeyT) -> Optional(ValueT); // ... } A user of this type will provide specific values for the key and value types: class Song { impl as Hashable { ... } // ... } var play_count: HashMap(Song, i32) = ...; var thriller_count: Optional(i32) = play_count.Find(Song(\"Thriller\")); Since the Find function is generic, it can only use the capabilities that HashMap requires of KeyT and ValueT . This allows us to evaluate when we can convert between two different arguments to a parameterized type. Consider two adapters of Song that implement Hashable : adapter PlayableSong for Song { impl as Hashable = Song; impl as Media { ... } } adapter SongHashedByTitle for Song { impl as Hashable { ... } } Song and PlayableSong have the same implementation of Hashable in addition to using the same data representation. This means that it is safe to convert between HashMap(Song, i32) and HashMap(PlayableSong, i32) , because the implementation of all the methods will use the same implementation of the Hashable interface. Carbon permits this conversion with an explicit cast. On the other hand, SongHashedByTitle has a different implementation of Hashable than Song . So even though Song and SongHashedByTitle are compatible types, HashMap(Song, i32) and HashMap(SongHashedByTitle, i32) are incompatible. This is important because we know that in practice the invariants of a HashMap implementation rely on the hashing function staying the same. Extending adapter Frequently we expect that the adapter type will want to preserve most or all of the API of the original type. The two most common cases expected are adding and replacing an interface implementation. Users would indicate that an adapter starts from the original type's existing API by using the extends keyword instead of for : class Song { impl as Hashable { ... } impl as Printable { ... } } adapter SongByArtist extends Song { // Add an implementation of a new interface impl as Comparable { ... } // Replace an existing implementation of an interface // with an alternative. impl as Hashable { ... } } The resulting type SongByArtist would: implement Comparable , unlike Song , implement Hashable , but differently than Song , and implement Printable , inherited from Song . Unlike the similar class B extends A notation, adapter B extends A is permitted even if A is a final class. Also, there is no implicit conversion from B to A , matching adapter ... for but unlike class extension. To avoid or resolve name conflicts between interfaces, an impl may be declared external . The names in that interface may then be pulled in individually or renamed using alias declarations. adapter SongRenderToPrintDriver extends Song { // Add a new `Print()` member function. fn Print[me: Self]() { ... } // Avoid name conflict with new `Print` function by making // the implementation of the `Printable` interface external. external impl as Printable = Song; // Make the `Print` function from `Printable` available // under the name `PrintToScreen`. alias PrintToScreen = Printable.Print; } Use case: Using independent libraries together Imagine we have two packages that are developed independently. Package CompareLib defines an interface CompareLib.Comparable and a generic algorithm CompareLib.Sort that operates on types that implement CompareLib.Comparable . Package SongLib defines a type SongLib.Song . Neither has a dependency on the other, so neither package defines an implementation for CompareLib.Comparable for type SongLib.Song . A user that wants to pass a value of type SongLib.Song to CompareLib.Sort has to define an adapter that provides an implementation of CompareLib.Comparable for SongLib.Song . This adapter will probably use the extends facility of adapters to preserve the SongLib.Song API. import CompareLib; import SongLib; adapter Song extends SongLib.Song { impl as CompareLib.Comparable { ... } } // Or, to keep the names from CompareLib.Comparable out of Song's API: adapter Song extends SongLib.Song { } external impl Song as CompareLib.Comparable { ... } // Or, equivalently: adapter Song extends SongLib.Song { external impl as CompareLib.Comparable { ... } } The caller can either convert SongLib.Song values to Song when calling CompareLib.Sort or just start with Song values in the first place. var lib_song: SongLib.Song = ...; CompareLib.Sort((lib_song as Song,)); var song: Song = ...; CompareLib.Sort((song,)); Use case: Defining an impl for use by other types Let's say we want to provide a possible implementation of an interface for use by types for which that implementation would be appropriate. We can do that by defining an adapter implementing the interface that is parameterized on the type it is adapting. That impl may then be pulled in using the impl as ... = ...; syntax. For example, given an interface Comparable for deciding which value is smaller: interface Comparable { fn Less[me: Self](rhs: Self) -> bool; } We might define an adapter that implements Comparable for types that define another interface Difference : interface Difference { fn Sub[me:Self](rhs: Self) -> i32; } adapter ComparableFromDifference(T:! Difference) for T { impl as Comparable { fn Less[me: Self](rhs: Self) -> bool { return (me as T).Sub(rhs) < 0; } } } class IntWrapper { var x: i32; impl as Difference { fn Sub[me: Self](rhs: Self) -> i32 { return left.x - right.x; } } impl as Comparable = ComparableFromDifferenceFn(IntWrapper); } TODO: If we support function types, we could potentially pass a function to use to the adapter instead: adapter ComparableFromDifferenceFn (T:! Type, Difference:! fnty(T, T)->i32) for T { impl as Comparable { fn Less[me: Self](rhs: Self) -> bool { return Difference(me, rhs) < 0; } } } class IntWrapper { var x: i32; fn Difference(left: Self, right: Self) { return left.x - right.x; } impl as Comparable = ComparableFromDifferenceFn(IntWrapper, Difference); } Use case: Private impl Adapter types can be used when a library publicly exposes a type, but only wants to say that type implements an interface as a private detail internal to the implementation of the type. In that case, instead of implementing the interface for the public type, the library can create a private adapter for that type and implement the interface on that instead. Any member of the class can cast its me parameter to the adapter type when it wants to make use of the private impl. // Public, in API file class Complex64 { // ... fn CloserToOrigin[me: Self](them: Self) -> bool; } // Private adapter ByReal extends Complex64 { // Complex numbers are not generally comparable, // but this comparison function is useful for some // method implementations. impl as Comparable { fn Less[me: Self](that: Self) -> bool { return me.Real() < that.Real(); } } } fn Complex64.CloserToOrigin[me: Self](them: Self) -> bool { var me_mag: ByReal = me * me.Conj() as ByReal; var them_mag: ByReal = them * them.Conj() as ByReal; return me_mag.Less(them_mag); } Use case: Accessing external names Consider a case where a function will call several functions from an interface that is implemented externally for a type. interface DrawingContext { fn SetPen[me: Self](...); fn SetFill[me: Self](...); fn DrawRectangle[me: Self](...); fn DrawLine[me: Self](...); ... } external impl Window as DrawingContext { ... } An adapter can make that much more convenient by making a compatible type where the interface is implemented internally . This avoids having to qualify each call to methods in the interface. adapter DrawInWindow for Window { impl as DrawingContext = Window; } fn Render(w: Window) { let d: DrawInWindow = w as DrawInWindow; d.SetPen(...); d.SetFill(...); d.DrawRectangle(...); ... } Adapter with stricter invariants Future work: Rust also uses the newtype idiom to create types with additional invariants or other information encoded in the type ( 1 , 2 , 3 ). This is used to record in the type system that some data has passed validation checks, like ValidDate with the same data layout as Date . Or to record the units associated with a value, such as Seconds versus Milliseconds or Feet versus Meters . We should have some way of restricting the casts between a type and an adapter to address this use case. Associated constants In addition to associated methods, we allow other kinds of associated entities . For consistency, we use the same syntax to describe a constant in an interface as in a type without assigning a value. As constants, they are declared using the let introducer. For example, a fixed-dimensional point type could have the dimension as an associated constant. interface NSpacePoint { let N:! i32; // The following require: 0 <= i < N. fn Get[addr me: Self*](i: i32) -> f64; fn Set[addr me: Self*](i: i32, value: f64); // Associated constants may be used in signatures: fn SetAll[addr me: Self*](value: Array(f64, N)); } An implementation of an interface specifies values for associated constants with a where clause . For example, implementations of NSpacePoint for different types might have different values for N : class Point2D { impl as NSpacePoint where .N = 2 { fn Get[addr me: Self*](i: i32) -> f64 { ... } fn Set[addr me: Self*](i: i32, value: f64) { ... } fn SetAll[addr me: Self*](value: Array(f64, 2)) { ... } } } class Point3D { impl as NSpacePoint where .N = 3 { fn Get[addr me: Self*](i: i32) -> f64 { ... } fn Set[addr me: Self*](i: i32, value: f64) { ... } fn SetAll[addr me: Self*](value: Array(f64, 3)) { ... } } } Multiple assignments to associated constants may be joined using the and keyword. The list of assignments is subject to two restrictions: An implementation of an interface cannot specify a value for a final associated constant. If an associated constant doesn't have a default value , every implementation must specify its value. These values may be accessed as members of the type: Assert(Point2D.N == 2); Assert(Point3D.N == 3); fn PrintPoint[PointT:! NSpacePoint](p: PointT) { for (var i: i32 = 0; i < PointT.N; ++i) { if (i > 0) { Print(\", \"); } Print(p.Get(i)); } } fn ExtractPoint[PointT:! NSpacePoint]( p: PointT, dest: Array(f64, PointT.N)*) { for (var i: i32 = 0; i < PointT.N; ++i) { (*dest)[i] = p.Get(i); } } Comparison with other languages: This feature is also called associated constants in Rust . Aside: In general, the use of :! here means these let declarations will only have compile-time and not runtime storage associated with them. Associated class functions To be consistent with normal class function declaration syntax, associated class functions are written using a fn declaration: interface DeserializeFromString { fn Deserialize(serialized: String) -> Self; } class MySerializableType { var i: i32; impl as DeserializeFromString { fn Deserialize(serialized: String) -> Self { return (.i = StringToInt(serialized)); } } } var x: MySerializableType = MySerializableType.Deserialize(\"3\"); fn Deserialize(T:! DeserializeFromString, serialized: String) -> T { return T.Deserialize(serialized); } var y: MySerializableType = Deserialize(MySerializableType, \"4\"); This is instead of declaring an associated constant using let with a function type. Together associated methods and associated class functions are called associated functions , much like together methods and class functions are called member functions . Associated types Associated types are associated entities that happen to be types. These are particularly interesting since they can be used in the signatures of associated methods or functions, to allow the signatures of methods to vary from implementation to implementation. We already have one example of this: the Self type discussed in the \"Interfaces\" section . For other cases, we can say that the interface declares that each implementation will provide a type under a specific name. For example: interface StackAssociatedType { let ElementType:! Type; fn Push[addr me: Self*](value: ElementType); fn Pop[addr me: Self*]() -> ElementType; fn IsEmpty[addr me: Self*]() -> bool; } Here we have an interface called StackAssociatedType which defines two methods, Push and Pop . The signatures of those two methods declare them as accepting or returning values with the type ElementType , which any implementer of StackAssociatedType must also define. For example, maybe DynamicArray implements StackAssociatedType : class DynamicArray(T:! Type) { class IteratorType { ... } fn Begin[addr me: Self*]() -> IteratorType; fn End[addr me: Self*]() -> IteratorType; fn Insert[addr me: Self*](pos: IteratorType, value: T); fn Remove[addr me: Self*](pos: IteratorType); // Set the associated type `ElementType` to `T`. impl as StackAssociatedType where .ElementType = T { fn Push[addr me: Self*](value: ElementType) { me->Insert(me->End(), value); } fn Pop[addr me: Self*]() -> ElementType { var pos: IteratorType = me->End(); Assert(pos != me->Begin()); --pos; returned var ret: ElementType = *pos; me->Remove(pos); return var; } fn IsEmpty[addr me: Self*]() -> bool { return me->Begin() == me->End(); } } } Alternatives considered: See other syntax options considered in #731 for specifying associated types . In particular, it was deemed that Swift's approach of inferring the associated type from method signatures in the impl was unneeded complexity. The definition of the StackAssociatedType is sufficient for writing a generic function that operates on anything implementing that interface, for example: fn PeekAtTopOfStack[StackType:! StackAssociatedType](s: StackType*) -> StackType.ElementType { var top: StackType.ElementType = s->Pop(); s->Push(top); return top; } Inside the generic function PeekAtTopOfStack , the ElementType associated type member of StackType is erased. This means StackType.ElementType has the API dictated by the declaration of ElementType in the interface StackAssociatedType . Outside the generic, associated types have the concrete type values determined by impl lookup, rather than the erased version of that type used inside a generic. var my_array: DynamicArray(i32) = (1, 2, 3); // PeekAtTopOfStack's `StackType` is set to `DynamicArray(i32)` // with `StackType.ElementType` set to `i32`. Assert(PeekAtTopOfStack(my_array) == 3); This is another part of achieving the goal that generic functions can be used in place of regular functions without changing the return type that callers see discussed in the return type section . Associated types can also be implemented using a member type . interface Container { let IteratorType:! Iterator; ... } class DynamicArray(T:! Type) { ... impl as Container { class IteratorType { impl Iterator { ... } } ... } } For context, see \"Interface type parameters and associated types\" in the generics terminology document . Comparison with other languages: Both Rust and Swift support associated types. Implementation model The associated type can be modeled by a witness table field in the interface's witness table. interface Iterator { fn Advance[addr me: Self*](); } interface Container { let IteratorType:! Iterator; fn Begin[addr me: Self*]() -> IteratorType; } is represented by: class Iterator(Self:! Type) { var Advance: fnty(this: Self*); ... } class Container(Self:! Type) { // Representation type for the iterator. let IteratorType:! Type; // Witness that IteratorType implements Iterator. var iterator_impl: Iterator(IteratorType)*; // Method var Begin: fnty (this: Self*) -> IteratorType; ... } Parameterized interfaces Associated types don't change the fact that a type can only implement an interface at most once. If instead you want a family of related interfaces, one per possible value of a type parameter, multiple of which could be implemented for a single type, you would use parameterized interfaces . To write a parameterized version of the stack interface, instead of using associated types, write a parameter list after the name of the interface instead of the associated type declaration: interface StackParameterized(ElementType:! Type) { fn Push[addr me: Self*](value: ElementType); fn Pop[addr me: Self*]() -> ElementType; fn IsEmpty[addr me: Self*]() -> bool; } Then StackParameterized(Fruit) and StackParameterized(Veggie) would be considered different interfaces, with distinct implementations. class Produce { var fruit: DynamicArray(Fruit); var veggie: DynamicArray(Veggie); impl as StackParameterized(Fruit) { fn Push[addr me: Self*](value: Fruit) { me->fruit.Push(value); } fn Pop[addr me: Self*]() -> Fruit { return me->fruit.Pop(); } fn IsEmpty[addr me: Self*]() -> bool { return me->fruit.IsEmpty(); } } impl as StackParameterized(Veggie) { fn Push[addr me: Self*](value: Veggie) { me->veggie.Push(value); } fn Pop[addr me: Self*]() -> Veggie { return me->veggie.Pop(); } fn IsEmpty[addr me: Self*]() -> bool { return me->veggie.IsEmpty(); } } } Unlike associated types in interfaces and parameters to types, interface parameters can't be deduced. For example, if we were to rewrite the PeekAtTopOfStack example in the \"associated types\" section for StackParameterized(T) it would generate a compile error: // \u274c Error: can't deduce interface parameter `T`. fn BrokenPeekAtTopOfStackParameterized [T:! Type, StackType:! StackParameterized(T)] (s: StackType*) -> T { ... } This error is because the compiler can not determine if T should be Fruit or Veggie when passing in argument of type Produce* . The function's signature would have to be changed so that the value for T could be determined from the explicit parameters. fn PeekAtTopOfStackParameterized [T:! Type, StackType:! StackParameterized(T)] (s: StackType*, _:! singleton_type_of(T)) -> T { ... } var produce: Produce = ...; var top_fruit: Fruit = PeekAtTopOfStackParameterized(&produce, Fruit); var top_veggie: Veggie = PeekAtTopOfStackParameterized(&produce, Veggie); The pattern _:! singleton_type_of(T) is a placeholder syntax for an expression that will only match T , until issue #578: Value patterns as function parameters is resolved. Using that pattern in the explicit parameter list allows us to make T available earlier in the declaration so it can be passed as the argument to the parameterized interface StackParameterized . This approach is useful for the ComparableTo(T) interface, where a type might be comparable with multiple other types, and in fact interfaces for operator overloads more generally. Example: interface EquatableWith(T:! Type) { fn Equals[me: Self](rhs: T) -> bool; ... } class Complex { var real: f64; var imag: f64; // Can implement this interface more than once // as long as it has different arguments. impl as EquatableWith(Complex) { ... } impl as EquatableWith(f64) { ... } } All interface parameters must be marked as \"generic\", using the :! syntax. This reflects these two properties of these parameters: They must be resolved at compile-time, and so can't be passed regular dynamic values. We allow either generic or template values to be passed in. Note: Interface parameters aren't required to be types, but that is the vast majority of cases. As an example, if we had an interface that allowed a type to define how the tuple-member-read operator would work, the index of the member could be an interface parameter: interface ReadTupleMember(index:! u32) { let T:! Type; // Returns me[index] fn Get[me: Self]() -> T; } This requires that the index be known at compile time, but allows different indices to be associated with different types. Caveat: When implementing an interface twice for a type, the interface parameters are required to always be different. For example: interface Map(FromType:! Type, ToType:! Type) { fn Map[addr me: Self*](needle: FromType) -> Optional(ToType); } class Bijection(FromType:! Type, ToType:! Type) { impl as Map(FromType, ToType) { ... } impl as Map(ToType, FromType) { ... } } // \u274c Error: Bijection has two impls of interface Map(String, String) var oops: Bijection(String, String) = ...; In this case, it would be better to have an adapting type to contain the impl for the reverse map lookup, instead of implementing the Map interface twice: class Bijection(FromType:! Type, ToType:! Type) { impl as Map(FromType, ToType) { ... } } adapter ReverseLookup(FromType:! Type, ToType:! Type) for Bijection(FromType, ToType) { impl as Map(ToType, FromType) { ... } } Comparison with other languages: Rust calls traits with type parameters \"generic traits\" and uses them for operator overloading . Rust uses the term \"type parameters\" for both interface type parameters and associated types. The difference is that interface parameters are \"inputs\" since they determine which impl to use, and associated types are \"outputs\" since they are determined by the impl , but play no role in selecting the impl . Impl lookup Let's say you have some interface I(T, U(V)) being implemented for some type A(B(C(D), E)) . To satisfy the orphan rule for coherence , that impl must be defined in some library that must be imported in any code that looks up whether that interface is implemented for that type. This requires that impl is defined in the same library that defines the interface or one of the names needed by the type. That is, the impl must be defined with one of I , T , U , V , A , B , C , D , or E . We further require anything looking up this impl to import the definitions of all of those names. Seeing a forward declaration of these names is insufficient, since you can presumably see forward declarations without seeing an impl with the definition. This accomplishes a few goals: The compiler can check that there is only one definition of any impl that is actually used, avoiding One Definition Rule (ODR) problems. Every attempt to use an impl will see the exact same impl , making the interpretation and semantics of code consistent no matter its context, in accordance with the low context-sensitivity principle . Allowing the impl to be defined with either the interface or the type addresses the expression problem . Note that the rules for specialization do allow there to be more than one impl to be defined for a type, by unambiguously picking one as most specific. References: Implementation coherence is defined in terminology , and is a goal for Carbon . More detail can be found in this appendix with the rationale and alternatives considered . Parameterized named constraints We should also allow the named constraint construct to support parameters. Parameters would work the same way as for interfaces. Where constraints So far, we have restricted a generic type parameter by saying it has to implement an interface or a set of interfaces. There are a variety of other constraints we would like to be able to express, such as applying restrictions to its associated types and associated constants. This is done using the where operator that adds constraints to a type-of-type. The where operator can be applied to a type-of-type in a declaration context: // Constraints on function parameters: fn F[V:! D where ...](v: V) { ... } // Constraints on a class parameter: class S(T:! B where ...) { // Constraints on a method: fn G[me: Self, V:! D where ...](v: V); } // Constraints on an interface parameter: interface A(T:! B where ...) { // Constraints on an associated type: let U:! C where ...; // Constraints on an associated method: fn G[me: Self, V:! D where ...](v: V); } We also allow you to name constraints using a where operator in a let or constraint definition. The expressions that can follow the where keyword are described in the \"constraint use cases\" section, but generally look like boolean expressions that should evaluate to true . The result of applying a where operator to a type-of-type is another type-of-type. Note that this expands the kinds of requirements that type-of-types can have from just interface requirements to also include the various kinds of constraints discussed later in this section. In addition, it can introduce relationships between different type variables, such as that a member of one is equal to the member of another. Comparison with other languages: Both Swift and Rust use where clauses on declarations instead of in the expression syntax. These happen after the type that is being constrained has been given a name and use that name to express the constraint. Rust also supports directly passing in the values for associated types when using a trait as a constraint. This is helpful when specifying concrete types for all associated types in a trait in order to make it object safe so it can be used to define a trait object type . Rust is adding trait aliases ( RFC , tracking issue ) to support naming some classes of constraints. Constraint use cases Set an associated constant to a specific value We might need to write a function that only works with a specific value of an associated constant N . In this case, the name of the associated constant is written first, followed by an = , and then the value: fn PrintPoint2D[PointT:! NSpacePoint where .N = 2](p: PointT) { Print(p.Get(0), \", \", p.Get(1)); } Similarly in an interface definition: interface Has2DPoint { let PointT:! NSpacePoint where .N = 2; } To name such a constraint, you may use a let or a constraint declaration: let Point2DInterface:! auto = NSpacePoint where .N = 2; constraint Point2DInterface { extends NSpacePoint where .N = 2; } This syntax is also used to specify the values of associated constants when implementing an interface for a type. Concern: Using = for this use case is not consistent with other where clauses that write a boolean expression that evaluates to true when the constraint is satisfied. A constraint to say that two associated constants should have the same value without specifying what specific value they should have must use == instead of = : interface PointCloud { let Dim:! i32; let PointT:! NSpacePoint where .N == Dim; } Same type constraints Set an associated type to a specific value Functions accepting a generic type might also want to constrain one of its associated types to be a specific, concrete type. For example, we might want to have a function only accept stacks containing integers: fn SumIntStack[T:! Stack where .ElementType = i32](s: T*) -> i32 { var sum: i32 = 0; while (!s->IsEmpty()) { // s->Pop() has type `T.ElementType` == i32: sum += s->Pop(); } return sum; } To name these sorts of constraints, we could use let declarations or constraint definitions: let IntStack:! auto = Stack where .ElementType = i32; constraint IntStack { extends Stack where .ElementType = i32; } This syntax is also used to specify the values of associated types when implementing an interface for a type. Equal generic types Alternatively, two generic types could be constrained to be equal to each other, without specifying what that type is. This uses == instead of = . For example, we could make the ElementType of an Iterator interface equal to the ElementType of a Container interface as follows: interface Iterator { let ElementType:! Type; ... } interface Container { let ElementType:! Type; let IteratorType:! Iterator where .ElementType == ElementType; ... } Given an interface with two associated types interface PairInterface { let Left:! Type; let Right:! Type; } we can constrain them to be equal in a function signature: fn F[MatchedPairType:! PairInterface where .Left == .Right] (x: MatchedPairType*); or in an interface definition: interface HasEqualPair { let P:! PairInterface where .Left == .Right; } This kind of constraint can be named: let EqualPair:! auto = PairInterface where .Left == .Right; constraint EqualPair { extends PairInterface where .Left == .Right; } Another example of same type constraints is when associated types of two different interfaces are constrained to be equal: fn Map[CT:! Container, FT:! Function where .InputType == CT.ElementType] (c: CT, f: FT) -> Vector(FT.OutputType); Satisfying both type-of-types If the two types being constrained to be equal have been declared with different type-of-types, then the actual type value they are set to will have to satisfy both constraints. For example, if SortedContainer.ElementType is declared to be Comparable , then in this declaration: fn Contains [SC:! SortedContainer, CT:! Container where .ElementType == SC.ElementType] (haystack: SC, needles: CT) -> bool; the where constraint means CT.ElementType must satisfy Comparable as well. However, inside the body of Contains , CT.ElementType will only act like the implementation of Comparable is external . That is, items from the needles container won't directly have a Compare method member, but can still be implicitly converted to Comparable and can still call Compare using the compound member access syntax, needle.(Comparable.Compare)(elt) . The rule is that an == where constraint between two type variables does not modify the set of member names of either type. (If you write where .ElementType = String with a = and a concrete type, then .ElementType is actually set to String including the complete String API.) Note that == constraints are symmetric, so the previous declaration of Contains is equivalent to an alternative declaration where CT is declared first and the where clause is attached to SortedContainer : fn Contains [CT:! Container, SC:! SortedContainer where .ElementType == CT.ElementType] (haystack: SC, needles: CT) -> bool; Type bound for associated type A where clause can express that a type must implement an interface. This is more flexible than the usual approach of including that interface in the type since it can be applied to associated type members as well. Type bounds on associated types in declarations In the following example, normally the ElementType of a Container can be any type. The SortContainer function, however, takes a pointer to a type satisfying Container with the additional constraint that its ElementType must satisfy the Comparable interface. interface Container { let ElementType:! Type; ... } fn SortContainer [ContainerType:! Container where .ElementType is Comparable] (container_to_sort: ContainerType*); In contrast to a same type constraint , this does not say what type ElementType exactly is, just that it must satisfy some type-of-type. Open question: How do you spell that? Provisionally we are writing is , following Swift, but maybe we should have another operator that more clearly returns a boolean like has_type ? Note: Container defines ElementType as having type Type , but ContainerType.ElementType has type Comparable . This is because ContainerType has type Container where .ElementType is Comparable , not Container . This means we need to be a bit careful when talking about the type of ContainerType when there is a where clause modifying it. Type bounds on associated types in interfaces Given these definitions (omitting ElementType for brevity): interface IteratorInterface { ... } interface ContainerInterface { let IteratorType:! IteratorInterface; ... } interface RandomAccessIterator { extends IteratorInterface; ... } We can then define a function that only accepts types that implement ContainerInterface where its IteratorType associated type implements RandomAccessIterator : fn F[ContainerType:! ContainerInterface where .IteratorType is RandomAccessIterator] (c: ContainerType); We would like to be able to name this constraint, defining a RandomAccessContainer to be a type-of-type whose types satisfy ContainerInterface with an IteratorType satisfying RandomAccessIterator . let RandomAccessContainer:! auto = ContainerInterface where .IteratorType is RandomAccessIterator; // or constraint RandomAccessContainer { extends ContainerInterface where .IteratorType is RandomAccessIterator; } // With the above definition: fn F[ContainerType:! RandomAccessContainer](c: ContainerType); // is equivalent to: fn F[ContainerType:! ContainerInterface where .IteratorType is RandomAccessIterator] (c: ContainerType); Combining constraints Constraints can be combined by separating constraint clauses with the and keyword. This example expresses a constraint that two associated types are equal and satisfy an interface: fn EqualContainers [CT1:! Container, CT2:! Container where .ElementType is HasEquality and .ElementType == CT1.ElementType] (c1: CT1*, c2: CT2*) -> bool; Comparison with other languages: Swift and Rust use commas , to separate constraint clauses, but that only works because they place the where in a different position in a declaration. In Carbon, the where is attached to a type in a parameter list that is already using commas to separate parameters. Recursive constraints We sometimes need to constrain a type to equal one of its associated types. In this first example, we want to represent the function Abs which will return Self for some but not all types, so we use an associated type MagnitudeType to encode the return type: interface HasAbs { extends Numeric; let MagnitudeType:! Numeric; fn Abs[me: Self]() -> MagnitudeType; } For types representing subsets of the real numbers, such as i32 or f32 , the MagnitudeType will match Self , the type implementing an interface. For types representing complex numbers, the types will be different. For example, the Abs() applied to a Complex64 value would produce a f32 result. The goal is to write a constraint to restrict to the first case. In a second example, when you take the slice of a type implementing Container you get a type implementing Container which may or may not be the same type as the original container type. However, taking the slice of a slice always gives you the same type, and some functions want to only operate on containers whose slice type is the same as the container type. To solve this problem, we think of Self as an actual associated type member of every interface. We can then address it using .Self in a where clause, like any other associated type member. fn Relu[T:! HasAbs where .MagnitudeType == .Self](x: T) { // T.MagnitudeType == T so the following is allowed: return (x.Abs() + x) / 2; } fn UseContainer[T:! Container where .SliceType == .Self](c: T) -> bool { // T.SliceType == T so `c` and `c.Slice(...)` can be compared: return c == c.Slice(...); } Notice that in an interface definition, Self refers to the type implementing this interface while .Self refers to the associated type currently being defined. interface Container { let ElementType:! Type; let SliceType:! Container where .ElementType == ElementType and .SliceType == .Self; fn GetSlice[addr me: Self*] (start: IteratorType, end: IteratorType) -> SliceType; } These recursive constraints can be named: let RealAbs:! auto = HasAbs where .MagnitudeType == .Self; constraint RealAbs { extends HasAbs where .MagnitudeType == Self; } let ContainerIsSlice:! auto = Container where .SliceType == .Self; constraint ContainerIsSlice { extends Container where .SliceType == Self; } Note that using the constraint approach we can name these constraints using Self instead of .Self , since they refer to the same type. Parameterized type implements interface There are times when a function will pass a generic type parameter of the function as an argument to a parameterized type, as in the previous case, and in addition the function needs the result to implement a specific interface. // Some parametized type. class Vector(T:! Type) { ... } // Parameterized type implements interface only for some arguments. external impl Vector(String) as Printable { ... } // Constraint: `T` such that `Vector(T)` implements `Printable` fn PrintThree [T:! Type where Vector(.Self) is Printable] (a: T, b: T, c: T) { var v: Vector(T) = (a, b, c); Print(v); } Comparison with other languages: This use case was part of the Rust rationale for adding support for where clauses . Another type implements parameterized interface In this case, we need some other type to implement an interface parameterized by a generic type parameter. The syntax for this case follows the previous case, except now the .Self parameter is on the interface to the right of the is . For example, we might need a type parameter T to support explicit conversion from an integer type like i32 : interface As(T:! Type) { fn Convert[me: Self]() -> T; } fn Double[T:! Mul where i32 is As(.Self)](x: T) -> T { return x * (2 as T); } Implied constraints Imagine we have a generic function that accepts an arbitrary HashMap : fn LookUp[KeyType:! Type](hm: HashMap(KeyType, i32)*, k: KeyType) -> i32; fn PrintValueOrDefault[KeyType:! Printable, ValueT:! Printable & HasDefault] (map: HashMap(KeyType, ValueT), key: KeyT); The KeyType in these declarations does not visibly satisfy the requirements of HashMap , which requires the type implement Hashable and other interfaces: class HashMap( KeyType:! Hashable & EqualityComparable & Movable, ...) { ... } In this case, KeyType gets Hashable and so on as implied constraints . Effectively that means that these functions are automatically rewritten to add a where constraint on KeyType attached to the HashMap type: fn LookUp[KeyType:! Type] (hm: HashMap(KeyType, i32)* where KeyType is Hashable & EqualityComparable & Movable, k: KeyType) -> i32; fn PrintValueOrDefault[KeyType:! Printable, ValueT:! Printable & HasDefault] (map: HashMap(KeyType, ValueT) where KeyType is Hashable & EqualityComparable & Movable, key: KeyT); In this case, Carbon will accept the definition and infer the needed constraints on the generic type parameter. This is both more concise for the author of the code and follows the \"don't repeat yourself\" principle . This redundancy is undesirable since it means if the needed constraints for HashMap are changed, then the code has to be updated in more locations. Further it can add noise that obscures relevant information. In practice, any user of these functions will have to pass in a valid HashMap instance, and so will have already satisfied these constraints. This implied constraint is equivalent to the explicit constraint that each parameter and return type is legal . Note: These implied constraints affect the requirements of a generic type parameter, but not its member names . This way you can always look at the declaration to see how name resolution works, without having to look up the definitions of everything it is used as an argument to. Limitation: To limit readability concerns and ambiguity, this feature is limited to a single signature. Consider this interface declaration: interface GraphNode { let Edge:! Type; fn EdgesFrom[me: Self]() -> HashSet(Edge); } One approach would be to say the use of HashSet(Edge) in the signature of the EdgesFrom function would imply that Edge satisfies the requirements of an argument to HashSet , such as being Hashable . Another approach would be to say that the EdgesFrom would only be conditionally available when Edge does satisfy the constraints on HashSet arguments. Instead, Carbon will reject this definition, requiring the user to include all the constraints required for the other declarations in the interface in the declaration of the Edge associated type. Similarly, a parameter to a class must be declared with all the constraints needed to declare the members of the class that depend on that parameter. Comparison with other languages: Both Swift ( 1 , 2 ) and Rust support some form of this feature as part of their type inference (and the Rust community is considering expanding support ). Must be legal type argument constraints Now consider the case that the generic type parameter is going to be used as an argument to a parameterized type in a function body, not in the signature. If the parameterized type was explicitly mentioned in the signature, the implied constraint feature would ensure all of its requirements were met. The developer can create a trivial parameterized type implements interface where constraint to just say the type is a legal with this argument, by saying that the parameterized type implements Type , which all types do. For example, a function that adds its parameters to a HashSet to deduplicate them, needs them to be Hashable and so on. To say \" T is a type where HashSet(T) is legal,\" we can write: fn NumDistinct[T:! Type where HashSet(.Self) is Type] (a: T, b: T, c: T) -> i32 { var set: HashSet(T); set.Add(a); set.Add(b); set.Add(c); return set.Size(); } This has the same advantages over repeating the constraints on HashSet arguments in the type of T as the general implied constraints above. Referencing names in the interface being defined The constraint in a where clause is required to only reference earlier names from this scope, as in this example: interface Graph { let E: Edge; let V: Vert where .E == E and .Self == E.V; } Manual type equality Imagine we have some function with generic parameters: fn F[T:! SomeInterface](x: T) { x.G(x.H()); } We want to know if the return type of method T.H is the same as the parameter type of T.G in order to typecheck the function. However, determining whether two type expressions are transitively equal is in general undecidable, as has been shown in Swift . Carbon's approach is to only allow implicit conversions between two type expressions that are constrained to be equal in a single where clause. This means that if two type expressions are only transitively equal, the user will need to include a sequence of casts or use an observe declaration to convert between them. Given this interface Transitive that has associated types that are constrained to all be equal, with interfaces P , Q , and R : interface P { fn InP[me:Self](); } interface Q { fn InQ[me:Self](); } interface R { fn InR[me:Self](); } interface Transitive { let A:! P; let B:! Q where .Self == A; let C:! R where .Self == B; fn GetA[me: Self]() -> A; fn TakesC[me:Self](c: C); } A cast to B is needed to call TakesC with a value of type A , so each step only relies on one equality: fn F[T:! Transitive](t: T) { // \u2705 Allowed t.TakesC(t.GetA() as T.B); // \u2705 Allowed let b: T.B = t.GetA(); t.TakesC(b); // \u274c Not allowed: t.TakesC(t.GetA()); } A value of type A , such as the return value of GetA() , has the API of P . Any such value also implements Q , and since the compiler can see that by way of a single where equality, values of type A are treated as if they implement Q externally . However, the compiler will require a cast to B or C to see that the type implements R . fn TakesPQR[U:! P & Q & R](u: U); fn G[T:! Transitive](t: T) { var a: T.A = t.GetA(); // \u2705 Allowed: `T.A` implements `P`. a.InP(); // \u2705 Allowed: `T.A` implements `Q` externally. a.(Q.InQ)(); // \u274c Not allowed: a.InQ(); // \u2705 Allowed: values of type `T.A` may be cast // to `T.B`, which implements `Q` internally. (a as T.B).InQ(); // \u2705 Allowed: `T.B` implements `R` externally. (a as T.B).(R.InR)(); // \u274c Not allowed: TakesPQR(a); // \u2705 Allowed: `T.B` implements `P`, `Q`, and // `R`, though the implementations of `P` // and `R` are external. TakesPQR(a as T.B); } The compiler may have several different where clauses to consider, particularly when an interface has associated types that recursively satisfy the same interface. For example, given this interface Commute : interface Commute { let X:! Commute; let Y:! Commute where .X == X.Y; fn GetX[me: Self]() -> X; fn GetY[me: Self]() -> Y; fn TakesXXY[me:Self](xxy: X.X.Y); } and a function H taking a value with some type implementing this interface, then the following would be legal statements in H : fn H[C: Commute](c: C) { // \u2705 Legal: argument has type `C.X.X.Y` c.TakesXXY(c.GetX().GetX().GetY()); // \u2705 Legal: argument has type `C.X.Y.X` which is equal // to `C.X.X.Y` following only one `where` clause. c.TakesXXY(c.GetX().GetY().GetX()); // \u2705 Legal: cast is legal since it matches a `where` // clause, and produces an argument that has type // `C.X.Y.X`. c.TakesXXY(c.GetY().GetX().GetX() as C.X.Y.X); } That last call would not be legal without the cast, though. Comparison with other languages: Other languages such as Swift and Rust instead perform automatic type equality. In practice this means that their compiler can reject some legal programs based on heuristics simply to avoid running for an unbounded length of time. The benefits of the manual approach include: fast compilation, since the compiler does not need to explore a potentially large set of combinations of equality restrictions, supporting Carbon's goal of fast and scalable development ; expressive and predictable semantics, since there are no limitations on how complex a set of constraints can be supported; and simplicity. The main downsides are: manual work for the source code author to prove to the compiler that types are equal; and verbosity. We expect that rich error messages and IDE tooling will be able to suggest changes to the source code when a single equality constraint is not sufficient to show two type expressions are equal, but a more extensive automated search can find a sequence that prove they are equal. observe declarations An observe declaration lists a sequence of type expressions that are equal by some same-type where constraints. These observe declarations may be included in an interface definition or a function body, as in: interface Commute { let X:! Commute; let Y:! Commute where .X == X.Y; ... observe X.X.Y == X.Y.X == Y.X.X; } fn H[C: Commute](c: C) { observe C.X.Y.Y == C.Y.X.Y == C.Y.Y.X; ... } Every type expression after the first must be equal to some earlier type expression in the sequence by a single where equality constraint. In this example, interface Commute { let X:! Commute; let Y:! Commute where .X == X.Y; ... // \u2705 Legal: observe X.X.Y.Y == X.Y.X.Y == Y.X.X.Y == X.Y.Y.X; } the expression X.Y.Y.X is one equality away from X.Y.X.Y and so it is allowed. This is even though X.Y.X.Y isn't the type expression immediately prior to X.Y.Y.X . After an observe declaration, all of the listed type expressions are considered equal to each other using a single where equality. In this example, the observe declaration in the Transitive interface definition provides the link between associated types A and C that allows function F to type check. interface P { fn InP[me:Self](); } interface Q { fn InQ[me:Self](); } interface R { fn InR[me:Self](); } interface Transitive { let A:! P; let B:! Q where .Self == A; let C:! R where .Self == B; fn GetA[me: Self]() -> A; fn TakesC[me:Self](c: C); // Without this `observe` declaration, the // calls in `F` below would not be allowed. observe A == B == C; } fn TakesPQR[U:! P & Q & R](u: U); fn F[T:! Transitive](t: T) { var a: T.A = t.GetA(); // \u2705 Allowed: `T.A` == `T.C` t.TakesC(a); a.(R.InR()); // \u2705 Allowed: `T.A` implements `P`, // `T.A` == `T.B` that implements `Q`, and // `T.A` == `T.C` that implements `R`. TakesPQR(a); } Since adding an observe declaration only adds external implementations of interfaces to generic types, they may be added without breaking existing code. Other constraints as type-of-types There are some constraints that we will naturally represent as named type-of-types. These can either be used directly to constrain a generic type parameter, or in a where ... is ... clause to constrain an associated type. The compiler determines which types implement these interfaces, developers can not explicitly implement these interfaces for their own types. Open question: Are these names part of the prelude or in a standard library? Is a derived class Given a type T , Extends(T) is a type-of-type whose values are types that are derived from T . That is, Extends(T) is the set of all types U that are subtypes of T . fn F[T:! Extends(BaseType)](p: T*); fn UpCast[T:! Type](p: T*, U:! Type where T is Extends(.Self)) -> U*; fn DownCast[T:! Type](p: T*, U:! Extends(T)) -> U*; Open question: Alternatively, we could define a new extends operator: fn F[T:! Type where .Self extends BaseType](p: T*); fn UpCast[T:! Type](p: T*, U:! Type where T extends .Self) -> U*; fn DownCast[T:! Type](p: T*, U:! Type where .Self extends T) -> U*; Comparison to other languages: In Swift, you can add a required superclass to a type bound using & . Type compatible with another type Given a type U , define the type-of-type CompatibleWith(U) as follows: CompatibleWith(U) is a type whose values are types T such that T and U are compatible . That is values of types T and U can be cast back and forth without any change in representation (for example T is an adapter for U ). To support this, we extend the requirements that type-of-types are allowed to have to include a \"data representation requirement\" option. CompatibleWith determines an equivalence relationship between types. Specifically, given two types T1 and T2 , they are equivalent if T1 is CompatibleWith(T2) . That is, if T1 has the type CompatibleWith(T2) . Note: Just like interface parameters, we require the user to supply U , they may not be deduced. Specifically, this code would be illegal: fn Illegal[U:! Type, T:! CompatibleWith(U)](x: T*) ... In general there would be multiple choices for U given a specific T here, and no good way of picking one. However, similar code is allowed if there is another way of determining U : fn Allowed[U:! Type, T:! CompatibleWith(U)](x: U*, y: T*) ... Same implementation restriction In some cases, we need to restrict to types that implement certain interfaces the same way as the type U . The values of type CompatibleWith(U, TT) are types satisfying CompatibleWith(U) that have the same implementation of TT as U . For example, if we have a type HashSet(T) : class HashSet(T:! Hashable) { ... } Then HashSet(T) may be cast to HashSet(U) if T is CompatibleWith(U, Hashable) . The one-parameter interpretation of CompatibleWith(U) is recovered by letting the default for the second TT parameter be Type . Example: Multiple implementations of the same interface This allows us to represent functions that accept multiple implementations of the same interface for a type. enum CompareResult { Less, Equal, Greater } interface Comparable { fn Compare[me: Self](rhs: Self) -> CompareResult; } fn CombinedLess[T:! Type](a: T, b: T, U:! CompatibleWith(T) & Comparable, V:! CompatibleWith(T) & Comparable) -> bool { match ((a as U).Compare(b as U)) { case CompareResult.Less => { return True; } case CompareResult.Greater => { return False; } case CompareResult.Equal => { return (a as V).Compare(b as V) == CompareResult.Less; } } } Used as: class Song { ... } adapter SongByArtist for Song { impl as Comparable { ... } } adapter SongByTitle for Song { impl as Comparable { ... } } var s1: Song = ...; var s2: Song = ...; assert(CombinedLess(s1, s2, SongByArtist, SongByTitle) == True); We might generalize this to a list of implementations: fn CombinedCompare[T:! Type] (a: T, b: T, CompareList:! List(CompatibleWith(T) & Comparable)) -> CompareResult { for (let U:! auto in CompareList) { var result: CompareResult = (a as U).Compare(b); if (result != CompareResult.Equal) { return result; } } return CompareResult.Equal; } assert(CombinedCompare(Song(...), Song(...), (SongByArtist, SongByTitle)) == CompareResult.Less); Open question: How are compile-time lists of types declared and iterated through? They will also be needed for variadic argument support . Example: Creating an impl out of other impls And then to package this functionality as an implementation of Comparable , we combine CompatibleWith with type adaptation : adapter ThenCompare( T:! Type, CompareList:! List(CompatibleWith(T) & Comparable)) for T { impl as Comparable { fn Compare[me: Self](rhs: Self) -> CompareResult { for (let U:! auto in CompareList) { var result: CompareResult = (me as U).Compare(rhs as U); if (result != CompareResult.Equal) { return result; } } return CompareResult.Equal; } } } let SongByArtistThenTitle: auto = ThenCompare(Song, (SongByArtist, SongByTitle)); var s1: Song = ...; var s2: SongByArtistThenTitle = Song(...) as SongByArtistThenTitle; assert((s1 as SongByArtistThenTitle).Compare(s2) == CompareResult.Less); Sized types and type-of-types What is the size of a type? It could be fully known and fixed at compile time -- this is true of primitive types ( i32 , f64 , and so on), most classes , and most other concrete types. It could be known generically. This means that it will be known at codegen time, but not at type-checking time. It could be dynamic. For example, it could be a dynamic type , a slice, variable-sized type (such as found in Rust ), or you could dereference a pointer to a base class that could actually point to a derived class . It could be unknown which category the type is in. In practice this will be essentially equivalent to having dynamic size. A type is called sized if it is in the first two categories, and unsized otherwise. Note: something with size 0 is still considered \"sized\". The type-of-type Sized is defined as follows: Sized is a type whose values are types T that are \"sized\" -- that is the size of T is known, though possibly only generically. Knowing a type is sized is a precondition to declaring variables of that type, taking values of that type as parameters, returning values of that type, and defining arrays of that type. Users will not typically need to express the Sized constraint explicitly, though, since it will usually be a dependency of some other constraint the type will need such as Movable or Concrete . Note: The compiler will determine which types are \"sized\", this is not something types will implement explicitly like ordinary interfaces. Example: // In the Carbon standard library interface DefaultConstructible { // Types must be sized to be default constructible. impl as Sized; fn Default() -> Self; } // Classes are \"sized\" by default. class Name { impl as DefaultConstructible { fn Default() -> Self { ... } } ... } fn F[T:! Type](x: T*) { // T is unsized. // \u2705 Allowed: may access unsized values through a pointer. var y: T* = x; // \u274c Illegal: T is unsized. var z: T; } // T is sized, but its size is only known generically. fn G[T: DefaultConstructible](x: T*) { // \u2705 Allowed: T is default constructible, which means sized. var y: T = T.Default(); } var z: Name = Name.Default();; // \u2705 Allowed: `Name` is sized and implements `DefaultConstructible`. G(&z); Open question: Even if the size is fixed, it won't be known at the time of compiling the generic function if we are using the dynamic strategy. Should we automatically box local variables when using the dynamic strategy? Or should we only allow MaybeBox values to be instantiated locally? Or should this just be a case where the compiler won't necessarily use the dynamic strategy? Open question: Should the Sized type-of-type expose an associated constant with the size? So you could say T.ByteSize in the above example to get a generic int value with the size of T . Similarly you might say T.ByteStride to get the number of bytes used for each element of an array of T . Implementation model This requires a special integer field be included in the witness table type to hold the size of the type. This field will only be known generically, so if its value is used for type checking, we need some way of evaluating those type tests symbolically. TypeId There are some capabilities every type can provide. For example, every type should be able to return its name or identify whether it is equal to another type. It is rare, however, for code to need to access these capabilities, so we relegate these capabilities to an interface called TypeId that all types automatically implement. This way generic code can indicate that it needs those capabilities by including TypeId in the list of requirements. In the case where no type capabilities are needed, for example the code is only manipulating pointers to the type, you would write T:! Type and get the efficiency of void* but without giving up type safety. fn SortByAddress[T:! Type](v: Vector(T*)*) { ... } In particular, the compiler should in general avoid monomorphizing to generate multiple instantiations of the function in this case. Open question: Should TypeId be implemented externally for types to avoid name pollution ( .TypeName , .TypeHash , etc.) unless the function specifically requests those capabilities? Destructor constraints There are four type-of-types related to the destructors of types : Concrete types may be local or member variables. Deletable types may be safely deallocated by pointer using the Delete method on the Allocator used to allocate it. Destructible types have a destructor and may be deallocated by pointer using the UnsafeDelete method on the correct Allocator , but it may be unsafe. The concerning case is deleting a pointer to a derived class through a pointer to its base class without a virtual destructor. TrivialDestructor types have empty destructors. This type-of-type may be used with specialization to unlock specific optimizations. Note: The names Deletable and Destructible are placeholders since they do not conform to the decision on question-for-leads issue #1058: \"How should interfaces for core functionality be named?\" . The type-of-types Concrete , Deletable , and TrivialDestructor all extend Destructible . Combinations of them may be formed using the & operator . For example, a generic function that both instantiates and deletes values of a type T would require T implement Concrete & Deletable . Types are forbidden from explicitly implementing these type-of-types directly. Instead they use destructor declarations in their class definition and the compiler uses them to determine which of these type-of-types are implemented. Generic let A let statement inside a function body may be used to get the change in type behavior of calling a generic function without having to introduce a function call. fn F(...) { ... let T:! C = U; X; Y; Z; } gets rewritten to: fn F(...) { ... fn Closure(T:! C where .Self == U) { X; Y; Z; } Closure(U); } The where .Self == U modifier allows values to implicitly convert between type T , the erased type, and type U , the concrete type. Note that implicit conversion is only performed across a single where equality . This can be used to switch to the API of C when it is external, as an alternative to using an adapter , or to simplify inlining of a generic function while preserving semantics. Parameterized impls There are cases where an impl definition should apply to more than a single type and interface combination. The solution is to parameterize the impl definition, so it applies to a family of types, interfaces, or both. This includes: Declare an impl for a parameterized type, which may be external or declared out-of-line. \"Conditional conformance\" where a parameterized type implements some interface if the parameter to the type satisfies some criteria, like implementing the same interface. \"Blanket\" impls where an interface is implemented for all types that implement another interface, or some other criteria beyond being a specific type. \"Wildcard\" impls where a family of interfaces are implemented for single type. Impl for a parameterized type Interfaces may be implemented for a parameterized type. This can be done lexically in the class' scope: class Vector(T:! Type) { impl as Iterable where .ElementType = T { ... } } This is equivalent to naming the type between impl and as : class Vector(T:! Type) { impl Vector(T) as Iterable where .ElementType = T { ... } } An impl may be declared external by adding an external keyword before impl . External impls may also be declared out-of-line, but all parameters must be declared in a forall clause: external impl forall [T:! Type] Vector(T) as Iterable where .ElementType = T { ... } The parameter for the type can be used as an argument to the interface being implemented: class HashMap(Key:! Hashable, Value:! Type) { impl as Has(Key) { ... } impl as Contains(HashSet(Key)) { ... } } or externally out-of-line: class HashMap(Key:! Hashable, Value:! Type) { ... } external impl forall [Key:! Hashable, Value:! Type] HashMap(Key, Value) as Has(Key) { ... } external impl forall [Key:! Hashable, Value:! Type] HashMap(Key, Value) as Contains(HashSet(Key)) { ... } Conditional conformance Conditional conformance is expressing that we have an impl of some interface for some type, but only if some additional type restrictions are met. Examples where this would be useful include being able to say that a container type, like Vector , implements some interface when its element type satisfies the same interface: A container is printable if its elements are. A container could be compared to another container with the same element type using a lexicographic comparison if the element type is comparable. A container is copyable if its elements are. To do this with an external impl , specify a more-specific Self type to the left of the as in the declaration: interface Printable { fn Print[me: Self](); } class Vector(T:! Type) { ... } // By saying \"T:! Printable\" instead of \"T:! Type\" here, // we constrain T to be Printable for this impl. external impl forall [T:! Printable] Vector(T) as Printable { fn Print[me: Self]() { for (let a: T in me) { // Can call `Print` on `a` since the constraint // on `T` ensures it implements `Printable`. a.Print(); } } } To define these impl s inline in a class definition, include a forall clause with a more-specific type between the impl and as keywords. class Array(T:! Type, template N:! Int) { impl forall [P:! Printable] Array(P, N) as Printable { ... } } It is legal to add the keyword external before the impl keyword to switch to an external impl defined lexically within the class scope. Inside the scope, both P and T refer to the same type, but P has the type-of-type of Printable and so has a Print member. The relationship between T and P is as if there was a where P == T clause. TODO: Need to resolve whether the T name can be reused, or if we require that you need to use new names, like P , when creating new type variables. Example: Consider a type with two parameters, like Pair(T, U) . In this example, the interface Foo(T) is only implemented when the two types are equal. interface Foo(T:! Type) { ... } class Pair(T:! Type, U:! Type) { ... } external impl forall [T:! Type] Pair(T, T) as Foo(T) { ... } You may also define the impl inline, in which case it can be internal: class Pair(T:! Type, U:! Type) { impl Pair(T, T) as Foo(T) { ... } } Clarification: Method lookup will look at all internal implementations, whether or not the conditions on those implementations hold for the Self type. If the conditions don't hold, then the call will be rejected because Self has the wrong type, just like any other argument/parameter type mismatch. This means types may not implement two different interfaces internally if they share a member name, even if their conditions are mutually exclusive: class X(T:! Type) { impl X(i32) as Foo { fn F[me: Self](); } impl X(i64) as Bar { // \u274c Illegal: name conflict between `Foo.F` and `Bar.F` fn F[me: Self](n: i64); } } However, the same interface may be implemented multiple times as long as there is no overlap in the conditions: class X(T:! Type) { impl X(i32) as Foo { fn F[me: Self](); } impl X(i64) as Foo { // \u2705 Allowed: `X(T).F` consistently means `X(T).(Foo.F)` fn F[me: Self](); } } This allows a type to express that it implements an interface for a list of types, possibly with different implementations. In general, X(T).F can only mean one thing, regardless of T . Comparison with other languages: Swift supports conditional conformance , but bans cases where there could be ambiguity from overlap. Rust also supports conditional conformance . Conditional methods A method could be defined conditionally for a type by using a more specific type in place of Self in the method declaration. For example, this is how to define a vector type that only has a Sort method if its elements implement the Comparable interface: class Vector(T:! Type) { // `Vector(T)` has a `Sort()` method if `T` is `Comparable`. fn Sort[C:! Comparable, addr me: Vector(C)*](); } Comparison with other languages: In Rust this feature is part of conditional conformance. Swift supports conditional methods using conditional extensions or contextual where clauses . Blanket impls A blanket impl is an impl that could apply to more than one root type, so the impl will use a type variable for the Self type. Here are some examples where blanket impls arise: Any type implementing Ordered should get an implementation of PartiallyOrdered . external impl forall [T:! Ordered] T as PartiallyOrdered { ... } T implements CommonType(T) for all T external impl forall [T:! Type] T as CommonType(T) where .Result = T { } This means that every type is the common type with itself. Blanket impls must always be external and defined lexically out-of-line. Difference between blanket impls and named constraints A blanket interface can be used to say \"any type implementing interface I also implements interface B .\" Compare this with defining a constraint C that requires I . In that case, C will also be implemented any time I is. There are differences though: There can be other implementations of interface B without a corresponding implementation of I , unless B has a requirement on I . However, the types implementing C will be the same as the types implementing I . More specialized implementations of B can override the blanket implementation. Wildcard impls A wildcard impl is an impl that defines a family of interfaces for a single Self type. For example, the BigInt type might implement AddTo(T) for all T that implement ImplicitAs(i32) . The implementation would first convert T to i32 and then add the i32 to the BigInt value. class BigInt { external impl forall [T:! ImplicitAs(i32)] as AddTo(T) { ... } } // Or out-of-line: external impl forall [T:! ImplicitAs(i32)] BigInt as AddTo(T) { ... } Wildcard impls must always be external , to avoid having the names in the interface defined for the type multiple times. Combinations The different kinds of parameters to impls may be combined. For example, if T implements As(U) , then this implements As(Optional(U)) for Optional(T) : external impl forall [U:! Type, T:! As(U)] Optional(T) as As(Optional(U)) { ... } This has a wildcard parameter U , and a condition on parameter T . Lookup resolution and specialization As much as possible, we want rules for where an impl is allowed to be defined and for selecting which impl to use that achieve these three goals: Implementations have coherence, as defined in terminology . This is a goal for Carbon . More detail can be found in this appendix with the rationale and alternatives considered . Libraries will work together as long as they pass their separate checks. A generic function can assume that some impl will be successfully selected if it can see an impl that applies, even though another more specific impl may be selected. For this to work, we need a rule that picks a single impl in the case where there are multiple impl definitions that match a particular type and interface combination. This is called specialization when the rule is that most specific implementation is chosen, for some definition of specific. Type structure of an impl declaration Given an impl declaration, find the type structure by deleting deduced parameters and replacing type parameters by a ? . The type structure of this declaration: impl forall [T:! ..., U:! ...] Foo(T, i32) as Bar(String, U) { ... } is: impl Foo(?, i32) as Bar(String, ?) To get a uniform representation across different impl definitions, before type parameters are replaced the declarations are normalized as follows: For impls declared lexically inline in a class definition, the type is added between the impl and as keywords if the type is left out. Pointer types T* are replaced with Ptr(T) . The external keyword is removed, if present. The forall clause introducing type parameters is removed, if present. Any where clauses that are setting associated constants or types are removed. The type structure will always contain a single interface name, which is the name of the interface being implemented, and some number of type names. Type names can be in the Self type to the left of the as keyword, or as parameters to other types or the interface. These names must always be defined either in the current library or be publicly defined in some library this library depends on. Orphan rule To achieve coherence, we need to ensure that any given impl can only be defined in a library that must be imported for it to apply. Specifically, given a specific type and specific interface, impls that can match can only be in libraries that must have been imported to name that type or interface. This is achieved with the orphan rule . Orphan rule: Some name from the type structure of an impl declaration must be defined in the same library as the impl , that is some name must be local . Only the implementing interface and types (self type and type parameters) in the type structure are relevant here; an interface mentioned in a constraint is not sufficient since it need not be imported . Since Carbon in addition requires there be no cyclic library dependencies, we conclude that there is at most one library that can define impls with a particular type structure. Overlap rule Given a specific concrete type, say Foo(bool, i32) , and an interface, say Bar(String, f32) , the overlap rule picks, among all the matching impls, which type structure is considered \"most specific\" to use as the implementation of that type for that interface. Given two different type structures of impls matching a query, for example: impl Foo(?, i32) as Bar(String, ?) impl Foo(?, ?) as Bar(String, f32) We pick the type structure with a non- ? at the first difference as most specific. Here we see a difference between Foo(?, i32) and Foo(?, ?) , so we select the one with Foo(?, i32) , ignoring the fact that it has another ? later in its type structure This rule corresponds to a depth-first traversal of the type tree to identify the first difference, and then picking the most specific choice at that difference. Prioritization rule Since at most one library can define impls with a given type structure, all impls with a given type structure must be in the same library. Furthermore by the impl declaration access rules , they will be defined in the API file for the library if they could match any query from outside the library. If there is more than one impl with that type structure, they must be defined or declared together in a prioritization block. Once a type structure is selected for a query, the first impl in the prioritization block that matches is selected. Open question: How are prioritization blocks written? A block starts with a keyword like match_first or impl_priority and then a sequence of impl declarations inside matching curly braces { ... } . match_first { // If T is Foo prioritized ahead of T is Bar impl forall [T:! Foo] T as Bar { ... } impl forall [T:! Baz] T as Bar { ... } } Open question: How do we pick between two different prioritization blocks when they contain a mixture of type structures? There are three options: Prioritization blocks implicitly define all non-empty intersections of contained impls, which are then selected by their type structure. The compiler first picks the impl with the type pattern most favored for the query, and then picks the definition of the highest priority matching impl in the same prioritization block. All the impls in a prioritization block are required to have the same type structure, at a cost in expressivity. To see the difference between the first two options, consider two libraries with type structures as follows: Library B has impl (A, ?, ?, D) as I and impl (?, B, ?, D) as I in the same prioritization block. Library C has impl (A, ?, C, ?) as I . For the query (A, B, C, D) as I , using the intersection rule, library B is considered to have the intersection impl with type structure impl (A, B, ?, D) as I which is the most specific. If we instead just considered the rules mentioned explicitly, then impl (A, ?, C, ?) as I from library C is the most specific. The advantage of the implicit intersection rule is that if library B is changed to add an impl with type structure impl (A, B, ?, D) as I , it won't shift which library is serving that query. Acyclic rule A cycle is when a query, such as \"does type T implement interface I ?\", considers an impl that might match, and whether that impl matches is ultimately dependent on whether that query is true. These are cycles in the graph of (type, interface) pairs where there is an edge from pair A to pair B if whether type A implements interface A determines whether type B implements interface B. The test for whether something forms a cycle needs to be precise enough, and not erase too much information when considering this graph, that these impls are not considered to form cycles with themselves: impl forall [T:! Printable] Optional(T) as Printable; impl forall [T:! Type, U:! ComparableTo(T)] U as ComparableTo(Optional(T)); Example: If T implements ComparableWith(U) , then U should implement ComparableWith(T) . external impl forall [U:! Type, T:! ComparableWith(U)] U as ComparableWith(T); This is a cycle where which types implement ComparableWith determines which types implement the same interface. Example: Cycles can create situations where there are multiple ways of selecting impls that are inconsistent with each other. Consider an interface with two blanket impl declarations: class Y {} class N {} interface True {} impl Y as True {} interface Z(T:! Type) { let Cond:! Type; } match_first { impl forall [T:! Type, U:! Z(T) where .Cond is True] T as Z(U) where .Cond = N { } impl forall [T:! Type, U:! Type] T as Z(U) where .Cond = Y { } } What is i8.(Z(i16).Cond) ? It depends on which of the two blanket impls are selected. An implementation of Z(i16) for i8 could come from the first blanket impl with T == i8 and U == i16 if i16 is Z(i8) and i16.(Z(i8).Cond) == Y . This condition is satisfied if i16 implements Z(i8) using the second blanket impl. In this case, i8.(Z(i16).Cond) == N . Equally well Z(i8) could be implemented for i16 using the first blanket impl and Z(i16) for i8 using the second. In this case, i8.(Z(i16).Cond) == Y . There is no reason to to prefer one of these outcomes over the other. Example: Further, cycles can create contradictions in the type system: class A {} class B {} class C {} interface D(T:! Type) { let Cond:! Type; } match_first { impl forall [T:! Type, U:! D(T) where .Cond = B] T as D(U) where .Cond = C { } impl forall [T:! Type, U:! D(T) where .Cond = A] T as D(U) where .Cond = B { } impl forall [T:! Type, U:! Type] T as D(U) where .Cond = A { } } What is i8.(D(i16).Cond) ? The answer is determined by which blanket impl is selected to implement D(i16) for i8 : If the third blanket impl is selected, then i8.(D(i16).Cond) == A . This implies that i16.(D(i8).Cond) == B using the second blanket impl. If that is true, though, then our first impl choice was incorrect, since the first blanket impl applies and is higher priority. So i8.(D(i16).Cond) == C . But that means that i16 as D(i8) can't use the second blanket impl. For the second blanket impl to be selected, so i8.(D(i16).Cond) == B , i16.(D(i8).Cond) would have to be A . This happens when i16 implements D(i8) using the third blanket impl. However, i8.(D(i16).Cond) == B means that there is a higher priority implementation of D(i8).Cond for i16 . In either case, we arrive at a contradiction. The workaround for this problem is to either split an interface in the cycle in two, with a blanket implementation of one from the other, or move some of the criteria into a named constraint . Concern: Cycles could be spread out across libraries with no dependencies between them. This means there can be problems created by a library that are only detected by its users. Open question: Should Carbon reject cycles in the absence of a query? The two options here are: Combining impls gives you an immediate error if there exists queries using those impls that have cycles. Only when a query reveals a cyclic dependency is an error reported. Open question: In the second case, should we ignore cycles if they don't affect the result of the query? For example, the cycle might be among implementations that are lower priority. Termination rule It is possible to define a set of impls where there isn't a cycle, but the graph is infinite. Without some rule to prevent exhaustive exploration of the graph, determining whether a type implements an interface could run forever. Example: It could be that A implements B , so A is B if Optional(A) is B , if Optional(Optional(A)) is B , and so on. This could be the result of a single impl: impl forall [A:! Type where Optional(.Self) is B] A as B { ... } This problem can also result from a chain of impls, as in A is B if A* is C , if Optional(A) is B , and so on. Rust solves this problem by imposing a recursion limit, much like C++ compilers use to terminate template recursion. This goes against Carbon's goal of predictability in generics , but at this time there are no known alternatives. Unfortunately, the approach Carbon uses to avoid undecidability for type equality, providing an explicit proof in the source , can't be used here. The code triggering the query asking whether some type implements an interface will typically be generic code with know specific knowledge about the types involved, and won't be in a position to provide a manual proof that the implementation should exist. Open question: Is there some restriction on impl declarations that would allow our desired use cases, but allow the compiler to detect non-terminating cases? Perhaps there is some sort of complexity measure Carbon can require doesn't increase when recursing? final impls There are cases where knowing that a parameterized impl won't be specialized is particularly valuable. This could let the compiler know the return type of a generic function call, such as using an operator: // Interface defining the behavior of the prefix-* operator interface Deref { let Result:! Type; fn DoDeref[me: Self]() -> Result; } // Types implementing `Deref` class Ptr(T:! Type) { ... external impl as Deref where .Result = T { fn DoDeref[me: Self]() -> Result { ... } } } class Optional(T:! Type) { ... external impl as Deref where .Result = T { fn DoDeref[me: Self]() -> Result { ... } } } fn F[T:! Type](x: T) { // uses Ptr(T) and Optional(T) in implementation } The concern is the possibility of specializing Optional(T) as Deref or Ptr(T) as Deref for a more specific T means that the compiler can't assume anything about the return type of Deref.DoDeref calls. This means F would in practice have to add a constraint, which is both verbose and exposes what should be implementation details: fn F[T:! Type where Optional(T).(Deref.Result) == .Self and Ptr(T).(Deref.Result) == .Self](x: T) { // uses Ptr(T) and Optional(T) in implementation } To mark an impl as not able to be specialized, prefix it with the keyword final : class Ptr(T:! Type) { ... // Note: added `final` final external impl as Deref where .Result = T { fn DoDeref[me: Self]() -> Result { ... } } } class Optional(T:! Type) { ... // Note: added `final` final external impl as Deref where .Result = T { fn DoDeref[me: Self]() -> Result { ... } } } // \u274c Illegal: external impl Ptr(i32) as Deref { ... } // \u274c Illegal: external impl Optional(i32) as Deref { ... } This prevents any higher-priority impl that overlaps a final impl from being defined. Further, if the Carbon compiler sees a matching final impl, it can assume it won't be specialized so it can use the assignments of the associated types in that impl definition. fn F[T:! Type](x: T) { var p: Ptr(T) = ...; // *p has type `T` var o: Optional(T) = ...; // *o has type `T` } Libraries that can contain final impls To prevent the possibility of two unrelated libraries defining conflicting impls, Carbon restricts which libraries may declare an impl as final to only: the library declaring the impl's interface and the library declaring the root of the Self type. This means: A blanket impl with type structure impl ? as MyInterface(...) may only be defined in the same library as MyInterface . An impl with type structure impl MyType(...) as MyInterface(...) may be defined in the library with MyType or MyInterface . These restrictions ensure that the Carbon compiler can locally check that no higher-priority impl is defined superseding a final impl. An impl with type structure impl MyType(...) as MyInterface(...) defined in the library with MyType must import the library defining MyInterface , and so will be able to see any final blanket impls. A blanket impl with type structure impl ? as MyInterface(...ParameterType(...)...) may be defined in the library with ParameterType , but that library must import the library defining MyInterface , and so will be able to see any final blanket impls that might overlap. A final impl with type structure impl MyType(...) as MyInterface(...) would be given priority over any overlapping blanket impl defined in the ParameterType library. An impl with type structure impl MyType(...ParameterType(...)...) as MyInterface(...) may be defined in the library with ParameterType , but that library must import the libraries defining MyType and MyInterface , and so will be able to see any final impls that might overlap. Comparison to Rust Rust has been designing a specialization feature, but it has not been completed. Luckily, Rust team members have done a lot of blogging during their design process, so Carbon can benefit from the work they have done. However, getting specialization to work for Rust is complicated by the need to maintain compatibility with existing Rust code. This motivates a number of Rust rules where Carbon can be simpler. As a result there are both similarities and differences between the Carbon and Rust plans: A Rust impl defaults to not being able to be specialized, with a default keyword used to opt-in to allowing specialization, reflecting the existing code base developed without specialization. Carbon impls default to allowing specialization, with restrictions on which may be declared final . Since Rust impls are not specializable by default, generic functions can assume that if a matching blanket impl is found, the associated types from that impl will be used. In Carbon, if a generic function requires an associated type to have a particular value, the function commonly will need to state that using an explicit constraint. Carbon will not have the \"fundamental\" attribute used by Rust on types or traits, as described in Rust RFC 1023: \"Rebalancing Coherence\" . Carbon will not use \"covering\" rules, as described in Rust RFC 2451: \"Re-Rebalancing Coherence\" and Little Orphan Impls: The covered rule . Like Rust, Carbon does use ordering, favoring the Self type and then the parameters to the interface in left-to-right order, see Rust RFC 1023: \"Rebalancing Coherence\" and Little Orphan Impls: The ordered rule , but the specifics are different. Carbon is not planning to support any inheritance of implementation between impls. This is more important to Rust since Rust does not support class inheritance for implementation reuse. Rust has considered multiple approaches here, see Aaron Turon: \"Specialize to Reuse\" and Supporting blanket impls in specialization . Supporting blanket impls in specialization proposes a specialization rule for Rust that considers type structure before other constraints, as in Carbon, though the details differ. Rust has more orphan restrictions to avoid there being cases where it is ambiguous which impl should be selected. Carbon instead has picked a total ordering on type structures, picking one as higher priority even without one being more specific in the sense of only applying to a subset of types. Forward declarations and cyclic references Interfaces, named constraints, and their implementations may be forward declared and then later defined. This is needed to allow cyclic references, for example when declaring the edges and nodes of a graph. It is also a tool that may be used to make code more readable. The interface , named constraint , and implementation sections describe the syntax for their definition , which consists of a declaration followed by a body contained in curly braces { ... } . A forward declaration is a declaration followed by a semicolon ; . A forward declaration is a promise that the entity being declared will be defined later. Between the first declaration of an entity, which may be in a forward declaration or the first part of a definition, and the end of the definition the interface or implementation is called incomplete . There are additional restrictions on how the name of an incomplete entity may be used. Declaring interfaces and named constraints The declaration for an interface or named constraint consists of: an optional access-control keyword like private , the keyword introducer interface , constraint , or template constraint , the name of the interface or constraint, and the parameter list, if any. The name of an interface or constraint can not be used until its first declaration is complete. In particular, it is illegal to use the name of the interface in its parameter list. There is a workaround for the use cases when this would come up. An expression forming a constraint, such as C & D , is incomplete if any of the interfaces or constraints used in the expression are incomplete. A constraint expression using a where clause , like C where ... , is invalid if C is incomplete, since there is no way to look up member names of C that appear after where . An interface or named constraint may be forward declared subject to these rules: The definition must be in the same file as the declaration. Only the first declaration may have an access-control keyword. An incomplete interface or named constraint may be used as constraints in declarations of types, functions, interfaces, or named constraints. This includes an impl as or extends declaration inside an interface or named constraint, but excludes specifying the values for associated constants because that would involve name lookup into the incomplete constraint. An attempt to define the body of a generic function using an incomplete interface or named constraint is illegal. An attempt to call a generic function using an incomplete interface or named constraint in its signature is illegal. Any name lookup into an incomplete interface or named constraint is an error. For example, it is illegal to attempt to access a member of an interface using MyInterface.MemberName or constrain a member using a where clause. Declaring implementations The declaration of an interface implementation consists of: optional modifier keywords final , external , the keyword introducer impl , an optional deduced parameter list in square brackets [ ... ] , a type, including an optional parameter pattern, the keyword as , and a type-of-type , including an optional parameter pattern and where clause assigning associated constants and associated types . An implementation of an interface for a type may be forward declared subject to these rules: The definition must be in the same library as the declaration. They must either be in the same file, or the declaration can be in the API file and the definition in an impl file. Future work: Carbon may require the definition of parameterized impls to be in the API file, to support separate compilation. If there is both a forward declaration and a definition, only the first declaration must specify the assignment of associated constants with a where clause. Later declarations may omit the where clause by writing where _ instead. You may forward declare an implementation of a defined interface but not an incomplete interface. This allows the assignment of associated constants in the impl declaration to be verified. An impl forward declaration may be for any declared type, whether it is incomplete or defined. Note that this does not apply to impl as declarations in an interface or named constraint definition, as those are considered interface requirements not forward declarations. Every internal implementation must be declared (or defined) inside the scope of the class definition. It may also be declared before the class definition or defined afterwards. Note that the class itself is incomplete in the scope of the class definition, but member function bodies defined inline are processed as if they appeared immediately after the end of the outermost enclosing class . For coherence , we require that any impl that matches an impl lookup query in the same file, must be declared before the query. This can be done with a definition or a forward declaration. Matching and agreeing Carbon needs to determine if two declarations match in order to say which definition a forward declaration corresponds to and to verify that nothing is defined twice. Declarations that match must also agree, meaning they are consistent with each other. Interface and named constraint declarations match if their names are the same after name and alias resolution. To agree: The introducer keyword or keywords much be the same. The types and order of parameters in the parameter list, if any, must match. The parameter names may be omitted, but if they are included in both declarations, they must match. Types agree if they correspond to the same expression tree, after name and alias resolution and canonicalization of parentheses. Note that no other evaluation of type expressions is performed. Interface implementation declarations match if the type and interface expressions match: If the type part is omitted, it is rewritten to Self in the context of the declaration. Self is rewritten to its meaning in the scope it is used. In a class scope, this should match the type name and optional parameter expression after class . So in class MyClass extends MyBase { ... } , Self is rewritten to MyClass . In class Vector(T:! Movable) { ... } , Self is rewritten to Vector(T:! Movable) . Types match if they have the same name after name and alias resolution and the same parameters, or are the same type parameter. Interfaces match if they have the same name after name and alias resolution and the same parameters. Note that a named constraint that is equivalent to an interface, as in constraint Equivalent { extends MyInterface; } , is not considered to match. For implementations to agree: The presence of modifier keywords such as external before impl must match between a forward declaration and definition. If either declaration includes a where clause, they must both include one. If neither uses where _ , they must match in that they produce the associated constants with the same values considered separately. Declaration examples // Forward declaration of interfaces interface Interface1; interface Interface2; interface Interface3; interface Interface4; interface Interface5; interface Interface6; // Forward declaration of class type class MyClass; // \u274c Illegal: Can't declare implementation of incomplete // interface. // external impl MyClass as Interface1; // Definition of interfaces that were previously declared interface Interface1 { let T1:! Type; } interface Interface2 { let T2:! Type; } interface Interface3 { let T3:! Type; } interface Interface4 { let T4:! Type; } // Forward declaration of external implementations external impl MyClass as Interface1 where .T1 = i32; external impl MyClass as Interface2 where .T2 = bool; // Forward declaration of an internal implementation impl MyClass as Interface3 where .T3 = f32; impl MyClass as Interface4 where .T4 = String; interface Interface5 { let T5:! Type; } interface Interface6 { let T6:! Type; } // Definition of the previously declared class type class MyClass { // Definition of previously declared external impl. // Note: no need to repeat assignments to associated // constants. external impl as Interface1 where _ { } // Definition of previously declared internal impl. // Note: allowed even though `MyClass` is incomplete. // Note: allowed but not required to repeat `where` // clause. impl as Interface3 where .T3 = f32 { } // Redeclaration of previously declared internal impl. // Every internal implementation must be declared in // the class definition. impl as Interface4 where _; // Forward declaration of external implementation. external impl MyClass as Interface5 where .T5 = u64; // Forward declaration of internal implementation. impl MyClass as Interface6 where .T6 = u8; } // It would be legal to move the following definitions // from the API file to the implementation file for // this library. // Definition of previously declared external impls. external impl MyClass as Interface2 where _ { } external impl MyClass as Interface5 where _ { } // Definition of previously declared internal impls. impl MyClass as Interface4 where _ { } impl MyClass as Interface6 where _ { } Example of declaring interfaces with cyclic references In this example, Node has an EdgeType associated type that is constrained to implement Edge , and Edge has a NodeType associated type that is constrained to implement Node . Furthermore, the NodeType of an EdgeType is the original type, and the other way around. This is accomplished by naming and then forward declaring the constraints that can't be stated directly: // Forward declare interfaces used in // parameter lists of constraints. interface Edge; interface Node; // Forward declare named constraints used in // interface definitions. private constraint EdgeFor(N:! Node); private constraint NodeFor(E:! Edge); // Define interfaces using named constraints. interface Edge { let NodeType:! NodeFor(Self); fn Head[me: Self]() -> NodeType; } interface Node { let EdgeType:! EdgeFor(Self); fn Edges[me: Self]() -> Vector(EdgeType); } // Now that the interfaces are defined, can // refer to members of the interface, so it is // now legal to define the named constraints. constraint EdgeFor(N:! Node) { extends Edge where .NodeType == N; } constraint NodeFor(E:! Edge) { extends Node where .EdgeType == E; } Interfaces with parameters constrained by the same interface To work around the restriction about not being able to name an interface in its parameter list , instead include that requirement in the body of the interface. // Want to require that `T` satisfies `CommonType(Self)`, // but that can't be done in the parameter list. interface CommonType(T:! Type) { let Result:! Type; // Instead add the requirement inside the definition. impl T as CommonType(Self); } Note however that CommonType is still incomplete inside its definition, so no constraints on members of CommonType are allowed. interface CommonType(T:! Type) { let Result:! Type; // \u274c Illegal: `CommonType` is incomplete impl T as CommonType(Self) where .Result == Result; } Instead, a forward-declared named constraint can be used in place of the constraint that can only be defined later. This is the same strategy used to work around cyclic references . private constraint CommonTypeResult(T:! Type, R:! Type); interface CommonType(T:! Type) { let Result:! Type; // \u2705 Allowed: `CommonTypeResult` is incomplete, but // no members are accessed. impl T as CommonTypeResult(Self, Result); } constraint CommonTypeResult(T:! Type, R:! Type) { extends CommonType(T) where .Result == R; } Interface members with definitions Interfaces may provide definitions for members, such as a function body for an associated function or method or a value for an associated constant. If these definitions may be overridden in implementations, they are called \"defaults\" and prefixed with the default keyword. Otherwise they are called \"final members\" and prefixed with the final keyword. Interface defaults An interface may provide a default implementation of methods in terms of other methods in the interface. interface Vector { fn Add[me: Self](b: Self) -> Self; fn Scale[me: Self](v: f64) -> Self; // Default definition of `Invert` calls `Scale`. default fn Invert[me: Self]() -> Self { return me.Scale(-1.0); } } A default function or method may also be defined out of line, later in the same file as the interface definition: interface Vector { fn Add[me: Self](b: Self) -> Self; fn Scale[me: Self](v: f64) -> Self; default fn Invert[me: Self]() -> Self; } // `Vector` is considered complete at this point, // even though `Vector.Invert` is still incomplete. fn Vector.Invert[me: Self]() -> Self { return me.Scale(-1.0); } An impl of that interface for a type may omit a definition of Invert to use the default, or provide a definition to override the default. Interface defaults are helpful for evolution , as well as reducing boilerplate. Defaults address the gap between the minimum necessary for a type to provide the desired functionality of an interface and the breadth of API that developers desire. As an example, in Rust the iterator trait only has one required method but dozens of \"provided methods\" with defaults. Defaults may also be provided for associated constants, such as associated types, and interface parameters, using the = <default value> syntax. interface Add(Right:! Type = Self) { default let Result:! Type = Self; fn DoAdd[me: Self](right: Right) -> Result; } impl String as Add() { // Right == Result == Self == String fn DoAdd[me: Self](right: Self) -> Self; } Note that Self is a legal default value for an associated type or type parameter. In this case the value of those names is not determined until Self is, so Add() is equivalent to the constraint: // Equivalent to Add() constraint AddDefault { extends Add(Self); } Note also that the parenthesis are required after Add , even when all parameters are left as their default values. More generally, default expressions may reference other associated types or Self as parameters to type constructors. For example: interface Iterator { let Element:! Type; default let Pointer:! Type = Element*; } Carbon does not support providing a default implementation of a required interface. interface TotalOrder { fn TotalLess[me: Self](right: Self) -> bool; // \u274c Illegal: May not provide definition // for required interface. impl as PartialOrder { fn PartialLess[me: Self](right: Self) -> bool { return me.TotalLess(right); } } } The workaround for this restriction is to use a blanket impl instead: interface TotalOrder { fn TotalLess[me: Self](right: Self) -> bool; impl as PartialOrder; } external impl forall [T:! TotalOrder] T as PartialOrder { fn PartialLess[me: Self](right: Self) -> bool { return me.TotalLess(right); } } Note that by the orphan rule , this blanket impl must be defined in the same library as PartialOrder . Comparison with other languages: Rust supports specifying defaults for methods , interface parameters , and associated constants . Rust has found them valuable. final members As an alternative to providing a definition of an interface member as a default, members marked with the final keyword will not allow that definition to be overridden in impls. interface TotalOrder { fn TotalLess[me: Self](right: Self) -> bool; final fn TotalGreater[me: Self](right: Self) -> bool { return right.TotalLess(me); } } class String { impl as TotalOrder { fn TotalLess[me: Self](right: Self) -> bool { ... } // \u274c Illegal: May not provide definition of final // method `TotalGreater`. fn TotalGreater[me: Self](right: Self) -> bool { ... } } } interface Add(T:! Type = Self) { // `AddWith` *always* equals `T` final let AddWith:! Type = T; // Has a *default* of `Self` let Result:! Type = Self; fn DoAdd[me: Self](right: AddWith) -> Result; } Final members may also be defined out-of-line: interface TotalOrder { fn TotalLess[me: Self](right: Self) -> bool; final fn TotalGreater[me: Self](right: Self) -> bool; } // `TotalOrder` is considered complete at this point, even // though `TotalOrder.TotalGreater` is not yet defined. fn TotalOrder.TotalGreater[me: Self](right: Self) -> bool { return right.TotalLess(me); } There are a few reasons for this feature: When overriding would be inappropriate. Matching the functionality of non-virtual methods in base classes, so interfaces can be a replacement for inheritance. Potentially reduce dynamic dispatch when using the interface in a DynPtr . Note that this applies to associated entities, not interface parameters. Interface requiring other interfaces revisited Recall that an interface can require another interface be implemented for the type , as in: interface Iterable { impl as Equatable; // ... } This states that the type implementing the interface Iterable , which in this context is called Self , must also implement the interface Equatable . As is done with conditional conformance , we allow another type to be specified between impl and as to say some type other than Self must implement an interface. For example, interface IntLike { impl i32 as As(Self); // ... } says that if Self implements IntLike , then i32 must implement As(Self) . Similarly, interface CommonTypeWith(T:! Type) { impl T as CommonTypeWith(Self); // ... } says that if Self implements CommonTypeWith(T) , then T must implement CommonTypeWith(Self) . The previous description of impl as in an interface definition matches the behavior of using a default of Self when the type between impl and as is omitted. So the previous definition of interface Iterable is equivalent to: interface Iterable { // ... impl Self as Equatable; // Equivalent to: impl as Equatable; } When implementing an interface with an impl as requirement, that requirement must be satisfied by an implementation in an imported library, an implementation somewhere in the same file, or a constraint in the impl declaration. Implementing the requiring interface is a promise that the requirement will be implemented. This is like a forward declaration of an impl except that the definition can be broader instead of being required to match exactly. // `Iterable` requires `Equatable`, so there must be some // impl of `Equatable` for `Vector(i32)` in this file. external impl Vector(i32) as Iterable { ... } fn RequiresEquatable[T:! Equatable](x: T) { ... } fn ProcessVector(v: Vector(i32)) { // \u2705 Allowed since `Vector(i32)` is known to // implement `Equatable`. RequiresEquatable(v); } // Satisfies the requirement that `Vector(i32)` must // implement `Equatable` since `i32` is `Equatable`. external impl forall [T:! Equatable] Vector(T) as Equatable { ... } In some cases, the interface's requirement can be trivially satisfied by the implementation itself, as in: impl forall [T:! Type] T as CommonTypeWith(T) { ... } Here is an example where the requirement of interface Iterable that the type implements interface Equatable is satisfied by a constraint in the impl declaration: class Foo(T:! Type) {} // This is allowed because we know that an `impl Foo(T) as Equatable` // will exist for all types `T` for which this impl is used, even // though there's neither an imported impl nor an impl in this file. external impl forall [T:! Type where Foo(T) is Equatable] Foo(T) as Iterable {} This might be used to provide an implementation of Equatable for types that already satisfy the requirement of implementing Iterable : class Bar {} external impl Foo(Bar) as Equatable {} // Gives `Foo(Bar) is Iterable` using the blanket impl of // `Iterable` for `Foo(T)`. Requirements with where constraints An interface implementation requirement with a where clause is harder to satisfy. Consider an interface B that has a requirement that interface A is also implemented. interface A(T:! Type) { let Result:! Type; } interface B(T:! Type) { impl as A(T) where .Result == i32; } An implementation of B for a set of types can only be valid if there is a visible implementation of A with the same T parameter for those types with the .Result associated type set to i32 . That is not sufficient , though, unless the implementation of A can't be specialized, either because it is marked final or is not parameterized . Implementations in other libraries can't make A be implemented for fewer types, but can cause .Result to have a different assignment. Observing a type implements an interface An observe declaration can be used to show that two types are equal so code can pass type checking without explicitly writing casts, without requiring the compiler to do a unbounded search that may not terminate. An observe declaration can also be used to show that a type implements an interface, in cases where the compiler will not work this out for itself. Observing interface requirements One situation where this occurs is when there is a chain of interfaces requiring other interfaces . During the impl validation done during type checking, Carbon will only consider the interfaces that are direct requirements of the interfaces the type is known to implement. An observe...is declaration can be used to add an interface that is a direct requirement to the set of interfaces whose direct requirements will be considered for that type. This allows a developer to provide a proof that there is a sequence of requirements that demonstrate that a type implements an interface, as in this example: interface A { } interface B { impl as A; } interface C { impl as B; } interface D { impl as C; } fn RequiresA[T:! A](x: T); fn RequiresC[T:! C](x: T); fn RequiresD[T:! D](x: T) { // \u2705 Allowed: `D` directly requires `C` to be implemented. RequiresC(x); // \u274c Illegal: No direct connection between `D` and `A`. // RequiresA(x); // `T` is `D` and `D` directly requires `C` to be // implemented. observe T is C; // `T` is `C` and `C` directly requires `B` to be // implemented. observe T is B; // \u2705 Allowed: `T` is `B` and `B` directly requires // `A` to be implemented. RequiresA(x); } Note that observe statements do not affect the selection of impls during code generation. For coherence, the impl used for a (type, interface) pair must always be the same, independent of context. The termination rule governs when compilation may fail when the compiler can't determine the impl to select. Observing blanket impls An observe...is declaration can also be used to observe that a type implements an interface because there is a blanket impl in terms of requirements a type is already known to satisfy. Without an observe declaration, Carbon will only use blanket impls that are directly satisfied. interface A { } interface B { } interface C { } interface D { } impl forall [T:! A] T as B { } impl forall [T:! B] T as C { } impl forall [T:! C] T as D { } fn RequiresD(T:! D)(x: T); fn RequiresB(T:! B)(x: T); fn RequiresA(T:! A)(x: T) { // \u2705 Allowed: There is a blanket implementation // of `B` for types implementing `A`. RequiresB(x); // \u274c Illegal: No implementation of `D` for type // `T` implementing `A` // RequiresD(x); // There is a blanket implementation of `B` for // types implementing `A`. observe T is B; // There is a blanket implementation of `C` for // types implementing `B`. observe T is C; // \u2705 Allowed: There is a blanket implementation // of `D` for types implementing `C`. RequiresD(x); } In the case of an error, a quality Carbon implementation will do a deeper search for chains of requirements and blanket impls and suggest observe declarations that would make the code compile if any solution is found. Operator overloading Operations are overloaded for a type by implementing an interface specific to that interface for that type. For example, types implement the Negatable interface to overload the unary - operator: // Unary `-`. interface Negatable { let Result:! Type = Self; fn Negate[me: Self]() -> Result; } Expressions using operators are rewritten into calls to these interface methods. For example, -x would be rewritten to x.(Negatable.Negate)() . The interfaces and rewrites used for a given operator may be found in the expressions design . Question-for-leads issue #1058 defines the naming scheme for these interfaces. Binary operators Binary operators will have an interface that is parameterized based on the second operand. For example, to say a type may be converted to another type using an as expression, implement the As interface : interface As(Dest:! Type) { fn Convert[me: Self]() -> Dest; } The expression x as U is rewritten to x.(As(U).Convert)() . Note that the parameterization of the interface means it can be implemented multiple times to support multiple operand types. Unlike as , for most binary operators the interface's argument will be the type of the right-hand operand instead of its value . Consider an interface for a binary operator like * : // Binary `*`. interface MultipliableWith(U:! Type) { let Result:! Type = Self; fn Multiply[me: Self](other: U) -> Result; } A use of binary * in source code will be rewritten to use this interface: var left: Meters = ...; var right: f64 = ...; var result: auto = left * right; // Equivalent to: var equivalent: left.(MultipliableWith(f64).Result) = left.(MultipliableWith(f64).Multiply)(right); Note that if the types of the two operands are different, then swapping the order of the operands will result in a different implementation being selected. It is up to the developer to make those consistent when that is appropriate. The standard library will provide adapters for defining the second implementation from the first, as in: interface ComparableWith(RHS:! Type) { fn Compare[me: Self](right: RHS) -> CompareResult; } adapter ReverseComparison (T:! Type, U:! ComparableWith(RHS)) for T { impl as ComparableWith(U) { fn Compare[me: Self](right: RHS) -> CompareResult { return ReverseCompareResult(right.Compare(me)); } } } external impl SongByTitle as ComparableWith(SongTitle); external impl SongTitle as ComparableWith(SongByTitle) = ReverseComparison(SongTitle, SongByTitle); In some cases the reverse operation may not be defined. For example, a library might support subtracting a vector from a point, but not the other way around. Further note that even if the reverse implementation exists, the impl prioritization rule might not pick it. For example, if we have two types that support comparison with anything implementing an interface that the other implements: interface IntLike { fn AsInt[me: Self]() -> i64; } class EvenInt { ... } external impl EvenInt as IntLike; external impl EvenInt as ComparableWith(EvenInt); // Allow `EvenInt` to be compared with anything that // implements `IntLike`, in either order. external impl forall [T:! IntLike] EvenInt as ComparableWith(T); external impl forall [T:! IntLike] T as ComparableWith(EvenInt); class PositiveInt { ... } external impl PositiveInt as IntLike; external impl PositiveInt as ComparableWith(PositiveInt); // Allow `PositiveInt` to be compared with anything that // implements `IntLike`, in either order. external impl forall [T:! IntLike] PositiveInt as ComparableWith(T); external impl forall [T:! IntLike] T as ComparableWith(PositiveInt); Then it will favor selecting the implementation based on the type of the left-hand operand: var even: EvenInt = ...; var positive: PositiveInt = ...; // Uses `EvenInt as ComparableWith(T)` impl if (even < positive) { ... } // Uses `PositiveInt as ComparableWith(T)` impl if (positive > even) { ... } like operator for implicit conversions Because the type of the operands is directly used to select the implementation to use, there are no automatic implicit conversions, unlike with function or method calls. Given both a method and an interface implementation for multiplying by a value of type f64 : class Meters { fn Scale[me: Self](s: f64) -> Self; } // \"Implementation One\" external impl Meters as MultipliableWith(f64) where .Result = Meters { fn Multiply[me: Self](other: f64) -> Result { return me.Scale(other); } } the method will work with any argument that can be implicitly converted to f64 but the operator overload will only work with values that have the specific type of f64 : var height: Meters = ...; var scale: f32 = 1.25; // \u2705 Allowed: `scale` implicitly converted // from `f32` to `f64`. var allowed: Meters = height.Scale(scale); // \u274c Illegal: `Meters` doesn't implement // `MultipliableWith(f32)`. var illegal: Meters = height * scale; The workaround is to define a parameterized implementation that performs the conversion. The implementation is for types that implement the ImplicitAs interface . // \"Implementation Two\" external impl forall [T:! ImplicitAs(f64)] Meters as MultipliableWith(T) where .Result = Meters { fn Multiply[me: Self](other: T) -> Result { // Carbon will implicitly convert `other` from type // `T` to `f64` to perform this call. return me.(Meters.(MultipliableWith(f64).Multiply))(other); } } // \u2705 Allowed: uses `Meters as MultipliableWith(T)` impl // with `T == f32` since `f32 is ImplicitAs(f64)`. var now_allowed: Meters = height * scale; Observe that the prioritization rule will still prefer the unparameterized impl when there is an exact match. To reduce the boilerplate needed to support these implicit conversions when defining operator overloads, Carbon has the like operator. This operator can only be used in the type or type-of-type part of an impl declaration, as part of a forward declaration or definition, in a place of a type. // Notice `f64` has been replaced by `like f64` // compared to \"implementation one\" above. external impl Meters as MultipliableWith(like f64) where .Result = Meters { fn Multiply[me: Self](other: f64) -> Result { return me.Scale(other); } } This impl definition actually defines two implementations. The first is the same as this definition with like f64 replaced by f64 , giving something equivalent to \"implementation one\". The second implementation replaces the like f64 with a parameter that ranges over types that can be implicitly converted to f64 , equivalent to \"implementation two\". In general, each like adds one additional impl. There is always the impl with all of the like expressions replaced by their arguments with the definition supplied in the source code. In addition, for each like expression, there is an impl with it replaced by a new parameter. These additional impls will delegate to the main impl, which will trigger implicit conversions according to Carbon's ordinary implicit conversion rules . In this example, there are two uses of like , producing three implementations external impl like Meters as MultipliableWith(like f64) where .Result = Meters { fn Multiply[me: Self](other: f64) -> Result { return me.Scale(other); } } is equivalent to \"implementation one\", \"implementation two\", and: external impl forall [T:! ImplicitAs(Meters)] T as MultipliableWith(f64) where .Result = Meters { fn Multiply[me: Self](other: f64) -> Result { // Will implicitly convert `me` to `Meters` in order to // match the signature of this `Multiply` method. return me.(Meters.(MultipliableWith(f64).Multiply))(other); } } like may be used in forward declarations in a way analogous to impl definitions. external impl like Meters as MultipliableWith(like f64) where .Result = Meters; } is equivalent to: // All `like`s removed. Same as the declaration part of // \"implementation one\", without the body of the definition. external impl Meters as MultipliableWith(f64) where .Result = Meters; // First `like` replaced with a wildcard. external impl forall [T:! ImplicitAs(Meters)] T as MultipliableWith(f64) where .Result = Meters; // Second `like` replaced with a wildcard. Same as the // declaration part of \"implementation two\", without the // body of the definition. external impl forall [T:! ImplicitAs(f64)] Meters as MultipliableWith(T) where .Result = Meters; In addition, the generated impl definition for a like is implicitly injected at the end of the (unique) source file in which the impl is first declared. That is, it is injected in the API file if the impl is declared in an API file, and in the sole impl file declaring the impl otherwise. This means an impl declaration using like in an API file also makes the parameterized definition If one impl declaration uses like , other declarations must use like in the same way to match. The like operator may be nested, as in: external impl like Vector(like String) as Printable; Which will generate implementations with declarations: external impl Vector(String) as Printable; external impl forall [T:! ImplicitAs(Vector(String))] T as Printable; external impl forall [T:! ImplicitAs(String)] Vector(T) as Printable; The generated implementations must be legal or the like is illegal. For example, it must be legal to define those impls in this library by the orphan rule . In addition, the generated impl definitions must only require implicit conversions that are guaranteed to exist. For example, there existing an implicit conversion from T to String does not imply that there is one from Vector(T) to Vector(String) , so the following use of like is illegal: // \u274c Illegal: Can't convert a value with type // `Vector(T:! ImplicitAs(String))` // to `Vector(String)` for `me` // parameter of `Printable.Print`. external impl Vector(like String) as Printable; Since the additional implementation definitions are generated eagerly, these errors will be reported in the file with the first declaration. The argument to like must either not mention any type parameters, or those parameters must be able to be determined due to being repeated outside of the like expression. // \u2705 Allowed: no parameters external impl like Meters as Printable; // \u274c Illegal: No other way to determine `T` external impl forall [T:! IntLike] like T as Printable; // \u274c Illegal: `T` being used in a `where` clause // is insufficient. external impl forall [T:! IntLike] like T as MultipliableWith(i64) where .Result = T; // \u274c Illegal: `like` can't be used in a `where` // clause. external impl Meters as MultipliableWith(f64) where .Result = like Meters; // \u2705 Allowed: `T` can be determined by another // part of the query. external impl forall [T:! IntLike] like T as MultipliableWith(T) where .Result = T; external impl forall [T:! IntLike] T as MultipliableWith(like T) where .Result = T; // \u2705 Allowed: Only one `like` used at a time, so this // is equivalent to the above two examples. external impl forall [T:! IntLike] like T as MultipliableWith(like T) where .Result = T; Parameterized types Types may have generic parameters. Those parameters may be used to specify types in the declarations of its members, such as data fields, member functions, and even interfaces being implemented. For example, a container type might be parameterized by the type of its elements: class HashMap( KeyType:! Hashable & EqualityComparable & Movable, ValueType:! Movable) { // `Self` is `HashMap(KeyType, ValueType)`. // Parameters may be used in function signatures. fn Insert[addr me: Self*](k: KeyType, v: ValueType); // Parameters may be used in field types. private var buckets: Vector((KeyType, ValueType)); // Parameters may be used in interfaces implemented. impl as Container where .ElementType = (KeyType, ValueType); impl as ComparableWith(HashMap(KeyType, ValueType)); } Note that, unlike functions, every parameter to a type must either be generic or template, using :! or template...:! , not dynamic, with a plain : . Two types are the same if they have the same name and the same arguments. Carbon's manual type equality approach means that the compiler may not always be able to tell when two type expressions are equal without help from the user, in the form of observe declarations . This means Carbon will not in general be able to determine when types are unequal. Unlike an interface's parameters , a type's parameters may be deduced , as in: fn ContainsKey[KeyType:! Movable, ValueType:! Movable] (haystack: HashMap(KeyType, ValueType), needle: KeyType) -> bool { ... } fn MyMapContains(s: String) { var map: HashMap(String, i32) = ((\"foo\", 3), (\"bar\", 5)); // \u2705 Deduces `KeyType` = `String` from the types of both arguments. // Deduces `ValueType` = `i32` from the type of the first argument. return ContainsKey(map, s); } Note that restrictions on the type's parameters from the type's declaration can be implied constraints on the function's parameters. Specialization Specialization is used to improve performance in specific cases when a general strategy would be inefficient. For example, you might use binary search for containers that support random access and keep their contents in sorted order but linear search in other cases. Types, like functions, may not be specialized directly in Carbon. This effect can be achieved, however, through delegation. For example, imagine we have a parameterized class Optional(T) that has a default storage strategy that works for all T , but for some types we have a more efficient approach. For pointers we can use a null value to represent \"no pointer\", and for booleans we can support True , False , and None in a single byte. Clients of the optional library may want to add additional specializations for their own types. We make an interface that represents \"the storage of Optional(T) for type T ,\" written here as OptionalStorage : interface OptionalStorage { let Storage:! Type; fn MakeNone() -> Storage; fn Make(x: Self) -> Storage; fn IsNone(x: Storage) -> bool; fn Unwrap(x: Storage) -> Self; } The default implementation of this interface is provided by a blanket implementation : // Default blanket implementation impl forall [T:! Movable] T as OptionalStorage where .Storage = (bool, T) { ... } This implementation can then be specialized for more specific type patterns: // Specialization for pointers, using nullptr == None final external impl forall [T:! Type] T* as OptionalStorage where .Storage = Array(Byte, sizeof(T*)) { ... } // Specialization for type `bool`. final external impl bool as OptionalStorage where .Storage = Byte { ... } Further, libraries can implement OptionalStorage for their own types, assuming the interface is not marked private . Then the implementation of Optional(T) can delegate to OptionalStorage for anything that can vary with T : class Optional(T:! Movable) { fn None() -> Self { return {.storage = T.(OptionalStorage.MakeNone)()}; } fn Some(x: T) -> Self { return {.storage = T.(OptionalStorage.Make)(x)}; } ... private var storage: T.(OptionalStorage.Storage); } Note that the constraint on T is just Movable , not Movable & OptionalStorage , since the Movable requirement is sufficient to guarantee that some implementation of OptionalStorage exists for T . Carbon does not require callers of Optional , even generic callers, to specify that the argument type implements OptionalStorage : // \u2705 Allowed: `T` just needs to be `Movable` to form `Optional(T)`. // A `T:! OptionalStorage` constraint is not required. fn First[T:! Movable & Eq](v: Vector(T)) -> Optional(T); Adding OptionalStorage to the constraints on the parameter to Optional would obscure what types can be used as arguments. OptionalStorage is an implementation detail of Optional and need not appear in its public API. In this example, a let is used to avoid repeating OptionalStorage in the definition of Optional , since it has no name conflicts with the members of Movable : class Optional(T:! Movable) { private let U:! Movable & OptionalStorage = T; fn None() -> Self { return {.storage = U.MakeNone()}; } fn Some(x: T) -> Self { return {.storage = u.Make(x)}; } ... private var storage: U.Storage; } Future work Dynamic types Generics provide enough structure to support runtime dispatch for values with types that vary at runtime, without giving up type safety. Both Rust and Swift have demonstrated the value of this feature. Runtime type parameters This feature is about allowing a function's type parameter to be passed in as a dynamic (non-generic) parameter. All values of that type would still be required to have the same type. Runtime type fields Instead of passing in a single type parameter to a function, we could store a type per value. This changes the data layout of the value, and so is a somewhat more invasive change. It also means that when a function operates on multiple values they could have different real types. Abstract return types This lets you return an anonymous type implementing an interface from a function. In Rust this is the impl Trait return type . In Swift, there are discussions about implementing this feature under the name \"reverse generics\" or \"opaque result types\": 1 , 2 , 3 , 4 , Swift is considering spelling this <V: Collection> V or some Collection . Evolution There are a collection of use cases for making different changes to interfaces that are already in use. These should be addressed either by describing how they can be accomplished with existing generics features, or by adding features. In addition, evolution from (C++ or Carbon) templates to generics needs to be supported and made safe. Testing The idea is that you would write tests alongside an interface that validate the expected behavior of any type implementing that interface. Impls with state A feature we might consider where an impl itself can have state. Generic associated types and higher-ranked types This would be some way to express the requirement that there is a way to go from a type to an implementation of an interface parameterized by that type. Generic associated types Generic associated types are about when this is a requirement of an interface. These are also called \"associated type constructors.\" Higher-ranked types Higher-ranked types are used to represent this requirement in a function signature. They can be emulated using generic associated types . Field requirements We might want to allow interfaces to express the requirement that any implementing type has a particular field. This would be to match the expressivity of inheritance, which can express \"all subtypes start with this list of fields.\" Bridge for C++ customization points See details in the goals document . Variadic arguments Some facility for allowing a function to generically take a variable number of arguments. Range constraints on generic integers We currently only support where clauses on type-of-types. We may want to also support constraints on generic integers. The constraint with the most expected value is the ability to do comparisons like < , or >= . For example, you might constrain the N member of NSpacePoint using an expression like PointT:! NSpacePoint where 2 <= .N and .N <= 3 . The concern here is supporting this at compile time with more benefit than complexity. For example, we probably don't want to support integer-range based types at runtime, and there are also concerns about reasoning about comparisons between multiple generic integer parameters. For example, if J < K and K <= L , can we call a function that requires J < L ? There is also a secondary syntactic concern about how to write this kind of constraint on a parameter, as opposed to an associated type, as in N:! u32 where ___ >= 2 . References #553: Generics details part 1 #731: Generics details 2: adapters, associated types, parameterized interfaces #818: Constraints for generics (generics details 3) #931: Generic impls access (details 4) #920: Generic parameterized impls (details 5) #950: Generic details 6: remove facets #983: Generic details 7: final impls #990: Generics details 8: interface default and final members #1013: Generics: Set associated constants using where constraints #1084: Generics details 9: forward declarations #1088: Generic details 10: interface-implemented requirements #1144: Generic details 11: operator overloading #1146: Generic details 12: parameterized types #1327: Generics: impl forall","title":"Generics: Details"},{"location":"design/generics/details/#generics-details","text":"","title":"Generics: Details"},{"location":"design/generics/details/#table-of-contents","text":"Overview Interfaces Implementing interfaces Implementing multiple interfaces External impl Qualified member names and compound member access Access Generics Return type Implementation model Interfaces recap Type-of-types Named constraints Subtyping between type-of-types Combining interfaces by anding type-of-types Interface requiring other interfaces Interface extension extends and impl with named constraints Diamond dependency issue Use case: overload resolution Adapting types Adapter compatibility Extending adapter Use case: Using independent libraries together Use case: Defining an impl for use by other types Use case: Private impl Use case: Accessing external names Adapter with stricter invariants Associated constants Associated class functions Associated types Implementation model Parameterized interfaces Impl lookup Parameterized named constraints Where constraints Constraint use cases Set an associated constant to a specific value Same type constraints Set an associated type to a specific value Equal generic types Satisfying both type-of-types Type bound for associated type Type bounds on associated types in declarations Type bounds on associated types in interfaces Combining constraints Recursive constraints Parameterized type implements interface Another type implements parameterized interface Implied constraints Must be legal type argument constraints Referencing names in the interface being defined Manual type equality observe declarations Other constraints as type-of-types Is a derived class Type compatible with another type Same implementation restriction Example: Multiple implementations of the same interface Example: Creating an impl out of other impls Sized types and type-of-types Implementation model TypeId Destructor constraints Generic let Parameterized impls Impl for a parameterized type Conditional conformance Conditional methods Blanket impls Difference between blanket impls and named constraints Wildcard impls Combinations Lookup resolution and specialization Type structure of an impl declaration Orphan rule Overlap rule Prioritization rule Acyclic rule Termination rule final impls Libraries that can contain final impls Comparison to Rust Forward declarations and cyclic references Declaring interfaces and named constraints Declaring implementations Matching and agreeing Declaration examples Example of declaring interfaces with cyclic references Interfaces with parameters constrained by the same interface Interface members with definitions Interface defaults final members Interface requiring other interfaces revisited Requirements with where constraints Observing a type implements an interface Observing interface requirements Observing blanket impls Operator overloading Binary operators like operator for implicit conversions Parameterized types Specialization Future work Dynamic types Runtime type parameters Runtime type fields Abstract return types Evolution Testing Impls with state Generic associated types and higher-ranked types Generic associated types Higher-ranked types Field requirements Bridge for C++ customization points Variadic arguments Range constraints on generic integers References","title":"Table of contents"},{"location":"design/generics/details/#overview","text":"This document goes into the details of the design of generic type parameters. Imagine we want to write a function parameterized by a type argument. Maybe our function is PrintToStdout and let's say we want to operate on values that have a type for which we have an implementation of the ConvertibleToString interface. The ConvertibleToString interface has a ToString method returning a string. To do this, we give the PrintToStdout function two parameters: one is the value to print, let's call that val , the other is the type of that value, let's call that T . The type of val is T , what is the type of T ? Well, since we want to let T be any type implementing the ConvertibleToString interface, we express that in the \"interfaces are type-of-types\" model by saying the type of T is ConvertibleToString . Since we can figure out T from the type of val , we don't need the caller to pass in T explicitly, so it can be a deduced parameter (also see deduced parameters in the Generics overview doc). Basically, the user passes in a value for val , and the type of val determines T . T still gets passed into the function though, and it plays an important role -- it defines the implementation of the interface. We can think of the interface as defining a struct type whose members are function pointers, and an implementation of an interface as a value of that struct with actual function pointer values. So an implementation is a table of function pointers (one per function defined in the interface) that gets passed into a function as the type argument. For more on this, see the implementation model section below. In addition to function pointer members, interfaces can include any constants that belong to a type. For example, the type's size (represented by an integer constant member of the type) could be a member of an interface and its implementation. There are a few cases why we would include another interface implementation as a member: associated types type parameters interface requirements The function expresses that the type argument is passed in statically , basically generating a separate function body for every different type passed in, by using the \"generic argument\" syntax :! , see the generics section below. The interface contains enough information to type and definition check the function body -- you can only call functions defined in the interface in the function body. Contrast this with making the type a template argument, where you could just use Type instead of an interface and it will work as long as the function is only called with types that allow the definition of the function to compile. The interface bound has other benefits: allows the compiler to deliver clearer error messages, documents expectations, and expresses that a type has certain semantics beyond what is captured in its member function names and signatures. The last piece of the puzzle is calling the function. For a value of type Song to be printed using the PrintToStdout function, Song needs to implement the ConvertibleToString interface. Interface implementations will usually be defined either with the type or with the interface. They may also be defined somewhere else as long as Carbon can be guaranteed to see the definition when needed. For more on this, see the implementing interfaces section below. Unless the implementation of ConvertibleToString for Song is defined as external , every member of ConvertibleToString is also a member of Song . This includes members of ConvertibleToString that are not explicitly named in the impl definition but have defaults. Whether the implementation is defined as internal or external , you may access the ToString function for a Song value s by a writing function call using a qualified member access expression , like s.(ConvertibleToString.ToString)() . If Song doesn't implement an interface or we would like to use a different implementation of that interface, we can define another type that also has the same data representation as Song that has whatever different interface implementations we want. However, Carbon won't implicitly convert to that other type, the user will have to explicitly cast to that type in order to select those alternate implementations. For more on this, see the adapting type section below.","title":"Overview"},{"location":"design/generics/details/#interfaces","text":"An interface , defines an API that a given type can implement. For example, an interface capturing a linear-algebra vector API might have two methods: interface Vector { // Here `Self` means \"the type implementing this interface\". fn Add[me: Self](b: Self) -> Self; fn Scale[me: Self](v: f64) -> Self; } The syntax here is to match how the same members would be defined in a type . Each declaration in the interface defines an associated entity . In this example, Vector has two associated methods, Add and Scale . An interface defines a type-of-type, that is a type whose values are types. The values of an interface are any types implementing the interface, and so provide definitions for all the functions (and other members) declared in the interface.","title":"Interfaces"},{"location":"design/generics/details/#implementing-interfaces","text":"Carbon interfaces are \"nominal\" , which means that types explicitly describe how they implement interfaces. An \"impl\" defines how one interface is implemented for a type. Every associated entity is given a definition. Different types satisfying Vector can have different definitions for Add and Scale , so we say their definitions are associated with what type is implementing Vector . The impl defines what is associated with the type for that interface. Impls may be defined inline inside the type definition: class Point { var x: f64; var y: f64; impl as Vector { // In this scope, \"Self\" is an alias for \"Point\". fn Add[me: Self](b: Self) -> Self { return {.x = a.x + b.x, .y = a.y + b.y}; } fn Scale[me: Self](v: f64) -> Self { return {.x = a.x * v, .y = a.y * v}; } } } Interfaces that are implemented inline contribute to the type's API: var p1: Point = {.x = 1.0, .y = 2.0}; var p2: Point = {.x = 2.0, .y = 4.0}; Assert(p1.Scale(2.0) == p2); Assert(p1.Add(p1) == p2); Note: A type may implement any number of different interfaces, but may provide at most one implementation of any single interface. This makes the act of selecting an implementation of an interface for a type unambiguous throughout the whole program. Comparison with other languages: Rust defines implementations lexically outside of the class definition. This Carbon approach means that a type's API is described by declarations inside the class definition and doesn't change afterwards. References: This interface implementation syntax was accepted in proposal #553 . In particular, see the alternatives considered .","title":"Implementing interfaces"},{"location":"design/generics/details/#implementing-multiple-interfaces","text":"To implement more than one interface when defining a type, simply include an impl block per interface. class Point { var x: f64; var y: f64; impl as Vector { fn Add[me: Self](b: Self) -> Self { ... } fn Scale[me: Self](v: f64) -> Self { ... } } impl as Drawable { fn Draw[me: Self]() { ... } } } In this case, all the functions Add , Scale , and Draw end up a part of the API for Point . This means you can't implement two interfaces that have a name in common (unless you use an external impl for one or both, as described below). class GameBoard { impl as Drawable { fn Draw[me: Self]() { ... } } impl as EndOfGame { // \u274c Error: `GameBoard` has two methods named // `Draw` with the same signature. fn Draw[me: Self]() { ... } fn Winner[me: Self](player: i32) { ... } } } Open question: Should we have some syntax for the case where you want both names to be given the same implementation? It seems like that might be a common case, but we won't really know if this is an important case until we get more experience. class Player { var name: String; impl as Icon { fn Name[me: Self]() -> String { return me.name; } // ... } impl as GameUnit { // Possible syntax options for defining // `GameUnit.Name` as the same as `Icon.Name`: alias Name = Icon.Name; fn Name[me: Self]() -> String = Icon.Name; // ... } }","title":"Implementing multiple interfaces"},{"location":"design/generics/details/#external-impl","text":"Interfaces may also be implemented for a type externally , by using the external impl construct. An external impl does not add the interface's methods to the type. class Point2 { var x: f64; var y: f64; external impl as Vector { // In this scope, `Self` is an alias for `Point2`. fn Add[me: Self](b: Self) -> Self { return {.x = a.x + b.x, .y = a.y + b.y}; } fn Scale[me: Self](v: f64) -> Self { return {.x = a.x * v, .y = a.y * v}; } } } var a: Point2 = {.x = 1.0, .y = 2.0}; // `a` does *not* have `Add` and `Scale` methods: // \u274c Error: a.Add(a.Scale(2.0)); An external impl may be defined out-of-line, by including the name of the existing type before as , which is otherwise optional: class Point3 { var x: f64; var y: f64; } external impl Point3 as Vector { // In this scope, `Self` is an alias for `Point3`. fn Add[me: Self](b: Self) -> Self { return {.x = a.x + b.x, .y = a.y + b.y}; } fn Scale[me: Self](v: f64) -> Self { return {.x = a.x * v, .y = a.y * v}; } } var a: Point3 = {.x = 1.0, .y = 2.0}; // `a` does *not* have `Add` and `Scale` methods: // \u274c Error: a.Add(a.Scale(2.0)); References: The external interface implementation syntax was decided in proposal #553 . In particular, see the alternatives considered . The external impl statement is allowed to be defined in a different library from Point3 , restricted by the coherence/orphan rules that ensure that the implementation of an interface can't change based on imports. In particular, the external impl statement is allowed in the library defining the interface ( Vector in this case) in addition to the library that defines the type ( Point3 here). This (at least partially) addresses the expression problem . Carbon requires impl s defined in a different library to be external so that the API of Point3 doesn't change based on what is imported. It would be particularly bad if two different libraries implemented interfaces with conflicting names that both affected the API of a single type. As a consequence of this restriction, you can find all the names of direct members (those available by simple member access ) of a type in the definition of that type. The only thing that may be in another library is an impl of an interface. You might also use external impl to implement an interface for a type to avoid cluttering the API of that type, for example to avoid a name collision. A syntax for reusing method implementations allows us to do this selectively when needed. In this case, the external impl may be declared lexically inside the class scope. class Point4a { var x: f64; var y: f64; fn Add[me: Self](b: Self) -> Self { return {.x = a.x + b.x, .y = a.y + b.y}; } external impl as Vector { alias Add = Point4a.Add; // Syntax TBD fn Scale[me: Self](v: f64) -> Self { return {.x = a.x * v, .y = a.y * v}; } } } // OR: class Point4b { var x: f64; var y: f64; external impl as Vector { fn Add[me: Self](b: Self) -> Self { return {.x = a.x + b.x, .y = a.y + b.y}; } fn Scale[me: Self](v: f64) -> Self { return {.x = a.x * v, .y = a.y * v}; } } alias Add = Vector.Add; // Syntax TBD } // OR: class Point4c { var x: f64; var y: f64; fn Add[me: Self](b: Self) -> Self { return {.x = a.x + b.x, .y = a.y + b.y}; } } external impl Point4c as Vector { alias Add = Point4c.Add; // Syntax TBD fn Scale[me: Self](v: f64) -> Self { return {.x = a.x * v, .y = a.y * v}; } } Being defined lexically inside the class means that implementation is available to other members defined in the class. For example, it would allow implementing another interface or method that requires this interface to be implemented. Open question: Do implementations need to be defined lexically inside the class to get access to private members, or is it sufficient to be defined in the same library as the class? Rejected alternative: We could allow types to have different APIs in different files based on explicit configuration in that file. For example, we could support a declaration that a given interface or a given method of an interface is \"in scope\" for a particular type in this file. With that declaration, the method could be called using simple member access . This avoids most concerns arising from name collisions between interfaces. It has a few downsides though: It increases variability between files, since the same type will have different APIs depending on these declarations. This makes it harder to copy-paste code between files. It makes reading code harder, since you have to search the file for these declarations that affect name lookup. Comparison with other languages: Both Rust and Swift support external implementation. Swift's syntax does this as an \"extension\" of the original type. In Rust, all implementations are external as in this example . Unlike Swift and Rust, we don't allow a type's API to be modified outside its definition. So in Carbon a type's API is consistent no matter what is imported, unlike Swift and Rust.","title":"External impl"},{"location":"design/generics/details/#qualified-member-names-and-compound-member-access","text":"Given a value of type Point3 and an interface Vector implemented for that type, you can access the methods from that interface using a qualified member access expression whether or not the implementation is done externally with an external impl declaration. The qualified member access expression writes the member's qualified name in the parentheses of the compound member access syntax : var p1: Point3 = {.x = 1.0, .y = 2.0}; var p2: Point3 = {.x = 2.0, .y = 4.0}; Assert(p1.(Vector.Scale)(2.0) == p2); Assert(p1.(Vector.Add)(p1) == p2); Note that the name in the parens is looked up in the containing scope, not in the names of members of Point3 . So if there was another interface Drawable with method Draw defined in the Plot package also implemented for Point3 , as in: package Plot; import Points; interface Drawable { fn Draw[me: Self](); } external impl Points.Point3 as Drawable { ... } You could access Draw with a qualified name: import Plot; import Points; var p: Points.Point3 = {.x = 1.0, .y = 2.0}; p.(Plot.Drawable.Draw)(); Comparison with other languages: This is intended to be analogous to, in C++, adding ClassName:: in front of a member name to disambiguate, such as names defined in both a parent and child class .","title":"Qualified member names and compound member access"},{"location":"design/generics/details/#access","text":"An impl must be visible to all code that can see both the type and the interface being implemented: If either the type or interface is private to a single file, then since the only way to define the impl is to use that private name, the impl must be defined private to that file as well. Otherwise, if the type or interface is private but declared in an API file, then the impl must be declared in the same file so the existence of that impl is visible to all files in that library. Otherwise, the impl must be defined in the public API file of the library, so it is visible in all places that might use it. No access control modifiers are allowed on impl declarations, an impl is always visible to the intersection of the visibility of all names used in the declaration of the impl .","title":"Access"},{"location":"design/generics/details/#generics","text":"Here is a function that can accept values of any type that has implemented the Vector interface: fn AddAndScaleGeneric[T:! Vector](a: T, b: T, s: f64) -> T { return a.Add(b).Scale(s); } var v: Point = AddAndScaleGeneric(a, w, 2.5); Here T is a type whose type is Vector . The :! syntax means that T is a generic parameter . That means it must be known to the caller, but we will only use the information present in the signature of the function to type check the body of AddAndScaleGeneric 's definition. In this case, we know that any value of type T implements the Vector interface and so has an Add and a Scale method. References: The :! syntax was accepted in proposal #676 . Names are looked up in the body of AddAndScaleGeneric for values of type T in Vector . This means that AddAndScaleGeneric is interpreted as equivalent to adding a Vector qualification to replace all simple member accesses of T : fn AddAndScaleGeneric[T:! Vector](a: T, b: T, s: Double) -> T { return a.(Vector.Add)(b).(Vector.Scale)(s); } With these qualifications, the function can be type-checked for any T implementing Vector . This type checking is equivalent to type checking the function with T set to an archetype of Vector . An archetype is a placeholder type considered to satisfy its constraint, which is Vector in this case, and no more. It acts as the most general type satisfying the interface. The effect of this is that an archetype of Vector acts like a supertype of any T implementing Vector . For name lookup purposes, an archetype is considered to have implemented its constraint internally . The only oddity is that the archetype may have different names for members than specific types T that implement interfaces from the constraint externally . This difference in names can also occur for supertypes in C++, for example members in a derived class can hide members in the base class with the same name, though it is not that common for it to come up in practice. The behavior of calling AddAndScaleGeneric with a value of a specific type like Point is to set T to Point after all the names have been qualified. // AddAndScaleGeneric with T = Point fn AddAndScaleForPoint(a: Point, b: Point, s: Double) -> Point { return a.(Vector.Add)(b).(Vector.Scale)(s); } This qualification gives a consistent interpretation to the body of the function even when the type supplied by the caller implements the interface externally , as Point2 does: // AddAndScaleGeneric with T = Point2 fn AddAndScaleForPoint2(a: Point2, b: Point2, s: Double) -> Point2 { // \u2705 This works even though `a.Add(b).Scale(s)` wouldn't. return a.(Vector.Add)(b).(Vector.Scale)(s); }","title":"Generics"},{"location":"design/generics/details/#return-type","text":"From the caller's perspective, the return type is the result of substituting the caller's values for the generic parameters into the return type expression. So AddAndScaleGeneric called with Point values returns a Point and called with Point2 values returns a Point2 . So looking up a member on the resulting value will look in Point or Point2 rather than Vector . This is part of realizing the goal that generic functions can be used in place of regular functions without changing the return type that callers see . In this example, AddAndScaleGeneric can be substituted for AddAndScaleForPoint and AddAndScaleForPoint2 without affecting the return types. This requires the return value to be converted to the type that the caller expects instead of the erased type used inside the generic function. A generic caller of a generic function performs the same substitution process to determine the return type, but the result may be generic. In this example of calling a generic from another generic, fn DoubleThreeTimes[U:! Vector](a: U) -> U { return AddAndScaleGeneric(a, a, 2.0).Scale(2.0); } the return type of AddAndScaleGeneric is found by substituting in the U from DoubleThreeTimes for the T from AddAndScaleGeneric in the return type expression of AddAndScaleGeneric . U is an archetype of Vector , and so implements Vector internally and therefore has a Scale method. If U had a more specific type, the return value would have the additional capabilities of U . For example, given a parameterized type GeneralPoint implementing Vector , and a function that takes a GeneralPoint and calls AddAndScaleGeneric with it: class GeneralPoint(C:! Numeric) { external impl as Vector { ... } fn Get[me: Self](i: i32) -> C; } fn CallWithGeneralPoint[C:! Numeric](p: GeneralPoint(C)) -> C { // `AddAndScaleGeneric` returns `T` and in these calls `T` is // deduced to be `GeneralPoint(C)`. // \u274c Illegal: AddAndScaleGeneric(p, p, 2.0).Scale(2.0); // `GeneralPoint(C)` implements `Vector` externally, and so // does not have a `Scale` method. // \u2705 Allowed: `GeneralPoint(C)` has a `Get` method AddAndScaleGeneric(p, p, 2.0).Get(0); // \u2705 Allowed: `GeneralPoint(C)` implements `Vector` // externally, and so has a `Vector.Scale` method. // `Vector.Scale` returns `Self` which is `GeneralPoint(C)` // again, and so has a `Get` method. return AddAndScaleGeneric(p, p, 2.0).(Vector.Scale)(2.0).Get(0); } The result of the call to AddAndScaleGeneric from CallWithGeneralPoint has type GeneralPoint(C) and so has a Get method and a Vector.Scale method. But, in contrast to how DoubleThreeTimes works, since Vector is implemented externally the return value in this case does not directly have a Scale method.","title":"Return type"},{"location":"design/generics/details/#implementation-model","text":"A possible model for generating code for a generic function is to use a witness table to represent how a type implements an interface: Interfaces are types of witness tables. Impls are witness table values. The compiler rewrites functions with an implicit type argument ( fn Foo[InterfaceName:! T](...) ) to have an actual argument with type determined by the interface, and supplied at the callsite using a value determined by the impl. For the example above, the Vector interface could be thought of defining a witness table type like: class Vector { // `Self` is the representation type, which is only // known at compile time. var Self:! Type; // `fnty` is **placeholder** syntax for a \"function type\", // so `Add` is a function that takes two `Self` parameters // and returns a value of type `Self`. var Add: fnty(a: Self, b: Self) -> Self; var Scale: fnty(a: Self, v: f64) -> Self; } The impl of Vector for Point would be a value of this type: var VectorForPoint: Vector = { .Self = Point, // `lambda` is **placeholder** syntax for defining a // function value. .Add = lambda(a: Point, b: Point) -> Point { return {.x = a.x + b.x, .y = a.y + b.y}; }, .Scale = lambda(a: Point, v: f64) -> Point { return {.x = a.x * v, .y = a.y * v}; }, }; Finally we can define a generic function and call it, like AddAndScaleGeneric from the \"Generics\" section by making the witness table an explicit argument to the function: fn AddAndScaleGeneric (t:! Vector, a: t.Self, b: t.Self, s: f64) -> t.Self { return t.Scale(t.Add(a, b), s); } // Point implements Vector. var v: Point = AddAndScaleGeneric(VectorForPoint, a, w, 2.5); The rule is that generic arguments (declared using :! ) are passed at compile time, so the actual value of the t argument here can be used to generate the code for AddAndScaleGeneric . So AddAndScaleGeneric is using a static-dispatch witness table . Note that this implementation strategy only works for impls that the caller knows the callee needs.","title":"Implementation model"},{"location":"design/generics/details/#interfaces-recap","text":"Interfaces have a name and a definition. The definition of an interface consists of a set of declarations. Each declaration defines a requirement for any impl that is in turn a capability that consumers of that impl can rely on. Typically those declarations also have names, useful for both saying how the impl satisfies the requirement and accessing the capability. Interfaces are \"nominal\" , which means their name is significant. So two interfaces with the same body definition but different names are different, just like two classes with the same definition but different names are considered different types. For example, lets say we define another interface, say LegoFish , with the same Add and Scale method signatures. Implementing Vector would not imply an implementation of LegoFish , because the impl definition explicitly refers to the name Vector . An interface's name may be used in a few different contexts: to define an impl for a type , as a namespace name in a qualified name , and as a type-of-type for a generic type parameter . While interfaces are examples of type-of-types, type-of-types are a more general concept, for which interfaces are a building block.","title":"Interfaces recap"},{"location":"design/generics/details/#type-of-types","text":"A type-of-type consists of a set of requirements and a set of names. Requirements are typically a set of interfaces that a type must satisfy, though other kinds of requirements are added below. The names are aliases for qualified names in those interfaces. An interface is one particularly simple example of a type-of-type. For example, Vector as a type-of-type has a set of requirements consisting of the single interface Vector . Its set of names consists of Add and Scale which are aliases for the corresponding qualified names inside Vector as a namespace. The requirements determine which types are values of a given type-of-type. The set of names in a type-of-type determines the API of a generic type value and define the result of member access into the type-of-type. This general structure of type-of-types holds not just for interfaces, but others described in the rest of this document.","title":"Type-of-types"},{"location":"design/generics/details/#named-constraints","text":"If the interfaces discussed above are the building blocks for type-of-types, generic named constraints describe how they may be composed together. Unlike interfaces which are nominal, the name of a named constraint is not a part of its value. Two different named constraints with the same definition are equivalent even if they have different names. This is because types don't explicitly specify which named constraints they implement, types automatically implement any named constraints they can satisfy. A named constraint definition can contain interface requirements using impl declarations and names using alias declarations. Note that this allows us to declare the aspects of a type-of-type directly. constraint VectorLegoFish { // Interface implementation requirements impl as Vector; impl as LegoFish; // Names alias Scale = Vector.Scale; alias VAdd = Vector.Add; alias LFAdd = LegoFish.Add; } We don't expect developers to directly define many named constraints, but other constructs we do expect them to use will be defined in terms of them. For example, we can define the Carbon builtin Type as: constraint Type { } That is, Type is the type-of-type with no requirements (so matches every type), and defines no names. fn Identity[T:! Type](x: T) -> T { // Can accept values of any type. But, since we know nothing about the // type, we don't know about any operations on `x` inside this function. return x; } var i: i32 = Identity(3); var s: String = Identity(\"string\"); Aside: We can define auto as syntactic sugar for (template _:! Type) . This definition allows you to use auto as the type for a local variable whose type can be statically determined by the compiler. It also allows you to use auto as the type of a function parameter, to mean \"accepts a value of any type, and this function will be instantiated separately for every different type.\" This is consistent with the use of auto in the C++20 Abbreviated function template feature . In general, the declarations in constraint definition match a subset of the declarations in an interface . Named constraints used with generics, as opposed to templates, should only include required interfaces and aliases to named members of those interfaces. To declare a named constraint that includes other declarations for use with template parameters, use the template keyword before constraint . Method, associated type, and associated function requirements may only be declared inside a template constraint . Note that a generic constraint ignores the names of members defined for a type, but a template constraint can depend on them. There is an analogy between declarations used in a constraint and in an interface definition. If an interface I has (non- alias ) declarations X , Y , and Z , like so: interface I { X; Y; Z; } Then a type implementing I would have impl as I with definitions for X , Y , and Z , as in: class ImplementsI { // ... impl as I { X { ... } Y { ... } Z { ... } } } But the corresponding constraint or template constraint , S : // or template constraint S { constraint S { X; Y; Z; } would match any type with definitions for X , Y , and Z directly: class ImplementsS { // ... X { ... } Y { ... } Z { ... } } TODO: Move the template constraint and auto content to the template design document, once it exists.","title":"Named constraints"},{"location":"design/generics/details/#subtyping-between-type-of-types","text":"There is a subtyping relationship between type-of-types that allows calls of one generic function from another as long as it has a subset of the requirements. Given a generic type variable T with type-of-type I1 , it satisfies a type-of-type I2 as long as the requirements of I1 are a superset of the requirements of I2 . This means a value x of type T may be passed to functions requiring types to satisfy I2 , as in this example: interface Printable { fn Print[me: Self](); } interface Renderable { fn Draw[me: Self](); } constraint PrintAndRender { impl as Printable; impl as Renderable; } constraint JustPrint { impl as Printable; } fn PrintIt[T2:! JustPrint](x2: T2) { x2.(Printable.Print)(); } fn PrintDrawPrint[T1:! PrintAndRender](x1: T1) { // x1 implements `Printable` and `Renderable`. x1.(Printable.Print)(); x1.(Renderable.Draw)(); // Can call `PrintIt` since `T1` satisfies `JustPrint` since // it implements `Printable` (in addition to `Renderable`). PrintIt(x1); }","title":"Subtyping between type-of-types"},{"location":"design/generics/details/#combining-interfaces-by-anding-type-of-types","text":"In order to support functions that require more than one interface to be implemented, we provide a combination operator on type-of-types, written & . This operator gives the type-of-type with the union of all the requirements and the union of the names minus any conflicts. interface Printable { fn Print[me: Self](); } interface Renderable { fn Center[me: Self]() -> (i32, i32); fn Draw[me: Self](); } // `Printable & Renderable` is syntactic sugar for this type-of-type: constraint { impl as Printable; impl as Renderable; alias Print = Printable.Print; alias Center = Renderable.Center; alias Draw = Renderable.Draw; } fn PrintThenDraw[T:! Printable & Renderable](x: T) { // Can use methods of `Printable` or `Renderable` on `x` here. x.Print(); // Same as `x.(Printable.Print)();`. x.Draw(); // Same as `x.(Renderable.Draw)();`. } class Sprite { // ... impl as Printable { fn Print[me: Self]() { ... } } impl as Renderable { fn Center[me: Self]() -> (i32, i32) { ... } fn Draw[me: Self]() { ... } } } var s: Sprite = ...; PrintThenDraw(s); Any conflicting names between the two types are replaced with a name that is an error to use. interface Renderable { fn Center[me: Self]() -> (i32, i32); fn Draw[me: Self](); } interface EndOfGame { fn Draw[me: Self](); fn Winner[me: Self](player: i32); } // `Renderable & EndOfGame` is syntactic sugar for this type-of-type: constraint { impl as Renderable; impl as EndOfGame; alias Center = Renderable.Center; // Open question: `forbidden`, `invalid`, or something else? forbidden Draw message \"Ambiguous, use either `(Renderable.Draw)` or `(EndOfGame.Draw)`.\"; alias Winner = EndOfGame.Winner; } Conflicts can be resolved at the call site using a qualified member access expression , or by defining a named constraint explicitly and renaming the methods: constraint RenderableAndEndOfGame { impl as Renderable; impl as EndOfGame; alias Center = Renderable.Center; alias RenderableDraw = Renderable.Draw; alias TieGame = EndOfGame.Draw; alias Winner = EndOfGame.Winner; } fn RenderTieGame[T:! RenderableAndEndOfGame](x: T) { // Calls Renderable.Draw() x.RenderableDraw(); // Calls EndOfGame.Draw() x.TieGame(); } Reserving the name when there is a conflict is part of resolving what happens when you combine more than two type-of-types. If x is forbidden in A , it is forbidden in A & B , whether or not B defines the name x . This makes & associative and commutative, and so it is well defined on sets of interfaces, or other type-of-types, independent of order. Note that we do not consider two type-of-types using the same name to mean the same thing to be a conflict. For example, combining a type-of-type with itself gives itself, MyTypeOfType & MyTypeOfType == MyTypeOfType . Also, given two interface extensions of a common base interface, the sum should not conflict on any names in the common base. Rejected alternative: Instead of using & as the combining operator, we considered using + , like Rust . See #531 for the discussion. Future work: We may want to define another operator on type-of-types for adding requirements to a type-of-type without affecting the names, and so avoid the possibility of name conflicts. Note this means the operation is not commutative. If we call this operator [&] , then A [&] B has the names of A and B [&] A has the names of B . // `Printable [&] Renderable` is syntactic sugar for this type-of-type: constraint { impl as Printable; impl as Renderable; alias Print = Printable.Print; } // `Renderable [&] EndOfGame` is syntactic sugar for this type-of-type: constraint { impl as Renderable; impl as EndOfGame; alias Center = Renderable.Center; alias Draw = Renderable.Draw; } Note that all three expressions A & B , A [&] B , and B [&] A have the same requirements, and so you would be able to switch a function declaration between them without affecting callers. Nothing in this design depends on the [&] operator, and having both & and [&] might be confusing for users, so it makes sense to postpone implementing [&] until we have a demonstrated need. The [&] operator seems most useful for adding requirements for interfaces used for operator overloading , where merely implementing the interface is enough to be able to use the operator to access the functionality. Alternatives considered: See Carbon: Access to interface methods . Comparison with other languages: This & operation on interfaces works very similarly to Rust's + operation, with the main difference being how you qualify names when there is a conflict .","title":"Combining interfaces by anding type-of-types"},{"location":"design/generics/details/#interface-requiring-other-interfaces","text":"Some interfaces will depend on other interfaces being implemented for the same type. For example, in C++, the Container concept requires all containers to also satisfy the requirements of DefaultConstructible , CopyConstructible , EqualityComparable , and Swappable . This is already a capability for type-of-types in general . For consistency we will use the same semantics and syntax as we do for named constraints : interface Equatable { fn Equals[me: Self](rhs: Self) -> bool; } interface Iterable { fn Advance[addr me: Self*]() -> bool; impl as Equatable; } def DoAdvanceAndEquals[T:! Iterable](x: T) { // `x` has type `T` that implements `Iterable`, and so has `Advance`. x.Advance(); // `Iterable` requires an implementation of `Equatable`, // so `T` also implements `Equatable`. x.(Equatable.Equals)(x); } class Iota { impl as Iterable { fn Advance[me: Self]() { ... } } impl as Equatable { fn Equals[me: Self](rhs: Self) -> bool { ... } } } var x: Iota; DoAdvanceAndEquals(x); Like with named constraints, an interface implementation requirement doesn't by itself add any names to the interface, but again those can be added with alias declarations: interface Hashable { fn Hash[me: Self]() -> u64; impl as Equatable; alias Equals = Equatable.Equals; } def DoHashAndEquals[T:! Hashable](x: T) { // Now both `Hash` and `Equals` are available directly: x.Hash(); x.Equals(x); } Comparison with other languages: This feature is called \"Supertraits\" in Rust . Note: The design for this feature is continued in a later section .","title":"Interface requiring other interfaces"},{"location":"design/generics/details/#interface-extension","text":"When implementing an interface, we should allow implementing the aliased names as well. In the case of Hashable above, this includes all the members of Equatable , obviating the need to implement Equatable itself: class Song { impl as Hashable { fn Hash[me: Self]() -> u64 { ... } fn Equals[me: Self](rhs: Self) -> bool { ... } } } var y: Song; DoHashAndEquals(y); This allows us to say that Hashable \"extends\" Equatable , with some benefits: This allows Equatable to be an implementation detail of Hashable . This allows types implementing Hashable to implement all of its API in one place. This reduces the boilerplate for types implementing Hashable . We expect this concept to be common enough to warrant dedicated syntax: interface Equatable { fn Equals[me: Self](rhs: Self) -> bool; } interface Hashable { extends Equatable; fn Hash[me: Self]() -> u64; } // is equivalent to the definition of Hashable from before: // interface Hashable { // impl as Equatable; // alias Equals = Equatable.Equals; // fn Hash[me: Self]() -> u64; // } No names in Hashable are allowed to conflict with names in Equatable (unless those names are marked as upcoming or deprecated as in evolution future work ). Hopefully this won't be a problem in practice, since interface extension is a very closely coupled relationship, but this may be something we will have to revisit in the future. Examples: The C++ Boost.Graph library graph concepts has many refining relationships between concepts. Carbon generics use case: graph library shows how those concepts might be translated into Carbon interfaces. The C++ concepts for containers, iterators, and concurrency include many requirement relationships. Swift protocols, such as Collection . To write an interface extending multiple interfaces, use multiple extends declarations. For example, the BinaryInteger protocol in Swift inherits from CustomStringConvertible , Hashable , Numeric , and Stridable . The SetAlgebra protocol extends Equatable and ExpressibleByArrayLiteral , which would be declared in Carbon: interface SetAlgebra { extends Equatable; extends ExpressibleByArrayLiteral; } Alternative considered: The extends declarations are in the body of the interface definition instead of the header so we can use associated types (defined below) also defined in the body in parameters or constraints of the interface being extended. // A type can implement `ConvertibleTo` many times, using // different values of `T`. interface ConvertibleTo(T:! Type) { ... } // A type can only implement `PreferredConversion` once. interface PreferredConversion { let AssociatedType:! Type; extends ConvertibleTo(AssociatedType); }","title":"Interface extension"},{"location":"design/generics/details/#extends-and-impl-with-named-constraints","text":"The extends declaration makes sense with the same meaning inside a constraint definition, and so is also supported. interface Media { fn Play[me: Self](); } interface Job { fn Run[me: Self](); } constraint Combined { extends Media; extends Job; } This definition of Combined is equivalent to requiring both the Media and Job interfaces being implemented, and aliases their methods. // Equivalent constraint Combined { impl as Media; alias Play = Media.Play; impl as Job; alias Run = Job.Run; } Notice how Combined has aliases for all the methods in the interfaces it requires. That condition is sufficient to allow a type to impl the named constraint: class Song { impl as Combined { fn Play[me: Self]() { ... } fn Run[me: Self]() { ... } } } This is equivalent to implementing the required interfaces directly: class Song { impl as Media { fn Play[me: Self]() { ... } } impl as Job { fn Run[me: Self]() { ... } } } This is just like when you get an implementation of Equatable by implementing Hashable when Hashable extends Equatable . This provides a tool useful for evolution . Conversely, an interface can extend a constraint : interface MovieCodec { extends Combined; fn Load[addr me: Self*](filename: String); } This gives MovieCodec the same requirements and names as Combined , and so is equivalent to: interface MovieCodec { impl as Media; alias Play = Media.Play; impl as Job; alias Run = Job.Run; fn Load[addr me: Self*](filename: String); }","title":"extends and impl with named constraints"},{"location":"design/generics/details/#diamond-dependency-issue","text":"Consider this set of interfaces, simplified from this example generic graph library doc : interface Graph { fn Source[addr me: Self*](e: EdgeDescriptor) -> VertexDescriptor; fn Target[addr me: Self*](e: EdgeDescriptor) -> VertexDescriptor; } interface IncidenceGraph { extends Graph; fn OutEdges[addr me: Self*](u: VertexDescriptor) -> (EdgeIterator, EdgeIterator); } interface EdgeListGraph { extends Graph; fn Edges[addr me: Self*]() -> (EdgeIterator, EdgeIterator); } We need to specify what happens when a graph type implements both IncidenceGraph and EdgeListGraph , since both interfaces extend the Graph interface. class MyEdgeListIncidenceGraph { impl as IncidenceGraph { ... } impl as EdgeListGraph { ... } } The rule is that we need one definition of each method of Graph . Each method though could be defined in the impl block of IncidenceGraph , EdgeListGraph , or Graph . These would all be valid: IncidenceGraph implements all methods of Graph , EdgeListGraph implements none of them. class MyEdgeListIncidenceGraph { impl as IncidenceGraph { fn Source[me: Self](e: EdgeDescriptor) -> VertexDescriptor { ... } fn Target[me: Self](e: EdgeDescriptor) -> VertexDescriptor { ... } fn OutEdges[addr me: Self*](u: VertexDescriptor) -> (EdgeIterator, EdgeIterator) { ... } } impl as EdgeListGraph { fn Edges[addr me: Self*]() -> (EdgeIterator, EdgeIterator) { ... } } } IncidenceGraph and EdgeListGraph implement all methods of Graph between them, but with no overlap. class MyEdgeListIncidenceGraph { impl as IncidenceGraph { fn Source[me: Self](e: EdgeDescriptor) -> VertexDescriptor { ... } fn OutEdges[addr me: Self*](u: VertexDescriptor) -> (EdgeIterator, EdgeIterator) { ... } } impl as EdgeListGraph { fn Target[me: Self](e: EdgeDescriptor) -> VertexDescriptor { ... } fn Edges[addr me: Self*]() -> (EdgeIterator, EdgeIterator) { ... } } } Explicitly implementing Graph . class MyEdgeListIncidenceGraph { impl as Graph { fn Source[me: Self](e: EdgeDescriptor) -> VertexDescriptor { ... } fn Target[me: Self](e: EdgeDescriptor) -> VertexDescriptor { ... } } impl as IncidenceGraph { ... } impl as EdgeListGraph { ... } } Implementing Graph externally. class MyEdgeListIncidenceGraph { impl as IncidenceGraph { ... } impl as EdgeListGraph { ... } } external impl MyEdgeListIncidenceGraph as Graph { fn Source[me: Self](e: EdgeDescriptor) -> VertexDescriptor { ... } fn Target[me: Self](e: EdgeDescriptor) -> VertexDescriptor { ... } } This last point means that there are situations where we can only detect a missing method definition by the end of the file. This doesn't delay other aspects of semantic checking, which will just assume that these methods will eventually be provided. Open question: We could require that the external impl of the required interface be declared lexically in the class scope in this case. That would allow earlier detection of missing definitions.","title":"Diamond dependency issue"},{"location":"design/generics/details/#use-case-overload-resolution","text":"Implementing an extended interface is an example of a more specific match for lookup resolution . For example, this could be used to provide different implementations of an algorithm depending on the capabilities of the iterator being passed in: interface ForwardIntIterator { fn Advance[addr me: Self*](); fn Get[me: Self]() -> i32; } interface BidirectionalIntIterator { extends ForwardIntIterator; fn Back[addr me: Self*](); } interface RandomAccessIntIterator { extends BidirectionalIntIterator; fn Skip[addr me: Self*](offset: i32); fn Difference[me: Self](rhs: Self) -> i32; } fn SearchInSortedList[IterT:! ForwardIntIterator] (begin: IterT, end: IterT, needle: i32) -> bool { ... // does linear search } // Will prefer the following overload when it matches // since it is more specific. fn SearchInSortedList[IterT:! RandomAccessIntIterator] (begin: IterT, end: IterT, needle: i32) -> bool { ... // does binary search } This would be an example of the more general rule that an interface A requiring an implementation of interface B means A is more specific than B .","title":"Use case: overload resolution"},{"location":"design/generics/details/#adapting-types","text":"Since interfaces may only be implemented for a type once, and we limit where implementations may be added to a type, there is a need to allow the user to switch the type of a value to access different interface implementations. Carbon therefore provides a way to create new types compatible with existing types with different APIs, in particular with different interface implementations, by adapting them: interface Printable { fn Print[me: Self](); } interface Comparable { fn Less[me: Self](rhs: Self) -> bool; } class Song { impl as Printable { fn Print[me: Self]() { ... } } } adapter SongByTitle for Song { impl as Comparable { fn Less[me: Self](rhs: Self) -> bool { ... } } } adapter FormattedSong for Song { impl as Printable { fn Print[me: Self]() { ... } } } adapter FormattedSongByTitle for Song { impl as Printable = FormattedSong; impl as Comparable = SongByTitle; } This allows developers to provide implementations of new interfaces (as in SongByTitle ), provide different implementations of the same interface (as in FormattedSong ), or mix and match implementations from other compatible types (as in FormattedSongByTitle ). The rules are: You can add any declaration that you could add to a class except for declarations that would change the representation of the type. This means you can add methods, functions, interface implementations, and aliases, but not fields, base classes, or virtual functions. The adapted type is compatible with the original type, and that relationship is an equivalence class, so all of Song , SongByTitle , FormattedSong , and FormattedSongByTitle end up compatible with each other. Since adapted types are compatible with the original type, you may explicitly cast between them, but there is no implicit conversion between these types. Inside an adapter, the Self type matches the adapter. Members of the original type may be accessed either by a cast: adapter SongByTitle for Song { impl as Comparable { fn Less[me: Self](rhs: Self) -> bool { return (me as Song).Title() < (rhs as Song).Title(); } } } or using a qualified member access expression: adapter SongByTitle for Song { impl as Comparable { fn Less[me: Self](rhs: Self) -> bool { return me.(Song.Title)() < rhs.(Song.Title)(); } } } Comparison with other languages: This matches the Rust idiom called \"newtype\", which is used to implement traits on types while avoiding coherence problems, see here and here . Rust's mechanism doesn't directly support reusing implementations, though some of that is provided by macros defined in libraries. Haskell has a newtype feature as well. Haskell's feature doesn't directly support reusing implementations either, but the most popular compiler provides it as an extension .","title":"Adapting types"},{"location":"design/generics/details/#adapter-compatibility","text":"Consider a type with a generic type parameter, like a hash map: interface Hashable { ... } class HashMap(KeyT:! Hashable, ValueT:! Type) { fn Find[me:Self](key: KeyT) -> Optional(ValueT); // ... } A user of this type will provide specific values for the key and value types: class Song { impl as Hashable { ... } // ... } var play_count: HashMap(Song, i32) = ...; var thriller_count: Optional(i32) = play_count.Find(Song(\"Thriller\")); Since the Find function is generic, it can only use the capabilities that HashMap requires of KeyT and ValueT . This allows us to evaluate when we can convert between two different arguments to a parameterized type. Consider two adapters of Song that implement Hashable : adapter PlayableSong for Song { impl as Hashable = Song; impl as Media { ... } } adapter SongHashedByTitle for Song { impl as Hashable { ... } } Song and PlayableSong have the same implementation of Hashable in addition to using the same data representation. This means that it is safe to convert between HashMap(Song, i32) and HashMap(PlayableSong, i32) , because the implementation of all the methods will use the same implementation of the Hashable interface. Carbon permits this conversion with an explicit cast. On the other hand, SongHashedByTitle has a different implementation of Hashable than Song . So even though Song and SongHashedByTitle are compatible types, HashMap(Song, i32) and HashMap(SongHashedByTitle, i32) are incompatible. This is important because we know that in practice the invariants of a HashMap implementation rely on the hashing function staying the same.","title":"Adapter compatibility"},{"location":"design/generics/details/#extending-adapter","text":"Frequently we expect that the adapter type will want to preserve most or all of the API of the original type. The two most common cases expected are adding and replacing an interface implementation. Users would indicate that an adapter starts from the original type's existing API by using the extends keyword instead of for : class Song { impl as Hashable { ... } impl as Printable { ... } } adapter SongByArtist extends Song { // Add an implementation of a new interface impl as Comparable { ... } // Replace an existing implementation of an interface // with an alternative. impl as Hashable { ... } } The resulting type SongByArtist would: implement Comparable , unlike Song , implement Hashable , but differently than Song , and implement Printable , inherited from Song . Unlike the similar class B extends A notation, adapter B extends A is permitted even if A is a final class. Also, there is no implicit conversion from B to A , matching adapter ... for but unlike class extension. To avoid or resolve name conflicts between interfaces, an impl may be declared external . The names in that interface may then be pulled in individually or renamed using alias declarations. adapter SongRenderToPrintDriver extends Song { // Add a new `Print()` member function. fn Print[me: Self]() { ... } // Avoid name conflict with new `Print` function by making // the implementation of the `Printable` interface external. external impl as Printable = Song; // Make the `Print` function from `Printable` available // under the name `PrintToScreen`. alias PrintToScreen = Printable.Print; }","title":"Extending adapter"},{"location":"design/generics/details/#use-case-using-independent-libraries-together","text":"Imagine we have two packages that are developed independently. Package CompareLib defines an interface CompareLib.Comparable and a generic algorithm CompareLib.Sort that operates on types that implement CompareLib.Comparable . Package SongLib defines a type SongLib.Song . Neither has a dependency on the other, so neither package defines an implementation for CompareLib.Comparable for type SongLib.Song . A user that wants to pass a value of type SongLib.Song to CompareLib.Sort has to define an adapter that provides an implementation of CompareLib.Comparable for SongLib.Song . This adapter will probably use the extends facility of adapters to preserve the SongLib.Song API. import CompareLib; import SongLib; adapter Song extends SongLib.Song { impl as CompareLib.Comparable { ... } } // Or, to keep the names from CompareLib.Comparable out of Song's API: adapter Song extends SongLib.Song { } external impl Song as CompareLib.Comparable { ... } // Or, equivalently: adapter Song extends SongLib.Song { external impl as CompareLib.Comparable { ... } } The caller can either convert SongLib.Song values to Song when calling CompareLib.Sort or just start with Song values in the first place. var lib_song: SongLib.Song = ...; CompareLib.Sort((lib_song as Song,)); var song: Song = ...; CompareLib.Sort((song,));","title":"Use case: Using independent libraries together"},{"location":"design/generics/details/#use-case-defining-an-impl-for-use-by-other-types","text":"Let's say we want to provide a possible implementation of an interface for use by types for which that implementation would be appropriate. We can do that by defining an adapter implementing the interface that is parameterized on the type it is adapting. That impl may then be pulled in using the impl as ... = ...; syntax. For example, given an interface Comparable for deciding which value is smaller: interface Comparable { fn Less[me: Self](rhs: Self) -> bool; } We might define an adapter that implements Comparable for types that define another interface Difference : interface Difference { fn Sub[me:Self](rhs: Self) -> i32; } adapter ComparableFromDifference(T:! Difference) for T { impl as Comparable { fn Less[me: Self](rhs: Self) -> bool { return (me as T).Sub(rhs) < 0; } } } class IntWrapper { var x: i32; impl as Difference { fn Sub[me: Self](rhs: Self) -> i32 { return left.x - right.x; } } impl as Comparable = ComparableFromDifferenceFn(IntWrapper); } TODO: If we support function types, we could potentially pass a function to use to the adapter instead: adapter ComparableFromDifferenceFn (T:! Type, Difference:! fnty(T, T)->i32) for T { impl as Comparable { fn Less[me: Self](rhs: Self) -> bool { return Difference(me, rhs) < 0; } } } class IntWrapper { var x: i32; fn Difference(left: Self, right: Self) { return left.x - right.x; } impl as Comparable = ComparableFromDifferenceFn(IntWrapper, Difference); }","title":"Use case: Defining an impl for use by other types"},{"location":"design/generics/details/#use-case-private-impl","text":"Adapter types can be used when a library publicly exposes a type, but only wants to say that type implements an interface as a private detail internal to the implementation of the type. In that case, instead of implementing the interface for the public type, the library can create a private adapter for that type and implement the interface on that instead. Any member of the class can cast its me parameter to the adapter type when it wants to make use of the private impl. // Public, in API file class Complex64 { // ... fn CloserToOrigin[me: Self](them: Self) -> bool; } // Private adapter ByReal extends Complex64 { // Complex numbers are not generally comparable, // but this comparison function is useful for some // method implementations. impl as Comparable { fn Less[me: Self](that: Self) -> bool { return me.Real() < that.Real(); } } } fn Complex64.CloserToOrigin[me: Self](them: Self) -> bool { var me_mag: ByReal = me * me.Conj() as ByReal; var them_mag: ByReal = them * them.Conj() as ByReal; return me_mag.Less(them_mag); }","title":"Use case: Private impl"},{"location":"design/generics/details/#use-case-accessing-external-names","text":"Consider a case where a function will call several functions from an interface that is implemented externally for a type. interface DrawingContext { fn SetPen[me: Self](...); fn SetFill[me: Self](...); fn DrawRectangle[me: Self](...); fn DrawLine[me: Self](...); ... } external impl Window as DrawingContext { ... } An adapter can make that much more convenient by making a compatible type where the interface is implemented internally . This avoids having to qualify each call to methods in the interface. adapter DrawInWindow for Window { impl as DrawingContext = Window; } fn Render(w: Window) { let d: DrawInWindow = w as DrawInWindow; d.SetPen(...); d.SetFill(...); d.DrawRectangle(...); ... }","title":"Use case: Accessing external names"},{"location":"design/generics/details/#adapter-with-stricter-invariants","text":"Future work: Rust also uses the newtype idiom to create types with additional invariants or other information encoded in the type ( 1 , 2 , 3 ). This is used to record in the type system that some data has passed validation checks, like ValidDate with the same data layout as Date . Or to record the units associated with a value, such as Seconds versus Milliseconds or Feet versus Meters . We should have some way of restricting the casts between a type and an adapter to address this use case.","title":"Adapter with stricter invariants"},{"location":"design/generics/details/#associated-constants","text":"In addition to associated methods, we allow other kinds of associated entities . For consistency, we use the same syntax to describe a constant in an interface as in a type without assigning a value. As constants, they are declared using the let introducer. For example, a fixed-dimensional point type could have the dimension as an associated constant. interface NSpacePoint { let N:! i32; // The following require: 0 <= i < N. fn Get[addr me: Self*](i: i32) -> f64; fn Set[addr me: Self*](i: i32, value: f64); // Associated constants may be used in signatures: fn SetAll[addr me: Self*](value: Array(f64, N)); } An implementation of an interface specifies values for associated constants with a where clause . For example, implementations of NSpacePoint for different types might have different values for N : class Point2D { impl as NSpacePoint where .N = 2 { fn Get[addr me: Self*](i: i32) -> f64 { ... } fn Set[addr me: Self*](i: i32, value: f64) { ... } fn SetAll[addr me: Self*](value: Array(f64, 2)) { ... } } } class Point3D { impl as NSpacePoint where .N = 3 { fn Get[addr me: Self*](i: i32) -> f64 { ... } fn Set[addr me: Self*](i: i32, value: f64) { ... } fn SetAll[addr me: Self*](value: Array(f64, 3)) { ... } } } Multiple assignments to associated constants may be joined using the and keyword. The list of assignments is subject to two restrictions: An implementation of an interface cannot specify a value for a final associated constant. If an associated constant doesn't have a default value , every implementation must specify its value. These values may be accessed as members of the type: Assert(Point2D.N == 2); Assert(Point3D.N == 3); fn PrintPoint[PointT:! NSpacePoint](p: PointT) { for (var i: i32 = 0; i < PointT.N; ++i) { if (i > 0) { Print(\", \"); } Print(p.Get(i)); } } fn ExtractPoint[PointT:! NSpacePoint]( p: PointT, dest: Array(f64, PointT.N)*) { for (var i: i32 = 0; i < PointT.N; ++i) { (*dest)[i] = p.Get(i); } } Comparison with other languages: This feature is also called associated constants in Rust . Aside: In general, the use of :! here means these let declarations will only have compile-time and not runtime storage associated with them.","title":"Associated constants"},{"location":"design/generics/details/#associated-class-functions","text":"To be consistent with normal class function declaration syntax, associated class functions are written using a fn declaration: interface DeserializeFromString { fn Deserialize(serialized: String) -> Self; } class MySerializableType { var i: i32; impl as DeserializeFromString { fn Deserialize(serialized: String) -> Self { return (.i = StringToInt(serialized)); } } } var x: MySerializableType = MySerializableType.Deserialize(\"3\"); fn Deserialize(T:! DeserializeFromString, serialized: String) -> T { return T.Deserialize(serialized); } var y: MySerializableType = Deserialize(MySerializableType, \"4\"); This is instead of declaring an associated constant using let with a function type. Together associated methods and associated class functions are called associated functions , much like together methods and class functions are called member functions .","title":"Associated class functions"},{"location":"design/generics/details/#associated-types","text":"Associated types are associated entities that happen to be types. These are particularly interesting since they can be used in the signatures of associated methods or functions, to allow the signatures of methods to vary from implementation to implementation. We already have one example of this: the Self type discussed in the \"Interfaces\" section . For other cases, we can say that the interface declares that each implementation will provide a type under a specific name. For example: interface StackAssociatedType { let ElementType:! Type; fn Push[addr me: Self*](value: ElementType); fn Pop[addr me: Self*]() -> ElementType; fn IsEmpty[addr me: Self*]() -> bool; } Here we have an interface called StackAssociatedType which defines two methods, Push and Pop . The signatures of those two methods declare them as accepting or returning values with the type ElementType , which any implementer of StackAssociatedType must also define. For example, maybe DynamicArray implements StackAssociatedType : class DynamicArray(T:! Type) { class IteratorType { ... } fn Begin[addr me: Self*]() -> IteratorType; fn End[addr me: Self*]() -> IteratorType; fn Insert[addr me: Self*](pos: IteratorType, value: T); fn Remove[addr me: Self*](pos: IteratorType); // Set the associated type `ElementType` to `T`. impl as StackAssociatedType where .ElementType = T { fn Push[addr me: Self*](value: ElementType) { me->Insert(me->End(), value); } fn Pop[addr me: Self*]() -> ElementType { var pos: IteratorType = me->End(); Assert(pos != me->Begin()); --pos; returned var ret: ElementType = *pos; me->Remove(pos); return var; } fn IsEmpty[addr me: Self*]() -> bool { return me->Begin() == me->End(); } } } Alternatives considered: See other syntax options considered in #731 for specifying associated types . In particular, it was deemed that Swift's approach of inferring the associated type from method signatures in the impl was unneeded complexity. The definition of the StackAssociatedType is sufficient for writing a generic function that operates on anything implementing that interface, for example: fn PeekAtTopOfStack[StackType:! StackAssociatedType](s: StackType*) -> StackType.ElementType { var top: StackType.ElementType = s->Pop(); s->Push(top); return top; } Inside the generic function PeekAtTopOfStack , the ElementType associated type member of StackType is erased. This means StackType.ElementType has the API dictated by the declaration of ElementType in the interface StackAssociatedType . Outside the generic, associated types have the concrete type values determined by impl lookup, rather than the erased version of that type used inside a generic. var my_array: DynamicArray(i32) = (1, 2, 3); // PeekAtTopOfStack's `StackType` is set to `DynamicArray(i32)` // with `StackType.ElementType` set to `i32`. Assert(PeekAtTopOfStack(my_array) == 3); This is another part of achieving the goal that generic functions can be used in place of regular functions without changing the return type that callers see discussed in the return type section . Associated types can also be implemented using a member type . interface Container { let IteratorType:! Iterator; ... } class DynamicArray(T:! Type) { ... impl as Container { class IteratorType { impl Iterator { ... } } ... } } For context, see \"Interface type parameters and associated types\" in the generics terminology document . Comparison with other languages: Both Rust and Swift support associated types.","title":"Associated types"},{"location":"design/generics/details/#implementation-model_1","text":"The associated type can be modeled by a witness table field in the interface's witness table. interface Iterator { fn Advance[addr me: Self*](); } interface Container { let IteratorType:! Iterator; fn Begin[addr me: Self*]() -> IteratorType; } is represented by: class Iterator(Self:! Type) { var Advance: fnty(this: Self*); ... } class Container(Self:! Type) { // Representation type for the iterator. let IteratorType:! Type; // Witness that IteratorType implements Iterator. var iterator_impl: Iterator(IteratorType)*; // Method var Begin: fnty (this: Self*) -> IteratorType; ... }","title":"Implementation model"},{"location":"design/generics/details/#parameterized-interfaces","text":"Associated types don't change the fact that a type can only implement an interface at most once. If instead you want a family of related interfaces, one per possible value of a type parameter, multiple of which could be implemented for a single type, you would use parameterized interfaces . To write a parameterized version of the stack interface, instead of using associated types, write a parameter list after the name of the interface instead of the associated type declaration: interface StackParameterized(ElementType:! Type) { fn Push[addr me: Self*](value: ElementType); fn Pop[addr me: Self*]() -> ElementType; fn IsEmpty[addr me: Self*]() -> bool; } Then StackParameterized(Fruit) and StackParameterized(Veggie) would be considered different interfaces, with distinct implementations. class Produce { var fruit: DynamicArray(Fruit); var veggie: DynamicArray(Veggie); impl as StackParameterized(Fruit) { fn Push[addr me: Self*](value: Fruit) { me->fruit.Push(value); } fn Pop[addr me: Self*]() -> Fruit { return me->fruit.Pop(); } fn IsEmpty[addr me: Self*]() -> bool { return me->fruit.IsEmpty(); } } impl as StackParameterized(Veggie) { fn Push[addr me: Self*](value: Veggie) { me->veggie.Push(value); } fn Pop[addr me: Self*]() -> Veggie { return me->veggie.Pop(); } fn IsEmpty[addr me: Self*]() -> bool { return me->veggie.IsEmpty(); } } } Unlike associated types in interfaces and parameters to types, interface parameters can't be deduced. For example, if we were to rewrite the PeekAtTopOfStack example in the \"associated types\" section for StackParameterized(T) it would generate a compile error: // \u274c Error: can't deduce interface parameter `T`. fn BrokenPeekAtTopOfStackParameterized [T:! Type, StackType:! StackParameterized(T)] (s: StackType*) -> T { ... } This error is because the compiler can not determine if T should be Fruit or Veggie when passing in argument of type Produce* . The function's signature would have to be changed so that the value for T could be determined from the explicit parameters. fn PeekAtTopOfStackParameterized [T:! Type, StackType:! StackParameterized(T)] (s: StackType*, _:! singleton_type_of(T)) -> T { ... } var produce: Produce = ...; var top_fruit: Fruit = PeekAtTopOfStackParameterized(&produce, Fruit); var top_veggie: Veggie = PeekAtTopOfStackParameterized(&produce, Veggie); The pattern _:! singleton_type_of(T) is a placeholder syntax for an expression that will only match T , until issue #578: Value patterns as function parameters is resolved. Using that pattern in the explicit parameter list allows us to make T available earlier in the declaration so it can be passed as the argument to the parameterized interface StackParameterized . This approach is useful for the ComparableTo(T) interface, where a type might be comparable with multiple other types, and in fact interfaces for operator overloads more generally. Example: interface EquatableWith(T:! Type) { fn Equals[me: Self](rhs: T) -> bool; ... } class Complex { var real: f64; var imag: f64; // Can implement this interface more than once // as long as it has different arguments. impl as EquatableWith(Complex) { ... } impl as EquatableWith(f64) { ... } } All interface parameters must be marked as \"generic\", using the :! syntax. This reflects these two properties of these parameters: They must be resolved at compile-time, and so can't be passed regular dynamic values. We allow either generic or template values to be passed in. Note: Interface parameters aren't required to be types, but that is the vast majority of cases. As an example, if we had an interface that allowed a type to define how the tuple-member-read operator would work, the index of the member could be an interface parameter: interface ReadTupleMember(index:! u32) { let T:! Type; // Returns me[index] fn Get[me: Self]() -> T; } This requires that the index be known at compile time, but allows different indices to be associated with different types. Caveat: When implementing an interface twice for a type, the interface parameters are required to always be different. For example: interface Map(FromType:! Type, ToType:! Type) { fn Map[addr me: Self*](needle: FromType) -> Optional(ToType); } class Bijection(FromType:! Type, ToType:! Type) { impl as Map(FromType, ToType) { ... } impl as Map(ToType, FromType) { ... } } // \u274c Error: Bijection has two impls of interface Map(String, String) var oops: Bijection(String, String) = ...; In this case, it would be better to have an adapting type to contain the impl for the reverse map lookup, instead of implementing the Map interface twice: class Bijection(FromType:! Type, ToType:! Type) { impl as Map(FromType, ToType) { ... } } adapter ReverseLookup(FromType:! Type, ToType:! Type) for Bijection(FromType, ToType) { impl as Map(ToType, FromType) { ... } } Comparison with other languages: Rust calls traits with type parameters \"generic traits\" and uses them for operator overloading . Rust uses the term \"type parameters\" for both interface type parameters and associated types. The difference is that interface parameters are \"inputs\" since they determine which impl to use, and associated types are \"outputs\" since they are determined by the impl , but play no role in selecting the impl .","title":"Parameterized interfaces"},{"location":"design/generics/details/#impl-lookup","text":"Let's say you have some interface I(T, U(V)) being implemented for some type A(B(C(D), E)) . To satisfy the orphan rule for coherence , that impl must be defined in some library that must be imported in any code that looks up whether that interface is implemented for that type. This requires that impl is defined in the same library that defines the interface or one of the names needed by the type. That is, the impl must be defined with one of I , T , U , V , A , B , C , D , or E . We further require anything looking up this impl to import the definitions of all of those names. Seeing a forward declaration of these names is insufficient, since you can presumably see forward declarations without seeing an impl with the definition. This accomplishes a few goals: The compiler can check that there is only one definition of any impl that is actually used, avoiding One Definition Rule (ODR) problems. Every attempt to use an impl will see the exact same impl , making the interpretation and semantics of code consistent no matter its context, in accordance with the low context-sensitivity principle . Allowing the impl to be defined with either the interface or the type addresses the expression problem . Note that the rules for specialization do allow there to be more than one impl to be defined for a type, by unambiguously picking one as most specific. References: Implementation coherence is defined in terminology , and is a goal for Carbon . More detail can be found in this appendix with the rationale and alternatives considered .","title":"Impl lookup"},{"location":"design/generics/details/#parameterized-named-constraints","text":"We should also allow the named constraint construct to support parameters. Parameters would work the same way as for interfaces.","title":"Parameterized named constraints"},{"location":"design/generics/details/#where-constraints","text":"So far, we have restricted a generic type parameter by saying it has to implement an interface or a set of interfaces. There are a variety of other constraints we would like to be able to express, such as applying restrictions to its associated types and associated constants. This is done using the where operator that adds constraints to a type-of-type. The where operator can be applied to a type-of-type in a declaration context: // Constraints on function parameters: fn F[V:! D where ...](v: V) { ... } // Constraints on a class parameter: class S(T:! B where ...) { // Constraints on a method: fn G[me: Self, V:! D where ...](v: V); } // Constraints on an interface parameter: interface A(T:! B where ...) { // Constraints on an associated type: let U:! C where ...; // Constraints on an associated method: fn G[me: Self, V:! D where ...](v: V); } We also allow you to name constraints using a where operator in a let or constraint definition. The expressions that can follow the where keyword are described in the \"constraint use cases\" section, but generally look like boolean expressions that should evaluate to true . The result of applying a where operator to a type-of-type is another type-of-type. Note that this expands the kinds of requirements that type-of-types can have from just interface requirements to also include the various kinds of constraints discussed later in this section. In addition, it can introduce relationships between different type variables, such as that a member of one is equal to the member of another. Comparison with other languages: Both Swift and Rust use where clauses on declarations instead of in the expression syntax. These happen after the type that is being constrained has been given a name and use that name to express the constraint. Rust also supports directly passing in the values for associated types when using a trait as a constraint. This is helpful when specifying concrete types for all associated types in a trait in order to make it object safe so it can be used to define a trait object type . Rust is adding trait aliases ( RFC , tracking issue ) to support naming some classes of constraints.","title":"Where constraints"},{"location":"design/generics/details/#constraint-use-cases","text":"","title":"Constraint use cases"},{"location":"design/generics/details/#set-an-associated-constant-to-a-specific-value","text":"We might need to write a function that only works with a specific value of an associated constant N . In this case, the name of the associated constant is written first, followed by an = , and then the value: fn PrintPoint2D[PointT:! NSpacePoint where .N = 2](p: PointT) { Print(p.Get(0), \", \", p.Get(1)); } Similarly in an interface definition: interface Has2DPoint { let PointT:! NSpacePoint where .N = 2; } To name such a constraint, you may use a let or a constraint declaration: let Point2DInterface:! auto = NSpacePoint where .N = 2; constraint Point2DInterface { extends NSpacePoint where .N = 2; } This syntax is also used to specify the values of associated constants when implementing an interface for a type. Concern: Using = for this use case is not consistent with other where clauses that write a boolean expression that evaluates to true when the constraint is satisfied. A constraint to say that two associated constants should have the same value without specifying what specific value they should have must use == instead of = : interface PointCloud { let Dim:! i32; let PointT:! NSpacePoint where .N == Dim; }","title":"Set an associated constant to a specific value"},{"location":"design/generics/details/#same-type-constraints","text":"","title":"Same type constraints"},{"location":"design/generics/details/#set-an-associated-type-to-a-specific-value","text":"Functions accepting a generic type might also want to constrain one of its associated types to be a specific, concrete type. For example, we might want to have a function only accept stacks containing integers: fn SumIntStack[T:! Stack where .ElementType = i32](s: T*) -> i32 { var sum: i32 = 0; while (!s->IsEmpty()) { // s->Pop() has type `T.ElementType` == i32: sum += s->Pop(); } return sum; } To name these sorts of constraints, we could use let declarations or constraint definitions: let IntStack:! auto = Stack where .ElementType = i32; constraint IntStack { extends Stack where .ElementType = i32; } This syntax is also used to specify the values of associated types when implementing an interface for a type.","title":"Set an associated type to a specific value"},{"location":"design/generics/details/#equal-generic-types","text":"Alternatively, two generic types could be constrained to be equal to each other, without specifying what that type is. This uses == instead of = . For example, we could make the ElementType of an Iterator interface equal to the ElementType of a Container interface as follows: interface Iterator { let ElementType:! Type; ... } interface Container { let ElementType:! Type; let IteratorType:! Iterator where .ElementType == ElementType; ... } Given an interface with two associated types interface PairInterface { let Left:! Type; let Right:! Type; } we can constrain them to be equal in a function signature: fn F[MatchedPairType:! PairInterface where .Left == .Right] (x: MatchedPairType*); or in an interface definition: interface HasEqualPair { let P:! PairInterface where .Left == .Right; } This kind of constraint can be named: let EqualPair:! auto = PairInterface where .Left == .Right; constraint EqualPair { extends PairInterface where .Left == .Right; } Another example of same type constraints is when associated types of two different interfaces are constrained to be equal: fn Map[CT:! Container, FT:! Function where .InputType == CT.ElementType] (c: CT, f: FT) -> Vector(FT.OutputType);","title":"Equal generic types"},{"location":"design/generics/details/#satisfying-both-type-of-types","text":"If the two types being constrained to be equal have been declared with different type-of-types, then the actual type value they are set to will have to satisfy both constraints. For example, if SortedContainer.ElementType is declared to be Comparable , then in this declaration: fn Contains [SC:! SortedContainer, CT:! Container where .ElementType == SC.ElementType] (haystack: SC, needles: CT) -> bool; the where constraint means CT.ElementType must satisfy Comparable as well. However, inside the body of Contains , CT.ElementType will only act like the implementation of Comparable is external . That is, items from the needles container won't directly have a Compare method member, but can still be implicitly converted to Comparable and can still call Compare using the compound member access syntax, needle.(Comparable.Compare)(elt) . The rule is that an == where constraint between two type variables does not modify the set of member names of either type. (If you write where .ElementType = String with a = and a concrete type, then .ElementType is actually set to String including the complete String API.) Note that == constraints are symmetric, so the previous declaration of Contains is equivalent to an alternative declaration where CT is declared first and the where clause is attached to SortedContainer : fn Contains [CT:! Container, SC:! SortedContainer where .ElementType == CT.ElementType] (haystack: SC, needles: CT) -> bool;","title":"Satisfying both type-of-types"},{"location":"design/generics/details/#type-bound-for-associated-type","text":"A where clause can express that a type must implement an interface. This is more flexible than the usual approach of including that interface in the type since it can be applied to associated type members as well.","title":"Type bound for associated type"},{"location":"design/generics/details/#type-bounds-on-associated-types-in-declarations","text":"In the following example, normally the ElementType of a Container can be any type. The SortContainer function, however, takes a pointer to a type satisfying Container with the additional constraint that its ElementType must satisfy the Comparable interface. interface Container { let ElementType:! Type; ... } fn SortContainer [ContainerType:! Container where .ElementType is Comparable] (container_to_sort: ContainerType*); In contrast to a same type constraint , this does not say what type ElementType exactly is, just that it must satisfy some type-of-type. Open question: How do you spell that? Provisionally we are writing is , following Swift, but maybe we should have another operator that more clearly returns a boolean like has_type ? Note: Container defines ElementType as having type Type , but ContainerType.ElementType has type Comparable . This is because ContainerType has type Container where .ElementType is Comparable , not Container . This means we need to be a bit careful when talking about the type of ContainerType when there is a where clause modifying it.","title":"Type bounds on associated types in declarations"},{"location":"design/generics/details/#type-bounds-on-associated-types-in-interfaces","text":"Given these definitions (omitting ElementType for brevity): interface IteratorInterface { ... } interface ContainerInterface { let IteratorType:! IteratorInterface; ... } interface RandomAccessIterator { extends IteratorInterface; ... } We can then define a function that only accepts types that implement ContainerInterface where its IteratorType associated type implements RandomAccessIterator : fn F[ContainerType:! ContainerInterface where .IteratorType is RandomAccessIterator] (c: ContainerType); We would like to be able to name this constraint, defining a RandomAccessContainer to be a type-of-type whose types satisfy ContainerInterface with an IteratorType satisfying RandomAccessIterator . let RandomAccessContainer:! auto = ContainerInterface where .IteratorType is RandomAccessIterator; // or constraint RandomAccessContainer { extends ContainerInterface where .IteratorType is RandomAccessIterator; } // With the above definition: fn F[ContainerType:! RandomAccessContainer](c: ContainerType); // is equivalent to: fn F[ContainerType:! ContainerInterface where .IteratorType is RandomAccessIterator] (c: ContainerType);","title":"Type bounds on associated types in interfaces"},{"location":"design/generics/details/#combining-constraints","text":"Constraints can be combined by separating constraint clauses with the and keyword. This example expresses a constraint that two associated types are equal and satisfy an interface: fn EqualContainers [CT1:! Container, CT2:! Container where .ElementType is HasEquality and .ElementType == CT1.ElementType] (c1: CT1*, c2: CT2*) -> bool; Comparison with other languages: Swift and Rust use commas , to separate constraint clauses, but that only works because they place the where in a different position in a declaration. In Carbon, the where is attached to a type in a parameter list that is already using commas to separate parameters.","title":"Combining constraints"},{"location":"design/generics/details/#recursive-constraints","text":"We sometimes need to constrain a type to equal one of its associated types. In this first example, we want to represent the function Abs which will return Self for some but not all types, so we use an associated type MagnitudeType to encode the return type: interface HasAbs { extends Numeric; let MagnitudeType:! Numeric; fn Abs[me: Self]() -> MagnitudeType; } For types representing subsets of the real numbers, such as i32 or f32 , the MagnitudeType will match Self , the type implementing an interface. For types representing complex numbers, the types will be different. For example, the Abs() applied to a Complex64 value would produce a f32 result. The goal is to write a constraint to restrict to the first case. In a second example, when you take the slice of a type implementing Container you get a type implementing Container which may or may not be the same type as the original container type. However, taking the slice of a slice always gives you the same type, and some functions want to only operate on containers whose slice type is the same as the container type. To solve this problem, we think of Self as an actual associated type member of every interface. We can then address it using .Self in a where clause, like any other associated type member. fn Relu[T:! HasAbs where .MagnitudeType == .Self](x: T) { // T.MagnitudeType == T so the following is allowed: return (x.Abs() + x) / 2; } fn UseContainer[T:! Container where .SliceType == .Self](c: T) -> bool { // T.SliceType == T so `c` and `c.Slice(...)` can be compared: return c == c.Slice(...); } Notice that in an interface definition, Self refers to the type implementing this interface while .Self refers to the associated type currently being defined. interface Container { let ElementType:! Type; let SliceType:! Container where .ElementType == ElementType and .SliceType == .Self; fn GetSlice[addr me: Self*] (start: IteratorType, end: IteratorType) -> SliceType; } These recursive constraints can be named: let RealAbs:! auto = HasAbs where .MagnitudeType == .Self; constraint RealAbs { extends HasAbs where .MagnitudeType == Self; } let ContainerIsSlice:! auto = Container where .SliceType == .Self; constraint ContainerIsSlice { extends Container where .SliceType == Self; } Note that using the constraint approach we can name these constraints using Self instead of .Self , since they refer to the same type.","title":"Recursive constraints"},{"location":"design/generics/details/#parameterized-type-implements-interface","text":"There are times when a function will pass a generic type parameter of the function as an argument to a parameterized type, as in the previous case, and in addition the function needs the result to implement a specific interface. // Some parametized type. class Vector(T:! Type) { ... } // Parameterized type implements interface only for some arguments. external impl Vector(String) as Printable { ... } // Constraint: `T` such that `Vector(T)` implements `Printable` fn PrintThree [T:! Type where Vector(.Self) is Printable] (a: T, b: T, c: T) { var v: Vector(T) = (a, b, c); Print(v); } Comparison with other languages: This use case was part of the Rust rationale for adding support for where clauses .","title":"Parameterized type implements interface"},{"location":"design/generics/details/#another-type-implements-parameterized-interface","text":"In this case, we need some other type to implement an interface parameterized by a generic type parameter. The syntax for this case follows the previous case, except now the .Self parameter is on the interface to the right of the is . For example, we might need a type parameter T to support explicit conversion from an integer type like i32 : interface As(T:! Type) { fn Convert[me: Self]() -> T; } fn Double[T:! Mul where i32 is As(.Self)](x: T) -> T { return x * (2 as T); }","title":"Another type implements parameterized interface"},{"location":"design/generics/details/#implied-constraints","text":"Imagine we have a generic function that accepts an arbitrary HashMap : fn LookUp[KeyType:! Type](hm: HashMap(KeyType, i32)*, k: KeyType) -> i32; fn PrintValueOrDefault[KeyType:! Printable, ValueT:! Printable & HasDefault] (map: HashMap(KeyType, ValueT), key: KeyT); The KeyType in these declarations does not visibly satisfy the requirements of HashMap , which requires the type implement Hashable and other interfaces: class HashMap( KeyType:! Hashable & EqualityComparable & Movable, ...) { ... } In this case, KeyType gets Hashable and so on as implied constraints . Effectively that means that these functions are automatically rewritten to add a where constraint on KeyType attached to the HashMap type: fn LookUp[KeyType:! Type] (hm: HashMap(KeyType, i32)* where KeyType is Hashable & EqualityComparable & Movable, k: KeyType) -> i32; fn PrintValueOrDefault[KeyType:! Printable, ValueT:! Printable & HasDefault] (map: HashMap(KeyType, ValueT) where KeyType is Hashable & EqualityComparable & Movable, key: KeyT); In this case, Carbon will accept the definition and infer the needed constraints on the generic type parameter. This is both more concise for the author of the code and follows the \"don't repeat yourself\" principle . This redundancy is undesirable since it means if the needed constraints for HashMap are changed, then the code has to be updated in more locations. Further it can add noise that obscures relevant information. In practice, any user of these functions will have to pass in a valid HashMap instance, and so will have already satisfied these constraints. This implied constraint is equivalent to the explicit constraint that each parameter and return type is legal . Note: These implied constraints affect the requirements of a generic type parameter, but not its member names . This way you can always look at the declaration to see how name resolution works, without having to look up the definitions of everything it is used as an argument to. Limitation: To limit readability concerns and ambiguity, this feature is limited to a single signature. Consider this interface declaration: interface GraphNode { let Edge:! Type; fn EdgesFrom[me: Self]() -> HashSet(Edge); } One approach would be to say the use of HashSet(Edge) in the signature of the EdgesFrom function would imply that Edge satisfies the requirements of an argument to HashSet , such as being Hashable . Another approach would be to say that the EdgesFrom would only be conditionally available when Edge does satisfy the constraints on HashSet arguments. Instead, Carbon will reject this definition, requiring the user to include all the constraints required for the other declarations in the interface in the declaration of the Edge associated type. Similarly, a parameter to a class must be declared with all the constraints needed to declare the members of the class that depend on that parameter. Comparison with other languages: Both Swift ( 1 , 2 ) and Rust support some form of this feature as part of their type inference (and the Rust community is considering expanding support ).","title":"Implied constraints"},{"location":"design/generics/details/#must-be-legal-type-argument-constraints","text":"Now consider the case that the generic type parameter is going to be used as an argument to a parameterized type in a function body, not in the signature. If the parameterized type was explicitly mentioned in the signature, the implied constraint feature would ensure all of its requirements were met. The developer can create a trivial parameterized type implements interface where constraint to just say the type is a legal with this argument, by saying that the parameterized type implements Type , which all types do. For example, a function that adds its parameters to a HashSet to deduplicate them, needs them to be Hashable and so on. To say \" T is a type where HashSet(T) is legal,\" we can write: fn NumDistinct[T:! Type where HashSet(.Self) is Type] (a: T, b: T, c: T) -> i32 { var set: HashSet(T); set.Add(a); set.Add(b); set.Add(c); return set.Size(); } This has the same advantages over repeating the constraints on HashSet arguments in the type of T as the general implied constraints above.","title":"Must be legal type argument constraints"},{"location":"design/generics/details/#referencing-names-in-the-interface-being-defined","text":"The constraint in a where clause is required to only reference earlier names from this scope, as in this example: interface Graph { let E: Edge; let V: Vert where .E == E and .Self == E.V; }","title":"Referencing names in the interface being defined"},{"location":"design/generics/details/#manual-type-equality","text":"Imagine we have some function with generic parameters: fn F[T:! SomeInterface](x: T) { x.G(x.H()); } We want to know if the return type of method T.H is the same as the parameter type of T.G in order to typecheck the function. However, determining whether two type expressions are transitively equal is in general undecidable, as has been shown in Swift . Carbon's approach is to only allow implicit conversions between two type expressions that are constrained to be equal in a single where clause. This means that if two type expressions are only transitively equal, the user will need to include a sequence of casts or use an observe declaration to convert between them. Given this interface Transitive that has associated types that are constrained to all be equal, with interfaces P , Q , and R : interface P { fn InP[me:Self](); } interface Q { fn InQ[me:Self](); } interface R { fn InR[me:Self](); } interface Transitive { let A:! P; let B:! Q where .Self == A; let C:! R where .Self == B; fn GetA[me: Self]() -> A; fn TakesC[me:Self](c: C); } A cast to B is needed to call TakesC with a value of type A , so each step only relies on one equality: fn F[T:! Transitive](t: T) { // \u2705 Allowed t.TakesC(t.GetA() as T.B); // \u2705 Allowed let b: T.B = t.GetA(); t.TakesC(b); // \u274c Not allowed: t.TakesC(t.GetA()); } A value of type A , such as the return value of GetA() , has the API of P . Any such value also implements Q , and since the compiler can see that by way of a single where equality, values of type A are treated as if they implement Q externally . However, the compiler will require a cast to B or C to see that the type implements R . fn TakesPQR[U:! P & Q & R](u: U); fn G[T:! Transitive](t: T) { var a: T.A = t.GetA(); // \u2705 Allowed: `T.A` implements `P`. a.InP(); // \u2705 Allowed: `T.A` implements `Q` externally. a.(Q.InQ)(); // \u274c Not allowed: a.InQ(); // \u2705 Allowed: values of type `T.A` may be cast // to `T.B`, which implements `Q` internally. (a as T.B).InQ(); // \u2705 Allowed: `T.B` implements `R` externally. (a as T.B).(R.InR)(); // \u274c Not allowed: TakesPQR(a); // \u2705 Allowed: `T.B` implements `P`, `Q`, and // `R`, though the implementations of `P` // and `R` are external. TakesPQR(a as T.B); } The compiler may have several different where clauses to consider, particularly when an interface has associated types that recursively satisfy the same interface. For example, given this interface Commute : interface Commute { let X:! Commute; let Y:! Commute where .X == X.Y; fn GetX[me: Self]() -> X; fn GetY[me: Self]() -> Y; fn TakesXXY[me:Self](xxy: X.X.Y); } and a function H taking a value with some type implementing this interface, then the following would be legal statements in H : fn H[C: Commute](c: C) { // \u2705 Legal: argument has type `C.X.X.Y` c.TakesXXY(c.GetX().GetX().GetY()); // \u2705 Legal: argument has type `C.X.Y.X` which is equal // to `C.X.X.Y` following only one `where` clause. c.TakesXXY(c.GetX().GetY().GetX()); // \u2705 Legal: cast is legal since it matches a `where` // clause, and produces an argument that has type // `C.X.Y.X`. c.TakesXXY(c.GetY().GetX().GetX() as C.X.Y.X); } That last call would not be legal without the cast, though. Comparison with other languages: Other languages such as Swift and Rust instead perform automatic type equality. In practice this means that their compiler can reject some legal programs based on heuristics simply to avoid running for an unbounded length of time. The benefits of the manual approach include: fast compilation, since the compiler does not need to explore a potentially large set of combinations of equality restrictions, supporting Carbon's goal of fast and scalable development ; expressive and predictable semantics, since there are no limitations on how complex a set of constraints can be supported; and simplicity. The main downsides are: manual work for the source code author to prove to the compiler that types are equal; and verbosity. We expect that rich error messages and IDE tooling will be able to suggest changes to the source code when a single equality constraint is not sufficient to show two type expressions are equal, but a more extensive automated search can find a sequence that prove they are equal.","title":"Manual type equality"},{"location":"design/generics/details/#observe-declarations","text":"An observe declaration lists a sequence of type expressions that are equal by some same-type where constraints. These observe declarations may be included in an interface definition or a function body, as in: interface Commute { let X:! Commute; let Y:! Commute where .X == X.Y; ... observe X.X.Y == X.Y.X == Y.X.X; } fn H[C: Commute](c: C) { observe C.X.Y.Y == C.Y.X.Y == C.Y.Y.X; ... } Every type expression after the first must be equal to some earlier type expression in the sequence by a single where equality constraint. In this example, interface Commute { let X:! Commute; let Y:! Commute where .X == X.Y; ... // \u2705 Legal: observe X.X.Y.Y == X.Y.X.Y == Y.X.X.Y == X.Y.Y.X; } the expression X.Y.Y.X is one equality away from X.Y.X.Y and so it is allowed. This is even though X.Y.X.Y isn't the type expression immediately prior to X.Y.Y.X . After an observe declaration, all of the listed type expressions are considered equal to each other using a single where equality. In this example, the observe declaration in the Transitive interface definition provides the link between associated types A and C that allows function F to type check. interface P { fn InP[me:Self](); } interface Q { fn InQ[me:Self](); } interface R { fn InR[me:Self](); } interface Transitive { let A:! P; let B:! Q where .Self == A; let C:! R where .Self == B; fn GetA[me: Self]() -> A; fn TakesC[me:Self](c: C); // Without this `observe` declaration, the // calls in `F` below would not be allowed. observe A == B == C; } fn TakesPQR[U:! P & Q & R](u: U); fn F[T:! Transitive](t: T) { var a: T.A = t.GetA(); // \u2705 Allowed: `T.A` == `T.C` t.TakesC(a); a.(R.InR()); // \u2705 Allowed: `T.A` implements `P`, // `T.A` == `T.B` that implements `Q`, and // `T.A` == `T.C` that implements `R`. TakesPQR(a); } Since adding an observe declaration only adds external implementations of interfaces to generic types, they may be added without breaking existing code.","title":"observe declarations"},{"location":"design/generics/details/#other-constraints-as-type-of-types","text":"There are some constraints that we will naturally represent as named type-of-types. These can either be used directly to constrain a generic type parameter, or in a where ... is ... clause to constrain an associated type. The compiler determines which types implement these interfaces, developers can not explicitly implement these interfaces for their own types. Open question: Are these names part of the prelude or in a standard library?","title":"Other constraints as type-of-types"},{"location":"design/generics/details/#is-a-derived-class","text":"Given a type T , Extends(T) is a type-of-type whose values are types that are derived from T . That is, Extends(T) is the set of all types U that are subtypes of T . fn F[T:! Extends(BaseType)](p: T*); fn UpCast[T:! Type](p: T*, U:! Type where T is Extends(.Self)) -> U*; fn DownCast[T:! Type](p: T*, U:! Extends(T)) -> U*; Open question: Alternatively, we could define a new extends operator: fn F[T:! Type where .Self extends BaseType](p: T*); fn UpCast[T:! Type](p: T*, U:! Type where T extends .Self) -> U*; fn DownCast[T:! Type](p: T*, U:! Type where .Self extends T) -> U*; Comparison to other languages: In Swift, you can add a required superclass to a type bound using & .","title":"Is a derived class"},{"location":"design/generics/details/#type-compatible-with-another-type","text":"Given a type U , define the type-of-type CompatibleWith(U) as follows: CompatibleWith(U) is a type whose values are types T such that T and U are compatible . That is values of types T and U can be cast back and forth without any change in representation (for example T is an adapter for U ). To support this, we extend the requirements that type-of-types are allowed to have to include a \"data representation requirement\" option. CompatibleWith determines an equivalence relationship between types. Specifically, given two types T1 and T2 , they are equivalent if T1 is CompatibleWith(T2) . That is, if T1 has the type CompatibleWith(T2) . Note: Just like interface parameters, we require the user to supply U , they may not be deduced. Specifically, this code would be illegal: fn Illegal[U:! Type, T:! CompatibleWith(U)](x: T*) ... In general there would be multiple choices for U given a specific T here, and no good way of picking one. However, similar code is allowed if there is another way of determining U : fn Allowed[U:! Type, T:! CompatibleWith(U)](x: U*, y: T*) ...","title":"Type compatible with another type"},{"location":"design/generics/details/#same-implementation-restriction","text":"In some cases, we need to restrict to types that implement certain interfaces the same way as the type U . The values of type CompatibleWith(U, TT) are types satisfying CompatibleWith(U) that have the same implementation of TT as U . For example, if we have a type HashSet(T) : class HashSet(T:! Hashable) { ... } Then HashSet(T) may be cast to HashSet(U) if T is CompatibleWith(U, Hashable) . The one-parameter interpretation of CompatibleWith(U) is recovered by letting the default for the second TT parameter be Type .","title":"Same implementation restriction"},{"location":"design/generics/details/#example-multiple-implementations-of-the-same-interface","text":"This allows us to represent functions that accept multiple implementations of the same interface for a type. enum CompareResult { Less, Equal, Greater } interface Comparable { fn Compare[me: Self](rhs: Self) -> CompareResult; } fn CombinedLess[T:! Type](a: T, b: T, U:! CompatibleWith(T) & Comparable, V:! CompatibleWith(T) & Comparable) -> bool { match ((a as U).Compare(b as U)) { case CompareResult.Less => { return True; } case CompareResult.Greater => { return False; } case CompareResult.Equal => { return (a as V).Compare(b as V) == CompareResult.Less; } } } Used as: class Song { ... } adapter SongByArtist for Song { impl as Comparable { ... } } adapter SongByTitle for Song { impl as Comparable { ... } } var s1: Song = ...; var s2: Song = ...; assert(CombinedLess(s1, s2, SongByArtist, SongByTitle) == True); We might generalize this to a list of implementations: fn CombinedCompare[T:! Type] (a: T, b: T, CompareList:! List(CompatibleWith(T) & Comparable)) -> CompareResult { for (let U:! auto in CompareList) { var result: CompareResult = (a as U).Compare(b); if (result != CompareResult.Equal) { return result; } } return CompareResult.Equal; } assert(CombinedCompare(Song(...), Song(...), (SongByArtist, SongByTitle)) == CompareResult.Less); Open question: How are compile-time lists of types declared and iterated through? They will also be needed for variadic argument support .","title":"Example: Multiple implementations of the same interface"},{"location":"design/generics/details/#example-creating-an-impl-out-of-other-impls","text":"And then to package this functionality as an implementation of Comparable , we combine CompatibleWith with type adaptation : adapter ThenCompare( T:! Type, CompareList:! List(CompatibleWith(T) & Comparable)) for T { impl as Comparable { fn Compare[me: Self](rhs: Self) -> CompareResult { for (let U:! auto in CompareList) { var result: CompareResult = (me as U).Compare(rhs as U); if (result != CompareResult.Equal) { return result; } } return CompareResult.Equal; } } } let SongByArtistThenTitle: auto = ThenCompare(Song, (SongByArtist, SongByTitle)); var s1: Song = ...; var s2: SongByArtistThenTitle = Song(...) as SongByArtistThenTitle; assert((s1 as SongByArtistThenTitle).Compare(s2) == CompareResult.Less);","title":"Example: Creating an impl out of other impls"},{"location":"design/generics/details/#sized-types-and-type-of-types","text":"What is the size of a type? It could be fully known and fixed at compile time -- this is true of primitive types ( i32 , f64 , and so on), most classes , and most other concrete types. It could be known generically. This means that it will be known at codegen time, but not at type-checking time. It could be dynamic. For example, it could be a dynamic type , a slice, variable-sized type (such as found in Rust ), or you could dereference a pointer to a base class that could actually point to a derived class . It could be unknown which category the type is in. In practice this will be essentially equivalent to having dynamic size. A type is called sized if it is in the first two categories, and unsized otherwise. Note: something with size 0 is still considered \"sized\". The type-of-type Sized is defined as follows: Sized is a type whose values are types T that are \"sized\" -- that is the size of T is known, though possibly only generically. Knowing a type is sized is a precondition to declaring variables of that type, taking values of that type as parameters, returning values of that type, and defining arrays of that type. Users will not typically need to express the Sized constraint explicitly, though, since it will usually be a dependency of some other constraint the type will need such as Movable or Concrete . Note: The compiler will determine which types are \"sized\", this is not something types will implement explicitly like ordinary interfaces. Example: // In the Carbon standard library interface DefaultConstructible { // Types must be sized to be default constructible. impl as Sized; fn Default() -> Self; } // Classes are \"sized\" by default. class Name { impl as DefaultConstructible { fn Default() -> Self { ... } } ... } fn F[T:! Type](x: T*) { // T is unsized. // \u2705 Allowed: may access unsized values through a pointer. var y: T* = x; // \u274c Illegal: T is unsized. var z: T; } // T is sized, but its size is only known generically. fn G[T: DefaultConstructible](x: T*) { // \u2705 Allowed: T is default constructible, which means sized. var y: T = T.Default(); } var z: Name = Name.Default();; // \u2705 Allowed: `Name` is sized and implements `DefaultConstructible`. G(&z); Open question: Even if the size is fixed, it won't be known at the time of compiling the generic function if we are using the dynamic strategy. Should we automatically box local variables when using the dynamic strategy? Or should we only allow MaybeBox values to be instantiated locally? Or should this just be a case where the compiler won't necessarily use the dynamic strategy? Open question: Should the Sized type-of-type expose an associated constant with the size? So you could say T.ByteSize in the above example to get a generic int value with the size of T . Similarly you might say T.ByteStride to get the number of bytes used for each element of an array of T .","title":"Sized types and type-of-types"},{"location":"design/generics/details/#implementation-model_2","text":"This requires a special integer field be included in the witness table type to hold the size of the type. This field will only be known generically, so if its value is used for type checking, we need some way of evaluating those type tests symbolically.","title":"Implementation model"},{"location":"design/generics/details/#typeid","text":"There are some capabilities every type can provide. For example, every type should be able to return its name or identify whether it is equal to another type. It is rare, however, for code to need to access these capabilities, so we relegate these capabilities to an interface called TypeId that all types automatically implement. This way generic code can indicate that it needs those capabilities by including TypeId in the list of requirements. In the case where no type capabilities are needed, for example the code is only manipulating pointers to the type, you would write T:! Type and get the efficiency of void* but without giving up type safety. fn SortByAddress[T:! Type](v: Vector(T*)*) { ... } In particular, the compiler should in general avoid monomorphizing to generate multiple instantiations of the function in this case. Open question: Should TypeId be implemented externally for types to avoid name pollution ( .TypeName , .TypeHash , etc.) unless the function specifically requests those capabilities?","title":"TypeId"},{"location":"design/generics/details/#destructor-constraints","text":"There are four type-of-types related to the destructors of types : Concrete types may be local or member variables. Deletable types may be safely deallocated by pointer using the Delete method on the Allocator used to allocate it. Destructible types have a destructor and may be deallocated by pointer using the UnsafeDelete method on the correct Allocator , but it may be unsafe. The concerning case is deleting a pointer to a derived class through a pointer to its base class without a virtual destructor. TrivialDestructor types have empty destructors. This type-of-type may be used with specialization to unlock specific optimizations. Note: The names Deletable and Destructible are placeholders since they do not conform to the decision on question-for-leads issue #1058: \"How should interfaces for core functionality be named?\" . The type-of-types Concrete , Deletable , and TrivialDestructor all extend Destructible . Combinations of them may be formed using the & operator . For example, a generic function that both instantiates and deletes values of a type T would require T implement Concrete & Deletable . Types are forbidden from explicitly implementing these type-of-types directly. Instead they use destructor declarations in their class definition and the compiler uses them to determine which of these type-of-types are implemented.","title":"Destructor constraints"},{"location":"design/generics/details/#generic-let","text":"A let statement inside a function body may be used to get the change in type behavior of calling a generic function without having to introduce a function call. fn F(...) { ... let T:! C = U; X; Y; Z; } gets rewritten to: fn F(...) { ... fn Closure(T:! C where .Self == U) { X; Y; Z; } Closure(U); } The where .Self == U modifier allows values to implicitly convert between type T , the erased type, and type U , the concrete type. Note that implicit conversion is only performed across a single where equality . This can be used to switch to the API of C when it is external, as an alternative to using an adapter , or to simplify inlining of a generic function while preserving semantics.","title":"Generic let"},{"location":"design/generics/details/#parameterized-impls","text":"There are cases where an impl definition should apply to more than a single type and interface combination. The solution is to parameterize the impl definition, so it applies to a family of types, interfaces, or both. This includes: Declare an impl for a parameterized type, which may be external or declared out-of-line. \"Conditional conformance\" where a parameterized type implements some interface if the parameter to the type satisfies some criteria, like implementing the same interface. \"Blanket\" impls where an interface is implemented for all types that implement another interface, or some other criteria beyond being a specific type. \"Wildcard\" impls where a family of interfaces are implemented for single type.","title":"Parameterized impls"},{"location":"design/generics/details/#impl-for-a-parameterized-type","text":"Interfaces may be implemented for a parameterized type. This can be done lexically in the class' scope: class Vector(T:! Type) { impl as Iterable where .ElementType = T { ... } } This is equivalent to naming the type between impl and as : class Vector(T:! Type) { impl Vector(T) as Iterable where .ElementType = T { ... } } An impl may be declared external by adding an external keyword before impl . External impls may also be declared out-of-line, but all parameters must be declared in a forall clause: external impl forall [T:! Type] Vector(T) as Iterable where .ElementType = T { ... } The parameter for the type can be used as an argument to the interface being implemented: class HashMap(Key:! Hashable, Value:! Type) { impl as Has(Key) { ... } impl as Contains(HashSet(Key)) { ... } } or externally out-of-line: class HashMap(Key:! Hashable, Value:! Type) { ... } external impl forall [Key:! Hashable, Value:! Type] HashMap(Key, Value) as Has(Key) { ... } external impl forall [Key:! Hashable, Value:! Type] HashMap(Key, Value) as Contains(HashSet(Key)) { ... }","title":"Impl for a parameterized type"},{"location":"design/generics/details/#conditional-conformance","text":"Conditional conformance is expressing that we have an impl of some interface for some type, but only if some additional type restrictions are met. Examples where this would be useful include being able to say that a container type, like Vector , implements some interface when its element type satisfies the same interface: A container is printable if its elements are. A container could be compared to another container with the same element type using a lexicographic comparison if the element type is comparable. A container is copyable if its elements are. To do this with an external impl , specify a more-specific Self type to the left of the as in the declaration: interface Printable { fn Print[me: Self](); } class Vector(T:! Type) { ... } // By saying \"T:! Printable\" instead of \"T:! Type\" here, // we constrain T to be Printable for this impl. external impl forall [T:! Printable] Vector(T) as Printable { fn Print[me: Self]() { for (let a: T in me) { // Can call `Print` on `a` since the constraint // on `T` ensures it implements `Printable`. a.Print(); } } } To define these impl s inline in a class definition, include a forall clause with a more-specific type between the impl and as keywords. class Array(T:! Type, template N:! Int) { impl forall [P:! Printable] Array(P, N) as Printable { ... } } It is legal to add the keyword external before the impl keyword to switch to an external impl defined lexically within the class scope. Inside the scope, both P and T refer to the same type, but P has the type-of-type of Printable and so has a Print member. The relationship between T and P is as if there was a where P == T clause. TODO: Need to resolve whether the T name can be reused, or if we require that you need to use new names, like P , when creating new type variables. Example: Consider a type with two parameters, like Pair(T, U) . In this example, the interface Foo(T) is only implemented when the two types are equal. interface Foo(T:! Type) { ... } class Pair(T:! Type, U:! Type) { ... } external impl forall [T:! Type] Pair(T, T) as Foo(T) { ... } You may also define the impl inline, in which case it can be internal: class Pair(T:! Type, U:! Type) { impl Pair(T, T) as Foo(T) { ... } } Clarification: Method lookup will look at all internal implementations, whether or not the conditions on those implementations hold for the Self type. If the conditions don't hold, then the call will be rejected because Self has the wrong type, just like any other argument/parameter type mismatch. This means types may not implement two different interfaces internally if they share a member name, even if their conditions are mutually exclusive: class X(T:! Type) { impl X(i32) as Foo { fn F[me: Self](); } impl X(i64) as Bar { // \u274c Illegal: name conflict between `Foo.F` and `Bar.F` fn F[me: Self](n: i64); } } However, the same interface may be implemented multiple times as long as there is no overlap in the conditions: class X(T:! Type) { impl X(i32) as Foo { fn F[me: Self](); } impl X(i64) as Foo { // \u2705 Allowed: `X(T).F` consistently means `X(T).(Foo.F)` fn F[me: Self](); } } This allows a type to express that it implements an interface for a list of types, possibly with different implementations. In general, X(T).F can only mean one thing, regardless of T . Comparison with other languages: Swift supports conditional conformance , but bans cases where there could be ambiguity from overlap. Rust also supports conditional conformance .","title":"Conditional conformance"},{"location":"design/generics/details/#conditional-methods","text":"A method could be defined conditionally for a type by using a more specific type in place of Self in the method declaration. For example, this is how to define a vector type that only has a Sort method if its elements implement the Comparable interface: class Vector(T:! Type) { // `Vector(T)` has a `Sort()` method if `T` is `Comparable`. fn Sort[C:! Comparable, addr me: Vector(C)*](); } Comparison with other languages: In Rust this feature is part of conditional conformance. Swift supports conditional methods using conditional extensions or contextual where clauses .","title":"Conditional methods"},{"location":"design/generics/details/#blanket-impls","text":"A blanket impl is an impl that could apply to more than one root type, so the impl will use a type variable for the Self type. Here are some examples where blanket impls arise: Any type implementing Ordered should get an implementation of PartiallyOrdered . external impl forall [T:! Ordered] T as PartiallyOrdered { ... } T implements CommonType(T) for all T external impl forall [T:! Type] T as CommonType(T) where .Result = T { } This means that every type is the common type with itself. Blanket impls must always be external and defined lexically out-of-line.","title":"Blanket impls"},{"location":"design/generics/details/#difference-between-blanket-impls-and-named-constraints","text":"A blanket interface can be used to say \"any type implementing interface I also implements interface B .\" Compare this with defining a constraint C that requires I . In that case, C will also be implemented any time I is. There are differences though: There can be other implementations of interface B without a corresponding implementation of I , unless B has a requirement on I . However, the types implementing C will be the same as the types implementing I . More specialized implementations of B can override the blanket implementation.","title":"Difference between blanket impls and named constraints"},{"location":"design/generics/details/#wildcard-impls","text":"A wildcard impl is an impl that defines a family of interfaces for a single Self type. For example, the BigInt type might implement AddTo(T) for all T that implement ImplicitAs(i32) . The implementation would first convert T to i32 and then add the i32 to the BigInt value. class BigInt { external impl forall [T:! ImplicitAs(i32)] as AddTo(T) { ... } } // Or out-of-line: external impl forall [T:! ImplicitAs(i32)] BigInt as AddTo(T) { ... } Wildcard impls must always be external , to avoid having the names in the interface defined for the type multiple times.","title":"Wildcard impls"},{"location":"design/generics/details/#combinations","text":"The different kinds of parameters to impls may be combined. For example, if T implements As(U) , then this implements As(Optional(U)) for Optional(T) : external impl forall [U:! Type, T:! As(U)] Optional(T) as As(Optional(U)) { ... } This has a wildcard parameter U , and a condition on parameter T .","title":"Combinations"},{"location":"design/generics/details/#lookup-resolution-and-specialization","text":"As much as possible, we want rules for where an impl is allowed to be defined and for selecting which impl to use that achieve these three goals: Implementations have coherence, as defined in terminology . This is a goal for Carbon . More detail can be found in this appendix with the rationale and alternatives considered . Libraries will work together as long as they pass their separate checks. A generic function can assume that some impl will be successfully selected if it can see an impl that applies, even though another more specific impl may be selected. For this to work, we need a rule that picks a single impl in the case where there are multiple impl definitions that match a particular type and interface combination. This is called specialization when the rule is that most specific implementation is chosen, for some definition of specific.","title":"Lookup resolution and specialization"},{"location":"design/generics/details/#type-structure-of-an-impl-declaration","text":"Given an impl declaration, find the type structure by deleting deduced parameters and replacing type parameters by a ? . The type structure of this declaration: impl forall [T:! ..., U:! ...] Foo(T, i32) as Bar(String, U) { ... } is: impl Foo(?, i32) as Bar(String, ?) To get a uniform representation across different impl definitions, before type parameters are replaced the declarations are normalized as follows: For impls declared lexically inline in a class definition, the type is added between the impl and as keywords if the type is left out. Pointer types T* are replaced with Ptr(T) . The external keyword is removed, if present. The forall clause introducing type parameters is removed, if present. Any where clauses that are setting associated constants or types are removed. The type structure will always contain a single interface name, which is the name of the interface being implemented, and some number of type names. Type names can be in the Self type to the left of the as keyword, or as parameters to other types or the interface. These names must always be defined either in the current library or be publicly defined in some library this library depends on.","title":"Type structure of an impl declaration"},{"location":"design/generics/details/#orphan-rule","text":"To achieve coherence, we need to ensure that any given impl can only be defined in a library that must be imported for it to apply. Specifically, given a specific type and specific interface, impls that can match can only be in libraries that must have been imported to name that type or interface. This is achieved with the orphan rule . Orphan rule: Some name from the type structure of an impl declaration must be defined in the same library as the impl , that is some name must be local . Only the implementing interface and types (self type and type parameters) in the type structure are relevant here; an interface mentioned in a constraint is not sufficient since it need not be imported . Since Carbon in addition requires there be no cyclic library dependencies, we conclude that there is at most one library that can define impls with a particular type structure.","title":"Orphan rule"},{"location":"design/generics/details/#overlap-rule","text":"Given a specific concrete type, say Foo(bool, i32) , and an interface, say Bar(String, f32) , the overlap rule picks, among all the matching impls, which type structure is considered \"most specific\" to use as the implementation of that type for that interface. Given two different type structures of impls matching a query, for example: impl Foo(?, i32) as Bar(String, ?) impl Foo(?, ?) as Bar(String, f32) We pick the type structure with a non- ? at the first difference as most specific. Here we see a difference between Foo(?, i32) and Foo(?, ?) , so we select the one with Foo(?, i32) , ignoring the fact that it has another ? later in its type structure This rule corresponds to a depth-first traversal of the type tree to identify the first difference, and then picking the most specific choice at that difference.","title":"Overlap rule"},{"location":"design/generics/details/#prioritization-rule","text":"Since at most one library can define impls with a given type structure, all impls with a given type structure must be in the same library. Furthermore by the impl declaration access rules , they will be defined in the API file for the library if they could match any query from outside the library. If there is more than one impl with that type structure, they must be defined or declared together in a prioritization block. Once a type structure is selected for a query, the first impl in the prioritization block that matches is selected. Open question: How are prioritization blocks written? A block starts with a keyword like match_first or impl_priority and then a sequence of impl declarations inside matching curly braces { ... } . match_first { // If T is Foo prioritized ahead of T is Bar impl forall [T:! Foo] T as Bar { ... } impl forall [T:! Baz] T as Bar { ... } } Open question: How do we pick between two different prioritization blocks when they contain a mixture of type structures? There are three options: Prioritization blocks implicitly define all non-empty intersections of contained impls, which are then selected by their type structure. The compiler first picks the impl with the type pattern most favored for the query, and then picks the definition of the highest priority matching impl in the same prioritization block. All the impls in a prioritization block are required to have the same type structure, at a cost in expressivity. To see the difference between the first two options, consider two libraries with type structures as follows: Library B has impl (A, ?, ?, D) as I and impl (?, B, ?, D) as I in the same prioritization block. Library C has impl (A, ?, C, ?) as I . For the query (A, B, C, D) as I , using the intersection rule, library B is considered to have the intersection impl with type structure impl (A, B, ?, D) as I which is the most specific. If we instead just considered the rules mentioned explicitly, then impl (A, ?, C, ?) as I from library C is the most specific. The advantage of the implicit intersection rule is that if library B is changed to add an impl with type structure impl (A, B, ?, D) as I , it won't shift which library is serving that query.","title":"Prioritization rule"},{"location":"design/generics/details/#acyclic-rule","text":"A cycle is when a query, such as \"does type T implement interface I ?\", considers an impl that might match, and whether that impl matches is ultimately dependent on whether that query is true. These are cycles in the graph of (type, interface) pairs where there is an edge from pair A to pair B if whether type A implements interface A determines whether type B implements interface B. The test for whether something forms a cycle needs to be precise enough, and not erase too much information when considering this graph, that these impls are not considered to form cycles with themselves: impl forall [T:! Printable] Optional(T) as Printable; impl forall [T:! Type, U:! ComparableTo(T)] U as ComparableTo(Optional(T)); Example: If T implements ComparableWith(U) , then U should implement ComparableWith(T) . external impl forall [U:! Type, T:! ComparableWith(U)] U as ComparableWith(T); This is a cycle where which types implement ComparableWith determines which types implement the same interface. Example: Cycles can create situations where there are multiple ways of selecting impls that are inconsistent with each other. Consider an interface with two blanket impl declarations: class Y {} class N {} interface True {} impl Y as True {} interface Z(T:! Type) { let Cond:! Type; } match_first { impl forall [T:! Type, U:! Z(T) where .Cond is True] T as Z(U) where .Cond = N { } impl forall [T:! Type, U:! Type] T as Z(U) where .Cond = Y { } } What is i8.(Z(i16).Cond) ? It depends on which of the two blanket impls are selected. An implementation of Z(i16) for i8 could come from the first blanket impl with T == i8 and U == i16 if i16 is Z(i8) and i16.(Z(i8).Cond) == Y . This condition is satisfied if i16 implements Z(i8) using the second blanket impl. In this case, i8.(Z(i16).Cond) == N . Equally well Z(i8) could be implemented for i16 using the first blanket impl and Z(i16) for i8 using the second. In this case, i8.(Z(i16).Cond) == Y . There is no reason to to prefer one of these outcomes over the other. Example: Further, cycles can create contradictions in the type system: class A {} class B {} class C {} interface D(T:! Type) { let Cond:! Type; } match_first { impl forall [T:! Type, U:! D(T) where .Cond = B] T as D(U) where .Cond = C { } impl forall [T:! Type, U:! D(T) where .Cond = A] T as D(U) where .Cond = B { } impl forall [T:! Type, U:! Type] T as D(U) where .Cond = A { } } What is i8.(D(i16).Cond) ? The answer is determined by which blanket impl is selected to implement D(i16) for i8 : If the third blanket impl is selected, then i8.(D(i16).Cond) == A . This implies that i16.(D(i8).Cond) == B using the second blanket impl. If that is true, though, then our first impl choice was incorrect, since the first blanket impl applies and is higher priority. So i8.(D(i16).Cond) == C . But that means that i16 as D(i8) can't use the second blanket impl. For the second blanket impl to be selected, so i8.(D(i16).Cond) == B , i16.(D(i8).Cond) would have to be A . This happens when i16 implements D(i8) using the third blanket impl. However, i8.(D(i16).Cond) == B means that there is a higher priority implementation of D(i8).Cond for i16 . In either case, we arrive at a contradiction. The workaround for this problem is to either split an interface in the cycle in two, with a blanket implementation of one from the other, or move some of the criteria into a named constraint . Concern: Cycles could be spread out across libraries with no dependencies between them. This means there can be problems created by a library that are only detected by its users. Open question: Should Carbon reject cycles in the absence of a query? The two options here are: Combining impls gives you an immediate error if there exists queries using those impls that have cycles. Only when a query reveals a cyclic dependency is an error reported. Open question: In the second case, should we ignore cycles if they don't affect the result of the query? For example, the cycle might be among implementations that are lower priority.","title":"Acyclic rule"},{"location":"design/generics/details/#termination-rule","text":"It is possible to define a set of impls where there isn't a cycle, but the graph is infinite. Without some rule to prevent exhaustive exploration of the graph, determining whether a type implements an interface could run forever. Example: It could be that A implements B , so A is B if Optional(A) is B , if Optional(Optional(A)) is B , and so on. This could be the result of a single impl: impl forall [A:! Type where Optional(.Self) is B] A as B { ... } This problem can also result from a chain of impls, as in A is B if A* is C , if Optional(A) is B , and so on. Rust solves this problem by imposing a recursion limit, much like C++ compilers use to terminate template recursion. This goes against Carbon's goal of predictability in generics , but at this time there are no known alternatives. Unfortunately, the approach Carbon uses to avoid undecidability for type equality, providing an explicit proof in the source , can't be used here. The code triggering the query asking whether some type implements an interface will typically be generic code with know specific knowledge about the types involved, and won't be in a position to provide a manual proof that the implementation should exist. Open question: Is there some restriction on impl declarations that would allow our desired use cases, but allow the compiler to detect non-terminating cases? Perhaps there is some sort of complexity measure Carbon can require doesn't increase when recursing?","title":"Termination rule"},{"location":"design/generics/details/#final-impls","text":"There are cases where knowing that a parameterized impl won't be specialized is particularly valuable. This could let the compiler know the return type of a generic function call, such as using an operator: // Interface defining the behavior of the prefix-* operator interface Deref { let Result:! Type; fn DoDeref[me: Self]() -> Result; } // Types implementing `Deref` class Ptr(T:! Type) { ... external impl as Deref where .Result = T { fn DoDeref[me: Self]() -> Result { ... } } } class Optional(T:! Type) { ... external impl as Deref where .Result = T { fn DoDeref[me: Self]() -> Result { ... } } } fn F[T:! Type](x: T) { // uses Ptr(T) and Optional(T) in implementation } The concern is the possibility of specializing Optional(T) as Deref or Ptr(T) as Deref for a more specific T means that the compiler can't assume anything about the return type of Deref.DoDeref calls. This means F would in practice have to add a constraint, which is both verbose and exposes what should be implementation details: fn F[T:! Type where Optional(T).(Deref.Result) == .Self and Ptr(T).(Deref.Result) == .Self](x: T) { // uses Ptr(T) and Optional(T) in implementation } To mark an impl as not able to be specialized, prefix it with the keyword final : class Ptr(T:! Type) { ... // Note: added `final` final external impl as Deref where .Result = T { fn DoDeref[me: Self]() -> Result { ... } } } class Optional(T:! Type) { ... // Note: added `final` final external impl as Deref where .Result = T { fn DoDeref[me: Self]() -> Result { ... } } } // \u274c Illegal: external impl Ptr(i32) as Deref { ... } // \u274c Illegal: external impl Optional(i32) as Deref { ... } This prevents any higher-priority impl that overlaps a final impl from being defined. Further, if the Carbon compiler sees a matching final impl, it can assume it won't be specialized so it can use the assignments of the associated types in that impl definition. fn F[T:! Type](x: T) { var p: Ptr(T) = ...; // *p has type `T` var o: Optional(T) = ...; // *o has type `T` }","title":"final impls"},{"location":"design/generics/details/#libraries-that-can-contain-final-impls","text":"To prevent the possibility of two unrelated libraries defining conflicting impls, Carbon restricts which libraries may declare an impl as final to only: the library declaring the impl's interface and the library declaring the root of the Self type. This means: A blanket impl with type structure impl ? as MyInterface(...) may only be defined in the same library as MyInterface . An impl with type structure impl MyType(...) as MyInterface(...) may be defined in the library with MyType or MyInterface . These restrictions ensure that the Carbon compiler can locally check that no higher-priority impl is defined superseding a final impl. An impl with type structure impl MyType(...) as MyInterface(...) defined in the library with MyType must import the library defining MyInterface , and so will be able to see any final blanket impls. A blanket impl with type structure impl ? as MyInterface(...ParameterType(...)...) may be defined in the library with ParameterType , but that library must import the library defining MyInterface , and so will be able to see any final blanket impls that might overlap. A final impl with type structure impl MyType(...) as MyInterface(...) would be given priority over any overlapping blanket impl defined in the ParameterType library. An impl with type structure impl MyType(...ParameterType(...)...) as MyInterface(...) may be defined in the library with ParameterType , but that library must import the libraries defining MyType and MyInterface , and so will be able to see any final impls that might overlap.","title":"Libraries that can contain final impls"},{"location":"design/generics/details/#comparison-to-rust","text":"Rust has been designing a specialization feature, but it has not been completed. Luckily, Rust team members have done a lot of blogging during their design process, so Carbon can benefit from the work they have done. However, getting specialization to work for Rust is complicated by the need to maintain compatibility with existing Rust code. This motivates a number of Rust rules where Carbon can be simpler. As a result there are both similarities and differences between the Carbon and Rust plans: A Rust impl defaults to not being able to be specialized, with a default keyword used to opt-in to allowing specialization, reflecting the existing code base developed without specialization. Carbon impls default to allowing specialization, with restrictions on which may be declared final . Since Rust impls are not specializable by default, generic functions can assume that if a matching blanket impl is found, the associated types from that impl will be used. In Carbon, if a generic function requires an associated type to have a particular value, the function commonly will need to state that using an explicit constraint. Carbon will not have the \"fundamental\" attribute used by Rust on types or traits, as described in Rust RFC 1023: \"Rebalancing Coherence\" . Carbon will not use \"covering\" rules, as described in Rust RFC 2451: \"Re-Rebalancing Coherence\" and Little Orphan Impls: The covered rule . Like Rust, Carbon does use ordering, favoring the Self type and then the parameters to the interface in left-to-right order, see Rust RFC 1023: \"Rebalancing Coherence\" and Little Orphan Impls: The ordered rule , but the specifics are different. Carbon is not planning to support any inheritance of implementation between impls. This is more important to Rust since Rust does not support class inheritance for implementation reuse. Rust has considered multiple approaches here, see Aaron Turon: \"Specialize to Reuse\" and Supporting blanket impls in specialization . Supporting blanket impls in specialization proposes a specialization rule for Rust that considers type structure before other constraints, as in Carbon, though the details differ. Rust has more orphan restrictions to avoid there being cases where it is ambiguous which impl should be selected. Carbon instead has picked a total ordering on type structures, picking one as higher priority even without one being more specific in the sense of only applying to a subset of types.","title":"Comparison to Rust"},{"location":"design/generics/details/#forward-declarations-and-cyclic-references","text":"Interfaces, named constraints, and their implementations may be forward declared and then later defined. This is needed to allow cyclic references, for example when declaring the edges and nodes of a graph. It is also a tool that may be used to make code more readable. The interface , named constraint , and implementation sections describe the syntax for their definition , which consists of a declaration followed by a body contained in curly braces { ... } . A forward declaration is a declaration followed by a semicolon ; . A forward declaration is a promise that the entity being declared will be defined later. Between the first declaration of an entity, which may be in a forward declaration or the first part of a definition, and the end of the definition the interface or implementation is called incomplete . There are additional restrictions on how the name of an incomplete entity may be used.","title":"Forward declarations and cyclic references"},{"location":"design/generics/details/#declaring-interfaces-and-named-constraints","text":"The declaration for an interface or named constraint consists of: an optional access-control keyword like private , the keyword introducer interface , constraint , or template constraint , the name of the interface or constraint, and the parameter list, if any. The name of an interface or constraint can not be used until its first declaration is complete. In particular, it is illegal to use the name of the interface in its parameter list. There is a workaround for the use cases when this would come up. An expression forming a constraint, such as C & D , is incomplete if any of the interfaces or constraints used in the expression are incomplete. A constraint expression using a where clause , like C where ... , is invalid if C is incomplete, since there is no way to look up member names of C that appear after where . An interface or named constraint may be forward declared subject to these rules: The definition must be in the same file as the declaration. Only the first declaration may have an access-control keyword. An incomplete interface or named constraint may be used as constraints in declarations of types, functions, interfaces, or named constraints. This includes an impl as or extends declaration inside an interface or named constraint, but excludes specifying the values for associated constants because that would involve name lookup into the incomplete constraint. An attempt to define the body of a generic function using an incomplete interface or named constraint is illegal. An attempt to call a generic function using an incomplete interface or named constraint in its signature is illegal. Any name lookup into an incomplete interface or named constraint is an error. For example, it is illegal to attempt to access a member of an interface using MyInterface.MemberName or constrain a member using a where clause.","title":"Declaring interfaces and named constraints"},{"location":"design/generics/details/#declaring-implementations","text":"The declaration of an interface implementation consists of: optional modifier keywords final , external , the keyword introducer impl , an optional deduced parameter list in square brackets [ ... ] , a type, including an optional parameter pattern, the keyword as , and a type-of-type , including an optional parameter pattern and where clause assigning associated constants and associated types . An implementation of an interface for a type may be forward declared subject to these rules: The definition must be in the same library as the declaration. They must either be in the same file, or the declaration can be in the API file and the definition in an impl file. Future work: Carbon may require the definition of parameterized impls to be in the API file, to support separate compilation. If there is both a forward declaration and a definition, only the first declaration must specify the assignment of associated constants with a where clause. Later declarations may omit the where clause by writing where _ instead. You may forward declare an implementation of a defined interface but not an incomplete interface. This allows the assignment of associated constants in the impl declaration to be verified. An impl forward declaration may be for any declared type, whether it is incomplete or defined. Note that this does not apply to impl as declarations in an interface or named constraint definition, as those are considered interface requirements not forward declarations. Every internal implementation must be declared (or defined) inside the scope of the class definition. It may also be declared before the class definition or defined afterwards. Note that the class itself is incomplete in the scope of the class definition, but member function bodies defined inline are processed as if they appeared immediately after the end of the outermost enclosing class . For coherence , we require that any impl that matches an impl lookup query in the same file, must be declared before the query. This can be done with a definition or a forward declaration.","title":"Declaring implementations"},{"location":"design/generics/details/#matching-and-agreeing","text":"Carbon needs to determine if two declarations match in order to say which definition a forward declaration corresponds to and to verify that nothing is defined twice. Declarations that match must also agree, meaning they are consistent with each other. Interface and named constraint declarations match if their names are the same after name and alias resolution. To agree: The introducer keyword or keywords much be the same. The types and order of parameters in the parameter list, if any, must match. The parameter names may be omitted, but if they are included in both declarations, they must match. Types agree if they correspond to the same expression tree, after name and alias resolution and canonicalization of parentheses. Note that no other evaluation of type expressions is performed. Interface implementation declarations match if the type and interface expressions match: If the type part is omitted, it is rewritten to Self in the context of the declaration. Self is rewritten to its meaning in the scope it is used. In a class scope, this should match the type name and optional parameter expression after class . So in class MyClass extends MyBase { ... } , Self is rewritten to MyClass . In class Vector(T:! Movable) { ... } , Self is rewritten to Vector(T:! Movable) . Types match if they have the same name after name and alias resolution and the same parameters, or are the same type parameter. Interfaces match if they have the same name after name and alias resolution and the same parameters. Note that a named constraint that is equivalent to an interface, as in constraint Equivalent { extends MyInterface; } , is not considered to match. For implementations to agree: The presence of modifier keywords such as external before impl must match between a forward declaration and definition. If either declaration includes a where clause, they must both include one. If neither uses where _ , they must match in that they produce the associated constants with the same values considered separately.","title":"Matching and agreeing"},{"location":"design/generics/details/#declaration-examples","text":"// Forward declaration of interfaces interface Interface1; interface Interface2; interface Interface3; interface Interface4; interface Interface5; interface Interface6; // Forward declaration of class type class MyClass; // \u274c Illegal: Can't declare implementation of incomplete // interface. // external impl MyClass as Interface1; // Definition of interfaces that were previously declared interface Interface1 { let T1:! Type; } interface Interface2 { let T2:! Type; } interface Interface3 { let T3:! Type; } interface Interface4 { let T4:! Type; } // Forward declaration of external implementations external impl MyClass as Interface1 where .T1 = i32; external impl MyClass as Interface2 where .T2 = bool; // Forward declaration of an internal implementation impl MyClass as Interface3 where .T3 = f32; impl MyClass as Interface4 where .T4 = String; interface Interface5 { let T5:! Type; } interface Interface6 { let T6:! Type; } // Definition of the previously declared class type class MyClass { // Definition of previously declared external impl. // Note: no need to repeat assignments to associated // constants. external impl as Interface1 where _ { } // Definition of previously declared internal impl. // Note: allowed even though `MyClass` is incomplete. // Note: allowed but not required to repeat `where` // clause. impl as Interface3 where .T3 = f32 { } // Redeclaration of previously declared internal impl. // Every internal implementation must be declared in // the class definition. impl as Interface4 where _; // Forward declaration of external implementation. external impl MyClass as Interface5 where .T5 = u64; // Forward declaration of internal implementation. impl MyClass as Interface6 where .T6 = u8; } // It would be legal to move the following definitions // from the API file to the implementation file for // this library. // Definition of previously declared external impls. external impl MyClass as Interface2 where _ { } external impl MyClass as Interface5 where _ { } // Definition of previously declared internal impls. impl MyClass as Interface4 where _ { } impl MyClass as Interface6 where _ { }","title":"Declaration examples"},{"location":"design/generics/details/#example-of-declaring-interfaces-with-cyclic-references","text":"In this example, Node has an EdgeType associated type that is constrained to implement Edge , and Edge has a NodeType associated type that is constrained to implement Node . Furthermore, the NodeType of an EdgeType is the original type, and the other way around. This is accomplished by naming and then forward declaring the constraints that can't be stated directly: // Forward declare interfaces used in // parameter lists of constraints. interface Edge; interface Node; // Forward declare named constraints used in // interface definitions. private constraint EdgeFor(N:! Node); private constraint NodeFor(E:! Edge); // Define interfaces using named constraints. interface Edge { let NodeType:! NodeFor(Self); fn Head[me: Self]() -> NodeType; } interface Node { let EdgeType:! EdgeFor(Self); fn Edges[me: Self]() -> Vector(EdgeType); } // Now that the interfaces are defined, can // refer to members of the interface, so it is // now legal to define the named constraints. constraint EdgeFor(N:! Node) { extends Edge where .NodeType == N; } constraint NodeFor(E:! Edge) { extends Node where .EdgeType == E; }","title":"Example of declaring interfaces with cyclic references"},{"location":"design/generics/details/#interfaces-with-parameters-constrained-by-the-same-interface","text":"To work around the restriction about not being able to name an interface in its parameter list , instead include that requirement in the body of the interface. // Want to require that `T` satisfies `CommonType(Self)`, // but that can't be done in the parameter list. interface CommonType(T:! Type) { let Result:! Type; // Instead add the requirement inside the definition. impl T as CommonType(Self); } Note however that CommonType is still incomplete inside its definition, so no constraints on members of CommonType are allowed. interface CommonType(T:! Type) { let Result:! Type; // \u274c Illegal: `CommonType` is incomplete impl T as CommonType(Self) where .Result == Result; } Instead, a forward-declared named constraint can be used in place of the constraint that can only be defined later. This is the same strategy used to work around cyclic references . private constraint CommonTypeResult(T:! Type, R:! Type); interface CommonType(T:! Type) { let Result:! Type; // \u2705 Allowed: `CommonTypeResult` is incomplete, but // no members are accessed. impl T as CommonTypeResult(Self, Result); } constraint CommonTypeResult(T:! Type, R:! Type) { extends CommonType(T) where .Result == R; }","title":"Interfaces with parameters constrained by the same interface"},{"location":"design/generics/details/#interface-members-with-definitions","text":"Interfaces may provide definitions for members, such as a function body for an associated function or method or a value for an associated constant. If these definitions may be overridden in implementations, they are called \"defaults\" and prefixed with the default keyword. Otherwise they are called \"final members\" and prefixed with the final keyword.","title":"Interface members with definitions"},{"location":"design/generics/details/#interface-defaults","text":"An interface may provide a default implementation of methods in terms of other methods in the interface. interface Vector { fn Add[me: Self](b: Self) -> Self; fn Scale[me: Self](v: f64) -> Self; // Default definition of `Invert` calls `Scale`. default fn Invert[me: Self]() -> Self { return me.Scale(-1.0); } } A default function or method may also be defined out of line, later in the same file as the interface definition: interface Vector { fn Add[me: Self](b: Self) -> Self; fn Scale[me: Self](v: f64) -> Self; default fn Invert[me: Self]() -> Self; } // `Vector` is considered complete at this point, // even though `Vector.Invert` is still incomplete. fn Vector.Invert[me: Self]() -> Self { return me.Scale(-1.0); } An impl of that interface for a type may omit a definition of Invert to use the default, or provide a definition to override the default. Interface defaults are helpful for evolution , as well as reducing boilerplate. Defaults address the gap between the minimum necessary for a type to provide the desired functionality of an interface and the breadth of API that developers desire. As an example, in Rust the iterator trait only has one required method but dozens of \"provided methods\" with defaults. Defaults may also be provided for associated constants, such as associated types, and interface parameters, using the = <default value> syntax. interface Add(Right:! Type = Self) { default let Result:! Type = Self; fn DoAdd[me: Self](right: Right) -> Result; } impl String as Add() { // Right == Result == Self == String fn DoAdd[me: Self](right: Self) -> Self; } Note that Self is a legal default value for an associated type or type parameter. In this case the value of those names is not determined until Self is, so Add() is equivalent to the constraint: // Equivalent to Add() constraint AddDefault { extends Add(Self); } Note also that the parenthesis are required after Add , even when all parameters are left as their default values. More generally, default expressions may reference other associated types or Self as parameters to type constructors. For example: interface Iterator { let Element:! Type; default let Pointer:! Type = Element*; } Carbon does not support providing a default implementation of a required interface. interface TotalOrder { fn TotalLess[me: Self](right: Self) -> bool; // \u274c Illegal: May not provide definition // for required interface. impl as PartialOrder { fn PartialLess[me: Self](right: Self) -> bool { return me.TotalLess(right); } } } The workaround for this restriction is to use a blanket impl instead: interface TotalOrder { fn TotalLess[me: Self](right: Self) -> bool; impl as PartialOrder; } external impl forall [T:! TotalOrder] T as PartialOrder { fn PartialLess[me: Self](right: Self) -> bool { return me.TotalLess(right); } } Note that by the orphan rule , this blanket impl must be defined in the same library as PartialOrder . Comparison with other languages: Rust supports specifying defaults for methods , interface parameters , and associated constants . Rust has found them valuable.","title":"Interface defaults"},{"location":"design/generics/details/#final-members","text":"As an alternative to providing a definition of an interface member as a default, members marked with the final keyword will not allow that definition to be overridden in impls. interface TotalOrder { fn TotalLess[me: Self](right: Self) -> bool; final fn TotalGreater[me: Self](right: Self) -> bool { return right.TotalLess(me); } } class String { impl as TotalOrder { fn TotalLess[me: Self](right: Self) -> bool { ... } // \u274c Illegal: May not provide definition of final // method `TotalGreater`. fn TotalGreater[me: Self](right: Self) -> bool { ... } } } interface Add(T:! Type = Self) { // `AddWith` *always* equals `T` final let AddWith:! Type = T; // Has a *default* of `Self` let Result:! Type = Self; fn DoAdd[me: Self](right: AddWith) -> Result; } Final members may also be defined out-of-line: interface TotalOrder { fn TotalLess[me: Self](right: Self) -> bool; final fn TotalGreater[me: Self](right: Self) -> bool; } // `TotalOrder` is considered complete at this point, even // though `TotalOrder.TotalGreater` is not yet defined. fn TotalOrder.TotalGreater[me: Self](right: Self) -> bool { return right.TotalLess(me); } There are a few reasons for this feature: When overriding would be inappropriate. Matching the functionality of non-virtual methods in base classes, so interfaces can be a replacement for inheritance. Potentially reduce dynamic dispatch when using the interface in a DynPtr . Note that this applies to associated entities, not interface parameters.","title":"final members"},{"location":"design/generics/details/#interface-requiring-other-interfaces-revisited","text":"Recall that an interface can require another interface be implemented for the type , as in: interface Iterable { impl as Equatable; // ... } This states that the type implementing the interface Iterable , which in this context is called Self , must also implement the interface Equatable . As is done with conditional conformance , we allow another type to be specified between impl and as to say some type other than Self must implement an interface. For example, interface IntLike { impl i32 as As(Self); // ... } says that if Self implements IntLike , then i32 must implement As(Self) . Similarly, interface CommonTypeWith(T:! Type) { impl T as CommonTypeWith(Self); // ... } says that if Self implements CommonTypeWith(T) , then T must implement CommonTypeWith(Self) . The previous description of impl as in an interface definition matches the behavior of using a default of Self when the type between impl and as is omitted. So the previous definition of interface Iterable is equivalent to: interface Iterable { // ... impl Self as Equatable; // Equivalent to: impl as Equatable; } When implementing an interface with an impl as requirement, that requirement must be satisfied by an implementation in an imported library, an implementation somewhere in the same file, or a constraint in the impl declaration. Implementing the requiring interface is a promise that the requirement will be implemented. This is like a forward declaration of an impl except that the definition can be broader instead of being required to match exactly. // `Iterable` requires `Equatable`, so there must be some // impl of `Equatable` for `Vector(i32)` in this file. external impl Vector(i32) as Iterable { ... } fn RequiresEquatable[T:! Equatable](x: T) { ... } fn ProcessVector(v: Vector(i32)) { // \u2705 Allowed since `Vector(i32)` is known to // implement `Equatable`. RequiresEquatable(v); } // Satisfies the requirement that `Vector(i32)` must // implement `Equatable` since `i32` is `Equatable`. external impl forall [T:! Equatable] Vector(T) as Equatable { ... } In some cases, the interface's requirement can be trivially satisfied by the implementation itself, as in: impl forall [T:! Type] T as CommonTypeWith(T) { ... } Here is an example where the requirement of interface Iterable that the type implements interface Equatable is satisfied by a constraint in the impl declaration: class Foo(T:! Type) {} // This is allowed because we know that an `impl Foo(T) as Equatable` // will exist for all types `T` for which this impl is used, even // though there's neither an imported impl nor an impl in this file. external impl forall [T:! Type where Foo(T) is Equatable] Foo(T) as Iterable {} This might be used to provide an implementation of Equatable for types that already satisfy the requirement of implementing Iterable : class Bar {} external impl Foo(Bar) as Equatable {} // Gives `Foo(Bar) is Iterable` using the blanket impl of // `Iterable` for `Foo(T)`.","title":"Interface requiring other interfaces revisited"},{"location":"design/generics/details/#requirements-with-where-constraints","text":"An interface implementation requirement with a where clause is harder to satisfy. Consider an interface B that has a requirement that interface A is also implemented. interface A(T:! Type) { let Result:! Type; } interface B(T:! Type) { impl as A(T) where .Result == i32; } An implementation of B for a set of types can only be valid if there is a visible implementation of A with the same T parameter for those types with the .Result associated type set to i32 . That is not sufficient , though, unless the implementation of A can't be specialized, either because it is marked final or is not parameterized . Implementations in other libraries can't make A be implemented for fewer types, but can cause .Result to have a different assignment.","title":"Requirements with where constraints"},{"location":"design/generics/details/#observing-a-type-implements-an-interface","text":"An observe declaration can be used to show that two types are equal so code can pass type checking without explicitly writing casts, without requiring the compiler to do a unbounded search that may not terminate. An observe declaration can also be used to show that a type implements an interface, in cases where the compiler will not work this out for itself.","title":"Observing a type implements an interface"},{"location":"design/generics/details/#observing-interface-requirements","text":"One situation where this occurs is when there is a chain of interfaces requiring other interfaces . During the impl validation done during type checking, Carbon will only consider the interfaces that are direct requirements of the interfaces the type is known to implement. An observe...is declaration can be used to add an interface that is a direct requirement to the set of interfaces whose direct requirements will be considered for that type. This allows a developer to provide a proof that there is a sequence of requirements that demonstrate that a type implements an interface, as in this example: interface A { } interface B { impl as A; } interface C { impl as B; } interface D { impl as C; } fn RequiresA[T:! A](x: T); fn RequiresC[T:! C](x: T); fn RequiresD[T:! D](x: T) { // \u2705 Allowed: `D` directly requires `C` to be implemented. RequiresC(x); // \u274c Illegal: No direct connection between `D` and `A`. // RequiresA(x); // `T` is `D` and `D` directly requires `C` to be // implemented. observe T is C; // `T` is `C` and `C` directly requires `B` to be // implemented. observe T is B; // \u2705 Allowed: `T` is `B` and `B` directly requires // `A` to be implemented. RequiresA(x); } Note that observe statements do not affect the selection of impls during code generation. For coherence, the impl used for a (type, interface) pair must always be the same, independent of context. The termination rule governs when compilation may fail when the compiler can't determine the impl to select.","title":"Observing interface requirements"},{"location":"design/generics/details/#observing-blanket-impls","text":"An observe...is declaration can also be used to observe that a type implements an interface because there is a blanket impl in terms of requirements a type is already known to satisfy. Without an observe declaration, Carbon will only use blanket impls that are directly satisfied. interface A { } interface B { } interface C { } interface D { } impl forall [T:! A] T as B { } impl forall [T:! B] T as C { } impl forall [T:! C] T as D { } fn RequiresD(T:! D)(x: T); fn RequiresB(T:! B)(x: T); fn RequiresA(T:! A)(x: T) { // \u2705 Allowed: There is a blanket implementation // of `B` for types implementing `A`. RequiresB(x); // \u274c Illegal: No implementation of `D` for type // `T` implementing `A` // RequiresD(x); // There is a blanket implementation of `B` for // types implementing `A`. observe T is B; // There is a blanket implementation of `C` for // types implementing `B`. observe T is C; // \u2705 Allowed: There is a blanket implementation // of `D` for types implementing `C`. RequiresD(x); } In the case of an error, a quality Carbon implementation will do a deeper search for chains of requirements and blanket impls and suggest observe declarations that would make the code compile if any solution is found.","title":"Observing blanket impls"},{"location":"design/generics/details/#operator-overloading","text":"Operations are overloaded for a type by implementing an interface specific to that interface for that type. For example, types implement the Negatable interface to overload the unary - operator: // Unary `-`. interface Negatable { let Result:! Type = Self; fn Negate[me: Self]() -> Result; } Expressions using operators are rewritten into calls to these interface methods. For example, -x would be rewritten to x.(Negatable.Negate)() . The interfaces and rewrites used for a given operator may be found in the expressions design . Question-for-leads issue #1058 defines the naming scheme for these interfaces.","title":"Operator overloading"},{"location":"design/generics/details/#binary-operators","text":"Binary operators will have an interface that is parameterized based on the second operand. For example, to say a type may be converted to another type using an as expression, implement the As interface : interface As(Dest:! Type) { fn Convert[me: Self]() -> Dest; } The expression x as U is rewritten to x.(As(U).Convert)() . Note that the parameterization of the interface means it can be implemented multiple times to support multiple operand types. Unlike as , for most binary operators the interface's argument will be the type of the right-hand operand instead of its value . Consider an interface for a binary operator like * : // Binary `*`. interface MultipliableWith(U:! Type) { let Result:! Type = Self; fn Multiply[me: Self](other: U) -> Result; } A use of binary * in source code will be rewritten to use this interface: var left: Meters = ...; var right: f64 = ...; var result: auto = left * right; // Equivalent to: var equivalent: left.(MultipliableWith(f64).Result) = left.(MultipliableWith(f64).Multiply)(right); Note that if the types of the two operands are different, then swapping the order of the operands will result in a different implementation being selected. It is up to the developer to make those consistent when that is appropriate. The standard library will provide adapters for defining the second implementation from the first, as in: interface ComparableWith(RHS:! Type) { fn Compare[me: Self](right: RHS) -> CompareResult; } adapter ReverseComparison (T:! Type, U:! ComparableWith(RHS)) for T { impl as ComparableWith(U) { fn Compare[me: Self](right: RHS) -> CompareResult { return ReverseCompareResult(right.Compare(me)); } } } external impl SongByTitle as ComparableWith(SongTitle); external impl SongTitle as ComparableWith(SongByTitle) = ReverseComparison(SongTitle, SongByTitle); In some cases the reverse operation may not be defined. For example, a library might support subtracting a vector from a point, but not the other way around. Further note that even if the reverse implementation exists, the impl prioritization rule might not pick it. For example, if we have two types that support comparison with anything implementing an interface that the other implements: interface IntLike { fn AsInt[me: Self]() -> i64; } class EvenInt { ... } external impl EvenInt as IntLike; external impl EvenInt as ComparableWith(EvenInt); // Allow `EvenInt` to be compared with anything that // implements `IntLike`, in either order. external impl forall [T:! IntLike] EvenInt as ComparableWith(T); external impl forall [T:! IntLike] T as ComparableWith(EvenInt); class PositiveInt { ... } external impl PositiveInt as IntLike; external impl PositiveInt as ComparableWith(PositiveInt); // Allow `PositiveInt` to be compared with anything that // implements `IntLike`, in either order. external impl forall [T:! IntLike] PositiveInt as ComparableWith(T); external impl forall [T:! IntLike] T as ComparableWith(PositiveInt); Then it will favor selecting the implementation based on the type of the left-hand operand: var even: EvenInt = ...; var positive: PositiveInt = ...; // Uses `EvenInt as ComparableWith(T)` impl if (even < positive) { ... } // Uses `PositiveInt as ComparableWith(T)` impl if (positive > even) { ... }","title":"Binary operators"},{"location":"design/generics/details/#like-operator-for-implicit-conversions","text":"Because the type of the operands is directly used to select the implementation to use, there are no automatic implicit conversions, unlike with function or method calls. Given both a method and an interface implementation for multiplying by a value of type f64 : class Meters { fn Scale[me: Self](s: f64) -> Self; } // \"Implementation One\" external impl Meters as MultipliableWith(f64) where .Result = Meters { fn Multiply[me: Self](other: f64) -> Result { return me.Scale(other); } } the method will work with any argument that can be implicitly converted to f64 but the operator overload will only work with values that have the specific type of f64 : var height: Meters = ...; var scale: f32 = 1.25; // \u2705 Allowed: `scale` implicitly converted // from `f32` to `f64`. var allowed: Meters = height.Scale(scale); // \u274c Illegal: `Meters` doesn't implement // `MultipliableWith(f32)`. var illegal: Meters = height * scale; The workaround is to define a parameterized implementation that performs the conversion. The implementation is for types that implement the ImplicitAs interface . // \"Implementation Two\" external impl forall [T:! ImplicitAs(f64)] Meters as MultipliableWith(T) where .Result = Meters { fn Multiply[me: Self](other: T) -> Result { // Carbon will implicitly convert `other` from type // `T` to `f64` to perform this call. return me.(Meters.(MultipliableWith(f64).Multiply))(other); } } // \u2705 Allowed: uses `Meters as MultipliableWith(T)` impl // with `T == f32` since `f32 is ImplicitAs(f64)`. var now_allowed: Meters = height * scale; Observe that the prioritization rule will still prefer the unparameterized impl when there is an exact match. To reduce the boilerplate needed to support these implicit conversions when defining operator overloads, Carbon has the like operator. This operator can only be used in the type or type-of-type part of an impl declaration, as part of a forward declaration or definition, in a place of a type. // Notice `f64` has been replaced by `like f64` // compared to \"implementation one\" above. external impl Meters as MultipliableWith(like f64) where .Result = Meters { fn Multiply[me: Self](other: f64) -> Result { return me.Scale(other); } } This impl definition actually defines two implementations. The first is the same as this definition with like f64 replaced by f64 , giving something equivalent to \"implementation one\". The second implementation replaces the like f64 with a parameter that ranges over types that can be implicitly converted to f64 , equivalent to \"implementation two\". In general, each like adds one additional impl. There is always the impl with all of the like expressions replaced by their arguments with the definition supplied in the source code. In addition, for each like expression, there is an impl with it replaced by a new parameter. These additional impls will delegate to the main impl, which will trigger implicit conversions according to Carbon's ordinary implicit conversion rules . In this example, there are two uses of like , producing three implementations external impl like Meters as MultipliableWith(like f64) where .Result = Meters { fn Multiply[me: Self](other: f64) -> Result { return me.Scale(other); } } is equivalent to \"implementation one\", \"implementation two\", and: external impl forall [T:! ImplicitAs(Meters)] T as MultipliableWith(f64) where .Result = Meters { fn Multiply[me: Self](other: f64) -> Result { // Will implicitly convert `me` to `Meters` in order to // match the signature of this `Multiply` method. return me.(Meters.(MultipliableWith(f64).Multiply))(other); } } like may be used in forward declarations in a way analogous to impl definitions. external impl like Meters as MultipliableWith(like f64) where .Result = Meters; } is equivalent to: // All `like`s removed. Same as the declaration part of // \"implementation one\", without the body of the definition. external impl Meters as MultipliableWith(f64) where .Result = Meters; // First `like` replaced with a wildcard. external impl forall [T:! ImplicitAs(Meters)] T as MultipliableWith(f64) where .Result = Meters; // Second `like` replaced with a wildcard. Same as the // declaration part of \"implementation two\", without the // body of the definition. external impl forall [T:! ImplicitAs(f64)] Meters as MultipliableWith(T) where .Result = Meters; In addition, the generated impl definition for a like is implicitly injected at the end of the (unique) source file in which the impl is first declared. That is, it is injected in the API file if the impl is declared in an API file, and in the sole impl file declaring the impl otherwise. This means an impl declaration using like in an API file also makes the parameterized definition If one impl declaration uses like , other declarations must use like in the same way to match. The like operator may be nested, as in: external impl like Vector(like String) as Printable; Which will generate implementations with declarations: external impl Vector(String) as Printable; external impl forall [T:! ImplicitAs(Vector(String))] T as Printable; external impl forall [T:! ImplicitAs(String)] Vector(T) as Printable; The generated implementations must be legal or the like is illegal. For example, it must be legal to define those impls in this library by the orphan rule . In addition, the generated impl definitions must only require implicit conversions that are guaranteed to exist. For example, there existing an implicit conversion from T to String does not imply that there is one from Vector(T) to Vector(String) , so the following use of like is illegal: // \u274c Illegal: Can't convert a value with type // `Vector(T:! ImplicitAs(String))` // to `Vector(String)` for `me` // parameter of `Printable.Print`. external impl Vector(like String) as Printable; Since the additional implementation definitions are generated eagerly, these errors will be reported in the file with the first declaration. The argument to like must either not mention any type parameters, or those parameters must be able to be determined due to being repeated outside of the like expression. // \u2705 Allowed: no parameters external impl like Meters as Printable; // \u274c Illegal: No other way to determine `T` external impl forall [T:! IntLike] like T as Printable; // \u274c Illegal: `T` being used in a `where` clause // is insufficient. external impl forall [T:! IntLike] like T as MultipliableWith(i64) where .Result = T; // \u274c Illegal: `like` can't be used in a `where` // clause. external impl Meters as MultipliableWith(f64) where .Result = like Meters; // \u2705 Allowed: `T` can be determined by another // part of the query. external impl forall [T:! IntLike] like T as MultipliableWith(T) where .Result = T; external impl forall [T:! IntLike] T as MultipliableWith(like T) where .Result = T; // \u2705 Allowed: Only one `like` used at a time, so this // is equivalent to the above two examples. external impl forall [T:! IntLike] like T as MultipliableWith(like T) where .Result = T;","title":"like operator for implicit conversions"},{"location":"design/generics/details/#parameterized-types","text":"Types may have generic parameters. Those parameters may be used to specify types in the declarations of its members, such as data fields, member functions, and even interfaces being implemented. For example, a container type might be parameterized by the type of its elements: class HashMap( KeyType:! Hashable & EqualityComparable & Movable, ValueType:! Movable) { // `Self` is `HashMap(KeyType, ValueType)`. // Parameters may be used in function signatures. fn Insert[addr me: Self*](k: KeyType, v: ValueType); // Parameters may be used in field types. private var buckets: Vector((KeyType, ValueType)); // Parameters may be used in interfaces implemented. impl as Container where .ElementType = (KeyType, ValueType); impl as ComparableWith(HashMap(KeyType, ValueType)); } Note that, unlike functions, every parameter to a type must either be generic or template, using :! or template...:! , not dynamic, with a plain : . Two types are the same if they have the same name and the same arguments. Carbon's manual type equality approach means that the compiler may not always be able to tell when two type expressions are equal without help from the user, in the form of observe declarations . This means Carbon will not in general be able to determine when types are unequal. Unlike an interface's parameters , a type's parameters may be deduced , as in: fn ContainsKey[KeyType:! Movable, ValueType:! Movable] (haystack: HashMap(KeyType, ValueType), needle: KeyType) -> bool { ... } fn MyMapContains(s: String) { var map: HashMap(String, i32) = ((\"foo\", 3), (\"bar\", 5)); // \u2705 Deduces `KeyType` = `String` from the types of both arguments. // Deduces `ValueType` = `i32` from the type of the first argument. return ContainsKey(map, s); } Note that restrictions on the type's parameters from the type's declaration can be implied constraints on the function's parameters.","title":"Parameterized types"},{"location":"design/generics/details/#specialization","text":"Specialization is used to improve performance in specific cases when a general strategy would be inefficient. For example, you might use binary search for containers that support random access and keep their contents in sorted order but linear search in other cases. Types, like functions, may not be specialized directly in Carbon. This effect can be achieved, however, through delegation. For example, imagine we have a parameterized class Optional(T) that has a default storage strategy that works for all T , but for some types we have a more efficient approach. For pointers we can use a null value to represent \"no pointer\", and for booleans we can support True , False , and None in a single byte. Clients of the optional library may want to add additional specializations for their own types. We make an interface that represents \"the storage of Optional(T) for type T ,\" written here as OptionalStorage : interface OptionalStorage { let Storage:! Type; fn MakeNone() -> Storage; fn Make(x: Self) -> Storage; fn IsNone(x: Storage) -> bool; fn Unwrap(x: Storage) -> Self; } The default implementation of this interface is provided by a blanket implementation : // Default blanket implementation impl forall [T:! Movable] T as OptionalStorage where .Storage = (bool, T) { ... } This implementation can then be specialized for more specific type patterns: // Specialization for pointers, using nullptr == None final external impl forall [T:! Type] T* as OptionalStorage where .Storage = Array(Byte, sizeof(T*)) { ... } // Specialization for type `bool`. final external impl bool as OptionalStorage where .Storage = Byte { ... } Further, libraries can implement OptionalStorage for their own types, assuming the interface is not marked private . Then the implementation of Optional(T) can delegate to OptionalStorage for anything that can vary with T : class Optional(T:! Movable) { fn None() -> Self { return {.storage = T.(OptionalStorage.MakeNone)()}; } fn Some(x: T) -> Self { return {.storage = T.(OptionalStorage.Make)(x)}; } ... private var storage: T.(OptionalStorage.Storage); } Note that the constraint on T is just Movable , not Movable & OptionalStorage , since the Movable requirement is sufficient to guarantee that some implementation of OptionalStorage exists for T . Carbon does not require callers of Optional , even generic callers, to specify that the argument type implements OptionalStorage : // \u2705 Allowed: `T` just needs to be `Movable` to form `Optional(T)`. // A `T:! OptionalStorage` constraint is not required. fn First[T:! Movable & Eq](v: Vector(T)) -> Optional(T); Adding OptionalStorage to the constraints on the parameter to Optional would obscure what types can be used as arguments. OptionalStorage is an implementation detail of Optional and need not appear in its public API. In this example, a let is used to avoid repeating OptionalStorage in the definition of Optional , since it has no name conflicts with the members of Movable : class Optional(T:! Movable) { private let U:! Movable & OptionalStorage = T; fn None() -> Self { return {.storage = U.MakeNone()}; } fn Some(x: T) -> Self { return {.storage = u.Make(x)}; } ... private var storage: U.Storage; }","title":"Specialization"},{"location":"design/generics/details/#future-work","text":"","title":"Future work"},{"location":"design/generics/details/#dynamic-types","text":"Generics provide enough structure to support runtime dispatch for values with types that vary at runtime, without giving up type safety. Both Rust and Swift have demonstrated the value of this feature.","title":"Dynamic types"},{"location":"design/generics/details/#runtime-type-parameters","text":"This feature is about allowing a function's type parameter to be passed in as a dynamic (non-generic) parameter. All values of that type would still be required to have the same type.","title":"Runtime type parameters"},{"location":"design/generics/details/#runtime-type-fields","text":"Instead of passing in a single type parameter to a function, we could store a type per value. This changes the data layout of the value, and so is a somewhat more invasive change. It also means that when a function operates on multiple values they could have different real types.","title":"Runtime type fields"},{"location":"design/generics/details/#abstract-return-types","text":"This lets you return an anonymous type implementing an interface from a function. In Rust this is the impl Trait return type . In Swift, there are discussions about implementing this feature under the name \"reverse generics\" or \"opaque result types\": 1 , 2 , 3 , 4 , Swift is considering spelling this <V: Collection> V or some Collection .","title":"Abstract return types"},{"location":"design/generics/details/#evolution","text":"There are a collection of use cases for making different changes to interfaces that are already in use. These should be addressed either by describing how they can be accomplished with existing generics features, or by adding features. In addition, evolution from (C++ or Carbon) templates to generics needs to be supported and made safe.","title":"Evolution"},{"location":"design/generics/details/#testing","text":"The idea is that you would write tests alongside an interface that validate the expected behavior of any type implementing that interface.","title":"Testing"},{"location":"design/generics/details/#impls-with-state","text":"A feature we might consider where an impl itself can have state.","title":"Impls with state"},{"location":"design/generics/details/#generic-associated-types-and-higher-ranked-types","text":"This would be some way to express the requirement that there is a way to go from a type to an implementation of an interface parameterized by that type.","title":"Generic associated types and higher-ranked types"},{"location":"design/generics/details/#generic-associated-types","text":"Generic associated types are about when this is a requirement of an interface. These are also called \"associated type constructors.\"","title":"Generic associated types"},{"location":"design/generics/details/#higher-ranked-types","text":"Higher-ranked types are used to represent this requirement in a function signature. They can be emulated using generic associated types .","title":"Higher-ranked types"},{"location":"design/generics/details/#field-requirements","text":"We might want to allow interfaces to express the requirement that any implementing type has a particular field. This would be to match the expressivity of inheritance, which can express \"all subtypes start with this list of fields.\"","title":"Field requirements"},{"location":"design/generics/details/#bridge-for-c-customization-points","text":"See details in the goals document .","title":"Bridge for C++ customization points"},{"location":"design/generics/details/#variadic-arguments","text":"Some facility for allowing a function to generically take a variable number of arguments.","title":"Variadic arguments"},{"location":"design/generics/details/#range-constraints-on-generic-integers","text":"We currently only support where clauses on type-of-types. We may want to also support constraints on generic integers. The constraint with the most expected value is the ability to do comparisons like < , or >= . For example, you might constrain the N member of NSpacePoint using an expression like PointT:! NSpacePoint where 2 <= .N and .N <= 3 . The concern here is supporting this at compile time with more benefit than complexity. For example, we probably don't want to support integer-range based types at runtime, and there are also concerns about reasoning about comparisons between multiple generic integer parameters. For example, if J < K and K <= L , can we call a function that requires J < L ? There is also a secondary syntactic concern about how to write this kind of constraint on a parameter, as opposed to an associated type, as in N:! u32 where ___ >= 2 .","title":"Range constraints on generic integers"},{"location":"design/generics/details/#references","text":"#553: Generics details part 1 #731: Generics details 2: adapters, associated types, parameterized interfaces #818: Constraints for generics (generics details 3) #931: Generic impls access (details 4) #920: Generic parameterized impls (details 5) #950: Generic details 6: remove facets #983: Generic details 7: final impls #990: Generics details 8: interface default and final members #1013: Generics: Set associated constants using where constraints #1084: Generics details 9: forward declarations #1088: Generic details 10: interface-implemented requirements #1144: Generic details 11: operator overloading #1146: Generic details 12: parameterized types #1327: Generics: impl forall","title":"References"},{"location":"design/generics/goals/","text":"Generics: Goals Table of contents Purpose of this document Background Generic parameters Interfaces Relationship to templates Goals Use cases Generic programming Upgrade path from C++ abstract interfaces Dependency injection Generics instead of open overloading and ADL Performance Better compiler experience Encapsulation Predictability Dispatch control Upgrade path from templates Path from regular functions Coherence No novel name lookup Learn from others Interfaces are nominal Interop and evolution Bridge for C++ customization points What we are not doing Not the full flexibility of templates Template use cases that are out of scope Generics will be checked when defined Specialization strategy References Purpose of this document This document attempts to clarify our goals for the design of the generics feature for Carbon. While these are not strict requirements, they represent the yardstick by which we evaluate design decisions. We do expect to achieve most of these goals, though some of these goals are somewhat aspirational or forward-looking. Background Carbon will support generics to support generic programming by way of parameterization of language constructs with early type checking and complete definition checking . This is in contrast with the compile-time duck typing approach of C++ templates, and in addition to template support in Carbon , if we decide to support templates in Carbon beyond interoperability with C++ templates. Generic parameters Generic functions and generic types will all take some \"generic parameters\", which will frequently be types, and in some cases will be deduced from the types of the values of explicit parameters. If a generic parameter is a type, the generic function's signature can specify constraints that the caller's type must satisfy. For example, a resizable array type (like C++'s std::vector ) might have a generic type parameter with the constraint that the type must be movable and have a static size. A sort function might apply to any array whose elements are comparable and movable. A constraint might involve multiple generic parameters. For example, a merge function might apply to two arbitrary containers so long as their elements have the same type. Interfaces We need some way to express the constraints on a generic type parameter. In Carbon we express these \"type constraints\" by saying we restrict to types that implement specific interfaces . Interfaces describe an API a type could implement; for example, it might specify a set of functions, including names and signatures. A type implementing an interface may be passed as a generic type argument to a function that has that interface as a requirement of its generic type parameter. Then, the functions defined in the interface may be called in the body of the function. Further, interfaces have names that allow them to be reused. Similar compile-time and run-time constructs may be found in other programming languages: Rust's traits Swift's protocols Java interfaces C++ concepts (compile-time only) Abstract base classes in C++, etc. (run-time only) Go interfaces (run-time only) In addition to specifying the methods available on a type, we may in the future expand the role of interfaces to allow other type constraints, such as on size, prefix of the data layout, specified method implementations, tests that must pass, etc. This might be part of making interfaces as expressive as classes, as part of a strategy to migrate to a future version of Carbon that uses interfaces instead of, rather than in addition to, standard inheritance-and-classes object-oriented language support. For the moment, everything beyond specifying the methods available is out of scope. Relationship to templates The entire idea of statically typed languages is that coding against specific types and interfaces is a better model and experience. Unfortunately, templates don't provide many of those benefits to programmers until it's too late, when users are consuming the API. Templates also come with high overhead, such as template error messages . We want Carbon code to move towards more rigorously type checked constructs. However, existing C++ code is full of unrestricted usage of compile-time duck-typed templates. They are incredibly convenient to write and so likely will continue to exist for a long time. The question of whether Carbon has direct support for templates is out of scope for this document. The generics design is not completely separate from templates, so it is written as if Carbon will have its own templating system. It is assumed to be similar to C++ templates with some specific changes: It may have some limitations to be more compatible with generics, much like how we restrict overloading . We likely will have a different method of selecting between different template instantiations, since SFINAE makes it difficult to deliver high quality compiler diagnostics. We assume Carbon will have templates for a few different reasons: Carbon generics will definitely have to interact with C++ templates, and many of the issues will be similar. We want to leave room in the design for templates, since it seems like it would be easier to remove templates if they are not pulling their weight than figure out how to add them in if they turn out to be needed. We may want to have templates in Carbon as a temporary measure, to make it easier for users to transition off of C++ templates. Goals Our goal for generics support in Carbon is to get most of the expressive benefits of C++ templates and open overloading with fewer downsides. Additionally, we want to support some dynamic dispatch use cases; for example, in cases that inheritance struggles with. Use cases To clarify the expressive range we are aming for, here are some specific use cases we expect Carbon generics to cover. Generic programming We in particular want to support generic programming , including: Containers: arrays, maps, lists, and more complicated data structures like trees and graphs Algorithms: sort, search Wrappers: optional, variant, expected/result, smart pointers Parameterized numeric types: std::complex<T> Configurable and parametric APIs: the storage-customized std::chrono APIs Policy-based design These would generally involve static, compile-time type arguments, and so would generally be used with static dispatch . Upgrade path from C++ abstract interfaces Interfaces in C++ are often represented by abstract base classes. Generics should offer an alternative that does not rely on inheritance. This means looser coupling and none of the problems of multiple inheritance. Some people, such as Sean Parent , advocate for runtime polymorphism patterns in C++ that avoid inheritance because it can cause runtime performance, correctness, and code maintenance problems in some situations. Those patterns require a lot of boilerplate and complexity in C++. It would be nice if those patterns were simpler to express with Carbon generics. More generally, Carbon generics will provide an alternative for those situations inheritance doesn't handle as well. As a specific example, we would like Carbon generics to supplant the need to support multiple inheritance in Carbon. This is a case that would use dynamic dispatch . Dependency injection Types which only support subclassing for test stubs and mocks, as in \"dependency injection\" , should be able to easily migrate to generics. This extends outside the realm of testing, allowing general configuration of how dependencies can be satisfied. For example, generics might be used to configure how a library writes logs. This would allow you to avoid the runtime overhead of virtual functions, using static dispatch without the poor build experience of templates . Generics instead of open overloading and ADL One name lookup problem we would like to avoid is caused by open overloading. Overloading is where you provide multiple implementations of a function with the same name, and the implementation used in a specific context is determined by the argument types. Open overloading is overloading where the overload set is not restricted to a single file or library. This works with Argument-dependent lookup , or ADL , a mechanism for enabling open overloading without having to reopen the namespace where the function was originally defined. Together these enable C++ customization points . This is commonly used to provide a type-specific implementation of some operation, but doesn't provide any enforcement of consistency across the different overloads. It makes the meaning of code dependent on which overloads are imported, and is at odds with being able to type check a function generically. Our goal is to address this use case, known more generally as the expression problem , with a generics mechanism that does enforce consistency so that type checking is possible without seeing all implementations. This will be Carbon's replacement for open overloading. As a consequence, Carbon generics will need to be able to support operator overloading. A specific example is the absolute value function Abs . We would like to write Abs(x) for a variety of types. For some types T , such as Int32 or Float64 , the return type will be the same T . For other types, such as Complex64 or Quaternion , the return type will be different. The generic functions that call Abs will need a way to specify whether they only operate on T such that Abs has signature T -> T . This does create an issue when interoperating with C++ code using open overloading, which will need to be addressed . Performance For any real-world C++ template, there shall be an idiomatic reformulation in Carbon generics that has equal or better performance. Performance is the top priority for Carbon , and we expect to use generics pervasively, and so they can't compromise that goal in release builds. Nice to have: There are cases where we should aim to do better than C++ templates. For example, the additional structure of generics should make it easier to reduce generated code duplication, reducing code size and cache misses. Better compiler experience Compared to C++ templates, we expect to reduce build times, particularly in development builds. We also expect the compiler to be able to report clearer errors, and report them earlier in the build process. One source of improvement is that the bodies of generic functions and types can be type checked once when they are defined, instead of every time they are used. This is both a reduction in the total work done, and how errors can be reported earlier. On use, the errors can be a lot clearer since they will be of the form \"argument did not satisfy function's contract as stated in its signature\" instead of \"substitution failed at this line of the function's implementation.\" Nice to have: In development builds, we will have the option of using dynamic dispatch to reduce build times. We may also be able to reduce the amount of redundant compilation work even with the static strategy by identifying instantiations with the same arguments or identical implementations and only generating code for them once. Encapsulation With a template, the implementation is part of the interface and types are only checked when the function is called and the template is instantiated. A generic function is type checked when it is defined, and type checking can't use any information that is only known when the function is instantiated such as the exact argument types. Furthermore, calls to a generic function may be type checked using only its declaration, not its body. You should be able to call a generic function using only a forward declaration. Predictability A general property of generics is they are more predictable than templates. They make clear when a type satisfies the requirements of a function; they have a documented contract. Further, that contract is enforced by the compiler, not sensitive to implementation details in the function body. This eases evolution by reducing (but not eliminating) the impact of Hyrum's law . Nice to have: We also want well-defined boundaries between what is legal and not. This is \"will my code be accepted by the compiler\" predictability. We would prefer to avoid algorithms in the compiler with the form \"run for up to N steps and report an error if it isn't resolved by then.\" For example, C++ compilers will typically have a template recursion limit. With generics, these problems arise due to trying to reason whether something is legal in all possible instantiations, rather than with specific, concrete types. Some of this is likely unavoidable or too costly to avoid, as most existing generics systems have undecidable aspects to their type system , including Rust and Swift . We fully expect there to be metaprogramming facilities in Carbon that will be able to execute arbitrary Turing machines, with infinite loops and undecidable stopping criteria. We don't see this as a problem though, just like we don't worry about trying to make the compiler reliably prevent you from writing programs that don't terminate. We would like to distinguish \"the executed steps are present in the program's source\" from \"the compiler has to search for a proof that the code is legal.\" In the former case, the compiler can surface a problem to the user by pointing to lines of code in a trace of execution. The user could employ traditional debugging techniques to refine their understanding until they can determine a fix. What we want to avoid is the latter case, since it has bad properties: Error messages end up in the form: \"this was too complicated to figure out, I eventually gave up.\" Little in the way of actionable feedback on how to fix problems. Not much the user can do to debug problems. If the compiler is currently right at a limit for figuring something out, it is easy to imagine a change to a distant dependency can cause it to suddenly stop compiling. If we can't find acceptable restrictions to make problems efficiently decidable, the next best solution is to require the proof to be in the source instead of derived by the compiler. If authoring the proof is too painful for the user, the we should invest in putting the proof search into IDEs or other tooling. Dispatch control Enable simple user control of whether to use dynamic or static dispatch. Implementation strategy: There are two strategies for generating code for generic functions: Static specialization strategy: Like template parameters, the values for generic parameters must be statically known at the callsite, or known to be a generic parameter to the calling function. This can generate separate, specialized versions of each combination of generic and template arguments, in order to optimize for those types or values. Dynamic strategy: This is when the compiler generates a single version of the function that uses runtime dispatch to get something semantically equivalent to separate instantiation, but likely with different size, build time, and performance characteristics. By default, we expect the implementation strategy to be controlled by the compiler, and not semantically visible to the user. For example, the compiler might use the static strategy for release builds and the dynamic strategy for development. Or it might choose between them on a more granular level based on code analysis, specific features used in the code, or profiling -- maybe some specific specializations are needed for performance, but others would just be code bloat. We require that all generic functions can be compiled using the static specialization strategy. For example, the values for generic parameters must be statically known at the callsite. Other limitations are listed below . Nice to have: It is desirable that the majority of functions with generic parameters also support the dynamic strategy. Specific features may prevent the compiler from using the dynamic strategy, but they should ideally be relatively rare, and easy to identify. Language features should avoid making it observable whether function code generated once or many times. For example, you should not be able to take the address of a function with generic parameters, or determine if a function was instantiated more than once using function-local static variables. There are a few obstacles to supporting dynamic dispatch efficiently, which may limit the extent it is used automatically by implementations. For example, the following features would benefit substantially from guaranteed monomorphization: Field packing in class layout. For example, packing a Bool into the lower bits of a pointer, or packing bit-fields with generic widths. Allocating local variables in stack storage. Without monomorphization, we would need to perform dynamic memory allocation -- whether on the stack or the heap -- for local variables whose sizes depend on generic parameters. Passing parameters to functions. We cannot pass values of generic types in registers. While it is possible to address these with dynamic dispatch, handling some of them might have far-reaching and surprising performance implications. We don't want to compromise our goal for predictable performance. We will allow the user to explicitly opt-in to using the dynamic strategy in specific cases. This could be just to control binary size in cases the user knows are not performance sensitive, or it could be to get the additional capability of operating on values with dynamic types. We may need to restrict this in various ways to maintain efficiency, like Rust does with object-safe traits. We also anticipate that the user may want to force the compiler to use the static strategy in specific cases. This might be to keep runtime performance acceptable even when running a development or debug build. Upgrade path from templates We want there to be a natural, incremental upgrade path from templated code to generic code. Assuming Carbon will support templates directly , the first step of migrating C++ template code would be to first convert it to a Carbon template. The problem is then how to convert templates to generics within Carbon. This gives us these sub-goals: Users should be able to convert a single template parameter to be generic at a time. A hybrid function with both template and generic parameters has all the limitations of a template function: it can't be completely definition checked, it can't use the dynamic strategy, etc. Even so, there are still benefits from enforcing the function's declared contract for those parameters that have been converted. Converting from a template parameter to a generic parameter should be safe. It should either work or fail to compile, never silently change semantics. We should minimize the effort to convert functions and types from templated to generic. Ideally it should just require specifying the type constraints, affecting just the signature of the function, not its body. Nice to have: It should be legal to call templated code from generic code when it would have the same semantics as if called from non-generic code, and an error otherwise. This is to allow more templated functions to be converted to generics, instead of requiring them to be converted specifically in bottom-up order. Nice to have: Provide a way to migrate from a template to a generic without immediately updating all of the types used with the template. For example, if the generic code requires types to implement a new interface, one possible solution would use the original template code to provide an implementation for that interface for any type that structurally has the methods used by the original template. If Carbon does not end up having direct support for templates, the transition will necessarily be less incremental. Path from regular functions Replacing a regular, non-parameterized function with a generic function should not affect existing callers of the function. There may be some differences, such as when taking the address of the function, but ordinary calls should not see any difference. In particular, the return type of a generic function should match, without any type erasure or additional named members. Coherence We want the generics system to have the coherence property , so that the implementation of an interface for a type is well defined. Since a generic function only depends on interface implementations, they will always behave consistently on a given type, independent of context. For more on this, see this description of what coherence is and why Rust enforces it . Coherence greatly simplifies the language design, since it reduces the need for complicated rules to picking an implementation when there are many candidates. It also has a number of benefits for users: It removes a way packages can conflict with each other. It makes the behavior of code more consistent and predictable. It means there is no need to provide a disambiguation mechanism. Disambiguation is particularly problematic since the ambiguous call is often in generic code rather than code you control. A consistent definition of a type is useful for instantiating a C++ or Carbon template on that type. The main downside of coherence is that there are some capabilities we would like for interfaces that are in tension with having an orphan rule limiting where implementations may be defined. For example, we would like to address the expression problem . We can get some of the way there by allowing the implementation of an interface for a type to be defined with either the interface or the type. But some use cases remain: They should be some way of selecting between multiple implementations of an interface for a given type. For example, a Song might support multiple orderings, such as by title or by artist. These would be represented by having multiple implementations of a Comparable interface. In order to allow libraries to be composed, there must be some way of saying a type implements an interface that is in another package that the authors of the type were unaware of. This is especially important since the library a type is defined in may not be able to see the interface definition without creating a dependency cycle or layering violation. We should have some mechanism for addressing these use cases. There are multiple approaches that could work: Interface implementations could be external to types and are passed in to generic functions separately. There could be some way to create multiple types that are compatible with a given value that you can switch between using casts to select different interface implementations. This is the approach used by Rust ( 1 , 2 ). Alternatives to coherence are discussed in an appendix . No novel name lookup We want to avoid adding rules for name lookup that are specific to generics. This is in contrast to Rust which has different lookup rules inside its traits. Instead, we should structure generics in a way that reuses existing name lookup facilities of the language. Nice to have: One application of this that would be nice to have is if the names of a type's members were all determined by a type's definition. So if x has type T , then if you write x.y you should be able to look up y in the definition of T . This might need to be somewhat indirect in some cases. For example, if T inherits from U , the name y might come from U and not be mentioned in the definition of T directly. We may have similar mechanisms where T gets methods that have default implementations in interfaces it implements, as long as the names of those interfaces are explicitly mentioned in the definition of T . Learn from others Many languages have implemented generics systems, and we should learn from those experiences. We should copy what works and makes sense in the context of Carbon, and change decisions that led to undesirable compromises. We are taking the strongest guidance from Rust and Swift, which have similar goals and significant experience with the implementation and usability of generics. They both use nominal interfaces, were designed with generics from the start, and produce native code. Contrast with Go which uses structural interfaces, or Java which targets a virtual machine that predated its generics feature. For example, Rust has found that supporting defaults for interface methods is a valuable feature. It is useful for evolution , implementation reuse, and for bridging the gap between the minimal functionality a type wants to implement and the rich API that users want to consume ( example ). We still have the flexibility to make simplifications that Rust cannot because they need to maintain compatibility. We could remove the concept of fundamental and explicit control over which methods may be specialized. These are complicated and impose coherence restrictions . Interfaces are nominal Interfaces can either be structural , as in Go, or nominal , as in Rust and Swift. Structural interfaces match any type that has the required methods, whereas nominal interfaces only match if there is an explicit declaration stating that the interface is implemented for that specific type. Carbon will support nominal interfaces, allowing them to designate semantics beyond the basic structure of the methods. This means that interfaces implicitly specify the intended semantics and invariants of and between those functions. Unlike the function signatures, this contract is between the implementers and the consumers of interfaces and is not enforced by Carbon itself. For example, a Draw method would mean different things when it is part of a GameResult interface versus an Image2D interface, even if those methods happen to have the same signature. Interop and evolution Evolution is a high priority for Carbon , and so will need mechanisms to support evolution when using generics. New additions to an interface might: need default implementations be marked \"upcoming\" to allow for a period of transition replace other APIs that need to be marked \"deprecated\" Experience with C++ concepts has shown that interfaces are hard to evolve without these kinds of supporting language mechanisms. Otherwise changes to interfaces need to made simultaneously with updates to types that implement the interface or functions that consume it. Another way of supporting evolution is to allow one interface to be substitutable for another. For example, a feature that lets you use an implementation of Interface1 for a type to automatically get an implementation of Interface2 , as well as the other way around, would help transitioning between those two interfaces. Evolution in particular means that the set of names in an interface can change, and so two interfaces that don't start with name conflicts can develop them. To handle name conflicts, interfaces should be separate, isolated namespaces. We should provide mechanisms to allow one type to implement two interfaces that accidentally use the same name for different things, and for functions to use interfaces with name conflicts together on a single type. Contrast this with Swift, where a type can only supply one associated type of a given name even when implementing multiple protocols. Similarly a function in Swift with a given name and signature can only have a single implementation for a type. Note this is possible since interfaces are nominal . The place where types specify that they implement an interface is also the vehicle for unambiguously designating which function implementation goes with what interface. Bridge for C++ customization points There will need to be some bridge for C++ extension points that currently rely on open overloading or ADL . For example, we need some way for C++ customization points like swap to work on Carbon types. We might define CPlusPlus.ADL.swap as a Carbon interface to be that bridge. Carbon types could implement that interface to work from C++, and Carbon functions could use that interface to invoke swap on C++ types. Similarly, we will want some way to implement Carbon interfaces for C++ types. For example, we might have a template implementation of an Addable interface for any C++ type that implements operator+ . What we are not doing What are we not doing with generics, particularly things that some other languages do? Not the full flexibility of templates Generics don't need to provide full flexibility of C++ templates: The current assumption is that Carbon templates will cover those cases that don't fit inside generics, such as code that relies on compile-time duck typing. We won't allow a specialization of some generic interface for some particular type to actually expose a different interface, with different methods or different types in method signatures. This would break modular type checking. Template metaprogramming will not be supported by Carbon generics. We expect to address those use cases with metaprogramming or templates in Carbon. Template use cases that are out of scope We will also not require Carbon generics to support expression templates , variadics , or variadic templates . Those are all out of scope. It would be fine for our generics system to support these features, but they won't drive any accommodation in the generics design, at least until we have some resolution about templates in Carbon. Generics will be checked when defined C++ compilers must defer full type checking of templates until they are instantiated by the user. Carbon will not defer type checking of generic definitions. Specialization strategy We want all generic Carbon code to support static dispatch . This means we won't support unbounded type families. Unbounded type families are when recursion creates an infinite collection of types, such as in this example from Swift or: fn Sort[T:! Comparable](list: List(T)) -> List(T) { if (list.size() == 1) return list; var chunks: List(List(T)) = FormChunks(list, sqrt(list.size())); chunks = chunks.ApplyToEach(Sort); chunks = Sort(chunks); return MergeSortedListOfSortedLists(chunks); } This, given an implementation of Comparable for any list with elements that are themselves Comparable , would recursively call itself to produce a set of types without bound. That is, calling Sort on a List(Int) would internally call Sort on a List(List(Int)) and so on recursively without any static limit. We won't require all generic Carbon code to support dynamic dispatch, but we would like it to be an implementation option for the compiler in the majority of cases. Lastly, runtime specialization is out of scope as an implementation strategy. That is, some language runtimes JIT a specialization when it is first needed, but it is not a goal for Carbon to support such an implementation strategy. References #24: Generics goals #950: Generic details 6: remove facets","title":"Generics: Goals"},{"location":"design/generics/goals/#generics-goals","text":"","title":"Generics: Goals"},{"location":"design/generics/goals/#table-of-contents","text":"Purpose of this document Background Generic parameters Interfaces Relationship to templates Goals Use cases Generic programming Upgrade path from C++ abstract interfaces Dependency injection Generics instead of open overloading and ADL Performance Better compiler experience Encapsulation Predictability Dispatch control Upgrade path from templates Path from regular functions Coherence No novel name lookup Learn from others Interfaces are nominal Interop and evolution Bridge for C++ customization points What we are not doing Not the full flexibility of templates Template use cases that are out of scope Generics will be checked when defined Specialization strategy References","title":"Table of contents"},{"location":"design/generics/goals/#purpose-of-this-document","text":"This document attempts to clarify our goals for the design of the generics feature for Carbon. While these are not strict requirements, they represent the yardstick by which we evaluate design decisions. We do expect to achieve most of these goals, though some of these goals are somewhat aspirational or forward-looking.","title":"Purpose of this document"},{"location":"design/generics/goals/#background","text":"Carbon will support generics to support generic programming by way of parameterization of language constructs with early type checking and complete definition checking . This is in contrast with the compile-time duck typing approach of C++ templates, and in addition to template support in Carbon , if we decide to support templates in Carbon beyond interoperability with C++ templates.","title":"Background"},{"location":"design/generics/goals/#generic-parameters","text":"Generic functions and generic types will all take some \"generic parameters\", which will frequently be types, and in some cases will be deduced from the types of the values of explicit parameters. If a generic parameter is a type, the generic function's signature can specify constraints that the caller's type must satisfy. For example, a resizable array type (like C++'s std::vector ) might have a generic type parameter with the constraint that the type must be movable and have a static size. A sort function might apply to any array whose elements are comparable and movable. A constraint might involve multiple generic parameters. For example, a merge function might apply to two arbitrary containers so long as their elements have the same type.","title":"Generic parameters"},{"location":"design/generics/goals/#interfaces","text":"We need some way to express the constraints on a generic type parameter. In Carbon we express these \"type constraints\" by saying we restrict to types that implement specific interfaces . Interfaces describe an API a type could implement; for example, it might specify a set of functions, including names and signatures. A type implementing an interface may be passed as a generic type argument to a function that has that interface as a requirement of its generic type parameter. Then, the functions defined in the interface may be called in the body of the function. Further, interfaces have names that allow them to be reused. Similar compile-time and run-time constructs may be found in other programming languages: Rust's traits Swift's protocols Java interfaces C++ concepts (compile-time only) Abstract base classes in C++, etc. (run-time only) Go interfaces (run-time only) In addition to specifying the methods available on a type, we may in the future expand the role of interfaces to allow other type constraints, such as on size, prefix of the data layout, specified method implementations, tests that must pass, etc. This might be part of making interfaces as expressive as classes, as part of a strategy to migrate to a future version of Carbon that uses interfaces instead of, rather than in addition to, standard inheritance-and-classes object-oriented language support. For the moment, everything beyond specifying the methods available is out of scope.","title":"Interfaces"},{"location":"design/generics/goals/#relationship-to-templates","text":"The entire idea of statically typed languages is that coding against specific types and interfaces is a better model and experience. Unfortunately, templates don't provide many of those benefits to programmers until it's too late, when users are consuming the API. Templates also come with high overhead, such as template error messages . We want Carbon code to move towards more rigorously type checked constructs. However, existing C++ code is full of unrestricted usage of compile-time duck-typed templates. They are incredibly convenient to write and so likely will continue to exist for a long time. The question of whether Carbon has direct support for templates is out of scope for this document. The generics design is not completely separate from templates, so it is written as if Carbon will have its own templating system. It is assumed to be similar to C++ templates with some specific changes: It may have some limitations to be more compatible with generics, much like how we restrict overloading . We likely will have a different method of selecting between different template instantiations, since SFINAE makes it difficult to deliver high quality compiler diagnostics. We assume Carbon will have templates for a few different reasons: Carbon generics will definitely have to interact with C++ templates, and many of the issues will be similar. We want to leave room in the design for templates, since it seems like it would be easier to remove templates if they are not pulling their weight than figure out how to add them in if they turn out to be needed. We may want to have templates in Carbon as a temporary measure, to make it easier for users to transition off of C++ templates.","title":"Relationship to templates"},{"location":"design/generics/goals/#goals","text":"Our goal for generics support in Carbon is to get most of the expressive benefits of C++ templates and open overloading with fewer downsides. Additionally, we want to support some dynamic dispatch use cases; for example, in cases that inheritance struggles with.","title":"Goals"},{"location":"design/generics/goals/#use-cases","text":"To clarify the expressive range we are aming for, here are some specific use cases we expect Carbon generics to cover.","title":"Use cases"},{"location":"design/generics/goals/#generic-programming","text":"We in particular want to support generic programming , including: Containers: arrays, maps, lists, and more complicated data structures like trees and graphs Algorithms: sort, search Wrappers: optional, variant, expected/result, smart pointers Parameterized numeric types: std::complex<T> Configurable and parametric APIs: the storage-customized std::chrono APIs Policy-based design These would generally involve static, compile-time type arguments, and so would generally be used with static dispatch .","title":"Generic programming"},{"location":"design/generics/goals/#upgrade-path-from-c-abstract-interfaces","text":"Interfaces in C++ are often represented by abstract base classes. Generics should offer an alternative that does not rely on inheritance. This means looser coupling and none of the problems of multiple inheritance. Some people, such as Sean Parent , advocate for runtime polymorphism patterns in C++ that avoid inheritance because it can cause runtime performance, correctness, and code maintenance problems in some situations. Those patterns require a lot of boilerplate and complexity in C++. It would be nice if those patterns were simpler to express with Carbon generics. More generally, Carbon generics will provide an alternative for those situations inheritance doesn't handle as well. As a specific example, we would like Carbon generics to supplant the need to support multiple inheritance in Carbon. This is a case that would use dynamic dispatch .","title":"Upgrade path from C++ abstract interfaces"},{"location":"design/generics/goals/#dependency-injection","text":"Types which only support subclassing for test stubs and mocks, as in \"dependency injection\" , should be able to easily migrate to generics. This extends outside the realm of testing, allowing general configuration of how dependencies can be satisfied. For example, generics might be used to configure how a library writes logs. This would allow you to avoid the runtime overhead of virtual functions, using static dispatch without the poor build experience of templates .","title":"Dependency injection"},{"location":"design/generics/goals/#generics-instead-of-open-overloading-and-adl","text":"One name lookup problem we would like to avoid is caused by open overloading. Overloading is where you provide multiple implementations of a function with the same name, and the implementation used in a specific context is determined by the argument types. Open overloading is overloading where the overload set is not restricted to a single file or library. This works with Argument-dependent lookup , or ADL , a mechanism for enabling open overloading without having to reopen the namespace where the function was originally defined. Together these enable C++ customization points . This is commonly used to provide a type-specific implementation of some operation, but doesn't provide any enforcement of consistency across the different overloads. It makes the meaning of code dependent on which overloads are imported, and is at odds with being able to type check a function generically. Our goal is to address this use case, known more generally as the expression problem , with a generics mechanism that does enforce consistency so that type checking is possible without seeing all implementations. This will be Carbon's replacement for open overloading. As a consequence, Carbon generics will need to be able to support operator overloading. A specific example is the absolute value function Abs . We would like to write Abs(x) for a variety of types. For some types T , such as Int32 or Float64 , the return type will be the same T . For other types, such as Complex64 or Quaternion , the return type will be different. The generic functions that call Abs will need a way to specify whether they only operate on T such that Abs has signature T -> T . This does create an issue when interoperating with C++ code using open overloading, which will need to be addressed .","title":"Generics instead of open overloading and ADL"},{"location":"design/generics/goals/#performance","text":"For any real-world C++ template, there shall be an idiomatic reformulation in Carbon generics that has equal or better performance. Performance is the top priority for Carbon , and we expect to use generics pervasively, and so they can't compromise that goal in release builds. Nice to have: There are cases where we should aim to do better than C++ templates. For example, the additional structure of generics should make it easier to reduce generated code duplication, reducing code size and cache misses.","title":"Performance"},{"location":"design/generics/goals/#better-compiler-experience","text":"Compared to C++ templates, we expect to reduce build times, particularly in development builds. We also expect the compiler to be able to report clearer errors, and report them earlier in the build process. One source of improvement is that the bodies of generic functions and types can be type checked once when they are defined, instead of every time they are used. This is both a reduction in the total work done, and how errors can be reported earlier. On use, the errors can be a lot clearer since they will be of the form \"argument did not satisfy function's contract as stated in its signature\" instead of \"substitution failed at this line of the function's implementation.\" Nice to have: In development builds, we will have the option of using dynamic dispatch to reduce build times. We may also be able to reduce the amount of redundant compilation work even with the static strategy by identifying instantiations with the same arguments or identical implementations and only generating code for them once.","title":"Better compiler experience"},{"location":"design/generics/goals/#encapsulation","text":"With a template, the implementation is part of the interface and types are only checked when the function is called and the template is instantiated. A generic function is type checked when it is defined, and type checking can't use any information that is only known when the function is instantiated such as the exact argument types. Furthermore, calls to a generic function may be type checked using only its declaration, not its body. You should be able to call a generic function using only a forward declaration.","title":"Encapsulation"},{"location":"design/generics/goals/#predictability","text":"A general property of generics is they are more predictable than templates. They make clear when a type satisfies the requirements of a function; they have a documented contract. Further, that contract is enforced by the compiler, not sensitive to implementation details in the function body. This eases evolution by reducing (but not eliminating) the impact of Hyrum's law . Nice to have: We also want well-defined boundaries between what is legal and not. This is \"will my code be accepted by the compiler\" predictability. We would prefer to avoid algorithms in the compiler with the form \"run for up to N steps and report an error if it isn't resolved by then.\" For example, C++ compilers will typically have a template recursion limit. With generics, these problems arise due to trying to reason whether something is legal in all possible instantiations, rather than with specific, concrete types. Some of this is likely unavoidable or too costly to avoid, as most existing generics systems have undecidable aspects to their type system , including Rust and Swift . We fully expect there to be metaprogramming facilities in Carbon that will be able to execute arbitrary Turing machines, with infinite loops and undecidable stopping criteria. We don't see this as a problem though, just like we don't worry about trying to make the compiler reliably prevent you from writing programs that don't terminate. We would like to distinguish \"the executed steps are present in the program's source\" from \"the compiler has to search for a proof that the code is legal.\" In the former case, the compiler can surface a problem to the user by pointing to lines of code in a trace of execution. The user could employ traditional debugging techniques to refine their understanding until they can determine a fix. What we want to avoid is the latter case, since it has bad properties: Error messages end up in the form: \"this was too complicated to figure out, I eventually gave up.\" Little in the way of actionable feedback on how to fix problems. Not much the user can do to debug problems. If the compiler is currently right at a limit for figuring something out, it is easy to imagine a change to a distant dependency can cause it to suddenly stop compiling. If we can't find acceptable restrictions to make problems efficiently decidable, the next best solution is to require the proof to be in the source instead of derived by the compiler. If authoring the proof is too painful for the user, the we should invest in putting the proof search into IDEs or other tooling.","title":"Predictability"},{"location":"design/generics/goals/#dispatch-control","text":"Enable simple user control of whether to use dynamic or static dispatch. Implementation strategy: There are two strategies for generating code for generic functions: Static specialization strategy: Like template parameters, the values for generic parameters must be statically known at the callsite, or known to be a generic parameter to the calling function. This can generate separate, specialized versions of each combination of generic and template arguments, in order to optimize for those types or values. Dynamic strategy: This is when the compiler generates a single version of the function that uses runtime dispatch to get something semantically equivalent to separate instantiation, but likely with different size, build time, and performance characteristics. By default, we expect the implementation strategy to be controlled by the compiler, and not semantically visible to the user. For example, the compiler might use the static strategy for release builds and the dynamic strategy for development. Or it might choose between them on a more granular level based on code analysis, specific features used in the code, or profiling -- maybe some specific specializations are needed for performance, but others would just be code bloat. We require that all generic functions can be compiled using the static specialization strategy. For example, the values for generic parameters must be statically known at the callsite. Other limitations are listed below . Nice to have: It is desirable that the majority of functions with generic parameters also support the dynamic strategy. Specific features may prevent the compiler from using the dynamic strategy, but they should ideally be relatively rare, and easy to identify. Language features should avoid making it observable whether function code generated once or many times. For example, you should not be able to take the address of a function with generic parameters, or determine if a function was instantiated more than once using function-local static variables. There are a few obstacles to supporting dynamic dispatch efficiently, which may limit the extent it is used automatically by implementations. For example, the following features would benefit substantially from guaranteed monomorphization: Field packing in class layout. For example, packing a Bool into the lower bits of a pointer, or packing bit-fields with generic widths. Allocating local variables in stack storage. Without monomorphization, we would need to perform dynamic memory allocation -- whether on the stack or the heap -- for local variables whose sizes depend on generic parameters. Passing parameters to functions. We cannot pass values of generic types in registers. While it is possible to address these with dynamic dispatch, handling some of them might have far-reaching and surprising performance implications. We don't want to compromise our goal for predictable performance. We will allow the user to explicitly opt-in to using the dynamic strategy in specific cases. This could be just to control binary size in cases the user knows are not performance sensitive, or it could be to get the additional capability of operating on values with dynamic types. We may need to restrict this in various ways to maintain efficiency, like Rust does with object-safe traits. We also anticipate that the user may want to force the compiler to use the static strategy in specific cases. This might be to keep runtime performance acceptable even when running a development or debug build.","title":"Dispatch control"},{"location":"design/generics/goals/#upgrade-path-from-templates","text":"We want there to be a natural, incremental upgrade path from templated code to generic code. Assuming Carbon will support templates directly , the first step of migrating C++ template code would be to first convert it to a Carbon template. The problem is then how to convert templates to generics within Carbon. This gives us these sub-goals: Users should be able to convert a single template parameter to be generic at a time. A hybrid function with both template and generic parameters has all the limitations of a template function: it can't be completely definition checked, it can't use the dynamic strategy, etc. Even so, there are still benefits from enforcing the function's declared contract for those parameters that have been converted. Converting from a template parameter to a generic parameter should be safe. It should either work or fail to compile, never silently change semantics. We should minimize the effort to convert functions and types from templated to generic. Ideally it should just require specifying the type constraints, affecting just the signature of the function, not its body. Nice to have: It should be legal to call templated code from generic code when it would have the same semantics as if called from non-generic code, and an error otherwise. This is to allow more templated functions to be converted to generics, instead of requiring them to be converted specifically in bottom-up order. Nice to have: Provide a way to migrate from a template to a generic without immediately updating all of the types used with the template. For example, if the generic code requires types to implement a new interface, one possible solution would use the original template code to provide an implementation for that interface for any type that structurally has the methods used by the original template. If Carbon does not end up having direct support for templates, the transition will necessarily be less incremental.","title":"Upgrade path from templates"},{"location":"design/generics/goals/#path-from-regular-functions","text":"Replacing a regular, non-parameterized function with a generic function should not affect existing callers of the function. There may be some differences, such as when taking the address of the function, but ordinary calls should not see any difference. In particular, the return type of a generic function should match, without any type erasure or additional named members.","title":"Path from regular functions"},{"location":"design/generics/goals/#coherence","text":"We want the generics system to have the coherence property , so that the implementation of an interface for a type is well defined. Since a generic function only depends on interface implementations, they will always behave consistently on a given type, independent of context. For more on this, see this description of what coherence is and why Rust enforces it . Coherence greatly simplifies the language design, since it reduces the need for complicated rules to picking an implementation when there are many candidates. It also has a number of benefits for users: It removes a way packages can conflict with each other. It makes the behavior of code more consistent and predictable. It means there is no need to provide a disambiguation mechanism. Disambiguation is particularly problematic since the ambiguous call is often in generic code rather than code you control. A consistent definition of a type is useful for instantiating a C++ or Carbon template on that type. The main downside of coherence is that there are some capabilities we would like for interfaces that are in tension with having an orphan rule limiting where implementations may be defined. For example, we would like to address the expression problem . We can get some of the way there by allowing the implementation of an interface for a type to be defined with either the interface or the type. But some use cases remain: They should be some way of selecting between multiple implementations of an interface for a given type. For example, a Song might support multiple orderings, such as by title or by artist. These would be represented by having multiple implementations of a Comparable interface. In order to allow libraries to be composed, there must be some way of saying a type implements an interface that is in another package that the authors of the type were unaware of. This is especially important since the library a type is defined in may not be able to see the interface definition without creating a dependency cycle or layering violation. We should have some mechanism for addressing these use cases. There are multiple approaches that could work: Interface implementations could be external to types and are passed in to generic functions separately. There could be some way to create multiple types that are compatible with a given value that you can switch between using casts to select different interface implementations. This is the approach used by Rust ( 1 , 2 ). Alternatives to coherence are discussed in an appendix .","title":"Coherence"},{"location":"design/generics/goals/#no-novel-name-lookup","text":"We want to avoid adding rules for name lookup that are specific to generics. This is in contrast to Rust which has different lookup rules inside its traits. Instead, we should structure generics in a way that reuses existing name lookup facilities of the language. Nice to have: One application of this that would be nice to have is if the names of a type's members were all determined by a type's definition. So if x has type T , then if you write x.y you should be able to look up y in the definition of T . This might need to be somewhat indirect in some cases. For example, if T inherits from U , the name y might come from U and not be mentioned in the definition of T directly. We may have similar mechanisms where T gets methods that have default implementations in interfaces it implements, as long as the names of those interfaces are explicitly mentioned in the definition of T .","title":"No novel name lookup"},{"location":"design/generics/goals/#learn-from-others","text":"Many languages have implemented generics systems, and we should learn from those experiences. We should copy what works and makes sense in the context of Carbon, and change decisions that led to undesirable compromises. We are taking the strongest guidance from Rust and Swift, which have similar goals and significant experience with the implementation and usability of generics. They both use nominal interfaces, were designed with generics from the start, and produce native code. Contrast with Go which uses structural interfaces, or Java which targets a virtual machine that predated its generics feature. For example, Rust has found that supporting defaults for interface methods is a valuable feature. It is useful for evolution , implementation reuse, and for bridging the gap between the minimal functionality a type wants to implement and the rich API that users want to consume ( example ). We still have the flexibility to make simplifications that Rust cannot because they need to maintain compatibility. We could remove the concept of fundamental and explicit control over which methods may be specialized. These are complicated and impose coherence restrictions .","title":"Learn from others"},{"location":"design/generics/goals/#interfaces-are-nominal","text":"Interfaces can either be structural , as in Go, or nominal , as in Rust and Swift. Structural interfaces match any type that has the required methods, whereas nominal interfaces only match if there is an explicit declaration stating that the interface is implemented for that specific type. Carbon will support nominal interfaces, allowing them to designate semantics beyond the basic structure of the methods. This means that interfaces implicitly specify the intended semantics and invariants of and between those functions. Unlike the function signatures, this contract is between the implementers and the consumers of interfaces and is not enforced by Carbon itself. For example, a Draw method would mean different things when it is part of a GameResult interface versus an Image2D interface, even if those methods happen to have the same signature.","title":"Interfaces are nominal"},{"location":"design/generics/goals/#interop-and-evolution","text":"Evolution is a high priority for Carbon , and so will need mechanisms to support evolution when using generics. New additions to an interface might: need default implementations be marked \"upcoming\" to allow for a period of transition replace other APIs that need to be marked \"deprecated\" Experience with C++ concepts has shown that interfaces are hard to evolve without these kinds of supporting language mechanisms. Otherwise changes to interfaces need to made simultaneously with updates to types that implement the interface or functions that consume it. Another way of supporting evolution is to allow one interface to be substitutable for another. For example, a feature that lets you use an implementation of Interface1 for a type to automatically get an implementation of Interface2 , as well as the other way around, would help transitioning between those two interfaces. Evolution in particular means that the set of names in an interface can change, and so two interfaces that don't start with name conflicts can develop them. To handle name conflicts, interfaces should be separate, isolated namespaces. We should provide mechanisms to allow one type to implement two interfaces that accidentally use the same name for different things, and for functions to use interfaces with name conflicts together on a single type. Contrast this with Swift, where a type can only supply one associated type of a given name even when implementing multiple protocols. Similarly a function in Swift with a given name and signature can only have a single implementation for a type. Note this is possible since interfaces are nominal . The place where types specify that they implement an interface is also the vehicle for unambiguously designating which function implementation goes with what interface.","title":"Interop and evolution"},{"location":"design/generics/goals/#bridge-for-c-customization-points","text":"There will need to be some bridge for C++ extension points that currently rely on open overloading or ADL . For example, we need some way for C++ customization points like swap to work on Carbon types. We might define CPlusPlus.ADL.swap as a Carbon interface to be that bridge. Carbon types could implement that interface to work from C++, and Carbon functions could use that interface to invoke swap on C++ types. Similarly, we will want some way to implement Carbon interfaces for C++ types. For example, we might have a template implementation of an Addable interface for any C++ type that implements operator+ .","title":"Bridge for C++ customization points"},{"location":"design/generics/goals/#what-we-are-not-doing","text":"What are we not doing with generics, particularly things that some other languages do?","title":"What we are not doing"},{"location":"design/generics/goals/#not-the-full-flexibility-of-templates","text":"Generics don't need to provide full flexibility of C++ templates: The current assumption is that Carbon templates will cover those cases that don't fit inside generics, such as code that relies on compile-time duck typing. We won't allow a specialization of some generic interface for some particular type to actually expose a different interface, with different methods or different types in method signatures. This would break modular type checking. Template metaprogramming will not be supported by Carbon generics. We expect to address those use cases with metaprogramming or templates in Carbon.","title":"Not the full flexibility of templates"},{"location":"design/generics/goals/#template-use-cases-that-are-out-of-scope","text":"We will also not require Carbon generics to support expression templates , variadics , or variadic templates . Those are all out of scope. It would be fine for our generics system to support these features, but they won't drive any accommodation in the generics design, at least until we have some resolution about templates in Carbon.","title":"Template use cases that are out of scope"},{"location":"design/generics/goals/#generics-will-be-checked-when-defined","text":"C++ compilers must defer full type checking of templates until they are instantiated by the user. Carbon will not defer type checking of generic definitions.","title":"Generics will be checked when defined"},{"location":"design/generics/goals/#specialization-strategy","text":"We want all generic Carbon code to support static dispatch . This means we won't support unbounded type families. Unbounded type families are when recursion creates an infinite collection of types, such as in this example from Swift or: fn Sort[T:! Comparable](list: List(T)) -> List(T) { if (list.size() == 1) return list; var chunks: List(List(T)) = FormChunks(list, sqrt(list.size())); chunks = chunks.ApplyToEach(Sort); chunks = Sort(chunks); return MergeSortedListOfSortedLists(chunks); } This, given an implementation of Comparable for any list with elements that are themselves Comparable , would recursively call itself to produce a set of types without bound. That is, calling Sort on a List(Int) would internally call Sort on a List(List(Int)) and so on recursively without any static limit. We won't require all generic Carbon code to support dynamic dispatch, but we would like it to be an implementation option for the compiler in the majority of cases. Lastly, runtime specialization is out of scope as an implementation strategy. That is, some language runtimes JIT a specialization when it is first needed, but it is not a goal for Carbon to support such an implementation strategy.","title":"Specialization strategy"},{"location":"design/generics/goals/#references","text":"#24: Generics goals #950: Generic details 6: remove facets","title":"References"},{"location":"design/generics/overview/","text":"Generics: Overview This document is a high-level description of Carbon's generics design, with pointers to other design documents that dive deeper into individual topics. Table of contents Goals Summary What are generics? Interfaces Defining interfaces Contrast with templates Implementing interfaces Accessing members of interfaces Type-of-types Generic functions Deduced parameters Generic type parameters Requiring or extending another interface Combining interfaces Named constraints Type erasure Adapting types Interface input and output types Associated types Parameterized interfaces Constraints Parameterized impls Operator overloading Future work References Goals The goal of Carbon generics is to provide an alternative to Carbon (or C++) templates. Generics in this form should provide many advantages, including: Function calls and bodies are checked independently against the function signatures. Clearer and earlier error messages. Fast builds, particularly development builds. Support for both static and dynamic dispatch. For more detail, see the detailed discussion of generics goals and generics terminology . Summary Summary of how Carbon generics work: Generics are parameterized functions and types that can apply generally. They are used to avoid writing specialized, near-duplicate code for similar situations. Generics are written using interfaces which have a name and describe methods, functions, and other entities for types to implement. Types must explicitly implement interfaces to indicate that they support its functionality. A given type may implement an interface at most once. Implementations may be part of the type's definition, in which case you can directly call the interface's methods on those types. Or, they may be external, in which case the implementation is allowed to be defined in the library defining the interface. Interfaces are used as the type of a generic type parameter, acting as a type-of-type . Type-of-types in general specify the capabilities and requirements of the type. Types define specific implementations of those capabilities. Inside such a generic function, the API of the type is erased , except for the names defined in the type-of-type. Deduced parameters are parameters whose values are determined by the values and (most commonly) the types of the explicit arguments. Generic type parameters are typically deduced. A function with a generic type parameter can have the same function body as an unparameterized one. Functions can freely mix generic, template, and regular parameters. Interfaces can require other interfaces be implemented. Interfaces can extend required interfaces. The & operation on type-of-types allows you conveniently combine interfaces. It gives you all the names that don't conflict. You may also declare a new type-of-type directly using \"named constraints\" . Named constraints can express requirements that multiple interfaces be implemented, and give you control over how name conflicts are handled. Alternatively, you may resolve name conflicts by using a qualified member access expression to directly call a function from a specific interface using a qualified name. What are generics? Generics are a mechanism for writing parameterized code that applies generally instead of making near-duplicates for very similar situations, much like C++ templates. For example, instead of having one function per type-you-can-sort: fn SortInt32Vector(a: Vector(i32)*) { ... } fn SortStringVector(a: Vector(String)*) { ... } ... You might have one generic function that could sort any array with comparable elements: fn SortVector(T:! Comparable, a: Vector(T)*) { ... } The syntax above adds a ! to indicate that the parameter named T is generic and the caller will have to provide a value known at compile time. Given an i32 vector iv , SortVector(i32, &iv) is equivalent to SortInt32Vector(&iv) . Similarly for a String vector sv , SortVector(String, &sv) is equivalent to SortStringVector(&sv) . Thus, we can sort any vector containing comparable elements using this single SortVector function. This ability to generalize makes SortVector a generic . Interfaces The SortVector function requires a definition of Comparable , with the goal that the compiler can: completely type check a generic definition without information from where it's called. completely type check a call to a generic with information only from the function's signature, and not from its body. In this example, then, Comparable is an interface . Interfaces describe all the requirements needed for the type T . Given that the compiler knows T satisfies those requirements, it can type check the body of the SortVector function. This includes checking that the Comparable requirement covers all of the uses of T inside the function. Later, when the compiler comes across a call to SortVector , it can type check against the requirements expressed in the function's signature. Using only the types at the call site, the compiler can check that the member elements of the passed-in array satisfy the function's requirements. There is no need to look at the body of the SortVector function, since we separately checked that those requirements were sufficient. Defining interfaces Interfaces, then, have a name and describe methods, functions, and other entities for types to implement. Example: interface Comparable { // `Less` is an associated method. fn Less[me: Self](rhs: Self) -> Bool; } Functions and methods may be given a default implementation by prefixing the declaration with default and putting the function body in curly braces { ... } in place of the terminating ; of the function declaration. To prevent that implementation from being overridden, use final instead of default . Interfaces describe functionality, but not data; no variables may be declared in an interface. Contrast with templates Contrast these generics with a C++ template, where the compiler may be able to do some checking given a function definition, but more checking of the definition is required after seeing the call sites once all the instantiations are known. Note: Generics terminology goes into more detail about the differences between generics and templates . Implementing interfaces Interfaces themselves only describe functionality by way of method descriptions. A type needs to implement an interface to indicate that it supports its functionality. A given type may implement an interface at most once. Consider this interface: interface Printable { fn Print[me: Self](); } The interface keyword is used to define a nominal interface . That means that types need to explicitly implement them, using an impl block, such as here: class Song { // ... // Implementing `Printable` for `Song` inside the definition of `Song` // without the keyword `external` means all names of `Printable`, such // as `F`, are included as a part of the `Song` API. impl as Printable { // Could use `Self` in place of `Song` here. fn Print[me: Song]() { ... } } } // Implement `Comparable` for `Song` without changing the API of `Song` // using an `external impl` declaration. This may be defined in either // the library defining `Song` or `Comparable`. external impl Song as Comparable { // Could use either `Self` or `Song` here. fn Less[me: Self](rhs: Self) -> Bool { ... } } Implementations may be defined within the class definition itself or out-of-line. Implementations may optionally be start with the external keyword to say the members of the interface are not members of the class. Out-of-line implementations must be external. External implementations may be defined in the library defining either the class or the interface. Accessing members of interfaces The methods of an interface implemented internally within the class definition may be called with the simple member access syntax . Methods of all implemented interfaces may be called with a qualified member access expression , whether they are defined internally or externally. var song: Song; // `song.Print()` is allowed, unlike `song.Play()`. song.Print(); // `Less` is defined in `Comparable`, which is // implemented externally for `Song` song.(Comparable.Less)(song); // Can also call `Print` using a qualified member // access expression, using the compound member access // syntax with the qualified name `Printable.Print`: song.(Printable.Print)(); Type-of-types To type check a function, the compiler needs to be able to verify that uses of a value match the capabilities of the value's type. In SortVector , the parameter T is a type, but that type is a generic parameter. That means that the specific type value assigned to T is not known when type checking the SortVector function. Instead it is the constraints on T that let the compiler know what operations may be performed on values of type T . Those constraints are represented by the type of T , a type-of-type . In general, a type-of-type describes the capabilities of a type, while a type defines specific implementations of those capabilities. An interface, like Comparable , may be used as a type-of-type. In that case, the constraint on the type is that it must implement the interface Comparable . A type-of-type also defines a set of names and a mapping to corresponding qualified names. Those names are used for simple member lookup in scopes where the value of the type is not known, such as when the type is a generic parameter. You may combine interfaces into new type-of-types using the & operator or named constraints . Generic functions We want to be able to call generic functions just like ordinary functions, and write generic function bodies like ordinary functions. There are only a few differences, like that you can't take the address of generic functions. Deduced parameters This SortVector function is explicitly providing type information that is already included in the type of the second argument. To eliminate the argument at the call site, use a deduced parameter . fn SortVectorDeduced[T:! Comparable](a: Vector(T)*) { ... } The T parameter is defined in square brackets before the explicit parameter list in parenthesis to indicate it should be deduced. This means you may call the function without the type argument, just like the ordinary functions SortInt32Vector or SortStringVector : SortVectorDeduced(&anIntVector); // or SortVectorDeduced(&aStringVector); and the compiler deduces that the T argument should be set to i32 or String from the type of the argument. Deduced arguments are always determined from the call and its explicit arguments. There is no syntax for specifying deduced arguments directly at the call site. // ERROR: can't determine `U` from explicit parameters fn Illegal[T:! Type, U:! Type](x: T) -> U { ... } Generic type parameters A function with a generic type parameter can have the same function body as an unparameterized one. fn PrintIt[T:! Printable](p: T*) { p->Print(); } fn PrintIt(p: Song*) { p->Print(); } Inside the function body, you can treat the generic type parameter just like any other type. There is no need to refer to or access generic parameters differently because they are defined as generic, as long as you only refer to the names defined by type-of-type for the type parameter. You may also refer to any of the methods of interfaces required by the type-of-type using a qualified member access expression , as shown in the following sections. A function can have a mix of generic, template, and regular parameters. Likewise, it's allowed to pass a template or generic value to a generic or regular parameter. However, passing a generic value to a template parameter is future work. Requiring or extending another interface Interfaces can require other interfaces be implemented: interface Equatable { fn IsEqual[me: Self](rhs: Self) -> Bool; } // `Iterable` requires that `Equatable` is implemented. interface Iterable { impl as Equatable; fn Advance[addr me: Self*](); } The extends keyword is used to extend another interface. If interface Derived extends interface Base , Base 's interface is both required and all its methods are included in Derived 's interface. // `Hashable` extends `Equatable`. interface Hashable { extends Equatable; fn Hash[me: Self]() -> u64; } // `Hashable` is equivalent to: interface Hashable { impl as Equatable; alias IsEqual = Equatable.IsEqual; fn Hash[me: Self]() -> u64; } A type may implement the base interface implicitly by implementing all the methods in the implementation of the derived interface. class Key { // ... impl as Hashable { fn IsEqual[me: Key](rhs: Key) -> Bool { ... } fn Hash[me: Key]() -> u64 { ... } } // No need to separately implement `Equatable`. } var k: Key = ...; k.Hash(); k.IsEqual(k); Combining interfaces The & operation on type-of-types allows you conveniently combine interfaces. It gives you all the names that don't conflict. interface Renderable { fn GetCenter[me: Self]() -> (i32, i32); // Draw the object to the screen fn Draw[me: Self](); } interface EndOfGame { fn SetWinner[addr me: Self*](player: i32); // Indicate the game was a draw fn Draw[addr me: Self*](); } fn F[T:! Renderable & EndOfGame](game_state: T*) -> (i32, i32) { game_state->SetWinner(1); return game_state->Center(); } Names with conflicts can be accessed using a qualified member access expression . fn BothDraws[T:! Renderable & EndOfGame](game_state: T*) { game_state->(Renderable.Draw)(); game_state->(GameState.Draw)(); } Named constraints You may also declare a new type-of-type directly using \"named constraints\" . Named constraints can express requirements that multiple interfaces be implemented, and give you control over how name conflicts are handled. Named constraints have other applications and capabilities not covered here. constraint Combined { impl as Renderable; impl as EndOfGame; alias Draw_Renderable = Renderable.Draw; alias Draw_EndOfGame = EndOfGame.Draw; alias SetWinner = EndOfGame.SetWinner; } fn CallItAll[T:! Combined](game_state: T*, int winner) { if (winner > 0) { game_state->SetWinner(winner); } else { game_state->Draw_EndOfGame(); } game_state->Draw_Renderable(); // Can still use a qualified member access expression // for names not defined in the named constraint. return game_state->(Renderable.Center)(); } Type erasure Inside a generic function, the API of a type argument is erased except for the names defined in the type-of-type. An equivalent model is to say an archetype is used for type checking and name lookup when the actual type is not known in that scope. The archetype has members dictated by the type-of-type. For example: If there were a class CDCover defined this way: class CDCover { impl as Printable { ... } } it can be passed to this PrintIt function: fn PrintIt[T:! Printable](p: T*) { p->Print(); } Inside PrintIt , T is an archetype with the API of Printable . A call to PrintIt with a value of type CDCover erases everything except the members or Printable . This includes the type connection to CDCover , so it is illegal to cast from T to CDCover . Adapting types Carbon has a mechanism called adapting types ) to create new types that are compatible with existing types but with different interface implementations. This could be used to add or replace implementations, or define implementations for reuse. In this example, we have multiple ways of sorting a collection of Song values. class Song { ... } adapter SongByArtist extends Song { impl as Comparable { ... } } adapter SongByTitle extends Song { impl as Comparable { ... } } Values of type Song may be cast to SongByArtist or SongByTitle to get a specific sort order. Interface input and output types Associated types and interface parameters allow function signatures to vary with the implementing type. The biggest difference between these is that associated types (\"output types\") may be deduced from a type, and types can implement the same interface multiple times with different interface parameters (\"input types\"). Associated types Expect types that vary in an interface to be associated types by default. Since associated types may be deduced, they are more convenient to use. Imagine a Stack interface. Different types implementing Stack will have different element types: interface Stack { let ElementType:! Movable; fn Push[addr me: Self*](value: ElementType); fn Pop[addr me: Self*]() -> ElementType; fn IsEmpty[addr me: Self*]() -> Bool; } ElementType is an associated type of the interface Stack . Types that implement Stack give ElementType a specific value of some type implementing Movable . Functions that accept a type implementing Stack can deduce the ElementType from the stack type. // \u2705 This is allowed, since the type of the stack will determine // `ElementType`. fn PeekAtTopOfStack[StackType:! Stack](s: StackType*) -> StackType.ElementType; Parameterized interfaces Parameterized interfaces are commonly associated with overloaded operators. Imagine an interface for determining if two values are equivalent that allows those types to be different. An element in a hash map might have type Pair(String, i64) that implements both Equatable(String) and Equatable(Pair(String, i64)) . interface Equatable(T:! Type) { fn IsEqual[me: Self](compare_to: T) -> Bool; } T is a parameter to interface Equatable . A type can implement Equatable multiple times as long as each time it is with a different value of the T parameter. Functions may accept types implementing Equatable(i32) or Equatable(f32) . Functions can't accept types implementing Equatable(T) in general, unless some other parameter determines T . // \u2705 This is allowed, since the value of `T` is determined by the // `v` parameter. fn FindInVector[T:! Type, U:! Equatable(T)](v: Vector(T), needle: U) -> Optional(i32); // \u274c This is forbidden. Since `U` could implement `Equatable` // multiple times, there is no way to determine the value for `T`. // Contrast with `PeekAtTopOfStack` in the associated type example. fn CompileError[T:! Type, U:! Equatable(T)](x: U) -> T; Constraints Type-of-types can be further constrained using a where clause: fn FindFirstPrime[T:! Container where .Element == i32] (c: T, i: i32) -> Optional(i32) { // The elements of `c` have type `T.Element`, which is `i32`. ... } fn PrintContainer[T:! Container where .Element is Printable](c: T) { // The type of the elements of `c` is not known, but we do know // that type satisfies the `Printable` interface. ... } Constraints limit the types that the generic function can operate on, but increase the knowledge that may be used in the body of the function to operate on values of those types. Constraints are also used when implementing an interface to specify the values of associated types (and other associated constants). class Vector(T:! Movable) { impl as Stack where .ElementType = T { ... } } Parameterized impls Implementations can be parameterized to apply to multiple types. Those parameters can have constraints to restrict when the implementation applies. When multiple implementations apply, there is a rule to pick which one is considered the most specific: All type parameters in each impl declaration are replaced with question marks ? . This is called the type structure of the impl declaration. Given two type structures, find the first difference when read from left-to-right. The one with a ? is less specific, the one with a concrete type name in that position is more specific. If there is more than one impl declaration with the most specific type structure, pick the one listed first in the priority ordering. To ensure coherence , an impl may only be declared in a library defining some name from its type structure. If a library defines multiple implementations with the same type structure, they must be listed in priority order in a prioritization block. Operator overloading To overload an operator, implement the corresponding interface from the standard library. For example, to define how the unary - operator behaves for a type, implement the Negatable interface for that type. The interfaces and rewrites used for a given operator may be found in the expressions design . As a convenience, there is a shorcut for defining an implementation that supports any type implicitly convertible to a specified type, using like : // Support multiplying values of type `Distance` with // values of type `f64` or any type implicitly // convertible to `f64`. external impl Distance as MultipliableWith(like f64) ... Future work Functions should have a way to accept types that vary at runtime. You should have the ability to mark entities as upcoming or deprecated to support evolution. There should be a way to define generic associated and higher-ranked/kinded types. References #524: Generics overview #731: Generics details 2: adapters, associated types, parameterized interfaces #818: Constraints for generics (generics details 3) #920: Generic parameterized impls (details 5) #950: Generic details 6: remove facets #1013: Generics: Set associated constants using where constraints #1084: Generics details 9: forward declarations","title":"Generics: Overview"},{"location":"design/generics/overview/#generics-overview","text":"This document is a high-level description of Carbon's generics design, with pointers to other design documents that dive deeper into individual topics.","title":"Generics: Overview"},{"location":"design/generics/overview/#table-of-contents","text":"Goals Summary What are generics? Interfaces Defining interfaces Contrast with templates Implementing interfaces Accessing members of interfaces Type-of-types Generic functions Deduced parameters Generic type parameters Requiring or extending another interface Combining interfaces Named constraints Type erasure Adapting types Interface input and output types Associated types Parameterized interfaces Constraints Parameterized impls Operator overloading Future work References","title":"Table of contents"},{"location":"design/generics/overview/#goals","text":"The goal of Carbon generics is to provide an alternative to Carbon (or C++) templates. Generics in this form should provide many advantages, including: Function calls and bodies are checked independently against the function signatures. Clearer and earlier error messages. Fast builds, particularly development builds. Support for both static and dynamic dispatch. For more detail, see the detailed discussion of generics goals and generics terminology .","title":"Goals"},{"location":"design/generics/overview/#summary","text":"Summary of how Carbon generics work: Generics are parameterized functions and types that can apply generally. They are used to avoid writing specialized, near-duplicate code for similar situations. Generics are written using interfaces which have a name and describe methods, functions, and other entities for types to implement. Types must explicitly implement interfaces to indicate that they support its functionality. A given type may implement an interface at most once. Implementations may be part of the type's definition, in which case you can directly call the interface's methods on those types. Or, they may be external, in which case the implementation is allowed to be defined in the library defining the interface. Interfaces are used as the type of a generic type parameter, acting as a type-of-type . Type-of-types in general specify the capabilities and requirements of the type. Types define specific implementations of those capabilities. Inside such a generic function, the API of the type is erased , except for the names defined in the type-of-type. Deduced parameters are parameters whose values are determined by the values and (most commonly) the types of the explicit arguments. Generic type parameters are typically deduced. A function with a generic type parameter can have the same function body as an unparameterized one. Functions can freely mix generic, template, and regular parameters. Interfaces can require other interfaces be implemented. Interfaces can extend required interfaces. The & operation on type-of-types allows you conveniently combine interfaces. It gives you all the names that don't conflict. You may also declare a new type-of-type directly using \"named constraints\" . Named constraints can express requirements that multiple interfaces be implemented, and give you control over how name conflicts are handled. Alternatively, you may resolve name conflicts by using a qualified member access expression to directly call a function from a specific interface using a qualified name.","title":"Summary"},{"location":"design/generics/overview/#what-are-generics","text":"Generics are a mechanism for writing parameterized code that applies generally instead of making near-duplicates for very similar situations, much like C++ templates. For example, instead of having one function per type-you-can-sort: fn SortInt32Vector(a: Vector(i32)*) { ... } fn SortStringVector(a: Vector(String)*) { ... } ... You might have one generic function that could sort any array with comparable elements: fn SortVector(T:! Comparable, a: Vector(T)*) { ... } The syntax above adds a ! to indicate that the parameter named T is generic and the caller will have to provide a value known at compile time. Given an i32 vector iv , SortVector(i32, &iv) is equivalent to SortInt32Vector(&iv) . Similarly for a String vector sv , SortVector(String, &sv) is equivalent to SortStringVector(&sv) . Thus, we can sort any vector containing comparable elements using this single SortVector function. This ability to generalize makes SortVector a generic .","title":"What are generics?"},{"location":"design/generics/overview/#interfaces","text":"The SortVector function requires a definition of Comparable , with the goal that the compiler can: completely type check a generic definition without information from where it's called. completely type check a call to a generic with information only from the function's signature, and not from its body. In this example, then, Comparable is an interface . Interfaces describe all the requirements needed for the type T . Given that the compiler knows T satisfies those requirements, it can type check the body of the SortVector function. This includes checking that the Comparable requirement covers all of the uses of T inside the function. Later, when the compiler comes across a call to SortVector , it can type check against the requirements expressed in the function's signature. Using only the types at the call site, the compiler can check that the member elements of the passed-in array satisfy the function's requirements. There is no need to look at the body of the SortVector function, since we separately checked that those requirements were sufficient.","title":"Interfaces"},{"location":"design/generics/overview/#defining-interfaces","text":"Interfaces, then, have a name and describe methods, functions, and other entities for types to implement. Example: interface Comparable { // `Less` is an associated method. fn Less[me: Self](rhs: Self) -> Bool; } Functions and methods may be given a default implementation by prefixing the declaration with default and putting the function body in curly braces { ... } in place of the terminating ; of the function declaration. To prevent that implementation from being overridden, use final instead of default . Interfaces describe functionality, but not data; no variables may be declared in an interface.","title":"Defining interfaces"},{"location":"design/generics/overview/#contrast-with-templates","text":"Contrast these generics with a C++ template, where the compiler may be able to do some checking given a function definition, but more checking of the definition is required after seeing the call sites once all the instantiations are known. Note: Generics terminology goes into more detail about the differences between generics and templates .","title":"Contrast with templates"},{"location":"design/generics/overview/#implementing-interfaces","text":"Interfaces themselves only describe functionality by way of method descriptions. A type needs to implement an interface to indicate that it supports its functionality. A given type may implement an interface at most once. Consider this interface: interface Printable { fn Print[me: Self](); } The interface keyword is used to define a nominal interface . That means that types need to explicitly implement them, using an impl block, such as here: class Song { // ... // Implementing `Printable` for `Song` inside the definition of `Song` // without the keyword `external` means all names of `Printable`, such // as `F`, are included as a part of the `Song` API. impl as Printable { // Could use `Self` in place of `Song` here. fn Print[me: Song]() { ... } } } // Implement `Comparable` for `Song` without changing the API of `Song` // using an `external impl` declaration. This may be defined in either // the library defining `Song` or `Comparable`. external impl Song as Comparable { // Could use either `Self` or `Song` here. fn Less[me: Self](rhs: Self) -> Bool { ... } } Implementations may be defined within the class definition itself or out-of-line. Implementations may optionally be start with the external keyword to say the members of the interface are not members of the class. Out-of-line implementations must be external. External implementations may be defined in the library defining either the class or the interface.","title":"Implementing interfaces"},{"location":"design/generics/overview/#accessing-members-of-interfaces","text":"The methods of an interface implemented internally within the class definition may be called with the simple member access syntax . Methods of all implemented interfaces may be called with a qualified member access expression , whether they are defined internally or externally. var song: Song; // `song.Print()` is allowed, unlike `song.Play()`. song.Print(); // `Less` is defined in `Comparable`, which is // implemented externally for `Song` song.(Comparable.Less)(song); // Can also call `Print` using a qualified member // access expression, using the compound member access // syntax with the qualified name `Printable.Print`: song.(Printable.Print)();","title":"Accessing members of interfaces"},{"location":"design/generics/overview/#type-of-types","text":"To type check a function, the compiler needs to be able to verify that uses of a value match the capabilities of the value's type. In SortVector , the parameter T is a type, but that type is a generic parameter. That means that the specific type value assigned to T is not known when type checking the SortVector function. Instead it is the constraints on T that let the compiler know what operations may be performed on values of type T . Those constraints are represented by the type of T , a type-of-type . In general, a type-of-type describes the capabilities of a type, while a type defines specific implementations of those capabilities. An interface, like Comparable , may be used as a type-of-type. In that case, the constraint on the type is that it must implement the interface Comparable . A type-of-type also defines a set of names and a mapping to corresponding qualified names. Those names are used for simple member lookup in scopes where the value of the type is not known, such as when the type is a generic parameter. You may combine interfaces into new type-of-types using the & operator or named constraints .","title":"Type-of-types"},{"location":"design/generics/overview/#generic-functions","text":"We want to be able to call generic functions just like ordinary functions, and write generic function bodies like ordinary functions. There are only a few differences, like that you can't take the address of generic functions.","title":"Generic functions"},{"location":"design/generics/overview/#deduced-parameters","text":"This SortVector function is explicitly providing type information that is already included in the type of the second argument. To eliminate the argument at the call site, use a deduced parameter . fn SortVectorDeduced[T:! Comparable](a: Vector(T)*) { ... } The T parameter is defined in square brackets before the explicit parameter list in parenthesis to indicate it should be deduced. This means you may call the function without the type argument, just like the ordinary functions SortInt32Vector or SortStringVector : SortVectorDeduced(&anIntVector); // or SortVectorDeduced(&aStringVector); and the compiler deduces that the T argument should be set to i32 or String from the type of the argument. Deduced arguments are always determined from the call and its explicit arguments. There is no syntax for specifying deduced arguments directly at the call site. // ERROR: can't determine `U` from explicit parameters fn Illegal[T:! Type, U:! Type](x: T) -> U { ... }","title":"Deduced parameters"},{"location":"design/generics/overview/#generic-type-parameters","text":"A function with a generic type parameter can have the same function body as an unparameterized one. fn PrintIt[T:! Printable](p: T*) { p->Print(); } fn PrintIt(p: Song*) { p->Print(); } Inside the function body, you can treat the generic type parameter just like any other type. There is no need to refer to or access generic parameters differently because they are defined as generic, as long as you only refer to the names defined by type-of-type for the type parameter. You may also refer to any of the methods of interfaces required by the type-of-type using a qualified member access expression , as shown in the following sections. A function can have a mix of generic, template, and regular parameters. Likewise, it's allowed to pass a template or generic value to a generic or regular parameter. However, passing a generic value to a template parameter is future work.","title":"Generic type parameters"},{"location":"design/generics/overview/#requiring-or-extending-another-interface","text":"Interfaces can require other interfaces be implemented: interface Equatable { fn IsEqual[me: Self](rhs: Self) -> Bool; } // `Iterable` requires that `Equatable` is implemented. interface Iterable { impl as Equatable; fn Advance[addr me: Self*](); } The extends keyword is used to extend another interface. If interface Derived extends interface Base , Base 's interface is both required and all its methods are included in Derived 's interface. // `Hashable` extends `Equatable`. interface Hashable { extends Equatable; fn Hash[me: Self]() -> u64; } // `Hashable` is equivalent to: interface Hashable { impl as Equatable; alias IsEqual = Equatable.IsEqual; fn Hash[me: Self]() -> u64; } A type may implement the base interface implicitly by implementing all the methods in the implementation of the derived interface. class Key { // ... impl as Hashable { fn IsEqual[me: Key](rhs: Key) -> Bool { ... } fn Hash[me: Key]() -> u64 { ... } } // No need to separately implement `Equatable`. } var k: Key = ...; k.Hash(); k.IsEqual(k);","title":"Requiring or extending another interface"},{"location":"design/generics/overview/#combining-interfaces","text":"The & operation on type-of-types allows you conveniently combine interfaces. It gives you all the names that don't conflict. interface Renderable { fn GetCenter[me: Self]() -> (i32, i32); // Draw the object to the screen fn Draw[me: Self](); } interface EndOfGame { fn SetWinner[addr me: Self*](player: i32); // Indicate the game was a draw fn Draw[addr me: Self*](); } fn F[T:! Renderable & EndOfGame](game_state: T*) -> (i32, i32) { game_state->SetWinner(1); return game_state->Center(); } Names with conflicts can be accessed using a qualified member access expression . fn BothDraws[T:! Renderable & EndOfGame](game_state: T*) { game_state->(Renderable.Draw)(); game_state->(GameState.Draw)(); }","title":"Combining interfaces"},{"location":"design/generics/overview/#named-constraints","text":"You may also declare a new type-of-type directly using \"named constraints\" . Named constraints can express requirements that multiple interfaces be implemented, and give you control over how name conflicts are handled. Named constraints have other applications and capabilities not covered here. constraint Combined { impl as Renderable; impl as EndOfGame; alias Draw_Renderable = Renderable.Draw; alias Draw_EndOfGame = EndOfGame.Draw; alias SetWinner = EndOfGame.SetWinner; } fn CallItAll[T:! Combined](game_state: T*, int winner) { if (winner > 0) { game_state->SetWinner(winner); } else { game_state->Draw_EndOfGame(); } game_state->Draw_Renderable(); // Can still use a qualified member access expression // for names not defined in the named constraint. return game_state->(Renderable.Center)(); }","title":"Named constraints"},{"location":"design/generics/overview/#type-erasure","text":"Inside a generic function, the API of a type argument is erased except for the names defined in the type-of-type. An equivalent model is to say an archetype is used for type checking and name lookup when the actual type is not known in that scope. The archetype has members dictated by the type-of-type. For example: If there were a class CDCover defined this way: class CDCover { impl as Printable { ... } } it can be passed to this PrintIt function: fn PrintIt[T:! Printable](p: T*) { p->Print(); } Inside PrintIt , T is an archetype with the API of Printable . A call to PrintIt with a value of type CDCover erases everything except the members or Printable . This includes the type connection to CDCover , so it is illegal to cast from T to CDCover .","title":"Type erasure"},{"location":"design/generics/overview/#adapting-types","text":"Carbon has a mechanism called adapting types ) to create new types that are compatible with existing types but with different interface implementations. This could be used to add or replace implementations, or define implementations for reuse. In this example, we have multiple ways of sorting a collection of Song values. class Song { ... } adapter SongByArtist extends Song { impl as Comparable { ... } } adapter SongByTitle extends Song { impl as Comparable { ... } } Values of type Song may be cast to SongByArtist or SongByTitle to get a specific sort order.","title":"Adapting types"},{"location":"design/generics/overview/#interface-input-and-output-types","text":"Associated types and interface parameters allow function signatures to vary with the implementing type. The biggest difference between these is that associated types (\"output types\") may be deduced from a type, and types can implement the same interface multiple times with different interface parameters (\"input types\").","title":"Interface input and output types"},{"location":"design/generics/overview/#associated-types","text":"Expect types that vary in an interface to be associated types by default. Since associated types may be deduced, they are more convenient to use. Imagine a Stack interface. Different types implementing Stack will have different element types: interface Stack { let ElementType:! Movable; fn Push[addr me: Self*](value: ElementType); fn Pop[addr me: Self*]() -> ElementType; fn IsEmpty[addr me: Self*]() -> Bool; } ElementType is an associated type of the interface Stack . Types that implement Stack give ElementType a specific value of some type implementing Movable . Functions that accept a type implementing Stack can deduce the ElementType from the stack type. // \u2705 This is allowed, since the type of the stack will determine // `ElementType`. fn PeekAtTopOfStack[StackType:! Stack](s: StackType*) -> StackType.ElementType;","title":"Associated types"},{"location":"design/generics/overview/#parameterized-interfaces","text":"Parameterized interfaces are commonly associated with overloaded operators. Imagine an interface for determining if two values are equivalent that allows those types to be different. An element in a hash map might have type Pair(String, i64) that implements both Equatable(String) and Equatable(Pair(String, i64)) . interface Equatable(T:! Type) { fn IsEqual[me: Self](compare_to: T) -> Bool; } T is a parameter to interface Equatable . A type can implement Equatable multiple times as long as each time it is with a different value of the T parameter. Functions may accept types implementing Equatable(i32) or Equatable(f32) . Functions can't accept types implementing Equatable(T) in general, unless some other parameter determines T . // \u2705 This is allowed, since the value of `T` is determined by the // `v` parameter. fn FindInVector[T:! Type, U:! Equatable(T)](v: Vector(T), needle: U) -> Optional(i32); // \u274c This is forbidden. Since `U` could implement `Equatable` // multiple times, there is no way to determine the value for `T`. // Contrast with `PeekAtTopOfStack` in the associated type example. fn CompileError[T:! Type, U:! Equatable(T)](x: U) -> T;","title":"Parameterized interfaces"},{"location":"design/generics/overview/#constraints","text":"Type-of-types can be further constrained using a where clause: fn FindFirstPrime[T:! Container where .Element == i32] (c: T, i: i32) -> Optional(i32) { // The elements of `c` have type `T.Element`, which is `i32`. ... } fn PrintContainer[T:! Container where .Element is Printable](c: T) { // The type of the elements of `c` is not known, but we do know // that type satisfies the `Printable` interface. ... } Constraints limit the types that the generic function can operate on, but increase the knowledge that may be used in the body of the function to operate on values of those types. Constraints are also used when implementing an interface to specify the values of associated types (and other associated constants). class Vector(T:! Movable) { impl as Stack where .ElementType = T { ... } }","title":"Constraints"},{"location":"design/generics/overview/#parameterized-impls","text":"Implementations can be parameterized to apply to multiple types. Those parameters can have constraints to restrict when the implementation applies. When multiple implementations apply, there is a rule to pick which one is considered the most specific: All type parameters in each impl declaration are replaced with question marks ? . This is called the type structure of the impl declaration. Given two type structures, find the first difference when read from left-to-right. The one with a ? is less specific, the one with a concrete type name in that position is more specific. If there is more than one impl declaration with the most specific type structure, pick the one listed first in the priority ordering. To ensure coherence , an impl may only be declared in a library defining some name from its type structure. If a library defines multiple implementations with the same type structure, they must be listed in priority order in a prioritization block.","title":"Parameterized impls"},{"location":"design/generics/overview/#operator-overloading","text":"To overload an operator, implement the corresponding interface from the standard library. For example, to define how the unary - operator behaves for a type, implement the Negatable interface for that type. The interfaces and rewrites used for a given operator may be found in the expressions design . As a convenience, there is a shorcut for defining an implementation that supports any type implicitly convertible to a specified type, using like : // Support multiplying values of type `Distance` with // values of type `f64` or any type implicitly // convertible to `f64`. external impl Distance as MultipliableWith(like f64) ...","title":"Operator overloading"},{"location":"design/generics/overview/#future-work","text":"Functions should have a way to accept types that vary at runtime. You should have the ability to mark entities as upcoming or deprecated to support evolution. There should be a way to define generic associated and higher-ranked/kinded types.","title":"Future work"},{"location":"design/generics/overview/#references","text":"#524: Generics overview #731: Generics details 2: adapters, associated types, parameterized interfaces #818: Constraints for generics (generics details 3) #920: Generic parameterized impls (details 5) #950: Generic details 6: remove facets #1013: Generics: Set associated constants using where constraints #1084: Generics details 9: forward declarations","title":"References"},{"location":"design/generics/terminology/","text":"Generics: Terminology Table of contents Parameterized language constructs Generic versus template parameters Polymorphism Parametric polymorphism Compile-time duck typing Ad-hoc polymorphism Constrained genericity Dependent names Definition checking Complete definition checking Early versus late type checking Deduced parameter Interface Structural interfaces Nominal interfaces Named constraints Associated entity Impls: Implementations of interfaces Internal impl External impl Member access Simple member access Qualified member access expression Compatible types Subtyping and casting Coherence Adapting a type Type erasure Archetype Extending an interface Witness tables Dynamic-dispatch witness table Static-dispatch witness table Instantiation Specialization Template specialization Generic specialization Conditional conformance Interface type parameters and associated types Type constraints Type-of-type References Parameterized language constructs Generally speaking, when we talk about either templates or a generics system, we are talking about generalizing some language construct by adding a parameter to it. Language constructs here primarily would include functions and types, but we may want to support parameterizing other language constructs like interfaces . This parameter broadens the scope of the language construct on an axis defined by that parameter, for example it could define a family of functions instead of a single one. Generic versus template parameters When we are distinguishing between generics and templates in Carbon, it is on an parameter by parameter basis. A single function can take a mix of regular, generic, and template parameters. Regular parameters , or \"dynamic parameters\", are designated using the \"<name> : <type>\" syntax (or \"<value>\"). Generic parameters are designated using :! between the name and the type (so it is \"<name> :! <type>\"). Template parameters are designated using \" template <name> :! <type>\". The syntax for generic and template parameters was decided in questions-for-leads issue #565 . Expected difference between generics and templates: Generics Templates bounded parametric polymorphism compile-time duck typing and ad-hoc polymorphism constrained genericity optional constraints name lookup resolved for definitions in isolation (\"early\") some name lookup may require information from calls (name lookup may be \"late\") sound to typecheck definitions in isolation (\"early\") complete type checking may require information from calls (may be \"late\") supports separate type checking; may also support separate compilation, for example when implemented using dynamic witness tables separate compilation only to the extent that C++ supports it allowed but not required to be implemented using dynamic dispatch does not support implementation by way of dynamic dispatch, just static by way of instantiation monomorphization is an optional optimization that cannot render the program invalid monomorphization is mandatory and can fail, resulting in the program being invalid Polymorphism Generics and templates provide different forms of polymorphism than object-oriented programming with inheritance. That uses subtype polymorphism where different descendants, or \"subtypes\", of a base class can provide different implementations of a method, subject to some compatibility restrictions on the signature. Parametric polymorphism Parametric polymorphism ( Wikipedia ) is when a function or a data type can be written generically so that it can handle values identically without depending on their type. Bounded parametric polymorphism is where the allowed types are restricted to satisfy some constraints. Within the set of allowed types, different types are treated uniformly. Compile-time duck typing Duck typing ( Wikipedia ) is when the legal types for arguments are determined implicitly by the usage of the values of those types in the body of the function. Compile-time duck typing is when the usages in the body of the function are checked at compile-time, along all code paths. Contrast this with ordinary duck typing in a dynamic language such as Python where type errors are only diagnosed at runtime when a usage is reached dynamically. Ad-hoc polymorphism Ad-hoc polymorphism ( Wikipedia ), also known as \"overloading\", is when a single function name has multiple implementations for handling different argument types. There is no enforcement of any consistency between the implementations. For example, the return type of each overload can be arbitrary, rather than being the result of some consistent rule being applied to the argument types. Templates work with ad-hoc polymorphism in two ways: A function with template parameters can be specialized in C++ as a form of ad-hoc polymorphism. A function with template parameters can call overloaded functions since it will only resolve that call after the types are known. In Carbon, we expect there to be a compile error if overloading of some name prevents a generic function from being typechecked from its definition alone. For example, let's say we have some overloaded function called F that has two overloads: fn F[template T:! Type](x: T*) -> T; fn F(x: Int) -> Bool; A generic function G can call F with a type like T* that can not possibly call the F(Int) overload for F , and so it can consistently determine the return type of F . But G can't call F with an argument that could match either overload. Note: It is undecided what to do in the situation where F is overloaded, but the signatures are consistent and so callers could still typecheck calls to F . This still poses problems for the dynamic strategy for compiling generics, in a similar way to impl specialization. Constrained genericity We will allow some way of specifying constraints as part of a function (or type or other parameterized language construct). These constraints are a limit on what callers are allowed to pass in. The distinction between constrained and unconstrained genericity is whether the body of the function is limited to just those operations that are guaranteed by the constraints. With templates using unconstrained genericity, you may perform any operation in the body of the function, and they will be checked against the specific types used in calls. You can still have constraints, but they are optional in that they could be removed and the function would still have the same capabilities. Constraints only affect the caller, which will use them to resolve overloaded calls to the template and provide clearer error messages. With generics using constrained genericity, the function body can be checked against the signature at the time of definition. Note that it is still perfectly permissible to have no constraints on a type; that just means that you can only perform operations that work for all types (such as manipulate pointers to values of that type) in the body of the function. Dependent names A name is said to be dependent if it depends on some generic or template parameter. Note: this matches the use of the term \"dependent\" in C++ , not as in dependent types . Definition checking Definition checking is the process of semantically checking the definition of parameterized code for correctness independently of any particular arguments. It includes type checking and other semantic checks. It is possible, even with templates, to check semantics of expressions that are not dependent on any template parameter in the definition. Adding constraints to template parameters and/or switching them to be generic allows the compiler to increase how much of the definition can be checked. Any remaining checks are delayed until instantiation , which can fail. Complete definition checking Complete definition checking is when the definition can be fully semantically checked, including type checking. It is an especially useful property because it enables separate semantic checking of the definition, a prerequisite to separate compilation. It also enables implementation strategies that don\u2019t instantiate the implementation (for example, type erasure or dynamic-dispatch witness tables ). Early versus late type checking Early type checking is where expressions and statements are type checked when the definition of the function body is compiled, as part of definition checking. This occurs for regular and generic values. Late type checking is where expressions and statements may only be fully typechecked once calling information is known. Late type checking delays complete definition checking. This occurs for template-dependent values. Deduced parameter An deduced parameter is listed in the optional [ ] section right after the function name in a function signature: fn <name> [ <deduced parameters> ]( <explicit parameters ) -> <return type> Deduced arguments are determined as a result of pattern matching the explicit argument values (usually the types of those values) to the explicit parameters. Note that function signatures can typically be rewritten to avoid using deduced parameters: fn F[template T:! Type](value: T); // is equivalent to: fn F(value: (template T:! Type)); See more here . Interface An interface is an API constraint used in a function signature to provide encapsulation. Encapsulation here means that callers of the function only need to know about the interface requirements to call the function, not anything about the implementation of the function body, and the compiler can check the function body without knowing anything more about the caller. Callers of the function provide a value that has an implementation of the API and the body of the function may then use that API (and nothing else). Structural interfaces A \"structural\" interface is one where we say a type satisfies the interface as long as it has members with a specific list of names, and for each name it must have some type or signature. A type can satisfy a structural interface without ever naming that interface, just by virtue of having members with the right form. Nominal interfaces A \"nominal\" interface is one where we say a type can only satisfy an interface if there is some explicit statement saying so, for example by defining an impl . This allows \"satisfies the interface\" to have additional semantic meaning beyond what is directly checkable by the compiler. For example, knowing whether the Draw function means \"render an image to the screen\" or \"take a card from the top of a deck of cards\"; or that a + operator is commutative (and not, say, string concatenation). We use the \"structural\" versus \"nominal\" terminology as a generalization of the same terms being used in a subtyping context . Named constraints Named constraints are \"structural\" in the sense that they match a type based on meeting some criteria rather than an explicit statement in the type's definition. The criteria for a named constraint, however, are less focused on the type's API and instead might include a set of nominal interfaces that the type must implement and constraints on the associated entities and interface type parameters . Associated entity An associated entity is a requirement in an interface that a type's implementation of the interface must satisfy by having a matching member. A requirement that the type define a value for a member constant is called an associated constant , and similarly an associated function or associated type . Different types can satisfy an interface with different definitions for a given member. These definitions are associated with what type is implementing the interface. An impl defines what is associated with the type for that interface. Rust uses the term \"associated item\" instead of associated entity. Impls: Implementations of interfaces An impl is an implementation of an interface for a specific type. It is the place where the function bodies are defined, values for associated types, etc. are given. Impls are needed for nominal interfaces ; structural interfaces and named constraints define conformance implicitly instead of by requiring an impl to be defined. In can still make sense to implement a named constraint as a way to implement all of the interfaces it requires. Internal impl A type that implements an interface internally has all the named members of the interface as named members of the type. This means that the members of the interface are available by way of both simple member access and qualified member access expressions . External impl In contrast, a type that implements an interface externally does not include the named members of the interface in the type. The members of the interface are still implemented by the type, though, and so may be accessed using qualified member access expressions for those members. Member access There are two different kinds of member access: simple and compound . See the member access design document for the details. The application to generics combines compound member access with qualified names, which we call a qualified member access expression . Simple member access Simple member access has the from object.member , where member is a word naming a member of object . This form may be used to access members of interfaces implemented internally by the type of object . If String implements Printable internally, then s1.Print() calls the Print method of Printable using simple member access. In this case, the name Print is used without qualifying it with the name of the interface it is a member of since it is recognized as a member of the type itself as well. Qualified member access expression Compound member access has the form object.(expression) , where expression is resolved in the containing scope. A compound member access where the member expression is a simple member access expression, as in a.(context.b) , is called a qualified member access expression . The member expression context.b may be the qualified member name of an interface member, that consists of the name of the interface, possibly qualified with a package or namespace name, a dot . and the name of the member. For example, if the Comparable interface has a Less member method, then the qualified name of that member is Comparable.Less . So if String implements Comparable , and s1 and s2 are variables of type String , then the Less method may be called using the qualified member name by writing the qualified member access expression s1.(Comparable.Less)(s2) . This form may be used to access any member of an interface implemented for a type, whether it is implemented internally or externally . Compatible types Two types are compatible if they have the same notional set of values and represent those values in the same way, even if they expose different APIs. The representation of a type describes how the values of that type are represented as a sequence of bits in memory. The set of values of a type includes properties that the compiler can't directly see, such as invariants that the type maintains. We can't just say two types are compatible based on structural reasons. Instead, we have specific constructs that create compatible types from existing types in ways that encourage preserving the programmer's intended semantics and invariants, such as implementing the API of the new type by calling (public) methods of the original API, instead of accessing any private implementation details. Subtyping and casting Both subtyping and casting are different names for changing the type of a value to a compatible type. Subtyping is a relationship between two types where you can safely operate on a value of one type using a variable of another. For example, using C++'s object-oriented features, you can operate on a value of a derived class using a pointer to the base class. In most cases, you can pass a more specific type to a function that can handle a more general type. Return types work the opposite way, a function can return a more specific type to a caller prepared to handle a more general type. This determines how function signatures can change from base class to derived class, see covariance and contravariance in Wikipedia . In a generics context, we are specifically interested in the subtyping relationships between type-of-types . In particular, a type-of-type encompasses a set of type constraints , and you can convert a type from a more-restrictive type-of-type to another type-of-type whose constraints are implied by the first. C++ concepts terminology uses the term \"subsumes\" to talk about this partial ordering of constraints, but we avoid that term since it is at odds with the use of the term in object-oriented subtyping terminology . Note that subtyping is a bit like coercion , except we want to make it clear that the data representation of the value is not changing, just its type as reflected in the API available to manipulate the value. Casting is indicated explicitly by way of some syntax in the source code. You might use a cast to switch between type adaptations , or to be explicit where an implicit conversion would otherwise occur. For now, we are saying \" x as y \" is the provisional syntax in Carbon for casting the value x to the type y . Note that outside of generics, the term \"casting\" includes any explicit type change, including those that change the data representation. In contexts where an expression of one type is provided and a different type is required, an implicit conversion is performed if it is considered safe to do so. Such an implicit conversion, if permitted, always has the same meaning as an explicit cast. Coherence A generics system has the implementation coherence property, or simply coherence , if there is a single answer to the question \"what is the implementation of this interface for this type, if any?\" independent of context, such as the libraries imported into a given file. This is typically enforced by making sure the definition of the implementation must be imported if you import both the interface and the type. This may be done by requiring the implementation to be in the same library as the interface or type. This is called an orphan rule , meaning we don't allow an implementation that is not with either of its parents (parent type or parent interface). Note that in addition to an orphan rule that implementations are visible when queried, coherence also requires a rule for resolving what happens if there are multiple non-orphan implementations. In Rust, this is called the overlap rule or overlap check . This could be just producing an error in that situation, or picking one using some specialization rule. Adapting a type A type can be adapted by creating a new type that is compatible with an existing type, but has a different API. In particular, the new type might implement different interfaces or provide different implementations of the same interfaces. Unlike extending a type (as with C++ class inheritance), you are not allowed to add new data fields onto the end of the representation -- you may only change the API. This means that it is safe to cast a value between those two types without any dynamic checks or danger of object slicing . This is called \"newtype\" in Rust, and is used for capturing additional information in types to improve type safety by moving some checking to compile time ( 1 , 2 , 3 ) and as a workaround for Rust's orphan rules for coherence . Type erasure \"Type erasure\" is where a type's API is replaced by a subset. Everything outside of the preserved subset is said to have been \"erased\". This can happen in a variety of contexts including both generics and runtime polymorphism. For generics, type erasure restricts a type to just the API required by the constraints on a generic function. An example of type erasure in runtime polymorphism in C++ is casting from a pointer of a derived type to a pointer to an abstract base type. Only the API of the base type is available on the result, even though the implementation of those methods still come from the derived type. The term \"type erasure\" can also refer to the specific strategy used by Java to implement generics . which includes erasing the identity of type parameters. This is not the meaning of \"type erasure\" used in Carbon. Archetype A placeholder type is used when type checking a function in place of a generic type parameter. This allows type checking when the specific type to be used is not known at type checking time. The type satisfies just its constraint and no more, so it acts as the most general type satisfying the interface. In this way the archetype is the supertype of all types satisfying the interface. In addition to satisfying all the requirements of its constraint, the archetype also has the member names of its constraint. Effectively it is considered to implement the constraint internally . Extending an interface An interface can be extended by defining an interface that includes the full API of another interface, plus some additional API. Types implementing the extended interface should automatically be considered to have implemented the narrower interface. Witness tables Witness tables are an implementation strategy where values passed to a generic parameter are compiled into a table of required functionality. That table is then filled in for a given passed-in type with references to the implementation on the original type. The generic is implemented using calls into entries in the witness table, which turn into calls to the original type. This doesn't necessarily imply a runtime indirection: it may be a purely compile-time separation of concerns. However, it insists on a full abstraction boundary between the generic user of a type and the concrete implementation. A simple way to imagine a witness table is as a struct of function pointers, one per method in the interface. However, in practice, it's more complex because it must model things like associated types and interfaces. Witness tables are called \"dictionary passing\" in Haskell. Outside of generics, a vtable is a witness table that witnesses that a class is a descendant of an abstract base class, and is passed as part of the object instead of separately. Dynamic-dispatch witness table For dynamic-dispatch witness tables, actual function pointers are formed and used as a dynamic, runtime indirection. As a result, the generic code will not be duplicated for different witness tables. Static-dispatch witness table For static-dispatch witness tables, the implementation is required to collapse the table indirections at compile time. As a result, the generic code will be duplicated for different witness tables. Static-dispatch may be implemented as a performance optimization for dynamic-dispatch that increases generated code size. The final compiled output may not retain the witness table. Instantiation Instantiation is the implementation strategy for templates in both C++ and Carbon. Instantiation explicitly creates a copy of the template code and replaces the template components with the concrete type and its implementation operations. It allows duck typing and lazy binding. Instantiation implies template code will be duplicated. Unlike static-dispatch witness tables and monomorphization (as in Rust) , this is done before type checking completes. Only when the template is used with a concrete type is the template fully type checked, and it type checks against the actual concrete type after substituting it into the template. This means that different instantiations may interpret the same construct in different ways, and that templates can include constructs that are not valid for some possible instantiations. However, it also means that some errors in the template implementation may not produce errors until the instantiation occurs, and other errors may only happen for some instantiations. Specialization Template specialization Specialization in C++ is essentially overloading in the context of a template. The template is overloaded to have a different definition for some subset of the possible template argument values. For example, the C++ type std::vector<T> might have a specialization std::vector<T*> that is implemented in terms of std::vector<void*> to reduce code size. In C++, even the interface of a templated type can be changed in a specialization, as happens for std::vector<bool> . Generic specialization Specialization of generics, or types used by generics, is restricted to changing the implementation without affecting the interface. This restriction is needed to preserve the ability to perform type checking of generic definitions that reference a type that can be specialized, without statically knowing which specialization will be used. While there is nothing fundamentally incompatible about specialization with generics, even when implemented using witness tables, the result may be surprising because the selection of the specialized generic happens outside of the witness-table-based indirection between the generic code and the concrete implementation. Provided all selection relies exclusively on interfaces, this still satisfies the fundamental constraints of generics. Conditional conformance Conditional conformance is when you have a parameterized type that has one API that it always supports, but satisfies additional interfaces under some conditions on the type argument. For example: Array(T) might implement Comparable if T itself implements Comparable , using lexicographical order. Interface type parameters and associated types Imagine an interface defining a container. Different containers will contain different types of values, and the container API will have to refer to that \"element type\" when defining the signature of methods like \"insert\" or \"find\". If that element type is a parameter (input) to the interface type, we say it is an interface type parameter ; if it is an output, we say it is an associated type . An associated type is a kind of associated entity . Interface type parameter example: interface StackTP(ElementType:! Type) fn Push[addr me: Self*](value: ElementType); fn Pop[addr me: Self*]() -> ElementType; } Associated type example: interface StackAT { let ElementType:! Type; fn Push[addr me: Self*](value: ElementType); fn Pop[addr me: Self*]() -> ElementType; } Associated types are particularly called for when the implementation controls the type, not the caller. For example, the iterator type for a container is specific to the container and not something you would expect a user of the interface to specify. interface Iterator { ... } interface Container { // This does not make sense as an parameter to the container interface, // since this type is determined from the container type. let IteratorType:! Iterator; ... fn Insert[addr me: Self*](position: IteratorType, value: ElementType); } class ListIterator(ElementType:! Type) { ... impl as Iterator; } class List(ElementType:! Type) { // Iterator type is determined by the container type. impl as Container where .IteratorType = ListIterator(ElementType) { fn Insert[addr me: Self*](position: IteratorType, value: ElementType) { ... } } } If you have an interface with type parameters, a type can have multiple impls for different combinations of type parameters. As a result, type parameters may not be deduced in a function call. However, if the interface parameters are specified, a type can only have a single implementation of the given interface. This unique implementation choice determines the values of associated types. For example, we might have an interface that says how to perform addition with another type: interface Addable(T:! Type) { let ResultType:! Type; fn Add[me: Self](rhs: T) -> ResultType; } An i32 value might support addition with i32 , u16 , and f64 values. impl i32 as Addable(i32) where .ResultType = i32 { ... } impl i32 as Addable(u16) where .ResultType = i32 { ... } impl i32 as Addable(f64) where .ResultType = f64 { ... } To write a generic function requiring a parameter to be Addable , there needs to be some way to determine the type to add to: // \u2705 This is allowed, since the value of `T` is determined by the // `y` parameter. fn DoAdd[T:! Type, U:! Addable(T)](x: U, y: T) -> U.ResultType { return x.Add(y); } // \u274c This is forbidden, can't uniquely determine `T`. fn CompileError[T:! Type, U:! Addable(T)](x: U) -> T; Once the interface parameter can be determined, that determines the values for associated types, such as ResultType in the example. As always, calls with types for which no implementation exists will be rejected at the call site: // \u274c This is forbidden, no implementation of `Addable(Orange)` // for `Apple`. DoAdd(apple, orange); Type constraints Type constraints restrict which types are legal for template or generic parameters or associated types. They help define semantics under which they should be called, and prevent incorrect calls. In general there are a number of different type relationships we would like to express, for example: This function accepts two containers. The container types may be different, but the element types need to match. For this container interface we have associated types for iterators and elements. The iterator type's element type needs to match the container's element type. An interface may define an associated type that needs to be constrained to implement some interfaces. This type must be compatible with another type. You might use this to define alternate implementations of a single interfaces, such as sorting order, for a single type. Note that type constraints can be a restriction on one type parameter or associated type, or can define a relationship between multiple types. Type-of-type A type-of-type is the type used when declaring some type parameter. It foremost determines which types are legal arguments for that type parameter, also known as type constraints . For template parameters, that is all a type-of-type does. For generic parameters, it also determines the API that is available in the body of the function. References #447: Generics terminology #731: Generics details 2: adapters, associated types, parameterized interfaces #950: Generic details 6: remove facets #1013: Generics: Set associated constants using where constraints","title":"Generics: Terminology"},{"location":"design/generics/terminology/#generics-terminology","text":"","title":"Generics: Terminology"},{"location":"design/generics/terminology/#table-of-contents","text":"Parameterized language constructs Generic versus template parameters Polymorphism Parametric polymorphism Compile-time duck typing Ad-hoc polymorphism Constrained genericity Dependent names Definition checking Complete definition checking Early versus late type checking Deduced parameter Interface Structural interfaces Nominal interfaces Named constraints Associated entity Impls: Implementations of interfaces Internal impl External impl Member access Simple member access Qualified member access expression Compatible types Subtyping and casting Coherence Adapting a type Type erasure Archetype Extending an interface Witness tables Dynamic-dispatch witness table Static-dispatch witness table Instantiation Specialization Template specialization Generic specialization Conditional conformance Interface type parameters and associated types Type constraints Type-of-type References","title":"Table of contents"},{"location":"design/generics/terminology/#parameterized-language-constructs","text":"Generally speaking, when we talk about either templates or a generics system, we are talking about generalizing some language construct by adding a parameter to it. Language constructs here primarily would include functions and types, but we may want to support parameterizing other language constructs like interfaces . This parameter broadens the scope of the language construct on an axis defined by that parameter, for example it could define a family of functions instead of a single one.","title":"Parameterized language constructs"},{"location":"design/generics/terminology/#generic-versus-template-parameters","text":"When we are distinguishing between generics and templates in Carbon, it is on an parameter by parameter basis. A single function can take a mix of regular, generic, and template parameters. Regular parameters , or \"dynamic parameters\", are designated using the \"<name> : <type>\" syntax (or \"<value>\"). Generic parameters are designated using :! between the name and the type (so it is \"<name> :! <type>\"). Template parameters are designated using \" template <name> :! <type>\". The syntax for generic and template parameters was decided in questions-for-leads issue #565 . Expected difference between generics and templates: Generics Templates bounded parametric polymorphism compile-time duck typing and ad-hoc polymorphism constrained genericity optional constraints name lookup resolved for definitions in isolation (\"early\") some name lookup may require information from calls (name lookup may be \"late\") sound to typecheck definitions in isolation (\"early\") complete type checking may require information from calls (may be \"late\") supports separate type checking; may also support separate compilation, for example when implemented using dynamic witness tables separate compilation only to the extent that C++ supports it allowed but not required to be implemented using dynamic dispatch does not support implementation by way of dynamic dispatch, just static by way of instantiation monomorphization is an optional optimization that cannot render the program invalid monomorphization is mandatory and can fail, resulting in the program being invalid","title":"Generic versus template parameters"},{"location":"design/generics/terminology/#polymorphism","text":"Generics and templates provide different forms of polymorphism than object-oriented programming with inheritance. That uses subtype polymorphism where different descendants, or \"subtypes\", of a base class can provide different implementations of a method, subject to some compatibility restrictions on the signature.","title":"Polymorphism"},{"location":"design/generics/terminology/#parametric-polymorphism","text":"Parametric polymorphism ( Wikipedia ) is when a function or a data type can be written generically so that it can handle values identically without depending on their type. Bounded parametric polymorphism is where the allowed types are restricted to satisfy some constraints. Within the set of allowed types, different types are treated uniformly.","title":"Parametric polymorphism"},{"location":"design/generics/terminology/#compile-time-duck-typing","text":"Duck typing ( Wikipedia ) is when the legal types for arguments are determined implicitly by the usage of the values of those types in the body of the function. Compile-time duck typing is when the usages in the body of the function are checked at compile-time, along all code paths. Contrast this with ordinary duck typing in a dynamic language such as Python where type errors are only diagnosed at runtime when a usage is reached dynamically.","title":"Compile-time duck typing"},{"location":"design/generics/terminology/#ad-hoc-polymorphism","text":"Ad-hoc polymorphism ( Wikipedia ), also known as \"overloading\", is when a single function name has multiple implementations for handling different argument types. There is no enforcement of any consistency between the implementations. For example, the return type of each overload can be arbitrary, rather than being the result of some consistent rule being applied to the argument types. Templates work with ad-hoc polymorphism in two ways: A function with template parameters can be specialized in C++ as a form of ad-hoc polymorphism. A function with template parameters can call overloaded functions since it will only resolve that call after the types are known. In Carbon, we expect there to be a compile error if overloading of some name prevents a generic function from being typechecked from its definition alone. For example, let's say we have some overloaded function called F that has two overloads: fn F[template T:! Type](x: T*) -> T; fn F(x: Int) -> Bool; A generic function G can call F with a type like T* that can not possibly call the F(Int) overload for F , and so it can consistently determine the return type of F . But G can't call F with an argument that could match either overload. Note: It is undecided what to do in the situation where F is overloaded, but the signatures are consistent and so callers could still typecheck calls to F . This still poses problems for the dynamic strategy for compiling generics, in a similar way to impl specialization.","title":"Ad-hoc polymorphism"},{"location":"design/generics/terminology/#constrained-genericity","text":"We will allow some way of specifying constraints as part of a function (or type or other parameterized language construct). These constraints are a limit on what callers are allowed to pass in. The distinction between constrained and unconstrained genericity is whether the body of the function is limited to just those operations that are guaranteed by the constraints. With templates using unconstrained genericity, you may perform any operation in the body of the function, and they will be checked against the specific types used in calls. You can still have constraints, but they are optional in that they could be removed and the function would still have the same capabilities. Constraints only affect the caller, which will use them to resolve overloaded calls to the template and provide clearer error messages. With generics using constrained genericity, the function body can be checked against the signature at the time of definition. Note that it is still perfectly permissible to have no constraints on a type; that just means that you can only perform operations that work for all types (such as manipulate pointers to values of that type) in the body of the function.","title":"Constrained genericity"},{"location":"design/generics/terminology/#dependent-names","text":"A name is said to be dependent if it depends on some generic or template parameter. Note: this matches the use of the term \"dependent\" in C++ , not as in dependent types .","title":"Dependent names"},{"location":"design/generics/terminology/#definition-checking","text":"Definition checking is the process of semantically checking the definition of parameterized code for correctness independently of any particular arguments. It includes type checking and other semantic checks. It is possible, even with templates, to check semantics of expressions that are not dependent on any template parameter in the definition. Adding constraints to template parameters and/or switching them to be generic allows the compiler to increase how much of the definition can be checked. Any remaining checks are delayed until instantiation , which can fail.","title":"Definition checking"},{"location":"design/generics/terminology/#complete-definition-checking","text":"Complete definition checking is when the definition can be fully semantically checked, including type checking. It is an especially useful property because it enables separate semantic checking of the definition, a prerequisite to separate compilation. It also enables implementation strategies that don\u2019t instantiate the implementation (for example, type erasure or dynamic-dispatch witness tables ).","title":"Complete definition checking"},{"location":"design/generics/terminology/#early-versus-late-type-checking","text":"Early type checking is where expressions and statements are type checked when the definition of the function body is compiled, as part of definition checking. This occurs for regular and generic values. Late type checking is where expressions and statements may only be fully typechecked once calling information is known. Late type checking delays complete definition checking. This occurs for template-dependent values.","title":"Early versus late type checking"},{"location":"design/generics/terminology/#deduced-parameter","text":"An deduced parameter is listed in the optional [ ] section right after the function name in a function signature: fn <name> [ <deduced parameters> ]( <explicit parameters ) -> <return type> Deduced arguments are determined as a result of pattern matching the explicit argument values (usually the types of those values) to the explicit parameters. Note that function signatures can typically be rewritten to avoid using deduced parameters: fn F[template T:! Type](value: T); // is equivalent to: fn F(value: (template T:! Type)); See more here .","title":"Deduced parameter"},{"location":"design/generics/terminology/#interface","text":"An interface is an API constraint used in a function signature to provide encapsulation. Encapsulation here means that callers of the function only need to know about the interface requirements to call the function, not anything about the implementation of the function body, and the compiler can check the function body without knowing anything more about the caller. Callers of the function provide a value that has an implementation of the API and the body of the function may then use that API (and nothing else).","title":"Interface"},{"location":"design/generics/terminology/#structural-interfaces","text":"A \"structural\" interface is one where we say a type satisfies the interface as long as it has members with a specific list of names, and for each name it must have some type or signature. A type can satisfy a structural interface without ever naming that interface, just by virtue of having members with the right form.","title":"Structural interfaces"},{"location":"design/generics/terminology/#nominal-interfaces","text":"A \"nominal\" interface is one where we say a type can only satisfy an interface if there is some explicit statement saying so, for example by defining an impl . This allows \"satisfies the interface\" to have additional semantic meaning beyond what is directly checkable by the compiler. For example, knowing whether the Draw function means \"render an image to the screen\" or \"take a card from the top of a deck of cards\"; or that a + operator is commutative (and not, say, string concatenation). We use the \"structural\" versus \"nominal\" terminology as a generalization of the same terms being used in a subtyping context .","title":"Nominal interfaces"},{"location":"design/generics/terminology/#named-constraints","text":"Named constraints are \"structural\" in the sense that they match a type based on meeting some criteria rather than an explicit statement in the type's definition. The criteria for a named constraint, however, are less focused on the type's API and instead might include a set of nominal interfaces that the type must implement and constraints on the associated entities and interface type parameters .","title":"Named constraints"},{"location":"design/generics/terminology/#associated-entity","text":"An associated entity is a requirement in an interface that a type's implementation of the interface must satisfy by having a matching member. A requirement that the type define a value for a member constant is called an associated constant , and similarly an associated function or associated type . Different types can satisfy an interface with different definitions for a given member. These definitions are associated with what type is implementing the interface. An impl defines what is associated with the type for that interface. Rust uses the term \"associated item\" instead of associated entity.","title":"Associated entity"},{"location":"design/generics/terminology/#impls-implementations-of-interfaces","text":"An impl is an implementation of an interface for a specific type. It is the place where the function bodies are defined, values for associated types, etc. are given. Impls are needed for nominal interfaces ; structural interfaces and named constraints define conformance implicitly instead of by requiring an impl to be defined. In can still make sense to implement a named constraint as a way to implement all of the interfaces it requires.","title":"Impls: Implementations of interfaces"},{"location":"design/generics/terminology/#internal-impl","text":"A type that implements an interface internally has all the named members of the interface as named members of the type. This means that the members of the interface are available by way of both simple member access and qualified member access expressions .","title":"Internal impl"},{"location":"design/generics/terminology/#external-impl","text":"In contrast, a type that implements an interface externally does not include the named members of the interface in the type. The members of the interface are still implemented by the type, though, and so may be accessed using qualified member access expressions for those members.","title":"External impl"},{"location":"design/generics/terminology/#member-access","text":"There are two different kinds of member access: simple and compound . See the member access design document for the details. The application to generics combines compound member access with qualified names, which we call a qualified member access expression .","title":"Member access"},{"location":"design/generics/terminology/#simple-member-access","text":"Simple member access has the from object.member , where member is a word naming a member of object . This form may be used to access members of interfaces implemented internally by the type of object . If String implements Printable internally, then s1.Print() calls the Print method of Printable using simple member access. In this case, the name Print is used without qualifying it with the name of the interface it is a member of since it is recognized as a member of the type itself as well.","title":"Simple member access"},{"location":"design/generics/terminology/#qualified-member-access-expression","text":"Compound member access has the form object.(expression) , where expression is resolved in the containing scope. A compound member access where the member expression is a simple member access expression, as in a.(context.b) , is called a qualified member access expression . The member expression context.b may be the qualified member name of an interface member, that consists of the name of the interface, possibly qualified with a package or namespace name, a dot . and the name of the member. For example, if the Comparable interface has a Less member method, then the qualified name of that member is Comparable.Less . So if String implements Comparable , and s1 and s2 are variables of type String , then the Less method may be called using the qualified member name by writing the qualified member access expression s1.(Comparable.Less)(s2) . This form may be used to access any member of an interface implemented for a type, whether it is implemented internally or externally .","title":"Qualified member access expression"},{"location":"design/generics/terminology/#compatible-types","text":"Two types are compatible if they have the same notional set of values and represent those values in the same way, even if they expose different APIs. The representation of a type describes how the values of that type are represented as a sequence of bits in memory. The set of values of a type includes properties that the compiler can't directly see, such as invariants that the type maintains. We can't just say two types are compatible based on structural reasons. Instead, we have specific constructs that create compatible types from existing types in ways that encourage preserving the programmer's intended semantics and invariants, such as implementing the API of the new type by calling (public) methods of the original API, instead of accessing any private implementation details.","title":"Compatible types"},{"location":"design/generics/terminology/#subtyping-and-casting","text":"Both subtyping and casting are different names for changing the type of a value to a compatible type. Subtyping is a relationship between two types where you can safely operate on a value of one type using a variable of another. For example, using C++'s object-oriented features, you can operate on a value of a derived class using a pointer to the base class. In most cases, you can pass a more specific type to a function that can handle a more general type. Return types work the opposite way, a function can return a more specific type to a caller prepared to handle a more general type. This determines how function signatures can change from base class to derived class, see covariance and contravariance in Wikipedia . In a generics context, we are specifically interested in the subtyping relationships between type-of-types . In particular, a type-of-type encompasses a set of type constraints , and you can convert a type from a more-restrictive type-of-type to another type-of-type whose constraints are implied by the first. C++ concepts terminology uses the term \"subsumes\" to talk about this partial ordering of constraints, but we avoid that term since it is at odds with the use of the term in object-oriented subtyping terminology . Note that subtyping is a bit like coercion , except we want to make it clear that the data representation of the value is not changing, just its type as reflected in the API available to manipulate the value. Casting is indicated explicitly by way of some syntax in the source code. You might use a cast to switch between type adaptations , or to be explicit where an implicit conversion would otherwise occur. For now, we are saying \" x as y \" is the provisional syntax in Carbon for casting the value x to the type y . Note that outside of generics, the term \"casting\" includes any explicit type change, including those that change the data representation. In contexts where an expression of one type is provided and a different type is required, an implicit conversion is performed if it is considered safe to do so. Such an implicit conversion, if permitted, always has the same meaning as an explicit cast.","title":"Subtyping and casting"},{"location":"design/generics/terminology/#coherence","text":"A generics system has the implementation coherence property, or simply coherence , if there is a single answer to the question \"what is the implementation of this interface for this type, if any?\" independent of context, such as the libraries imported into a given file. This is typically enforced by making sure the definition of the implementation must be imported if you import both the interface and the type. This may be done by requiring the implementation to be in the same library as the interface or type. This is called an orphan rule , meaning we don't allow an implementation that is not with either of its parents (parent type or parent interface). Note that in addition to an orphan rule that implementations are visible when queried, coherence also requires a rule for resolving what happens if there are multiple non-orphan implementations. In Rust, this is called the overlap rule or overlap check . This could be just producing an error in that situation, or picking one using some specialization rule.","title":"Coherence"},{"location":"design/generics/terminology/#adapting-a-type","text":"A type can be adapted by creating a new type that is compatible with an existing type, but has a different API. In particular, the new type might implement different interfaces or provide different implementations of the same interfaces. Unlike extending a type (as with C++ class inheritance), you are not allowed to add new data fields onto the end of the representation -- you may only change the API. This means that it is safe to cast a value between those two types without any dynamic checks or danger of object slicing . This is called \"newtype\" in Rust, and is used for capturing additional information in types to improve type safety by moving some checking to compile time ( 1 , 2 , 3 ) and as a workaround for Rust's orphan rules for coherence .","title":"Adapting a type"},{"location":"design/generics/terminology/#type-erasure","text":"\"Type erasure\" is where a type's API is replaced by a subset. Everything outside of the preserved subset is said to have been \"erased\". This can happen in a variety of contexts including both generics and runtime polymorphism. For generics, type erasure restricts a type to just the API required by the constraints on a generic function. An example of type erasure in runtime polymorphism in C++ is casting from a pointer of a derived type to a pointer to an abstract base type. Only the API of the base type is available on the result, even though the implementation of those methods still come from the derived type. The term \"type erasure\" can also refer to the specific strategy used by Java to implement generics . which includes erasing the identity of type parameters. This is not the meaning of \"type erasure\" used in Carbon.","title":"Type erasure"},{"location":"design/generics/terminology/#archetype","text":"A placeholder type is used when type checking a function in place of a generic type parameter. This allows type checking when the specific type to be used is not known at type checking time. The type satisfies just its constraint and no more, so it acts as the most general type satisfying the interface. In this way the archetype is the supertype of all types satisfying the interface. In addition to satisfying all the requirements of its constraint, the archetype also has the member names of its constraint. Effectively it is considered to implement the constraint internally .","title":"Archetype"},{"location":"design/generics/terminology/#extending-an-interface","text":"An interface can be extended by defining an interface that includes the full API of another interface, plus some additional API. Types implementing the extended interface should automatically be considered to have implemented the narrower interface.","title":"Extending an interface"},{"location":"design/generics/terminology/#witness-tables","text":"Witness tables are an implementation strategy where values passed to a generic parameter are compiled into a table of required functionality. That table is then filled in for a given passed-in type with references to the implementation on the original type. The generic is implemented using calls into entries in the witness table, which turn into calls to the original type. This doesn't necessarily imply a runtime indirection: it may be a purely compile-time separation of concerns. However, it insists on a full abstraction boundary between the generic user of a type and the concrete implementation. A simple way to imagine a witness table is as a struct of function pointers, one per method in the interface. However, in practice, it's more complex because it must model things like associated types and interfaces. Witness tables are called \"dictionary passing\" in Haskell. Outside of generics, a vtable is a witness table that witnesses that a class is a descendant of an abstract base class, and is passed as part of the object instead of separately.","title":"Witness tables"},{"location":"design/generics/terminology/#dynamic-dispatch-witness-table","text":"For dynamic-dispatch witness tables, actual function pointers are formed and used as a dynamic, runtime indirection. As a result, the generic code will not be duplicated for different witness tables.","title":"Dynamic-dispatch witness table"},{"location":"design/generics/terminology/#static-dispatch-witness-table","text":"For static-dispatch witness tables, the implementation is required to collapse the table indirections at compile time. As a result, the generic code will be duplicated for different witness tables. Static-dispatch may be implemented as a performance optimization for dynamic-dispatch that increases generated code size. The final compiled output may not retain the witness table.","title":"Static-dispatch witness table"},{"location":"design/generics/terminology/#instantiation","text":"Instantiation is the implementation strategy for templates in both C++ and Carbon. Instantiation explicitly creates a copy of the template code and replaces the template components with the concrete type and its implementation operations. It allows duck typing and lazy binding. Instantiation implies template code will be duplicated. Unlike static-dispatch witness tables and monomorphization (as in Rust) , this is done before type checking completes. Only when the template is used with a concrete type is the template fully type checked, and it type checks against the actual concrete type after substituting it into the template. This means that different instantiations may interpret the same construct in different ways, and that templates can include constructs that are not valid for some possible instantiations. However, it also means that some errors in the template implementation may not produce errors until the instantiation occurs, and other errors may only happen for some instantiations.","title":"Instantiation"},{"location":"design/generics/terminology/#specialization","text":"","title":"Specialization"},{"location":"design/generics/terminology/#template-specialization","text":"Specialization in C++ is essentially overloading in the context of a template. The template is overloaded to have a different definition for some subset of the possible template argument values. For example, the C++ type std::vector<T> might have a specialization std::vector<T*> that is implemented in terms of std::vector<void*> to reduce code size. In C++, even the interface of a templated type can be changed in a specialization, as happens for std::vector<bool> .","title":"Template specialization"},{"location":"design/generics/terminology/#generic-specialization","text":"Specialization of generics, or types used by generics, is restricted to changing the implementation without affecting the interface. This restriction is needed to preserve the ability to perform type checking of generic definitions that reference a type that can be specialized, without statically knowing which specialization will be used. While there is nothing fundamentally incompatible about specialization with generics, even when implemented using witness tables, the result may be surprising because the selection of the specialized generic happens outside of the witness-table-based indirection between the generic code and the concrete implementation. Provided all selection relies exclusively on interfaces, this still satisfies the fundamental constraints of generics.","title":"Generic specialization"},{"location":"design/generics/terminology/#conditional-conformance","text":"Conditional conformance is when you have a parameterized type that has one API that it always supports, but satisfies additional interfaces under some conditions on the type argument. For example: Array(T) might implement Comparable if T itself implements Comparable , using lexicographical order.","title":"Conditional conformance"},{"location":"design/generics/terminology/#interface-type-parameters-and-associated-types","text":"Imagine an interface defining a container. Different containers will contain different types of values, and the container API will have to refer to that \"element type\" when defining the signature of methods like \"insert\" or \"find\". If that element type is a parameter (input) to the interface type, we say it is an interface type parameter ; if it is an output, we say it is an associated type . An associated type is a kind of associated entity . Interface type parameter example: interface StackTP(ElementType:! Type) fn Push[addr me: Self*](value: ElementType); fn Pop[addr me: Self*]() -> ElementType; } Associated type example: interface StackAT { let ElementType:! Type; fn Push[addr me: Self*](value: ElementType); fn Pop[addr me: Self*]() -> ElementType; } Associated types are particularly called for when the implementation controls the type, not the caller. For example, the iterator type for a container is specific to the container and not something you would expect a user of the interface to specify. interface Iterator { ... } interface Container { // This does not make sense as an parameter to the container interface, // since this type is determined from the container type. let IteratorType:! Iterator; ... fn Insert[addr me: Self*](position: IteratorType, value: ElementType); } class ListIterator(ElementType:! Type) { ... impl as Iterator; } class List(ElementType:! Type) { // Iterator type is determined by the container type. impl as Container where .IteratorType = ListIterator(ElementType) { fn Insert[addr me: Self*](position: IteratorType, value: ElementType) { ... } } } If you have an interface with type parameters, a type can have multiple impls for different combinations of type parameters. As a result, type parameters may not be deduced in a function call. However, if the interface parameters are specified, a type can only have a single implementation of the given interface. This unique implementation choice determines the values of associated types. For example, we might have an interface that says how to perform addition with another type: interface Addable(T:! Type) { let ResultType:! Type; fn Add[me: Self](rhs: T) -> ResultType; } An i32 value might support addition with i32 , u16 , and f64 values. impl i32 as Addable(i32) where .ResultType = i32 { ... } impl i32 as Addable(u16) where .ResultType = i32 { ... } impl i32 as Addable(f64) where .ResultType = f64 { ... } To write a generic function requiring a parameter to be Addable , there needs to be some way to determine the type to add to: // \u2705 This is allowed, since the value of `T` is determined by the // `y` parameter. fn DoAdd[T:! Type, U:! Addable(T)](x: U, y: T) -> U.ResultType { return x.Add(y); } // \u274c This is forbidden, can't uniquely determine `T`. fn CompileError[T:! Type, U:! Addable(T)](x: U) -> T; Once the interface parameter can be determined, that determines the values for associated types, such as ResultType in the example. As always, calls with types for which no implementation exists will be rejected at the call site: // \u274c This is forbidden, no implementation of `Addable(Orange)` // for `Apple`. DoAdd(apple, orange);","title":"Interface type parameters and associated types"},{"location":"design/generics/terminology/#type-constraints","text":"Type constraints restrict which types are legal for template or generic parameters or associated types. They help define semantics under which they should be called, and prevent incorrect calls. In general there are a number of different type relationships we would like to express, for example: This function accepts two containers. The container types may be different, but the element types need to match. For this container interface we have associated types for iterators and elements. The iterator type's element type needs to match the container's element type. An interface may define an associated type that needs to be constrained to implement some interfaces. This type must be compatible with another type. You might use this to define alternate implementations of a single interfaces, such as sorting order, for a single type. Note that type constraints can be a restriction on one type parameter or associated type, or can define a relationship between multiple types.","title":"Type constraints"},{"location":"design/generics/terminology/#type-of-type","text":"A type-of-type is the type used when declaring some type parameter. It foremost determines which types are legal arguments for that type parameter, also known as type constraints . For template parameters, that is all a type-of-type does. For generic parameters, it also determines the API that is available in the body of the function.","title":"Type-of-type"},{"location":"design/generics/terminology/#references","text":"#447: Generics terminology #731: Generics details 2: adapters, associated types, parameterized interfaces #950: Generic details 6: remove facets #1013: Generics: Set associated constants using where constraints","title":"References"},{"location":"design/interoperability/","text":"Bidirectional interoperability with C/C++ Table of contents Philosophy and goals Overview Philosophy and goals The C++ interoperability layer of Carbon allows a subset of C++ APIs to be accessed from Carbon code, and similarly a subset of Carbon APIs to be accessed from C++ code. This requires expressing one language as a subset of the other. Bridge code may be needed to map some APIs into the relevant subset, but the constraints on expressivity should be loose enough to keep the amount of such bridge code sustainable. The interoperability philosophy and goals provide more detail. Overview TODO","title":"Bidirectional interoperability with C/C++"},{"location":"design/interoperability/#bidirectional-interoperability-with-cc","text":"","title":"Bidirectional interoperability with C/C++"},{"location":"design/interoperability/#table-of-contents","text":"Philosophy and goals Overview","title":"Table of contents"},{"location":"design/interoperability/#philosophy-and-goals","text":"The C++ interoperability layer of Carbon allows a subset of C++ APIs to be accessed from Carbon code, and similarly a subset of Carbon APIs to be accessed from C++ code. This requires expressing one language as a subset of the other. Bridge code may be needed to map some APIs into the relevant subset, but the constraints on expressivity should be loose enough to keep the amount of such bridge code sustainable. The interoperability philosophy and goals provide more detail.","title":"Philosophy and goals"},{"location":"design/interoperability/#overview","text":"TODO","title":"Overview"},{"location":"design/interoperability/philosophy_and_goals/","text":"Interoperability philosophy and goals Table of contents Background Other interoperability layers Philosophy Language goal influences Performance-critical software Software and language evolution Code that is easy to read, understand, and write Practical safety guarantees and testing mechanisms Fast and scalable development Modern OS platforms, hardware architectures, and environments Interoperability with and migration from existing C++ code Goals Support mixing Carbon and C++ toolchains Compatibility with the C++ memory model Minimize bridge code Unsurprising mappings between C++ and Carbon types Allow C++ bridge code in Carbon files Carbon inheritance from C++ types Support use of advanced C++ features Support basic C interoperability Non-goals Full parity between a Carbon-only toolchain and mixing C++/Carbon toolchains Never require bridge code Convert all C++ types to Carbon types Support for C++ exceptions without bridge code Cross-language metaprogramming Offer equivalent support for languages other than C++ Open questions to be resolved later Carbon type inheritance from non-pure interface C++ types CRTP support Object lifetimes References Background Interoperability with and migration from C++ are a language goal . However, performance and evolution are higher priorities . This interaction of priorities is important to understanding Carbon's interoperability goals and trade-offs. Other interoperability layers Other language interoperability layers that may offer useful examples are: Java/Kotlin should be a comparable interoperability story. The languages are different, but share an underlying runtime. This may be closest to the model we desire for Carbon. JavaScript/TypeScript is similar to C/C++, where one language is essentially a subset of the other, allowing high interoperability. This is an interesting reference point, but we are looking at a different approach with a clearer boundary. C++/Java is an example of requiring specialized code for the bridge layer, making interoperability more burden on developers. The burden of the approach may be considered to correspond to the difference in language memory models and other language design choices. Regardless, the result can be considered higher maintenance for developers than we want for Carbon. C++/Go is similar to C++/Java. However, Go notably allows C++ bridge code to exist in the .go files, which can ease maintenance of the bridge layer, and is desirable for Carbon. Philosophy The C++ interoperability layer of Carbon allows a subset of C++ APIs to be accessed from Carbon code, and similarly a subset of Carbon APIs to be accessed from C++ code. This requires expressing one language as a subset of the other. Bridge code may be needed to map some APIs into the relevant subset, but the constraints on expressivity should be loose enough to keep the amount of such bridge code sustainable. The design for interoperability between Carbon and C++ hinges on: The ability to interoperate with a wide variety of code, such as classes/structs and templates, not just free functions. A willingness to expose the idioms of C++ into Carbon code, and the other way around, when necessary to maximize performance of the interoperability layer. The use of wrappers and generic programming, including templates, to minimize or eliminate runtime overhead. These things come together when looking at how custom data structures in C++ are exposed into Carbon, and the other way around. In both languages, it is reasonable and even common to have customized low-level data structures, such as associative containers. For example, there are numerous data structures for mapping from a key to a value that might be best for a particular use case, including hash tables, linked hash tables, sorted vectors, and btrees. Even for a given data structure, there may be slow but meaningful evolution in implementations strategies. The result is that it will often be reasonable to directly expose a C++ data structure to Carbon without converting it to a \"native\" or \"idiomatic\" Carbon data structure. Although interfaces may differ, a trivial adapter wrapper should be sufficient. Many Carbon data structures should also be able to support multiple implementations with C++ data structures being one such implementation, allowing for idiomatic use of C++ hidden behind Carbon. The reverse is also true. C++ code will often not care, or can be refactored to not care, what specific data structure is used. Carbon data structures can be exposed as yet another implementation in C++, and wrapped to match C++ idioms and even templates. For example, a C++ class template like std::vector<T> should be usable without wrapper code or runtime overhead, and passing a Carbon type as T . The resulting type should be equally usable from either C++ or Carbon code. It should also be easy to wrap std::vector<T> with a Carbon interface for transparent use in idiomatic Carbon code. Language goal influences Performance-critical software Interoperability with C++ will be frequently used in Carbon, whether it's C++ developers trying out Carbon, incrementally migrating a large C++ codebase, or continuing to use a C++ library long-term. In all cases, it must be possible to write interoperable code with zero overhead; copies must not be required. Software and language evolution Interoperability will require the addition of features to Carbon which exist primarily to support interoperability use cases. However, these features must not unduly impinge the overall evolution of Carbon. In particular, only a subset of Carbon features will support interoperability with C++. To do otherwise would restrict Carbon's feature set. Code that is easy to read, understand, and write Interoperability-related Carbon code will likely be more difficult to read than other, more idiomatic Carbon code. This is okay: aiming to make Carbon code readable doesn't mean that it needs to all be trivial to read. At the same time, the extra costs that interoperability exerts on Carbon developers should be minimized. Practical safety guarantees and testing mechanisms Safety is important to maintain around interoperability code, and mitigations should be provided where possible. However, safety guarantees will be focused on native Carbon code. C++ code will not benefit from the same set of safety mechanisms that Carbon offers, so Carbon code calling into C++ will accept higher safety risks. Fast and scalable development The interoperability layer will likely have tooling limitations similar to C++. For example, Carbon aims to compile quickly. However, C++ interoperability hinges on compiling C++ code, which is relatively slow. Carbon libraries that use interoperability will see bottlenecks from C++ compile time. Improving C++ is outside the scope of Carbon. Modern OS platforms, hardware architectures, and environments Interoperability will apply to the intersection of environments supported by both Carbon and C++. Pragmatically, Carbon will likely be the limiting factor here. Interoperability with and migration from existing C++ code Carbon's language goal for interoperability will focus on C++17 compatibility. The language design must be mindful of the prioritization; trade-offs harming other goals may still be made so long as they offer greater benefits for interoperability and Carbon as a whole. Although the below interoperability-specific goals will focus on interoperability, it's also important to consider how migration would be affected. If interoperability requires complex work, particularly to avoid performance impacts, it could impair the ability to incrementally migrate C++ codebases to Carbon. Goals Support mixing Carbon and C++ toolchains The Carbon toolchain will support compiling C++ code. It will contain a customized C++ compiler that enables some more advanced interoperability features, such as calling Carbon templates from C++. Mixing toolchains will also be supported in both directions: C++ libraries compiled by a non-Carbon toolchain will be usable from Carbon, so long as they are ABI-compatible with Carbon's C++ toolchain. The Carbon toolchain will support, as an option, generating a C++ header and object file from a Carbon library, with an ABI that's suitable for use with non-Carbon toolchains. Mixing toolchains restricts functionality to what's feasible with the C++ ABI. For example, developers should expect that Carbon templates will be callable from C++ when using the Carbon toolchain, and will not be available when mixing toolchains because it would require a substantially different and more complex interoperability implementation. This degraded interoperability should still be sufficient for most developers, albeit with the potential of more bridge code. Any C++ interoperability code that works when mixing toolchains must work when using the native Carbon toolchain. The mixed toolchain support must not have semantic divergence. The converse is not true, and the native Carbon toolchain may have additional language support and optimizations. Compatibility with the C++ memory model It must be straightforward for any Carbon interoperability code to be compatible with the C++ memory model. This does not mean that Carbon must exclusively use the C++ memory model, only that it must be supported. Minimize bridge code The majority of simple C++ functions and types should be usable from Carbon without any custom bridge code and without any runtime overhead. That is, Carbon code should be able to call most C++ code without any code changes to add support for interoperability, even if that code was built with a non-Carbon toolchain. This includes instantiating Carbon templates or generics using C++ types. In the other direction, Carbon may need some minimal markup to expose functions and types to C++. This should help avoid requiring Carbon to generate C++-compatible endpoints unconditionally, which could have compile and linking overheads that may in many cases be unnecessary. Also, it should help produce errors that indicate when a function or type may require additional changes to make compatible with C++. Carbon's priority developers should be able to easily reuse the mature ecosystem of C++ libraries provided by third-parties. A third-party library's language choice should not be a barrier to Carbon adoption. Even for first-party libraries, migration of C++ codebases to Carbon will often be incremental due to human costs of executing and verifying source migrations. Minimizing the amount of bridge code required should be expected to simplify such migrations. Unsurprising mappings between C++ and Carbon types Carbon will provide unsurprising mappings for common types. Primitive types will have mappings with zero overhead conversions. They are frequently used, making it important that interoperability code be able to use them seamlessly. The storage and representation will need to be equivalent in both languages. For example, if a C++ __int64 maps to Carbon's Int64 , the memory layout of both types must be identical. Semantics need to be similar, but edge-case behaviors don't need to be identical, allowing Carbon flexibility to evolve. For example, where C++ would have modulo wrapping on integers, Carbon could instead have trapping behavior on the default-mapped primitive types. Carbon may have versions of these types with no C++ mapping, such as Int256 . Non-owning vocabulary types , such as pointers and references, will have transparent, automatic translation between C++ and Carbon non-owning vocabulary types with zero overhead. Other vocabulary types will typically have reasonable, but potentially non-zero overhead, conversions available to map into Carbon vocabulary types. Code using these may choose whether to pay the overhead to convert. They may also use the C++ type directly from Carbon code, and the other way around. Incomplete types must have a mapping with similar semantics, similar to primitive types. Allow C++ bridge code in Carbon files Carbon files should support inline bridge code written in C++. Where bridge code is necessary, this will allow for maintenance of it directly alongside the code that uses it. Carbon inheritance from C++ types Carbon will support inheritance from C++ types for interoperability, although the syntax constructs may look different from C++ inheritance. This is considered necessary to address cases where a C++ library API expects users to inherit from a given C++ type. This might be restricted to pure interface types; see the open question . Support use of advanced C++ features There should be support for most idiomatic usage of advanced C++ features. A few examples are templates, overload sets, attributes and ADL . Although these features can be considered \"advanced\", their use is widespread throughout C++ code, including STL. Support for such features is key to supporting migration from C++ features. Support basic C interoperability C interoperability support must be sufficient for Carbon code to call popular APIs that are written in C. The ability of C to call Carbon will be more restricted, limited to where it echoes C++ interoperability support. Basic C interoperability will include functions, primitive types, and structs that only contain member variables. Features where interoperability will rely on more advanced C++-specific features, such as templates, inheritance, and class functions, need not be supported for C. These would require a C-specific interoperability model that will not be included. Non-goals Full parity between a Carbon-only toolchain and mixing C++/Carbon toolchains Making mixed C++/Carbon toolchain support equivalent to Carbon-only toolchain support affects all interoperability features. Mixed toolchains will have degraded support because full parity would be too expensive. The feature of calling Carbon templates from C++ code is key when analyzing this option. Template instantiation during compilation is pervasive in C++. With a Carbon toolchain compiling both Carbon and C++ code, the C++ compiler can be modified to handle Carbon templates differently. Carbon templates can be handled by exposing the Carbon compiler's AST to the C++ compiler directly, as a compiler extension. While this approach is still complex and may not always work, it should offer substantial value and ability to migrate C++ code to Carbon without requiring parallel maintenance of implementations in C++. With a mixed toolchain, the C++ compiler cannot be modified to handle Carbon templates differently. The only way to support template instantiation would be by having Carbon templates converted into equivalent C++ templates in C++ headers; in other words, template support would require source-to-source translation. Supporting Carbon to C++ code translations would be a complex and high cost feature to achieve full parity for mixed toolchains. Requiring bridge code for mixed toolchains is the likely solution to avoid this cost. Note that this issue differs when considering interoperability for Carbon code instantiating C++ templates. The C++ templates must be in C++ headers for re-use, which in turn must compile with the Carbon toolchain to re-use the built C++ code, regardless of whether a separate C++ toolchain is in use. This may also be considered a constraint on mixed toolchain interoperability, but it's simpler to address and less likely to burden developers. To summarize, developers should expect that while most features will work equivalently for mixed toolchains, there will never be full parity. Never require bridge code Corner cases of C++ will not receive equal support to common cases: the complexity of supporting any given construct must be balanced by the real world need for that support. For example: Interoperability will target C++17. Any interoperability support for future versions of C++, including features such as C++20 modules, will be based on a cost-benefit analysis. Exhaustive support should not be assumed. Support will be focused on idiomatic code, interfaces, and patterns used in widespread open source libraries or by other key constituencies. C++ code will have edge cases where the benefits of limiting Carbon's maintenance costs by avoiding complex interoperability outweighs the value of avoiding bridge code. Support for low-level C ABIs may be focused on modern 64-bit ABIs, including Linux, POSIX, and a small subset of Windows' calling conventions. Convert all C++ types to Carbon types Non-zero overhead conversions should only be supported , never required , in order to offer reliable, unsurprising performance behaviors. This does not mean that conversions will always be supported, as support is a cost-benefit decision for specific type mappings. For example, consider conversions between std::vector<T> and an equivalent, idiomatic Carbon type: Making conversions zero-overhead would require the Carbon type to mirror the memory layout and implementation semantics of std::vector<T> . However, doing so would constrain the evolution of the Carbon type to match C++. Although some constraints are accepted for most primitive types, it would pose a major burden on Carbon's evolution to constrain Carbon's types to match C++ vocabulary type implementations. These conversions may not always be present, but std::vector<T> is a frequently used type. As a result, it can be expected that there will be functions supporting a copy-based conversion to the idiomatic Carbon type. An interface which can hide the difference between whether std::vector<T> or the equivalent, idiomatic Carbon type is in use may also be offered for common types. It will still be normal to handle C++ types in Carbon code without conversions. Developers should be given the choice of when to convert. Support for C++ exceptions without bridge code Carbon may not provide seamless interoperability support for C++ exceptions. For example, translating C++ exceptions to or from Carbon errors might require annotations or bridge code, and those translations may have some performance overhead or lose information. Furthermore, if Carbon code calls a C++ function without suitable annotations or bridging, and that function exits with an exception, the program might terminate. Cross-language metaprogramming Carbon's metaprogramming design will be more restrictive than C++'s preprocessor macros. Although interoperability should handle simple cases, such as #define STDIN_FILENO 0 , complex metaprogramming libraries may require a deep ability to understand code rewrites. It should be reasonable to have these instead rewritten to use Carbon's metaprogramming model. Offer equivalent support for languages other than C++ Long-term, it should be anticipated that Carbon will add interoperability with non-C++ languages. However, interoperability discussions will be focused on C++ in order to support the language goal . Although we should work to consider extensibility when building interoperability facilities, C++ should be expected to have more robust support. Many languages do offer interoperability layers with C. Carbon's C interoperability will likely offer a degree of multi-language interoperability using C as an intermediary. Open questions to be resolved later Carbon type inheritance from non-pure interface C++ types Some C++ APIs will expect that consumers use classes that inherit from a type provided by the API. It's desirable to have Carbon support, in some way, inheritance from API types in order to use these APIs. It may be sufficient to require the parent type be a pure interface, and that APIs with either use bridge code or switch implementations. That will be determined later. CRTP support Although CRTP is a common technique in C++, interoperability support may require substantial work. Libraries based on use of CRTP may require bridge code or a rewrite for Carbon interoperability. More analysis should be done on the cost-benefit of supporting CRTP before making a support decision. Object lifetimes Carbon may have a different object lifetime design than C++. For example, Carbon may choose different rules for determining the lifetime of temporaries. This could affect idiomatic use of C++ APIs, turning code that would be safe in C++ into unsafe Carbon code, requiring developers to learn new coding patterns. More analysis should be done on object lifetimes and potential Carbon designs for it before deciding how to treat object lifetimes in the scope of interoperability. References Proposal #175: C++ interoperability goals","title":"Interoperability philosophy and goals"},{"location":"design/interoperability/philosophy_and_goals/#interoperability-philosophy-and-goals","text":"","title":"Interoperability philosophy and goals"},{"location":"design/interoperability/philosophy_and_goals/#table-of-contents","text":"Background Other interoperability layers Philosophy Language goal influences Performance-critical software Software and language evolution Code that is easy to read, understand, and write Practical safety guarantees and testing mechanisms Fast and scalable development Modern OS platforms, hardware architectures, and environments Interoperability with and migration from existing C++ code Goals Support mixing Carbon and C++ toolchains Compatibility with the C++ memory model Minimize bridge code Unsurprising mappings between C++ and Carbon types Allow C++ bridge code in Carbon files Carbon inheritance from C++ types Support use of advanced C++ features Support basic C interoperability Non-goals Full parity between a Carbon-only toolchain and mixing C++/Carbon toolchains Never require bridge code Convert all C++ types to Carbon types Support for C++ exceptions without bridge code Cross-language metaprogramming Offer equivalent support for languages other than C++ Open questions to be resolved later Carbon type inheritance from non-pure interface C++ types CRTP support Object lifetimes References","title":"Table of contents"},{"location":"design/interoperability/philosophy_and_goals/#background","text":"Interoperability with and migration from C++ are a language goal . However, performance and evolution are higher priorities . This interaction of priorities is important to understanding Carbon's interoperability goals and trade-offs.","title":"Background"},{"location":"design/interoperability/philosophy_and_goals/#other-interoperability-layers","text":"Other language interoperability layers that may offer useful examples are: Java/Kotlin should be a comparable interoperability story. The languages are different, but share an underlying runtime. This may be closest to the model we desire for Carbon. JavaScript/TypeScript is similar to C/C++, where one language is essentially a subset of the other, allowing high interoperability. This is an interesting reference point, but we are looking at a different approach with a clearer boundary. C++/Java is an example of requiring specialized code for the bridge layer, making interoperability more burden on developers. The burden of the approach may be considered to correspond to the difference in language memory models and other language design choices. Regardless, the result can be considered higher maintenance for developers than we want for Carbon. C++/Go is similar to C++/Java. However, Go notably allows C++ bridge code to exist in the .go files, which can ease maintenance of the bridge layer, and is desirable for Carbon.","title":"Other interoperability layers"},{"location":"design/interoperability/philosophy_and_goals/#philosophy","text":"The C++ interoperability layer of Carbon allows a subset of C++ APIs to be accessed from Carbon code, and similarly a subset of Carbon APIs to be accessed from C++ code. This requires expressing one language as a subset of the other. Bridge code may be needed to map some APIs into the relevant subset, but the constraints on expressivity should be loose enough to keep the amount of such bridge code sustainable. The design for interoperability between Carbon and C++ hinges on: The ability to interoperate with a wide variety of code, such as classes/structs and templates, not just free functions. A willingness to expose the idioms of C++ into Carbon code, and the other way around, when necessary to maximize performance of the interoperability layer. The use of wrappers and generic programming, including templates, to minimize or eliminate runtime overhead. These things come together when looking at how custom data structures in C++ are exposed into Carbon, and the other way around. In both languages, it is reasonable and even common to have customized low-level data structures, such as associative containers. For example, there are numerous data structures for mapping from a key to a value that might be best for a particular use case, including hash tables, linked hash tables, sorted vectors, and btrees. Even for a given data structure, there may be slow but meaningful evolution in implementations strategies. The result is that it will often be reasonable to directly expose a C++ data structure to Carbon without converting it to a \"native\" or \"idiomatic\" Carbon data structure. Although interfaces may differ, a trivial adapter wrapper should be sufficient. Many Carbon data structures should also be able to support multiple implementations with C++ data structures being one such implementation, allowing for idiomatic use of C++ hidden behind Carbon. The reverse is also true. C++ code will often not care, or can be refactored to not care, what specific data structure is used. Carbon data structures can be exposed as yet another implementation in C++, and wrapped to match C++ idioms and even templates. For example, a C++ class template like std::vector<T> should be usable without wrapper code or runtime overhead, and passing a Carbon type as T . The resulting type should be equally usable from either C++ or Carbon code. It should also be easy to wrap std::vector<T> with a Carbon interface for transparent use in idiomatic Carbon code.","title":"Philosophy"},{"location":"design/interoperability/philosophy_and_goals/#language-goal-influences","text":"","title":"Language goal influences"},{"location":"design/interoperability/philosophy_and_goals/#performance-critical-software","text":"Interoperability with C++ will be frequently used in Carbon, whether it's C++ developers trying out Carbon, incrementally migrating a large C++ codebase, or continuing to use a C++ library long-term. In all cases, it must be possible to write interoperable code with zero overhead; copies must not be required.","title":"Performance-critical software"},{"location":"design/interoperability/philosophy_and_goals/#software-and-language-evolution","text":"Interoperability will require the addition of features to Carbon which exist primarily to support interoperability use cases. However, these features must not unduly impinge the overall evolution of Carbon. In particular, only a subset of Carbon features will support interoperability with C++. To do otherwise would restrict Carbon's feature set.","title":"Software and language evolution"},{"location":"design/interoperability/philosophy_and_goals/#code-that-is-easy-to-read-understand-and-write","text":"Interoperability-related Carbon code will likely be more difficult to read than other, more idiomatic Carbon code. This is okay: aiming to make Carbon code readable doesn't mean that it needs to all be trivial to read. At the same time, the extra costs that interoperability exerts on Carbon developers should be minimized.","title":"Code that is easy to read, understand, and write"},{"location":"design/interoperability/philosophy_and_goals/#practical-safety-guarantees-and-testing-mechanisms","text":"Safety is important to maintain around interoperability code, and mitigations should be provided where possible. However, safety guarantees will be focused on native Carbon code. C++ code will not benefit from the same set of safety mechanisms that Carbon offers, so Carbon code calling into C++ will accept higher safety risks.","title":"Practical safety guarantees and testing mechanisms"},{"location":"design/interoperability/philosophy_and_goals/#fast-and-scalable-development","text":"The interoperability layer will likely have tooling limitations similar to C++. For example, Carbon aims to compile quickly. However, C++ interoperability hinges on compiling C++ code, which is relatively slow. Carbon libraries that use interoperability will see bottlenecks from C++ compile time. Improving C++ is outside the scope of Carbon.","title":"Fast and scalable development"},{"location":"design/interoperability/philosophy_and_goals/#modern-os-platforms-hardware-architectures-and-environments","text":"Interoperability will apply to the intersection of environments supported by both Carbon and C++. Pragmatically, Carbon will likely be the limiting factor here.","title":"Modern OS platforms, hardware architectures, and environments"},{"location":"design/interoperability/philosophy_and_goals/#interoperability-with-and-migration-from-existing-c-code","text":"Carbon's language goal for interoperability will focus on C++17 compatibility. The language design must be mindful of the prioritization; trade-offs harming other goals may still be made so long as they offer greater benefits for interoperability and Carbon as a whole. Although the below interoperability-specific goals will focus on interoperability, it's also important to consider how migration would be affected. If interoperability requires complex work, particularly to avoid performance impacts, it could impair the ability to incrementally migrate C++ codebases to Carbon.","title":"Interoperability with and migration from existing C++ code"},{"location":"design/interoperability/philosophy_and_goals/#goals","text":"","title":"Goals"},{"location":"design/interoperability/philosophy_and_goals/#support-mixing-carbon-and-c-toolchains","text":"The Carbon toolchain will support compiling C++ code. It will contain a customized C++ compiler that enables some more advanced interoperability features, such as calling Carbon templates from C++. Mixing toolchains will also be supported in both directions: C++ libraries compiled by a non-Carbon toolchain will be usable from Carbon, so long as they are ABI-compatible with Carbon's C++ toolchain. The Carbon toolchain will support, as an option, generating a C++ header and object file from a Carbon library, with an ABI that's suitable for use with non-Carbon toolchains. Mixing toolchains restricts functionality to what's feasible with the C++ ABI. For example, developers should expect that Carbon templates will be callable from C++ when using the Carbon toolchain, and will not be available when mixing toolchains because it would require a substantially different and more complex interoperability implementation. This degraded interoperability should still be sufficient for most developers, albeit with the potential of more bridge code. Any C++ interoperability code that works when mixing toolchains must work when using the native Carbon toolchain. The mixed toolchain support must not have semantic divergence. The converse is not true, and the native Carbon toolchain may have additional language support and optimizations.","title":"Support mixing Carbon and C++ toolchains"},{"location":"design/interoperability/philosophy_and_goals/#compatibility-with-the-c-memory-model","text":"It must be straightforward for any Carbon interoperability code to be compatible with the C++ memory model. This does not mean that Carbon must exclusively use the C++ memory model, only that it must be supported.","title":"Compatibility with the C++ memory model"},{"location":"design/interoperability/philosophy_and_goals/#minimize-bridge-code","text":"The majority of simple C++ functions and types should be usable from Carbon without any custom bridge code and without any runtime overhead. That is, Carbon code should be able to call most C++ code without any code changes to add support for interoperability, even if that code was built with a non-Carbon toolchain. This includes instantiating Carbon templates or generics using C++ types. In the other direction, Carbon may need some minimal markup to expose functions and types to C++. This should help avoid requiring Carbon to generate C++-compatible endpoints unconditionally, which could have compile and linking overheads that may in many cases be unnecessary. Also, it should help produce errors that indicate when a function or type may require additional changes to make compatible with C++. Carbon's priority developers should be able to easily reuse the mature ecosystem of C++ libraries provided by third-parties. A third-party library's language choice should not be a barrier to Carbon adoption. Even for first-party libraries, migration of C++ codebases to Carbon will often be incremental due to human costs of executing and verifying source migrations. Minimizing the amount of bridge code required should be expected to simplify such migrations.","title":"Minimize bridge code"},{"location":"design/interoperability/philosophy_and_goals/#unsurprising-mappings-between-c-and-carbon-types","text":"Carbon will provide unsurprising mappings for common types. Primitive types will have mappings with zero overhead conversions. They are frequently used, making it important that interoperability code be able to use them seamlessly. The storage and representation will need to be equivalent in both languages. For example, if a C++ __int64 maps to Carbon's Int64 , the memory layout of both types must be identical. Semantics need to be similar, but edge-case behaviors don't need to be identical, allowing Carbon flexibility to evolve. For example, where C++ would have modulo wrapping on integers, Carbon could instead have trapping behavior on the default-mapped primitive types. Carbon may have versions of these types with no C++ mapping, such as Int256 . Non-owning vocabulary types , such as pointers and references, will have transparent, automatic translation between C++ and Carbon non-owning vocabulary types with zero overhead. Other vocabulary types will typically have reasonable, but potentially non-zero overhead, conversions available to map into Carbon vocabulary types. Code using these may choose whether to pay the overhead to convert. They may also use the C++ type directly from Carbon code, and the other way around. Incomplete types must have a mapping with similar semantics, similar to primitive types.","title":"Unsurprising mappings between C++ and Carbon types"},{"location":"design/interoperability/philosophy_and_goals/#allow-c-bridge-code-in-carbon-files","text":"Carbon files should support inline bridge code written in C++. Where bridge code is necessary, this will allow for maintenance of it directly alongside the code that uses it.","title":"Allow C++ bridge code in Carbon files"},{"location":"design/interoperability/philosophy_and_goals/#carbon-inheritance-from-c-types","text":"Carbon will support inheritance from C++ types for interoperability, although the syntax constructs may look different from C++ inheritance. This is considered necessary to address cases where a C++ library API expects users to inherit from a given C++ type. This might be restricted to pure interface types; see the open question .","title":"Carbon inheritance from C++ types"},{"location":"design/interoperability/philosophy_and_goals/#support-use-of-advanced-c-features","text":"There should be support for most idiomatic usage of advanced C++ features. A few examples are templates, overload sets, attributes and ADL . Although these features can be considered \"advanced\", their use is widespread throughout C++ code, including STL. Support for such features is key to supporting migration from C++ features.","title":"Support use of advanced C++ features"},{"location":"design/interoperability/philosophy_and_goals/#support-basic-c-interoperability","text":"C interoperability support must be sufficient for Carbon code to call popular APIs that are written in C. The ability of C to call Carbon will be more restricted, limited to where it echoes C++ interoperability support. Basic C interoperability will include functions, primitive types, and structs that only contain member variables. Features where interoperability will rely on more advanced C++-specific features, such as templates, inheritance, and class functions, need not be supported for C. These would require a C-specific interoperability model that will not be included.","title":"Support basic C interoperability"},{"location":"design/interoperability/philosophy_and_goals/#non-goals","text":"","title":"Non-goals"},{"location":"design/interoperability/philosophy_and_goals/#full-parity-between-a-carbon-only-toolchain-and-mixing-ccarbon-toolchains","text":"Making mixed C++/Carbon toolchain support equivalent to Carbon-only toolchain support affects all interoperability features. Mixed toolchains will have degraded support because full parity would be too expensive. The feature of calling Carbon templates from C++ code is key when analyzing this option. Template instantiation during compilation is pervasive in C++. With a Carbon toolchain compiling both Carbon and C++ code, the C++ compiler can be modified to handle Carbon templates differently. Carbon templates can be handled by exposing the Carbon compiler's AST to the C++ compiler directly, as a compiler extension. While this approach is still complex and may not always work, it should offer substantial value and ability to migrate C++ code to Carbon without requiring parallel maintenance of implementations in C++. With a mixed toolchain, the C++ compiler cannot be modified to handle Carbon templates differently. The only way to support template instantiation would be by having Carbon templates converted into equivalent C++ templates in C++ headers; in other words, template support would require source-to-source translation. Supporting Carbon to C++ code translations would be a complex and high cost feature to achieve full parity for mixed toolchains. Requiring bridge code for mixed toolchains is the likely solution to avoid this cost. Note that this issue differs when considering interoperability for Carbon code instantiating C++ templates. The C++ templates must be in C++ headers for re-use, which in turn must compile with the Carbon toolchain to re-use the built C++ code, regardless of whether a separate C++ toolchain is in use. This may also be considered a constraint on mixed toolchain interoperability, but it's simpler to address and less likely to burden developers. To summarize, developers should expect that while most features will work equivalently for mixed toolchains, there will never be full parity.","title":"Full parity between a Carbon-only toolchain and mixing C++/Carbon toolchains"},{"location":"design/interoperability/philosophy_and_goals/#never-require-bridge-code","text":"Corner cases of C++ will not receive equal support to common cases: the complexity of supporting any given construct must be balanced by the real world need for that support. For example: Interoperability will target C++17. Any interoperability support for future versions of C++, including features such as C++20 modules, will be based on a cost-benefit analysis. Exhaustive support should not be assumed. Support will be focused on idiomatic code, interfaces, and patterns used in widespread open source libraries or by other key constituencies. C++ code will have edge cases where the benefits of limiting Carbon's maintenance costs by avoiding complex interoperability outweighs the value of avoiding bridge code. Support for low-level C ABIs may be focused on modern 64-bit ABIs, including Linux, POSIX, and a small subset of Windows' calling conventions.","title":"Never require bridge code"},{"location":"design/interoperability/philosophy_and_goals/#convert-all-c-types-to-carbon-types","text":"Non-zero overhead conversions should only be supported , never required , in order to offer reliable, unsurprising performance behaviors. This does not mean that conversions will always be supported, as support is a cost-benefit decision for specific type mappings. For example, consider conversions between std::vector<T> and an equivalent, idiomatic Carbon type: Making conversions zero-overhead would require the Carbon type to mirror the memory layout and implementation semantics of std::vector<T> . However, doing so would constrain the evolution of the Carbon type to match C++. Although some constraints are accepted for most primitive types, it would pose a major burden on Carbon's evolution to constrain Carbon's types to match C++ vocabulary type implementations. These conversions may not always be present, but std::vector<T> is a frequently used type. As a result, it can be expected that there will be functions supporting a copy-based conversion to the idiomatic Carbon type. An interface which can hide the difference between whether std::vector<T> or the equivalent, idiomatic Carbon type is in use may also be offered for common types. It will still be normal to handle C++ types in Carbon code without conversions. Developers should be given the choice of when to convert.","title":"Convert all C++ types to Carbon types"},{"location":"design/interoperability/philosophy_and_goals/#support-for-c-exceptions-without-bridge-code","text":"Carbon may not provide seamless interoperability support for C++ exceptions. For example, translating C++ exceptions to or from Carbon errors might require annotations or bridge code, and those translations may have some performance overhead or lose information. Furthermore, if Carbon code calls a C++ function without suitable annotations or bridging, and that function exits with an exception, the program might terminate.","title":"Support for C++ exceptions without bridge code"},{"location":"design/interoperability/philosophy_and_goals/#cross-language-metaprogramming","text":"Carbon's metaprogramming design will be more restrictive than C++'s preprocessor macros. Although interoperability should handle simple cases, such as #define STDIN_FILENO 0 , complex metaprogramming libraries may require a deep ability to understand code rewrites. It should be reasonable to have these instead rewritten to use Carbon's metaprogramming model.","title":"Cross-language metaprogramming"},{"location":"design/interoperability/philosophy_and_goals/#offer-equivalent-support-for-languages-other-than-c","text":"Long-term, it should be anticipated that Carbon will add interoperability with non-C++ languages. However, interoperability discussions will be focused on C++ in order to support the language goal . Although we should work to consider extensibility when building interoperability facilities, C++ should be expected to have more robust support. Many languages do offer interoperability layers with C. Carbon's C interoperability will likely offer a degree of multi-language interoperability using C as an intermediary.","title":"Offer equivalent support for languages other than C++"},{"location":"design/interoperability/philosophy_and_goals/#open-questions-to-be-resolved-later","text":"","title":"Open questions to be resolved later"},{"location":"design/interoperability/philosophy_and_goals/#carbon-type-inheritance-from-non-pure-interface-c-types","text":"Some C++ APIs will expect that consumers use classes that inherit from a type provided by the API. It's desirable to have Carbon support, in some way, inheritance from API types in order to use these APIs. It may be sufficient to require the parent type be a pure interface, and that APIs with either use bridge code or switch implementations. That will be determined later.","title":"Carbon type inheritance from non-pure interface C++ types"},{"location":"design/interoperability/philosophy_and_goals/#crtp-support","text":"Although CRTP is a common technique in C++, interoperability support may require substantial work. Libraries based on use of CRTP may require bridge code or a rewrite for Carbon interoperability. More analysis should be done on the cost-benefit of supporting CRTP before making a support decision.","title":"CRTP support"},{"location":"design/interoperability/philosophy_and_goals/#object-lifetimes","text":"Carbon may have a different object lifetime design than C++. For example, Carbon may choose different rules for determining the lifetime of temporaries. This could affect idiomatic use of C++ APIs, turning code that would be safe in C++ into unsafe Carbon code, requiring developers to learn new coding patterns. More analysis should be done on object lifetimes and potential Carbon designs for it before deciding how to treat object lifetimes in the scope of interoperability.","title":"Object lifetimes"},{"location":"design/interoperability/philosophy_and_goals/#references","text":"Proposal #175: C++ interoperability goals","title":"References"},{"location":"design/lexical_conventions/","text":"Lexical conventions Table of contents Lexical elements Lexical elements The first stage of processing a source file is the division of the source file into lexical elements. A lexical element is one of the following: a maximal sequence of whitespace characters a word a literal: a numeric literal a string literal TODO: operators, comments, ... The sequence of lexical elements is formed by repeatedly removing the longest initial sequence of characters that forms a valid lexical element.","title":"Lexical conventions"},{"location":"design/lexical_conventions/#lexical-conventions","text":"","title":"Lexical conventions"},{"location":"design/lexical_conventions/#table-of-contents","text":"Lexical elements","title":"Table of contents"},{"location":"design/lexical_conventions/#lexical-elements","text":"The first stage of processing a source file is the division of the source file into lexical elements. A lexical element is one of the following: a maximal sequence of whitespace characters a word a literal: a numeric literal a string literal TODO: operators, comments, ... The sequence of lexical elements is formed by repeatedly removing the longest initial sequence of characters that forms a valid lexical element.","title":"Lexical elements"},{"location":"design/lexical_conventions/numeric_literals/","text":"Numeric literals Table of contents Overview Details Integer literals Real-number literals Digit separators Divergence from other languages Alternatives considered References Overview The following syntaxes are supported: Integer literals 12345 (decimal) 0x1FE (hexadecimal) 0b1010 (binary) Real-number literals 123.456 (digits on both sides of the . ) 123.456e789 (optional + or - after the e ) 0x1.2p123 (optional + or - after the p ) Digit separators ( _ ) may be used, with some restrictions Note that real-number literals always contain a . with digits on both sides, and integer literals never contain a . . Literals are case-sensitive. Unlike in C++, literals do not have a suffix to indicate their type. Details Integer literals Decimal integers are written as a non-zero decimal digit followed by zero or more additional decimal digits, or as a single 0 . Integers in other bases are written as a 0 followed by a base specifier character, followed by a sequence of digits in the corresponding base. The available base specifiers and corresponding bases are: Base specifier Base Digits b 2 0 and 1 x 16 0 ... 9 , A ... F The above table is case-sensitive. For example, 0b1 and 0x1A are valid, and 0B1 , 0X1A , and 0x1a are invalid. A zero at the start of a literal can never be followed by another digit: either the literal is 0 , the 0 begins a base specifier, or the next character is a decimal point (see below). No support is provided for octal literals, and any C or C++ octal literal (other than 0 ) is invalid in Carbon. Real-number literals Real numbers are written as a decimal or hexadecimal integer followed by a period ( . ) followed by a sequence of one or more decimal or hexadecimal digits, respectively. A digit is required on each side of the period. 0. and .3 are both invalid. A real number can be followed by an exponent character, an optional + or - (defaulting to + if absent), and a character sequence matching the grammar of a decimal integer with some value N . For a decimal real number, the exponent character is e , and the effect is to multiply the given value by 10 \u00b1 N . For a hexadecimal real number, the exponent character is p , and the effect is to multiply the given value by 2 \u00b1 N . The exponent suffix is optional for both decimal and hexadecimal real numbers. Note that a decimal integer followed by e is not a real-number literal. For example, 3e10 is not a valid literal. When a real-number literal is interpreted as a value of a real-number type, its value is the representable real number closest to the value of the literal. In the case of a tie, the nearest value whose mantissa is even is selected. The decimal real number syntax allows for any decimal fraction to be expressed -- that is, any number of the form a x 10 - b , where a is an integer and b is a non-negative integer. Because the decimal fractions are dense in the reals and the set of values of the real-number type is assumed to be discrete, every value of the real-number type can be expressed as a real number literal. However, for certain applications, directly expressing the intended real-number representation may be more convenient than producing a decimal equivalent that is known to convert to the intended value. Hexadecimal real-number literals are provided in order to permit values of binary floating or fixed point real-number types to be expressed directly. Digit separators If digit separators ( _ ) are included in literals, they must meet the respective condition: For decimal integers, the digit separators shall occur every three digits starting from the right. For example, 2_147_483_648 . For hexadecimal integers, the digit separators shall occur every four digits starting from the right. For example, 0x7FFF_FFFF . For real-number literals, digit separators can appear in the decimal and hexadecimal integer portions (prior to the period and after the optional e or mandatory p ) as described in the previous bullets. For example, 2_147.483648e12_345 or 0x1_00CA.FEF00Dp+24 For binary literals, digit separators can appear between any two digits. For example, 0b1_000_101_11 . Divergence from other languages The design provides a syntax that is deliberately close to that used both by C++ and many other languages, so it should feel familiar to developers. However, it selects a reasonably minimal subset of the syntaxes. This minimal approach provides benefits directly in line with the goal that Carbon code should be easy to read, understand, and write : Reduces unnecessary choices for programmers. Simplifies the syntax rules of the language. Improves consistency of written Carbon code. That said, it still provides sufficient variations to address important use cases for the goal of not leaving room for a lower level language: Hexadecimal and binary integer literals. Scientific notation floating point literals. Hexadecimal (scientific) floating point literals. Alternatives considered Integer bases Octal literals Decimal literals Case sensitivity Real number syntax Disallow ties Digit separator syntax References Proposal #143: Numeric literals Proposal #866: Allow ties in floating literals","title":"Numeric literals"},{"location":"design/lexical_conventions/numeric_literals/#numeric-literals","text":"","title":"Numeric literals"},{"location":"design/lexical_conventions/numeric_literals/#table-of-contents","text":"Overview Details Integer literals Real-number literals Digit separators Divergence from other languages Alternatives considered References","title":"Table of contents"},{"location":"design/lexical_conventions/numeric_literals/#overview","text":"The following syntaxes are supported: Integer literals 12345 (decimal) 0x1FE (hexadecimal) 0b1010 (binary) Real-number literals 123.456 (digits on both sides of the . ) 123.456e789 (optional + or - after the e ) 0x1.2p123 (optional + or - after the p ) Digit separators ( _ ) may be used, with some restrictions Note that real-number literals always contain a . with digits on both sides, and integer literals never contain a . . Literals are case-sensitive. Unlike in C++, literals do not have a suffix to indicate their type.","title":"Overview"},{"location":"design/lexical_conventions/numeric_literals/#details","text":"","title":"Details"},{"location":"design/lexical_conventions/numeric_literals/#integer-literals","text":"Decimal integers are written as a non-zero decimal digit followed by zero or more additional decimal digits, or as a single 0 . Integers in other bases are written as a 0 followed by a base specifier character, followed by a sequence of digits in the corresponding base. The available base specifiers and corresponding bases are: Base specifier Base Digits b 2 0 and 1 x 16 0 ... 9 , A ... F The above table is case-sensitive. For example, 0b1 and 0x1A are valid, and 0B1 , 0X1A , and 0x1a are invalid. A zero at the start of a literal can never be followed by another digit: either the literal is 0 , the 0 begins a base specifier, or the next character is a decimal point (see below). No support is provided for octal literals, and any C or C++ octal literal (other than 0 ) is invalid in Carbon.","title":"Integer literals"},{"location":"design/lexical_conventions/numeric_literals/#real-number-literals","text":"Real numbers are written as a decimal or hexadecimal integer followed by a period ( . ) followed by a sequence of one or more decimal or hexadecimal digits, respectively. A digit is required on each side of the period. 0. and .3 are both invalid. A real number can be followed by an exponent character, an optional + or - (defaulting to + if absent), and a character sequence matching the grammar of a decimal integer with some value N . For a decimal real number, the exponent character is e , and the effect is to multiply the given value by 10 \u00b1 N . For a hexadecimal real number, the exponent character is p , and the effect is to multiply the given value by 2 \u00b1 N . The exponent suffix is optional for both decimal and hexadecimal real numbers. Note that a decimal integer followed by e is not a real-number literal. For example, 3e10 is not a valid literal. When a real-number literal is interpreted as a value of a real-number type, its value is the representable real number closest to the value of the literal. In the case of a tie, the nearest value whose mantissa is even is selected. The decimal real number syntax allows for any decimal fraction to be expressed -- that is, any number of the form a x 10 - b , where a is an integer and b is a non-negative integer. Because the decimal fractions are dense in the reals and the set of values of the real-number type is assumed to be discrete, every value of the real-number type can be expressed as a real number literal. However, for certain applications, directly expressing the intended real-number representation may be more convenient than producing a decimal equivalent that is known to convert to the intended value. Hexadecimal real-number literals are provided in order to permit values of binary floating or fixed point real-number types to be expressed directly.","title":"Real-number literals"},{"location":"design/lexical_conventions/numeric_literals/#digit-separators","text":"If digit separators ( _ ) are included in literals, they must meet the respective condition: For decimal integers, the digit separators shall occur every three digits starting from the right. For example, 2_147_483_648 . For hexadecimal integers, the digit separators shall occur every four digits starting from the right. For example, 0x7FFF_FFFF . For real-number literals, digit separators can appear in the decimal and hexadecimal integer portions (prior to the period and after the optional e or mandatory p ) as described in the previous bullets. For example, 2_147.483648e12_345 or 0x1_00CA.FEF00Dp+24 For binary literals, digit separators can appear between any two digits. For example, 0b1_000_101_11 .","title":"Digit separators"},{"location":"design/lexical_conventions/numeric_literals/#divergence-from-other-languages","text":"The design provides a syntax that is deliberately close to that used both by C++ and many other languages, so it should feel familiar to developers. However, it selects a reasonably minimal subset of the syntaxes. This minimal approach provides benefits directly in line with the goal that Carbon code should be easy to read, understand, and write : Reduces unnecessary choices for programmers. Simplifies the syntax rules of the language. Improves consistency of written Carbon code. That said, it still provides sufficient variations to address important use cases for the goal of not leaving room for a lower level language: Hexadecimal and binary integer literals. Scientific notation floating point literals. Hexadecimal (scientific) floating point literals.","title":"Divergence from other languages"},{"location":"design/lexical_conventions/numeric_literals/#alternatives-considered","text":"Integer bases Octal literals Decimal literals Case sensitivity Real number syntax Disallow ties Digit separator syntax","title":"Alternatives considered"},{"location":"design/lexical_conventions/numeric_literals/#references","text":"Proposal #143: Numeric literals Proposal #866: Allow ties in floating literals","title":"References"},{"location":"design/lexical_conventions/string_literals/","text":"String literals Table of contents Overview Details Simple and block string literals Escape sequences Raw string literals Encoding Alternatives considered References Overview Carbon supports both simple literals that are single-line using one double quotation mark ( \" ) and block literals that are multi-line using three double quotation marks ( \"\"\" ). A block string literal may have a file type indicator after the first \"\"\" ; this does not affect the string itself, but may assist other tooling. For example: // Simple string literal: var simple: String = \"example\"; // Block string literal: var block: String = \"\"\" The winds grow high; so do your stomachs, lords. How irksome is this music to my heart! When such strings jar, what hope of harmony? I pray, my lords, let me compound this strife. -- History of Henry VI, Part II, Act II, Scene 1, W. Shakespeare \"\"\"; // Block string literal with file type indicator: var code_block: String = \"\"\"cpp #include <iostream> int main() { std::cout << \"Hello world!\"; return 0; } \"\"\" The indentation of a block string literal's terminating line is removed from all preceding lines. As a consequence, in the above code_block example, only std::cout and return are indented in the resulting string, and by 4 spaces each. Escape sequences introduced by a backslash ( \\ ) and are used to express special character or code unit sequences, such as \\n for a newline character. Raw string literals are additionally delimited with one or more # ; these require an equal number of hash symbols ( # ) after the \\ to indicate an escape sequence. Raw string literals are used to more easily write literal \\ s in strings. Both simple and block string literals have raw forms. For example: // Raw simple string literal with newline escape sequence: var newline: String = \"line one\\nline two\"; // Raw simple string literal with literal `\\n`, not a newline: var raw: String = #\"line one\\nstill line one\"#; // Raw simple string literal with newline escape sequence: var raw_newline: String = #\"line one\\#nline two\"#; Details Simple and block string literals A simple string literal is formed of a sequence of: Characters other than \\ and \" . Only space characters (U+0020) are valid whitespace in a string literal. Other horizontal whitespace , including tabs, are disallowed but parse as part of the string for error recovery purposes. Vertical whitespace will not parse as part of a simple string literal. Escape sequences . Each escape sequence is replaced with the corresponding character sequence or code unit sequence. Similarly to invalid whitespace, invalid escape sequences such as \\z parse as part of the string. This sequence is enclosed in \" s. For example, this is a simple string literal: var String: lucius = \"The strings, my lord, are false.\"; A block string literal starts with \"\"\" , followed by an optional file type indicator, followed by a newline, and ends at the next instance of three double quotation marks whose first \" is not part of a \\\" escape sequence. The closing \"\"\" shall be the first non-whitespace characters on that line. The lines between the opening line and the closing line (exclusive) are content lines . The content lines shall not contain \\ characters that do not form part of an escape sequence. The indentation of a block string literal is the sequence of horizontal whitespace preceding the closing \"\"\" . Each non-empty content line shall begin with the indentation of the string literal. The content of the literal is formed as follows: The indentation of the closing line is removed from each non-empty content line. All trailing whitespace on each line, including the line terminator, is replaced with a single line feed (U+000A) character. The resulting lines are concatenated. Each escape sequence is replaced with the corresponding character sequence or code unit sequence. A content line is considered empty if it contains only whitespace characters. var String: w = \"\"\" This is a string literal. Its first character is 'T' and its last character is a newline character. It contains another newline between 'is' and 'a'. \"\"\"; // This string literal is invalid because the \"\"\" after 'closing' terminates // the literal, but is not at the start of the line. var String: invalid = \"\"\" error: closing \"\"\" is not on its own line. \"\"\"; A file type indicator is any sequence of non-whitespace characters other than \" or # . The file type indicator has no semantic meaning to the Carbon compiler, but some file type indicators are understood by the language tooling (for example, syntax highlighter, code formatter) as indicating the structure of the string literal's content. // This is a block string literal. Its first two characters are spaces, and its // last character is a line feed. It has a file type of 'c++'. var String: starts_with_whitespace = \"\"\"c++ int x = 1; // This line starts with two spaces. int y = 2; // This line starts with two spaces. \"\"\"; The file type indicator might contain semantic information beyond the file type itself, such as instructions to the code formatter to disable formatting for the code block. Open question: There is no concrete set of recognized file type indicators. It would be useful to informally specify a set of well-known indicators, so that tools have a common understanding of what those indicators mean, perhaps in a best practices guide. Escape sequences Within a string literal, the following escape sequences are recognized: Escape Meaning \\t U+0009 CHARACTER TABULATION \\n U+000A LINE FEED \\r U+000D CARRIAGE RETURN \\\" U+0022 QUOTATION MARK ( \" ) \\' U+0027 APOSTROPHE ( ' ) \\\\ U+005C REVERSE SOLIDUS ( \\ ) \\0 Code unit with value 0 \\0D Invalid, reserved for evolution \\xHH Code unit with value HH 16 \\u{HHHH...} Unicode code point U+HHHH... \\<newline> No string literal content produced (block literals only) Hex characters ( H ) must be uppercase ( \\xAA , not \\xaa ). This includes all C++ escape sequences except: \\? , which was historically used to escape trigraphs in string literals, and no longer serves any purpose. \\ooo octal escapes, which are removed because Carbon does not support octal literals; \\0 is retained as a special case, which is expected to be important for C interoperability. \\uABCD , which is replaced by \\u{ABCD} . \\U0010FFFF , which is replaced by \\u{10FFFF} . \\a (bell), \\b (backspace), \\v (vertical tab), and \\f (form feed). \\a and \\b are obsolescent, and \\f and \\v are largely obsolete. These characters can be expressed with \\x07 , \\x08 , \\x0B , and \\x0C respectively if needed. Note that this is the same set of escape sequences supported by Swift and Rust , except that, unlike in Swift, support for \\xHH is provided. While octal escape sequences are expected to remain not permitted (even though \\0D is reserved), the decision to not support \\1 .. \\7 or more generally \\DDDD is experimental . In the above table, H represents an arbitrary hexadecimal character, 0 - 9 or A - F (case-sensitive). Unlike in C++, but like in Python, \\x expects exactly two hexadecimal digits. As in JavaScript, Rust, and Swift, Unicode code points can be expressed by number using \\u{10FFFF} notation, which accepts any number of hexadecimal characters. Any numeric code point in the ranges 0 16 -D7FF 16 or E000 16 -10FFFF 16 can be expressed this way. Open question: Some programming languages (notably Python) support a \\N{unicode character name} syntax. We could add such an escape sequence. Future proposals considering adding such support should pay attention to work done by C++'s Unicode study group in this area. The escape sequence \\0 shall not be followed by a decimal digit. In cases where a null byte should be followed by a decimal digit, \\x00 can be used instead: \"foo\\x00123\" . The intent is to preserve the possibility of permitting decimal escape sequences in the future. A backslash followed by a line feed character is an escape sequence that produces no string contents. This escape sequence is experimental , and can only appear in block string literals. This escape sequence is processed after trailing whitespace is replaced by a line feed character, so a \\ followed by horizontal whitespace followed by a line terminator removes the whitespace up to and including the line terminator. Unlike in Rust, but like in Swift, leading whitespace on the line after an escaped newline is not removed, other than whitespace that matches the indentation of the terminating \"\"\" . A character sequence starting with a backslash that doesn't match any known escape sequence is invalid. Whitespace characters other than space and, for block string literals, new line optionally preceded by carriage return are disallowed. All other characters (including non-printable characters) are preserved verbatim. Because all Carbon source files are required to be valid sequences of Unicode characters, code unit sequences that are not valid UTF-8 can only be produced by \\x escape sequences. The decision to disallow raw tab characters in string literals is experimental . var String: fret = \"I would 'twere something that would fret the string,\\n\" + \"The master-cord on's \\u{2764}\\u{FE0F}!\"; // This string contains two characters (prior to encoding in UTF-8): // U+1F3F9 (BOW AND ARROW) followed by U+0032 (DIGIT TWO) var String: password = \"\\u{1F3F9}2\"; // This string contains no newline characters. var String: type_mismatch = \"\"\" Shall I compare thee to a summer's day? Thou art \\ more lovely and more temperate.\\ \"\"\"; var String: trailing_whitespace = \"\"\" This line ends in a space followed by a newline. \\n\\ This line starts with four spaces. \"\"\"; Raw string literals In order to allow strings whose contents include \\ s and \" s, the delimiters of string literals can be customized by prefixing the opening delimiter with N # characters. A closing delimiter for such a string is only recognized if it is followed by N # characters, and similarly, escape sequences in such string literals are recognized only if the \\ is also followed by N # characters. A \\ , \" , or \"\"\" not followed by N # characters has no special meaning. Opening delimiter Escape sequence introducer Closing delimiter \" / \"\"\" \\ (for example, \\n ) \" / \"\"\" #\" / #\"\"\" \\# (for example, \\#n ) \"# / \"\"\"# ##\" / ##\"\"\" \\## (for example, \\##n ) \"## / \"\"\"## ###\" / ###\"\"\" \\### (for example, \\###n ) \"### / \"\"\"### ... ... ... For example: var String: x = #\"\"\" This is the content of the string. The 'T' is the first character of the string. \"\"\" <-- This is not the end of the string. \"\"\"#; // But the preceding line does end the string. // OK, final character is \\ var String: y = #\"Hello\\\"#; var String: z = ##\"Raw strings #\"nesting\"#\"##; var String: w = #\"Tab is expressed as \\t. Example: '\\#t'\"#; Note that both a raw simple string literal and a raw block string literal can begin with #\"\"\" . These cases can be distinguished by the presence or absence of additional \" s later in the same line: In a raw simple string literal, there must be a \" and one or more # s later in the same line terminating the string. In a raw block string literal, the rest of the line is a file type indicator, which can contain neither \" nor # . // This string is a single-line raw string literal. // The contents of this string start and end with exactly two \"s. var String: ambig1 = #\"\"\"This is a raw string literal starting with \"\"\"#; // This string is a raw block string literal with file-type 'This', whose // contents start with \"is a \". var String: ambig2 = #\"\"\"This is a block string literal with file type 'This', first character 'i', and last character 'X': X\\# \"\"\"#; // This is a single-line raw string literal, equivalent to \"\\\"\". var String: ambig3 = #\"\"\"#; Encoding A string literal results in a sequence of 8-bit bytes. Like Carbon source files, string literals are encoded in UTF-8. There is no guarantee that the string is valid UTF-8, however, because arbitrary byte sequences can be inserted by way of \\xHH escape sequences. This is experimental , and should be revisited if we find sufficient motivation for directly expressing string literals in other encodings. Similarly, as library support for a string type evolves, we should consider including string literal syntax (perhaps as the default) that guarantees the string content is a valid UTF-8 encoding, so that valid UTF-8 can be distinguished from an arbitrary string in the type system. In such string literals, we should consider rejecting \\xHH escapes in which HH is greater than 7F 16 , as in Rust. Alternatives considered Block string literals Leading whitespace removal Terminating newline Escape sequences Raw string literals Trailing whitespace Line separators Internal whitespace References Proposal #199: String literals","title":"String literals"},{"location":"design/lexical_conventions/string_literals/#string-literals","text":"","title":"String literals"},{"location":"design/lexical_conventions/string_literals/#table-of-contents","text":"Overview Details Simple and block string literals Escape sequences Raw string literals Encoding Alternatives considered References","title":"Table of contents"},{"location":"design/lexical_conventions/string_literals/#overview","text":"Carbon supports both simple literals that are single-line using one double quotation mark ( \" ) and block literals that are multi-line using three double quotation marks ( \"\"\" ). A block string literal may have a file type indicator after the first \"\"\" ; this does not affect the string itself, but may assist other tooling. For example: // Simple string literal: var simple: String = \"example\"; // Block string literal: var block: String = \"\"\" The winds grow high; so do your stomachs, lords. How irksome is this music to my heart! When such strings jar, what hope of harmony? I pray, my lords, let me compound this strife. -- History of Henry VI, Part II, Act II, Scene 1, W. Shakespeare \"\"\"; // Block string literal with file type indicator: var code_block: String = \"\"\"cpp #include <iostream> int main() { std::cout << \"Hello world!\"; return 0; } \"\"\" The indentation of a block string literal's terminating line is removed from all preceding lines. As a consequence, in the above code_block example, only std::cout and return are indented in the resulting string, and by 4 spaces each. Escape sequences introduced by a backslash ( \\ ) and are used to express special character or code unit sequences, such as \\n for a newline character. Raw string literals are additionally delimited with one or more # ; these require an equal number of hash symbols ( # ) after the \\ to indicate an escape sequence. Raw string literals are used to more easily write literal \\ s in strings. Both simple and block string literals have raw forms. For example: // Raw simple string literal with newline escape sequence: var newline: String = \"line one\\nline two\"; // Raw simple string literal with literal `\\n`, not a newline: var raw: String = #\"line one\\nstill line one\"#; // Raw simple string literal with newline escape sequence: var raw_newline: String = #\"line one\\#nline two\"#;","title":"Overview"},{"location":"design/lexical_conventions/string_literals/#details","text":"","title":"Details"},{"location":"design/lexical_conventions/string_literals/#simple-and-block-string-literals","text":"A simple string literal is formed of a sequence of: Characters other than \\ and \" . Only space characters (U+0020) are valid whitespace in a string literal. Other horizontal whitespace , including tabs, are disallowed but parse as part of the string for error recovery purposes. Vertical whitespace will not parse as part of a simple string literal. Escape sequences . Each escape sequence is replaced with the corresponding character sequence or code unit sequence. Similarly to invalid whitespace, invalid escape sequences such as \\z parse as part of the string. This sequence is enclosed in \" s. For example, this is a simple string literal: var String: lucius = \"The strings, my lord, are false.\"; A block string literal starts with \"\"\" , followed by an optional file type indicator, followed by a newline, and ends at the next instance of three double quotation marks whose first \" is not part of a \\\" escape sequence. The closing \"\"\" shall be the first non-whitespace characters on that line. The lines between the opening line and the closing line (exclusive) are content lines . The content lines shall not contain \\ characters that do not form part of an escape sequence. The indentation of a block string literal is the sequence of horizontal whitespace preceding the closing \"\"\" . Each non-empty content line shall begin with the indentation of the string literal. The content of the literal is formed as follows: The indentation of the closing line is removed from each non-empty content line. All trailing whitespace on each line, including the line terminator, is replaced with a single line feed (U+000A) character. The resulting lines are concatenated. Each escape sequence is replaced with the corresponding character sequence or code unit sequence. A content line is considered empty if it contains only whitespace characters. var String: w = \"\"\" This is a string literal. Its first character is 'T' and its last character is a newline character. It contains another newline between 'is' and 'a'. \"\"\"; // This string literal is invalid because the \"\"\" after 'closing' terminates // the literal, but is not at the start of the line. var String: invalid = \"\"\" error: closing \"\"\" is not on its own line. \"\"\"; A file type indicator is any sequence of non-whitespace characters other than \" or # . The file type indicator has no semantic meaning to the Carbon compiler, but some file type indicators are understood by the language tooling (for example, syntax highlighter, code formatter) as indicating the structure of the string literal's content. // This is a block string literal. Its first two characters are spaces, and its // last character is a line feed. It has a file type of 'c++'. var String: starts_with_whitespace = \"\"\"c++ int x = 1; // This line starts with two spaces. int y = 2; // This line starts with two spaces. \"\"\"; The file type indicator might contain semantic information beyond the file type itself, such as instructions to the code formatter to disable formatting for the code block. Open question: There is no concrete set of recognized file type indicators. It would be useful to informally specify a set of well-known indicators, so that tools have a common understanding of what those indicators mean, perhaps in a best practices guide.","title":"Simple and block string literals"},{"location":"design/lexical_conventions/string_literals/#escape-sequences","text":"Within a string literal, the following escape sequences are recognized: Escape Meaning \\t U+0009 CHARACTER TABULATION \\n U+000A LINE FEED \\r U+000D CARRIAGE RETURN \\\" U+0022 QUOTATION MARK ( \" ) \\' U+0027 APOSTROPHE ( ' ) \\\\ U+005C REVERSE SOLIDUS ( \\ ) \\0 Code unit with value 0 \\0D Invalid, reserved for evolution \\xHH Code unit with value HH 16 \\u{HHHH...} Unicode code point U+HHHH... \\<newline> No string literal content produced (block literals only) Hex characters ( H ) must be uppercase ( \\xAA , not \\xaa ). This includes all C++ escape sequences except: \\? , which was historically used to escape trigraphs in string literals, and no longer serves any purpose. \\ooo octal escapes, which are removed because Carbon does not support octal literals; \\0 is retained as a special case, which is expected to be important for C interoperability. \\uABCD , which is replaced by \\u{ABCD} . \\U0010FFFF , which is replaced by \\u{10FFFF} . \\a (bell), \\b (backspace), \\v (vertical tab), and \\f (form feed). \\a and \\b are obsolescent, and \\f and \\v are largely obsolete. These characters can be expressed with \\x07 , \\x08 , \\x0B , and \\x0C respectively if needed. Note that this is the same set of escape sequences supported by Swift and Rust , except that, unlike in Swift, support for \\xHH is provided. While octal escape sequences are expected to remain not permitted (even though \\0D is reserved), the decision to not support \\1 .. \\7 or more generally \\DDDD is experimental . In the above table, H represents an arbitrary hexadecimal character, 0 - 9 or A - F (case-sensitive). Unlike in C++, but like in Python, \\x expects exactly two hexadecimal digits. As in JavaScript, Rust, and Swift, Unicode code points can be expressed by number using \\u{10FFFF} notation, which accepts any number of hexadecimal characters. Any numeric code point in the ranges 0 16 -D7FF 16 or E000 16 -10FFFF 16 can be expressed this way. Open question: Some programming languages (notably Python) support a \\N{unicode character name} syntax. We could add such an escape sequence. Future proposals considering adding such support should pay attention to work done by C++'s Unicode study group in this area. The escape sequence \\0 shall not be followed by a decimal digit. In cases where a null byte should be followed by a decimal digit, \\x00 can be used instead: \"foo\\x00123\" . The intent is to preserve the possibility of permitting decimal escape sequences in the future. A backslash followed by a line feed character is an escape sequence that produces no string contents. This escape sequence is experimental , and can only appear in block string literals. This escape sequence is processed after trailing whitespace is replaced by a line feed character, so a \\ followed by horizontal whitespace followed by a line terminator removes the whitespace up to and including the line terminator. Unlike in Rust, but like in Swift, leading whitespace on the line after an escaped newline is not removed, other than whitespace that matches the indentation of the terminating \"\"\" . A character sequence starting with a backslash that doesn't match any known escape sequence is invalid. Whitespace characters other than space and, for block string literals, new line optionally preceded by carriage return are disallowed. All other characters (including non-printable characters) are preserved verbatim. Because all Carbon source files are required to be valid sequences of Unicode characters, code unit sequences that are not valid UTF-8 can only be produced by \\x escape sequences. The decision to disallow raw tab characters in string literals is experimental . var String: fret = \"I would 'twere something that would fret the string,\\n\" + \"The master-cord on's \\u{2764}\\u{FE0F}!\"; // This string contains two characters (prior to encoding in UTF-8): // U+1F3F9 (BOW AND ARROW) followed by U+0032 (DIGIT TWO) var String: password = \"\\u{1F3F9}2\"; // This string contains no newline characters. var String: type_mismatch = \"\"\" Shall I compare thee to a summer's day? Thou art \\ more lovely and more temperate.\\ \"\"\"; var String: trailing_whitespace = \"\"\" This line ends in a space followed by a newline. \\n\\ This line starts with four spaces. \"\"\";","title":"Escape sequences"},{"location":"design/lexical_conventions/string_literals/#raw-string-literals","text":"In order to allow strings whose contents include \\ s and \" s, the delimiters of string literals can be customized by prefixing the opening delimiter with N # characters. A closing delimiter for such a string is only recognized if it is followed by N # characters, and similarly, escape sequences in such string literals are recognized only if the \\ is also followed by N # characters. A \\ , \" , or \"\"\" not followed by N # characters has no special meaning. Opening delimiter Escape sequence introducer Closing delimiter \" / \"\"\" \\ (for example, \\n ) \" / \"\"\" #\" / #\"\"\" \\# (for example, \\#n ) \"# / \"\"\"# ##\" / ##\"\"\" \\## (for example, \\##n ) \"## / \"\"\"## ###\" / ###\"\"\" \\### (for example, \\###n ) \"### / \"\"\"### ... ... ... For example: var String: x = #\"\"\" This is the content of the string. The 'T' is the first character of the string. \"\"\" <-- This is not the end of the string. \"\"\"#; // But the preceding line does end the string. // OK, final character is \\ var String: y = #\"Hello\\\"#; var String: z = ##\"Raw strings #\"nesting\"#\"##; var String: w = #\"Tab is expressed as \\t. Example: '\\#t'\"#; Note that both a raw simple string literal and a raw block string literal can begin with #\"\"\" . These cases can be distinguished by the presence or absence of additional \" s later in the same line: In a raw simple string literal, there must be a \" and one or more # s later in the same line terminating the string. In a raw block string literal, the rest of the line is a file type indicator, which can contain neither \" nor # . // This string is a single-line raw string literal. // The contents of this string start and end with exactly two \"s. var String: ambig1 = #\"\"\"This is a raw string literal starting with \"\"\"#; // This string is a raw block string literal with file-type 'This', whose // contents start with \"is a \". var String: ambig2 = #\"\"\"This is a block string literal with file type 'This', first character 'i', and last character 'X': X\\# \"\"\"#; // This is a single-line raw string literal, equivalent to \"\\\"\". var String: ambig3 = #\"\"\"#;","title":"Raw string literals"},{"location":"design/lexical_conventions/string_literals/#encoding","text":"A string literal results in a sequence of 8-bit bytes. Like Carbon source files, string literals are encoded in UTF-8. There is no guarantee that the string is valid UTF-8, however, because arbitrary byte sequences can be inserted by way of \\xHH escape sequences. This is experimental , and should be revisited if we find sufficient motivation for directly expressing string literals in other encodings. Similarly, as library support for a string type evolves, we should consider including string literal syntax (perhaps as the default) that guarantees the string content is a valid UTF-8 encoding, so that valid UTF-8 can be distinguished from an arbitrary string in the type system. In such string literals, we should consider rejecting \\xHH escapes in which HH is greater than 7F 16 , as in Rust.","title":"Encoding"},{"location":"design/lexical_conventions/string_literals/#alternatives-considered","text":"Block string literals Leading whitespace removal Terminating newline Escape sequences Raw string literals Trailing whitespace Line separators Internal whitespace","title":"Alternatives considered"},{"location":"design/lexical_conventions/string_literals/#references","text":"Proposal #199: String literals","title":"References"},{"location":"design/lexical_conventions/whitespace/","text":"Whitespace Table of contents Overview References Overview The exact lexical form of Carbon whitespace has not yet been settled. However, Carbon will follow lexical conventions for whitespace based on Unicode Annex #31 . TODO: Update this once the precise rules are decided; see the Unicode source files proposal. Unicode Annex #31 suggests selecting whitespace characters based on the characters with Unicode property Pattern_White_Space , which is currently these 11 characters: Horizontal whitespace: U+0009 CHARACTER TABULATION (horizontal tab) U+0020 SPACE U+200E LEFT-TO-RIGHT MARK U+200F RIGHT-TO-LEFT MARK Vertical whitespace: U+000A LINE FEED (traditional newline) U+000B LINE TABULATION (vertical tab) U+000C FORM FEED (page break) U+000D CARRIAGE RETURN U+0085 NEXT LINE (Unicode newline) U+2028 LINE SEPARATOR U+2029 PARAGRAPH SEPARATOR The quantity and kind of whitespace separating tokens is ignored except where otherwise specified. References Proposal #142: Unicode source files","title":"Whitespace"},{"location":"design/lexical_conventions/whitespace/#whitespace","text":"","title":"Whitespace"},{"location":"design/lexical_conventions/whitespace/#table-of-contents","text":"Overview References","title":"Table of contents"},{"location":"design/lexical_conventions/whitespace/#overview","text":"The exact lexical form of Carbon whitespace has not yet been settled. However, Carbon will follow lexical conventions for whitespace based on Unicode Annex #31 . TODO: Update this once the precise rules are decided; see the Unicode source files proposal. Unicode Annex #31 suggests selecting whitespace characters based on the characters with Unicode property Pattern_White_Space , which is currently these 11 characters: Horizontal whitespace: U+0009 CHARACTER TABULATION (horizontal tab) U+0020 SPACE U+200E LEFT-TO-RIGHT MARK U+200F RIGHT-TO-LEFT MARK Vertical whitespace: U+000A LINE FEED (traditional newline) U+000B LINE TABULATION (vertical tab) U+000C FORM FEED (page break) U+000D CARRIAGE RETURN U+0085 NEXT LINE (Unicode newline) U+2028 LINE SEPARATOR U+2029 PARAGRAPH SEPARATOR The quantity and kind of whitespace separating tokens is ignored except where otherwise specified.","title":"Overview"},{"location":"design/lexical_conventions/whitespace/#references","text":"Proposal #142: Unicode source files","title":"References"},{"location":"design/lexical_conventions/words/","text":"Words Table of contents Overview Keywords Alternatives considered References Overview A word is a lexical element formed from a sequence of letters or letter-like characters, such as fn or Foo or Int . The exact lexical form of words has not yet been settled. However, Carbon will follow lexical conventions for identifiers based on Unicode Annex #31 . TODO: Update this once the precise rules are decided; see the Unicode source files proposal. Keywords The following words are interpreted as keywords: abstract addr alias and api as auto base break case class constraint continue default else extends external final fn for forall friend if impl import in interface is let library like match namespace not observe or override package partial private protected return returned then var virtual where while Alternatives considered Character encoding: We could restrict words to ASCII. References Proposal #142: Unicode source files","title":"Words"},{"location":"design/lexical_conventions/words/#words","text":"","title":"Words"},{"location":"design/lexical_conventions/words/#table-of-contents","text":"Overview Keywords Alternatives considered References","title":"Table of contents"},{"location":"design/lexical_conventions/words/#overview","text":"A word is a lexical element formed from a sequence of letters or letter-like characters, such as fn or Foo or Int . The exact lexical form of words has not yet been settled. However, Carbon will follow lexical conventions for identifiers based on Unicode Annex #31 . TODO: Update this once the precise rules are decided; see the Unicode source files proposal.","title":"Overview"},{"location":"design/lexical_conventions/words/#keywords","text":"The following words are interpreted as keywords: abstract addr alias and api as auto base break case class constraint continue default else extends external final fn for forall friend if impl import in interface is let library like match namespace not observe or override package partial private protected return returned then var virtual where while","title":"Keywords"},{"location":"design/lexical_conventions/words/#alternatives-considered","text":"Character encoding: We could restrict words to ASCII.","title":"Alternatives considered"},{"location":"design/lexical_conventions/words/#references","text":"Proposal #142: Unicode source files","title":"References"},{"location":"guides/","text":"Guides This directory contains end-user documentation on how to use Carbon, focused on people trying to use and write code in Carbon. Glossary","title":"Guides"},{"location":"guides/#guides","text":"This directory contains end-user documentation on how to use Carbon, focused on people trying to use and write code in Carbon. Glossary","title":"Guides"},{"location":"guides/glossary/","text":"Glossary entity An entity is a named item with an associated name path, such as a function, type, interface, or namespace. For example, in fn GetTime() , GetTime refers to an entity which is a function. identifier An identifier is the token which names an entity, and is also used in code to refer to the entity. For example, in fn GetTime() , GetTime is the identifier for the function. library A library is a group of files that form an importable API and its implementation. Carbon encourages small libraries, bundled into larger packages. For example, given package Geometry library Shapes; , Shapes is a library in the Geometry package. name path A name path is the dot-separated identifier list that indicates a relative or full path of a name. For example, given fn GetArea(var Geometry.Circle: x) , Geometry.Circle is a name path. GetArea is also a name path, albeit with only one identifier needed. namespace A namespace is a entity that contains entities, and may be nested. For example, given a name path of Geometry.Circle , Geometry is a namespace containing Circle . package A package is a group of libraries in Carbon, and is the standard unit for distribution. The package name also serves as the root namespace for all name paths in its libraries. The package name should be a single, globally-unique identifier. For example, given package Geometry; in a file, Geometry is the package and root namespace.","title":"Glossary"},{"location":"guides/glossary/#glossary","text":"","title":"Glossary"},{"location":"guides/glossary/#entity","text":"An entity is a named item with an associated name path, such as a function, type, interface, or namespace. For example, in fn GetTime() , GetTime refers to an entity which is a function.","title":"entity"},{"location":"guides/glossary/#identifier","text":"An identifier is the token which names an entity, and is also used in code to refer to the entity. For example, in fn GetTime() , GetTime is the identifier for the function.","title":"identifier"},{"location":"guides/glossary/#library","text":"A library is a group of files that form an importable API and its implementation. Carbon encourages small libraries, bundled into larger packages. For example, given package Geometry library Shapes; , Shapes is a library in the Geometry package.","title":"library"},{"location":"guides/glossary/#name-path","text":"A name path is the dot-separated identifier list that indicates a relative or full path of a name. For example, given fn GetArea(var Geometry.Circle: x) , Geometry.Circle is a name path. GetArea is also a name path, albeit with only one identifier needed.","title":"name path"},{"location":"guides/glossary/#namespace","text":"A namespace is a entity that contains entities, and may be nested. For example, given a name path of Geometry.Circle , Geometry is a namespace containing Circle .","title":"namespace"},{"location":"guides/glossary/#package","text":"A package is a group of libraries in Carbon, and is the standard unit for distribution. The package name also serves as the root namespace for all name paths in its libraries. The package name should be a single, globally-unique identifier. For example, given package Geometry; in a file, Geometry is the package and root namespace.","title":"package"},{"location":"images/snippets/","text":"Front page snippets for Carbon Images Images are managed in Google Drive . Quicksort A sample of quicksort in Carbon. package Sorting api; fn Partition[T:! Comparable & Movable](s: Slice(T)) -> i64 { var i: i64 = -1; for (e: T in s) { if (e <= s.Last()) { ++i; Swap(&s[i], &e); } } return i; } fn QuickSort[T:! Comparable & Movable](s: Slice(T)) { if (s.Size() <= 1) { return; } let p: i64 = Partition(s); QuickSort(s[:p - 1]); QuickSort(s[p + 1:]); } Carbon and C++ C++ // C++: #include <math.h> #include <iostream> #include <span> #include <vector> struct Circle { float r; }; void PrintTotalArea(std::span<Circle> circles) { float area = 0; for (const Circle& c : circles) { area += M_PI * c.r * c.r; } std::cout << \"Total area: \" << area << \"\\n\"; } auto main(int argc, char** argv) -> int { std::vector<Circle> circles = {{1.0}, {2.0}}; // Implicitly constructors `span` from `vector`. PrintTotalArea(circles); return 0; } Carbon // Carbon: package Geometry api; import Math; class Circle { var r: f32; } fn PrintTotalArea(circles: Slice(Circle)) { var area: f32 = 0; for (c: Circle in circles) { area += Math.Pi * c.r * c.r; } Print(\"Total area: {0}\", area); } fn Main() -> i32 { // A dynamically sized array, like `std::vector`. var circles: Array(Circle) = ({.r = 1.0}, {.r = 2.0}); // Implicitly constructs `Slice` from `Array`. PrintTotalArea(circles); return 0; } Mixed // C++ code used in both Carbon and C++: struct Circle { float r; }; // Carbon exposing a function for C++: package Geometry api; import Cpp library \"circle.h\"; import Math; fn PrintTotalArea(circles: Slice(Cpp.Circle)) { var area: f32 = 0; for (c: Cpp.Circle in circles) { area += Math.Pi * c.r * c.r; } Print(\"Total area: {0}\", area); } // C++ calling Carbon: #include <vector> #include \"circle.h\" #include \"geometry.carbon.h\" auto main(int argc, char** argv) -> int { std::vector<Circle> circles = {{1.0}, {2.0}}; // Carbon's `Slice` supports implicit construction from `std::vector`, // similar to `std::span`. Geometry::PrintTotalArea(circles); return 0; }","title":"Front page snippets for Carbon"},{"location":"images/snippets/#front-page-snippets-for-carbon","text":"","title":"Front page snippets for Carbon"},{"location":"images/snippets/#images","text":"Images are managed in Google Drive .","title":"Images"},{"location":"images/snippets/#quicksort","text":"A sample of quicksort in Carbon. package Sorting api; fn Partition[T:! Comparable & Movable](s: Slice(T)) -> i64 { var i: i64 = -1; for (e: T in s) { if (e <= s.Last()) { ++i; Swap(&s[i], &e); } } return i; } fn QuickSort[T:! Comparable & Movable](s: Slice(T)) { if (s.Size() <= 1) { return; } let p: i64 = Partition(s); QuickSort(s[:p - 1]); QuickSort(s[p + 1:]); }","title":"Quicksort"},{"location":"images/snippets/#carbon-and-c","text":"","title":"Carbon and C++"},{"location":"images/snippets/#c","text":"// C++: #include <math.h> #include <iostream> #include <span> #include <vector> struct Circle { float r; }; void PrintTotalArea(std::span<Circle> circles) { float area = 0; for (const Circle& c : circles) { area += M_PI * c.r * c.r; } std::cout << \"Total area: \" << area << \"\\n\"; } auto main(int argc, char** argv) -> int { std::vector<Circle> circles = {{1.0}, {2.0}}; // Implicitly constructors `span` from `vector`. PrintTotalArea(circles); return 0; }","title":"C++"},{"location":"images/snippets/#carbon","text":"// Carbon: package Geometry api; import Math; class Circle { var r: f32; } fn PrintTotalArea(circles: Slice(Circle)) { var area: f32 = 0; for (c: Circle in circles) { area += Math.Pi * c.r * c.r; } Print(\"Total area: {0}\", area); } fn Main() -> i32 { // A dynamically sized array, like `std::vector`. var circles: Array(Circle) = ({.r = 1.0}, {.r = 2.0}); // Implicitly constructs `Slice` from `Array`. PrintTotalArea(circles); return 0; }","title":"Carbon"},{"location":"images/snippets/#mixed","text":"// C++ code used in both Carbon and C++: struct Circle { float r; }; // Carbon exposing a function for C++: package Geometry api; import Cpp library \"circle.h\"; import Math; fn PrintTotalArea(circles: Slice(Cpp.Circle)) { var area: f32 = 0; for (c: Cpp.Circle in circles) { area += Math.Pi * c.r * c.r; } Print(\"Total area: {0}\", area); } // C++ calling Carbon: #include <vector> #include \"circle.h\" #include \"geometry.carbon.h\" auto main(int argc, char** argv) -> int { std::vector<Circle> circles = {{1.0}, {2.0}}; // Carbon's `Slice` supports implicit construction from `std::vector`, // similar to `std::span`. Geometry::PrintTotalArea(circles); return 0; }","title":"Mixed"},{"location":"project/","text":"Project This directory contains project-related documentation for Carbon. Information about how the project works, goals, and community information belong here. Goals , and principles derived from those goals Roadmap and the process for updating it Carbon's process for evolution and governance Groups used for contacting key contributors and determining access Contributing to Carbon: Tools used when contributing to Carbon Style guides for language design and C++ code How Carbon does code review Trunk-based pull-request GitHub workflow used by Carbon","title":"Project"},{"location":"project/#project","text":"This directory contains project-related documentation for Carbon. Information about how the project works, goals, and community information belong here. Goals , and principles derived from those goals Roadmap and the process for updating it Carbon's process for evolution and governance Groups used for contacting key contributors and determining access Contributing to Carbon: Tools used when contributing to Carbon Style guides for language design and C++ code How Carbon does code review Trunk-based pull-request GitHub workflow used by Carbon","title":"Project"},{"location":"project/code_review/","text":"Code review Table of contents High level goals of code review What requires review? Who should review? GitHub pull request mechanics Code author guide Write good change descriptions First line Body Make small changes Responding to review comments Responding to questions or confusion Understand the feedback in the comments Code reviewer guide How quickly should you respond to a review request? What should be covered by a review? Writing review comments Approving the change Merging pull requests Merge commit descriptions Resolving an impasse or conflict Escalation High level goals of code review Code review serves several goals in the Carbon project. It directly improves the correctness, clarity, and consistency of contributions, including both code and documentation. These improvements range from the high-level functionality down through the design and implementation details. It also promotes team ownership and spreads knowledge across the team. More detailed discussions can be found in chapter 9 \"Code Review\" of the book Software Engineering at Google and chapter 21 \"Collaborative Construction\" in Code Complete: A Practical Handbook of Software Construction . However, these details aren't essential to understanding code review and how it works in the Carbon project. All of the important details are provided in the project documentation. What requires review? Every change to Carbon's repositories requires code review. Even formal evolution decisions which have been approved should have their specific changes to the repository reviewed. Many changes to Carbon repositories may only require code review. Typically, these include bug fixes, and development or documentation improvements clearly in line with accepted designs. It may in some rare cases extend to exploring experimental or prototype directions whose design is under active consideration. The term \"code review\" in the Carbon project is not only about \"code\". We expect changes to any files to be reviewed, including documentation and any other material stored in the repository. Who should review? Everyone should feel free to review Carbon changes. Even providing small or partial review can be a good way to start contributing to Carbon. Contributors with specific domain expertise or familiarity should also try to provide review on changes touching relevant parts of the project. Additionally, at least one developer with commit access must review each change. In Carbon, developers will focus on particular areas, loosely broken down as: Carbon leads : proposals and other important project documents, including the Main README , Code of Conduct , license , and goals . Implementation team : general changes. We split out auto-assignment by explorer , toolchain , and other files, including documentation . Auto-assignment will help find owners, but won't always be perfect -- developers may take a PR they weren't auto-assigned in order to help review go quickly. Contributors can also request multiple reviewers, but it can be daunting to get feedback from a large number of reviewers, so we suggest keeping the number of reviewers reasonably small. Any reviews that explicitly request changes should be addressed, either with the changes or an explanation of why not, before a pull request is merged. Further, any owners who have requested changes should explicitly confirm they're happy with the resolution before the change is merged. When a team gives an affirm decision on an evolution proposal , each team member should explicitly note any of their comments on the pull request that, while not blocking the decision , still need to be resolved as part of code review prior to it being merged. These might, for example, be trivial or minor wording tweaks or improvements. Otherwise, the decision is assumed to mean the prior review comments from members of that team are addressed; the author is free to merge once the pull request is approved, possibly with a code review separate from the proposal's review. GitHub pull request mechanics Carbon uses GitHub pull requests for code review, and we recommend some mechanical best practices to most effectively navigate them. Be aware that the main thread of pull request doesn't support threaded discussions or \"resolving\" a comment. If either of those would be useful, you'll probably want to comment on a file. You can quote comments in the main conversation thread in a reply by clicking the three-dot menu on the original comment and selecting \"Quote reply\". If you will want to comment on files, don't comment in the pull request conversation. Always go to the Files Changed tab. Make any in-file comments needed, but add them to a pending review rather than sending them directly. Finish the review and add any top-level review comments there. If you are an owner who will be providing approval for the change, then make sure to mark a review as requesting changes when you want the author to begin addressing your comment. Only use the \"comment\" review state if you are still in the process of reviewing and don't expect the author to begin working on further changes. If you are not an owner asked to approve, use the difference between a comment and requesting a change to help the author know whether to circle back with you before landing the pull request if the relevant owner(s) approve it. Don't reply to in-file comment threads in the conversation view, or with direct single reply comments. Add all replies to in-file comment threads using the Files Changed tab and by adding each reply to a new review, and posting them as a batch when done. You can get to the appropriate Files Changed tab by clicking on the change listed in the conversation view with the incoming set of in-file comments. This flow ensures an explicit update in the overall pull request that can help both the author and other reviewers note that new replies have arrived. Don't reply to an in-file comment and then mark it as resolved. No one will see your reply as the thread will be hidden immediately when marked as resolved. Generally, the person who started the comment thread should mark it as resolved when their comments are sufficiently addressed. If another reviewer is also on the thread and should also agree, just state that you're happy and the last reviewer can mark it resolved. Trivially resolved threads can just be marked as \"resolved\" without further update. Examples: a suggested change that has been successfully applied, or a thread where the relevant reviewers have clearly indicated they're happy. Code author guide The goal of an author should be to ensure their change improves the overall code, repository, and/or project. Within the context of code review, the goal is to get a reviewer to validate that the change succeeds at this goal. That involves finding an effective reviewer given the particular nature of the change, helping them understand the change fully, and addressing any feedback they provide. Write good change descriptions The change description in the pull request is the first thing your reviewers will see. This sets the context for the entire review, and is very important. First line The first line of a commit, or the subject of the pull request, should be a short summary of specifically what is being done by that change. It should be a complete sentence, written as though it was an order. Try to keep it short, focused, and to the point. Body The description body may need to explain several important aspects of the change to provide context for the reviewer when it isn't obvious from the change itself: The problem being solved by the change. Why the approach taken is the best one. Any issues, concerns, or shortcomings of the approach. Any alternatives considered or attempted. Relevant supporting data such as examples or benchmarks. Try to anticipate what information the reviewer of your change will need to have in order to be effective. Also consider what information someone else will need a year in the future when doing archaeology on the codebase and they come across your change without any context. Make small changes Small changes have many benefits: Faster review. More thorough review. Easier to merge. Easier to revert if needed. The ideal size of a change is as small as possible while it remains self-contained. It should address only one thing . Often, this results in a change only addressing part of a feature rather than the whole thing at once. This makes work more incremental, letting the reviewer understand it piece by piece. It can also make it much easier to critically evaluate whether each part of a feature is adequately tested by showing it in isolation. That said, a change should not be so small that its implications cannot easily be understood. It is fine to provide the reviewer context or a framework of a series of changes so they understand the big picture, but that will only go so far. It is still possible to shrink a change so much that it becomes nonsensical in isolation. For example, a change without appropriate tests is not self-contained. You may want to use a set of stacked pull requests rather than a single, larger pull request in order to keep changes easy to review. TODO: link to the stacked pull request documentation when available. Responding to review comments Many comments have easy and simple responses. The easiest is \"Done\" . When the comment is a concrete suggestion that makes sense and you implement it, you can simply let the reviewer know their suggestion has been incorporated. If the way you implemented the suggestion might need clarification, add that as well. For example, consider mentioning tweaks to the suggestion or when the suggestion was applied in more places. When a suggestion from the reviewer is explicitly optional, you may also have a simple response that you're not going to make the change. This is totally fine -- if it weren't, the reviewer shouldn't have listed it as optional -- but it may be helpful to explain your reasoning to the reviewer so they understand better why the optional suggestion didn't make sense to you. Sometimes comments, even optional ones, center around slight differences or preferences around the code. Consider that the reviewer may be a good proxy for future readers. If the suggestion is essentially equivalent to your original code, consider adopting it as it may make the code easier to read for others. But if you feel the current choice is better , even if only slightly, stand up for yourself and keep it. The reviewer can always push for a change and justify it if needed. For non-optional comments, this section provides several suggestions on how best to make progress. If none of these work, you may need to resolve an impasse or conflict . Responding to questions or confusion Some comments in code review will be questions or confusion as the reviewer tries to understand the code in question or why a particular approach was used. Don't assume that questions are a request for a change. Reviewers should be explicit if they think a change is needed rather than merely asking questions. You should assume a question or confusion is something which only needs to be clarified. However, when responding to a question or confusion, consider making changes to improve clarity in addition to responding within the review, such as by adding comments or changing code structure. The reviewer may not be the last person to need more clarity, and you should use their comments as a signal for improvement. Once done, the review response should typically focus on verifying that the clarifications made in the code are sufficient for the reviewer. Understand the feedback in the comments At times, review comments may be confusing or frustrating for you. While this is something we always want reviewers to minimize, it will still happen at some times and to some degree. It helps to remember that the goal of the review is to ensure the change results in the project improving over time. If the review comment doesn't make sense, ask the reviewer to help you understand the feedback better. If it isn't constructive or doesn't seem to provide any meaningful path forward, ask the reviewer to provide this. Making comments both clear and constructive are part of the reviewers' responsibilities. Once there is a clear and effectively communicated comment that you understand, it may still feel wrong or like it is unnecessarily blocking your progress. It is important to try to step back in this situation and, no matter how certain you are, genuinely consider whether there is valuable feedback. You should be asking yourself whether the reviewer might be correct, potentially in an unexpected or surprising way. If you can't decide this definitively, you may need to work to get a deeper understanding. If you are confident that the reviewer's comment is incorrect, that is OK . The reviewer is also only human and is certain to make mistakes and miss things. The response needs to try to explain what it is that leads you to be confident in your assessment. Lay out the information you have and how you are reasoning about the issue to arrive at the conclusion. Try not to make assumptions about what the reviewer knows or why they made the comment. Instead, focus on surfacing explicitly your perspective on the issue. These parts of a review will often be a discussion and may need to iterate a few times. That isn't intrinsically bad, but try to make sure that it doesn't result in reiterating positions or repeating things. Make sure the discussion is progressing towards deeper understanding and recognize when you reach an impasse or conflict and shift strategy to resolve that . It is also useful to avoid long delays between these iterations. Consider discussing over Discord chat or scheduling a quick video chat on the specific issue. This can avoid multi-hour -- or multi-day -- round trips. Code reviewer guide The specific goal for a particular review should always be to ensure that the overall health of the code, repository, and/or project improves over time. This requires that contributions make progress -- otherwise, nothing can improve. However, the review should ensure that quality of changes does not cause the health of the project to decrease over time. The primary responsibility for ensuring that code review remains constructive, productive, and helpful resides in the reviewer . As a reviewer, you are in a position of power and asked to critique the authors hard work. With this power comes responsibility for conducting the review well. How quickly should you respond to a review request? Try to respond to code review requests as soon as you can without interrupting a focused task. At the latest, the next day you are working on the project. Note that the review isn't expected to necessarily be complete after a single review. It is more valuable to give reasonably quick but partial feedback than to delay feedback in order to complete it. If leaving partial feedback, make it clear to the author which parts are covered and which you haven't gotten to yet. Large changes are especially important to give incremental feedback on in order to do so in a timely fashion. One of the first things to consider with large changes is whether it can be split apart into smaller changes that are easier to review promptly. This timeliness guidance doesn't apply to the higher-level evolution process reviews. Evaluating those proposals will often require a larger time investment and have their own timelines spelled out in the process. Here, we are talking about simply reviewing changes themselves orthogonally to any evolutionary discussion and evaluation. What should be covered by a review? Things to consider and evaluate when reviewing changes: Is the code well designed? Is the resulting functionality, including its interface, good for the users of the code? Does the resulting design facilitate long-term maintenance? Can the code be simplified? Is there unnecessary complexity? Are things being implemented that aren't yet needed and only might be needed in the future? Is the code free of bugs and well tested? Is memory safely managed? Is any parallel or concurrent programming done safely? Do unit tests cover relevant behaviors and edge cases? Do any integration tests need to be extended or added? Do any fuzz tests need to be extended or added? Are any tests well designed to be both thorough but also maintainable over time? Is the code easy to read? Are the names used in the code clear? Are all important or non-obvious aspects of the code well commented? Do the comments focus on why instead of what ? Is there appropriate high level documentation for the change? Does the change adhere to all relevant style guides? Is the change consistent with other parts of the project? Writing review comments These are general guidelines for writing effective code review comments: Be kind. Detailed review, especially in an open source project, can be stressful and difficult for the author. As a reviewer, part of the job is to ensure the review experience ends up positive and constructive for the author. Stay constructive. Focus your comments on suggesting specific ways to improve the change. If you need to explain why an improvement is necessary, focus on objective ways the improvement helps and avoid both subjective assessments and anchoring on problems with the current state. Explain why. It is important for the author to understand not merely the mechanical suggested change but what motivates it and why it matters. This may help clear up misunderstandings, help the suggestion be understood and applied more effectively, and allow internalizing improvements for future contributions. Provide a path forward. The author needs to understand what they will need to do to respond to your comments. For example, always provide alternatives when commenting that the current approach won't work. Keep in mind that the goal is to improve the overall health of the code, repository, and/or project over time. Sometimes, there will be pushback on review comments. Consider carefully if the author is correct -- they may be closer to the technical issues than you are and may have important insight. Also consider whether the suggestion is necessary to achieve the overall goal. If the suggestion isn't critical to make the change an overall improvement, it may be fine for it to move forward as-is. As with all communication in the Carbon project, it is critical that your comments are not unkind, unwelcoming, angry, ad-hominem attacks, or otherwise violating our community's code of conduct . Approving the change Be explicit and unambiguous at the end of your review. Select \"Approve\" when submitting the review to mark this in GitHub. You can always include a message, often \"LGTM\" or \"Looks Good To Me\" is often used. If you don't feel like you're in a position to approve the change and are simply helping out with review feedback, make that explicit as well. You should set the review to a \"Comment\" in GitHub, but also state this explicitly in the message since this is the default and doesn't indicate that your feedback is addressed. For example, say that \"my comments are addressed, but leaving the final review to others\" to clearly indicate that you're happy but are deferring the decision to others. If you are an owner and deferring to someone else, it is essential to suggest specific other reviewers. Otherwise, we risk all the owners assuming another is going to approve the change. An important technique to make progress, especially with different working hours and timezones, is to approve changes even with outstanding comments. For example, if the comments you have are straightforward and have unambiguous fixes or suggested edits, you should give an LGTM with those comments addressed. The author can always come back to you if they have questions, and we can always revert changes if the resolution for some reason diverges wildly from your expectations. Merging pull requests Pull requests are ready to be merged when reviewers have indicated they're happy (for example, \"LGTM\" or \"Looks good to me\") or have approved the pull request. While all merges require at least one approval, a reviewer might approve before others are finished reviewing; all reviewers should be given time to comment to ensure there's a consensus. Either the author or reviewer may merge and resolve conflicts. The author may indicate they want to merge by informing the reviewer and adding the DO NOT MERGE label. The reviewer is encouraged to coordinate with the author about merge timing if there are concerns about breaks. In either case, the developer doing the merge is expected to be available to help address post-commit issues, whether through a fix-forward or a rollback. Merge commit descriptions When squashing and merging, GitHub tries to generate a description, but it's recommended to use the first comment on the pull request review for the squashed commit description. Authors should keep it up-to-date so that reviewers can merge when the change is ready. Reviewers shouldn't edit or rewrite this message themselves, and instead ask the author make those changes (possibly with suggestions) just like other parts of the code review. It's important that the commit message is one the author is comfortable with when merged. When suggested edits have been merged into a pull request, GitHub will append a Co-authored-by: line to its default proposed commit message for each reviewer who suggested edits that were applied. These lines should be retained and appended to the message from the initial comment. Resolving an impasse or conflict At some point, a review may reach an impasse or a genuine conflict. While our goal is always to resolve these by building consensus in review, it may not be possible. Both the author and any reviewers should be careful to recognize when this point arrives and address it directly. Continuing the review is unlikely to be productive and has a high risk of becoming acrimonious or worse. There are two techniques to use to resolve these situations that should be tried early on: Bring another person into the review to help address the specific issue. Typically they should at least be an owner, and may usefully be a Carbon lead . Ask the specific question in a broader forum, such as Discord, in order to get a broad set of perspectives on a particular area or issue. The goal of these steps isn't to override the author or the reviewer, but to get more perspectives and voices involved. Often this will clarify the issue and its trade-offs, and provide a simple resolution that all parties are happy with. However, in some cases, the underlying conflict isn't actually addressed. While there is a desire to generally bias towards the direction of the owners during reviews, reviews should not turn into a voting process. The reason for proceeding in a specific direction should always be explained sufficiently that all parties on the review are satisfied by the explanation and don't feel the need to escalate. Fundamentally, both reviewers and the author need to agree on the direction to move forward. If reaching that agreement proves impossible, the review should be escalated . If you feel like an escalation is needed in a review, be explicit and clear in requesting it. There is nothing bad about going through this process, but it should only occur when needed and so it helps to be very clear. Once the impasse or conflict is addressed, it is essential to commit to that direction. It can be especially difficult for the author to accept a direction that they initially disagree with and make changes to their code as a result. An essential skill is the ability to disagree and commit . Escalation At the explicit request of any Carbon lead or to resolve any fundamental impasse in a review, the change should move to a formal proposal . Ultimately, the Carbon project governance structure is always available as an escalation path. Before escalating an impasse or conflict in code review, try asking another reviewer to help resolve the issue or bridge any communication gaps. Consider scheduling a quick video chat to discuss and better understand each others' concerns and position. Note that the formal evolution process is heavyweight and relatively slow. The expectation is that this is rarely used and only to resolve serious and severe disagreements. If this becomes a more common problem, lighter weight processes may be needed to help ensure a reasonable rate of progress.","title":"Code review"},{"location":"project/code_review/#code-review","text":"","title":"Code review"},{"location":"project/code_review/#table-of-contents","text":"High level goals of code review What requires review? Who should review? GitHub pull request mechanics Code author guide Write good change descriptions First line Body Make small changes Responding to review comments Responding to questions or confusion Understand the feedback in the comments Code reviewer guide How quickly should you respond to a review request? What should be covered by a review? Writing review comments Approving the change Merging pull requests Merge commit descriptions Resolving an impasse or conflict Escalation","title":"Table of contents"},{"location":"project/code_review/#high-level-goals-of-code-review","text":"Code review serves several goals in the Carbon project. It directly improves the correctness, clarity, and consistency of contributions, including both code and documentation. These improvements range from the high-level functionality down through the design and implementation details. It also promotes team ownership and spreads knowledge across the team. More detailed discussions can be found in chapter 9 \"Code Review\" of the book Software Engineering at Google and chapter 21 \"Collaborative Construction\" in Code Complete: A Practical Handbook of Software Construction . However, these details aren't essential to understanding code review and how it works in the Carbon project. All of the important details are provided in the project documentation.","title":"High level goals of code review"},{"location":"project/code_review/#what-requires-review","text":"Every change to Carbon's repositories requires code review. Even formal evolution decisions which have been approved should have their specific changes to the repository reviewed. Many changes to Carbon repositories may only require code review. Typically, these include bug fixes, and development or documentation improvements clearly in line with accepted designs. It may in some rare cases extend to exploring experimental or prototype directions whose design is under active consideration. The term \"code review\" in the Carbon project is not only about \"code\". We expect changes to any files to be reviewed, including documentation and any other material stored in the repository.","title":"What requires review?"},{"location":"project/code_review/#who-should-review","text":"Everyone should feel free to review Carbon changes. Even providing small or partial review can be a good way to start contributing to Carbon. Contributors with specific domain expertise or familiarity should also try to provide review on changes touching relevant parts of the project. Additionally, at least one developer with commit access must review each change. In Carbon, developers will focus on particular areas, loosely broken down as: Carbon leads : proposals and other important project documents, including the Main README , Code of Conduct , license , and goals . Implementation team : general changes. We split out auto-assignment by explorer , toolchain , and other files, including documentation . Auto-assignment will help find owners, but won't always be perfect -- developers may take a PR they weren't auto-assigned in order to help review go quickly. Contributors can also request multiple reviewers, but it can be daunting to get feedback from a large number of reviewers, so we suggest keeping the number of reviewers reasonably small. Any reviews that explicitly request changes should be addressed, either with the changes or an explanation of why not, before a pull request is merged. Further, any owners who have requested changes should explicitly confirm they're happy with the resolution before the change is merged. When a team gives an affirm decision on an evolution proposal , each team member should explicitly note any of their comments on the pull request that, while not blocking the decision , still need to be resolved as part of code review prior to it being merged. These might, for example, be trivial or minor wording tweaks or improvements. Otherwise, the decision is assumed to mean the prior review comments from members of that team are addressed; the author is free to merge once the pull request is approved, possibly with a code review separate from the proposal's review.","title":"Who should review?"},{"location":"project/code_review/#github-pull-request-mechanics","text":"Carbon uses GitHub pull requests for code review, and we recommend some mechanical best practices to most effectively navigate them. Be aware that the main thread of pull request doesn't support threaded discussions or \"resolving\" a comment. If either of those would be useful, you'll probably want to comment on a file. You can quote comments in the main conversation thread in a reply by clicking the three-dot menu on the original comment and selecting \"Quote reply\". If you will want to comment on files, don't comment in the pull request conversation. Always go to the Files Changed tab. Make any in-file comments needed, but add them to a pending review rather than sending them directly. Finish the review and add any top-level review comments there. If you are an owner who will be providing approval for the change, then make sure to mark a review as requesting changes when you want the author to begin addressing your comment. Only use the \"comment\" review state if you are still in the process of reviewing and don't expect the author to begin working on further changes. If you are not an owner asked to approve, use the difference between a comment and requesting a change to help the author know whether to circle back with you before landing the pull request if the relevant owner(s) approve it. Don't reply to in-file comment threads in the conversation view, or with direct single reply comments. Add all replies to in-file comment threads using the Files Changed tab and by adding each reply to a new review, and posting them as a batch when done. You can get to the appropriate Files Changed tab by clicking on the change listed in the conversation view with the incoming set of in-file comments. This flow ensures an explicit update in the overall pull request that can help both the author and other reviewers note that new replies have arrived. Don't reply to an in-file comment and then mark it as resolved. No one will see your reply as the thread will be hidden immediately when marked as resolved. Generally, the person who started the comment thread should mark it as resolved when their comments are sufficiently addressed. If another reviewer is also on the thread and should also agree, just state that you're happy and the last reviewer can mark it resolved. Trivially resolved threads can just be marked as \"resolved\" without further update. Examples: a suggested change that has been successfully applied, or a thread where the relevant reviewers have clearly indicated they're happy.","title":"GitHub pull request mechanics"},{"location":"project/code_review/#code-author-guide","text":"The goal of an author should be to ensure their change improves the overall code, repository, and/or project. Within the context of code review, the goal is to get a reviewer to validate that the change succeeds at this goal. That involves finding an effective reviewer given the particular nature of the change, helping them understand the change fully, and addressing any feedback they provide.","title":"Code author guide"},{"location":"project/code_review/#write-good-change-descriptions","text":"The change description in the pull request is the first thing your reviewers will see. This sets the context for the entire review, and is very important.","title":"Write good change descriptions"},{"location":"project/code_review/#first-line","text":"The first line of a commit, or the subject of the pull request, should be a short summary of specifically what is being done by that change. It should be a complete sentence, written as though it was an order. Try to keep it short, focused, and to the point.","title":"First line"},{"location":"project/code_review/#body","text":"The description body may need to explain several important aspects of the change to provide context for the reviewer when it isn't obvious from the change itself: The problem being solved by the change. Why the approach taken is the best one. Any issues, concerns, or shortcomings of the approach. Any alternatives considered or attempted. Relevant supporting data such as examples or benchmarks. Try to anticipate what information the reviewer of your change will need to have in order to be effective. Also consider what information someone else will need a year in the future when doing archaeology on the codebase and they come across your change without any context.","title":"Body"},{"location":"project/code_review/#make-small-changes","text":"Small changes have many benefits: Faster review. More thorough review. Easier to merge. Easier to revert if needed. The ideal size of a change is as small as possible while it remains self-contained. It should address only one thing . Often, this results in a change only addressing part of a feature rather than the whole thing at once. This makes work more incremental, letting the reviewer understand it piece by piece. It can also make it much easier to critically evaluate whether each part of a feature is adequately tested by showing it in isolation. That said, a change should not be so small that its implications cannot easily be understood. It is fine to provide the reviewer context or a framework of a series of changes so they understand the big picture, but that will only go so far. It is still possible to shrink a change so much that it becomes nonsensical in isolation. For example, a change without appropriate tests is not self-contained. You may want to use a set of stacked pull requests rather than a single, larger pull request in order to keep changes easy to review. TODO: link to the stacked pull request documentation when available.","title":"Make small changes"},{"location":"project/code_review/#responding-to-review-comments","text":"Many comments have easy and simple responses. The easiest is \"Done\" . When the comment is a concrete suggestion that makes sense and you implement it, you can simply let the reviewer know their suggestion has been incorporated. If the way you implemented the suggestion might need clarification, add that as well. For example, consider mentioning tweaks to the suggestion or when the suggestion was applied in more places. When a suggestion from the reviewer is explicitly optional, you may also have a simple response that you're not going to make the change. This is totally fine -- if it weren't, the reviewer shouldn't have listed it as optional -- but it may be helpful to explain your reasoning to the reviewer so they understand better why the optional suggestion didn't make sense to you. Sometimes comments, even optional ones, center around slight differences or preferences around the code. Consider that the reviewer may be a good proxy for future readers. If the suggestion is essentially equivalent to your original code, consider adopting it as it may make the code easier to read for others. But if you feel the current choice is better , even if only slightly, stand up for yourself and keep it. The reviewer can always push for a change and justify it if needed. For non-optional comments, this section provides several suggestions on how best to make progress. If none of these work, you may need to resolve an impasse or conflict .","title":"Responding to review comments"},{"location":"project/code_review/#responding-to-questions-or-confusion","text":"Some comments in code review will be questions or confusion as the reviewer tries to understand the code in question or why a particular approach was used. Don't assume that questions are a request for a change. Reviewers should be explicit if they think a change is needed rather than merely asking questions. You should assume a question or confusion is something which only needs to be clarified. However, when responding to a question or confusion, consider making changes to improve clarity in addition to responding within the review, such as by adding comments or changing code structure. The reviewer may not be the last person to need more clarity, and you should use their comments as a signal for improvement. Once done, the review response should typically focus on verifying that the clarifications made in the code are sufficient for the reviewer.","title":"Responding to questions or confusion"},{"location":"project/code_review/#understand-the-feedback-in-the-comments","text":"At times, review comments may be confusing or frustrating for you. While this is something we always want reviewers to minimize, it will still happen at some times and to some degree. It helps to remember that the goal of the review is to ensure the change results in the project improving over time. If the review comment doesn't make sense, ask the reviewer to help you understand the feedback better. If it isn't constructive or doesn't seem to provide any meaningful path forward, ask the reviewer to provide this. Making comments both clear and constructive are part of the reviewers' responsibilities. Once there is a clear and effectively communicated comment that you understand, it may still feel wrong or like it is unnecessarily blocking your progress. It is important to try to step back in this situation and, no matter how certain you are, genuinely consider whether there is valuable feedback. You should be asking yourself whether the reviewer might be correct, potentially in an unexpected or surprising way. If you can't decide this definitively, you may need to work to get a deeper understanding. If you are confident that the reviewer's comment is incorrect, that is OK . The reviewer is also only human and is certain to make mistakes and miss things. The response needs to try to explain what it is that leads you to be confident in your assessment. Lay out the information you have and how you are reasoning about the issue to arrive at the conclusion. Try not to make assumptions about what the reviewer knows or why they made the comment. Instead, focus on surfacing explicitly your perspective on the issue. These parts of a review will often be a discussion and may need to iterate a few times. That isn't intrinsically bad, but try to make sure that it doesn't result in reiterating positions or repeating things. Make sure the discussion is progressing towards deeper understanding and recognize when you reach an impasse or conflict and shift strategy to resolve that . It is also useful to avoid long delays between these iterations. Consider discussing over Discord chat or scheduling a quick video chat on the specific issue. This can avoid multi-hour -- or multi-day -- round trips.","title":"Understand the feedback in the comments"},{"location":"project/code_review/#code-reviewer-guide","text":"The specific goal for a particular review should always be to ensure that the overall health of the code, repository, and/or project improves over time. This requires that contributions make progress -- otherwise, nothing can improve. However, the review should ensure that quality of changes does not cause the health of the project to decrease over time. The primary responsibility for ensuring that code review remains constructive, productive, and helpful resides in the reviewer . As a reviewer, you are in a position of power and asked to critique the authors hard work. With this power comes responsibility for conducting the review well.","title":"Code reviewer guide"},{"location":"project/code_review/#how-quickly-should-you-respond-to-a-review-request","text":"Try to respond to code review requests as soon as you can without interrupting a focused task. At the latest, the next day you are working on the project. Note that the review isn't expected to necessarily be complete after a single review. It is more valuable to give reasonably quick but partial feedback than to delay feedback in order to complete it. If leaving partial feedback, make it clear to the author which parts are covered and which you haven't gotten to yet. Large changes are especially important to give incremental feedback on in order to do so in a timely fashion. One of the first things to consider with large changes is whether it can be split apart into smaller changes that are easier to review promptly. This timeliness guidance doesn't apply to the higher-level evolution process reviews. Evaluating those proposals will often require a larger time investment and have their own timelines spelled out in the process. Here, we are talking about simply reviewing changes themselves orthogonally to any evolutionary discussion and evaluation.","title":"How quickly should you respond to a review request?"},{"location":"project/code_review/#what-should-be-covered-by-a-review","text":"Things to consider and evaluate when reviewing changes: Is the code well designed? Is the resulting functionality, including its interface, good for the users of the code? Does the resulting design facilitate long-term maintenance? Can the code be simplified? Is there unnecessary complexity? Are things being implemented that aren't yet needed and only might be needed in the future? Is the code free of bugs and well tested? Is memory safely managed? Is any parallel or concurrent programming done safely? Do unit tests cover relevant behaviors and edge cases? Do any integration tests need to be extended or added? Do any fuzz tests need to be extended or added? Are any tests well designed to be both thorough but also maintainable over time? Is the code easy to read? Are the names used in the code clear? Are all important or non-obvious aspects of the code well commented? Do the comments focus on why instead of what ? Is there appropriate high level documentation for the change? Does the change adhere to all relevant style guides? Is the change consistent with other parts of the project?","title":"What should be covered by a review?"},{"location":"project/code_review/#writing-review-comments","text":"These are general guidelines for writing effective code review comments: Be kind. Detailed review, especially in an open source project, can be stressful and difficult for the author. As a reviewer, part of the job is to ensure the review experience ends up positive and constructive for the author. Stay constructive. Focus your comments on suggesting specific ways to improve the change. If you need to explain why an improvement is necessary, focus on objective ways the improvement helps and avoid both subjective assessments and anchoring on problems with the current state. Explain why. It is important for the author to understand not merely the mechanical suggested change but what motivates it and why it matters. This may help clear up misunderstandings, help the suggestion be understood and applied more effectively, and allow internalizing improvements for future contributions. Provide a path forward. The author needs to understand what they will need to do to respond to your comments. For example, always provide alternatives when commenting that the current approach won't work. Keep in mind that the goal is to improve the overall health of the code, repository, and/or project over time. Sometimes, there will be pushback on review comments. Consider carefully if the author is correct -- they may be closer to the technical issues than you are and may have important insight. Also consider whether the suggestion is necessary to achieve the overall goal. If the suggestion isn't critical to make the change an overall improvement, it may be fine for it to move forward as-is. As with all communication in the Carbon project, it is critical that your comments are not unkind, unwelcoming, angry, ad-hominem attacks, or otherwise violating our community's code of conduct .","title":"Writing review comments"},{"location":"project/code_review/#approving-the-change","text":"Be explicit and unambiguous at the end of your review. Select \"Approve\" when submitting the review to mark this in GitHub. You can always include a message, often \"LGTM\" or \"Looks Good To Me\" is often used. If you don't feel like you're in a position to approve the change and are simply helping out with review feedback, make that explicit as well. You should set the review to a \"Comment\" in GitHub, but also state this explicitly in the message since this is the default and doesn't indicate that your feedback is addressed. For example, say that \"my comments are addressed, but leaving the final review to others\" to clearly indicate that you're happy but are deferring the decision to others. If you are an owner and deferring to someone else, it is essential to suggest specific other reviewers. Otherwise, we risk all the owners assuming another is going to approve the change. An important technique to make progress, especially with different working hours and timezones, is to approve changes even with outstanding comments. For example, if the comments you have are straightforward and have unambiguous fixes or suggested edits, you should give an LGTM with those comments addressed. The author can always come back to you if they have questions, and we can always revert changes if the resolution for some reason diverges wildly from your expectations.","title":"Approving the change"},{"location":"project/code_review/#merging-pull-requests","text":"Pull requests are ready to be merged when reviewers have indicated they're happy (for example, \"LGTM\" or \"Looks good to me\") or have approved the pull request. While all merges require at least one approval, a reviewer might approve before others are finished reviewing; all reviewers should be given time to comment to ensure there's a consensus. Either the author or reviewer may merge and resolve conflicts. The author may indicate they want to merge by informing the reviewer and adding the DO NOT MERGE label. The reviewer is encouraged to coordinate with the author about merge timing if there are concerns about breaks. In either case, the developer doing the merge is expected to be available to help address post-commit issues, whether through a fix-forward or a rollback.","title":"Merging pull requests"},{"location":"project/code_review/#merge-commit-descriptions","text":"When squashing and merging, GitHub tries to generate a description, but it's recommended to use the first comment on the pull request review for the squashed commit description. Authors should keep it up-to-date so that reviewers can merge when the change is ready. Reviewers shouldn't edit or rewrite this message themselves, and instead ask the author make those changes (possibly with suggestions) just like other parts of the code review. It's important that the commit message is one the author is comfortable with when merged. When suggested edits have been merged into a pull request, GitHub will append a Co-authored-by: line to its default proposed commit message for each reviewer who suggested edits that were applied. These lines should be retained and appended to the message from the initial comment.","title":"Merge commit descriptions"},{"location":"project/code_review/#resolving-an-impasse-or-conflict","text":"At some point, a review may reach an impasse or a genuine conflict. While our goal is always to resolve these by building consensus in review, it may not be possible. Both the author and any reviewers should be careful to recognize when this point arrives and address it directly. Continuing the review is unlikely to be productive and has a high risk of becoming acrimonious or worse. There are two techniques to use to resolve these situations that should be tried early on: Bring another person into the review to help address the specific issue. Typically they should at least be an owner, and may usefully be a Carbon lead . Ask the specific question in a broader forum, such as Discord, in order to get a broad set of perspectives on a particular area or issue. The goal of these steps isn't to override the author or the reviewer, but to get more perspectives and voices involved. Often this will clarify the issue and its trade-offs, and provide a simple resolution that all parties are happy with. However, in some cases, the underlying conflict isn't actually addressed. While there is a desire to generally bias towards the direction of the owners during reviews, reviews should not turn into a voting process. The reason for proceeding in a specific direction should always be explained sufficiently that all parties on the review are satisfied by the explanation and don't feel the need to escalate. Fundamentally, both reviewers and the author need to agree on the direction to move forward. If reaching that agreement proves impossible, the review should be escalated . If you feel like an escalation is needed in a review, be explicit and clear in requesting it. There is nothing bad about going through this process, but it should only occur when needed and so it helps to be very clear. Once the impasse or conflict is addressed, it is essential to commit to that direction. It can be especially difficult for the author to accept a direction that they initially disagree with and make changes to their code as a result. An essential skill is the ability to disagree and commit .","title":"Resolving an impasse or conflict"},{"location":"project/code_review/#escalation","text":"At the explicit request of any Carbon lead or to resolve any fundamental impasse in a review, the change should move to a formal proposal . Ultimately, the Carbon project governance structure is always available as an escalation path. Before escalating an impasse or conflict in code review, try asking another reviewer to help resolve the issue or bridge any communication gaps. Consider scheduling a quick video chat to discuss and better understand each others' concerns and position. Note that the formal evolution process is heavyweight and relatively slow. The expectation is that this is rarely used and only to resolve serious and severe disagreements. If this becomes a more common problem, lighter weight processes may be needed to help ensure a reasonable rate of progress.","title":"Escalation"},{"location":"project/contribution_tools/","text":"Contribution tools The Carbon language project has a number of tools used to assist in preparing contributions. Table of contents Tool setup flow Package managers Linux and MacOS Homebrew python3 and pip3 Main tools Bazel and Bazelisk Clang and LLVM Manual installations (not recommended) pre-commit Optional tools Carbon-maintained new_proposal.py pr_comments.py GitHub gh CLI GitHub Desktop rs-git-fsmonitor and Watchman Vim vim-prettier Visual Studio Code pre-commit enabled tools black codespell Prettier Tool setup flow In order to set up a machine and git repository for developing on Carbon, a typical tool setup flow is: Install package managers . Install main tools and any desired optional tools . Set up the git repository: In GitHub, create a fork for development at https://github.com/carbon-language/carbon-lang. gh repo clone USER/carbon-lang , or otherwise clone the fork. cd carbon-lang to go into the cloned fork's directory. pre-commit install to set up pre-commit in the clone. Validate your installation by invoking `bazel test //...:all' from the project root. All tests should pass. Package managers Instructions for installing tools can be helpful for installing tooling. These instructions will try to rely on a minimum of managers. Linux and MacOS Homebrew Homebrew is a package manager, and can help install several tools that we recommend. Our recommended way of installing is to run the canonical install command . To get the latest version of brew packages, it will be necessary to periodically run brew upgrade . python3 and pip3 Carbon requires Python 3.9 or newer. The included pip3 should typically be used for Python package installation rather than other package managers. NOTE : Carbon will focus support on Homebrew installs of Python 3.9, but it may not be necessary if you have Python 3.9 installed another way. If you're trying to use a non-Homebrew Python but have issues involving Carbon and Python, please try Homebrew's Python. Our recommended way of installing is: brew install python@3.9 pip3 install -U pip NOTE : pip3 runs may print deprecation warnings referencing https://github.com/Homebrew/homebrew-core/issues/76621. These will need to be addressed in the future, but as of August 2021 can be ignored. To get the latest version of pip3 packages, it will be necessary to periodically run pip3 list --outdated , then pip3 install -U <package> to upgrade desired packages. Keep in mind when upgrading that version dependencies may mean packages should be outdated, and not be upgraded. Main tools These tools are key for contributions, primarily focused on validating contributions. Bazel and Bazelisk Bazel is Carbon's standard build system. Bazelisk is recommended for installing Bazel. Our recommended way of installing is: brew install bazelisk Clang and LLVM Clang and LLVM are used to compile and link Carbon as part of its build. Bazel will also download and build against a specific upstream LLVM commit. While the Bazel uses upstream LLVM sources, the project expects the LLVM 12 release (or newer) to be installed with Clang and other tools in your PATH for use in building Carbon itself. Our recommended way of installing is: brew install llvm On MacOS only (not Linux), llvm is keg-only; bear in mind this requires updating PATH for it because it's not part of the standard Homebrew path. Read the output of brew install for the necessary path changes, or add something to your PATH like: export PATH=\"$(brew --prefix llvm)/bin:${PATH}\" Carbon expects the PATH to include the installed tooling. If set, CC should also point at clang . Our build environment will detect the clang binary using CC then PATH , and will expect the rest of the LLVM toolchain to be available in the same directory as clang . However, various scripts and tools assume that the LLVM toolchain will be in PATH , particularly for tools like clang-format and clang-tidy . TODO: We'd like to use apt , but standard LLVM Debian packages are not configured correctly for our needs. We are currently aware of two libc++ issues, 43604 and 46321 . Manual installations (not recommended) You can also build and install LLVM yourself if you prefer. The essential CMake options to pass in order for this to work reliably include: -DLLVM_ENABLE_PROJECTS=clang;clang-tools-extra;lld -DLLVM_ENABLE_RUNTIMES=compiler-rt;libcxx;libcxxabi;libunwind -DRUNTIMES_CMAKE_ARGS=-DLLVM_ENABLE_PER_TARGET_RUNTIME_DIR=OFF;-DCMAKE_POSITION_INDEPENDENT_CODE=ON;-DLIBCXX_ENABLE_STATIC_ABI_LIBRARY=ON;-DLIBCXX_STATICALLY_LINK_ABI_IN_SHARED_LIBRARY=OFF;-DLIBCXX_STATICALLY_LINK_ABI_IN_STATIC_LIBRARY=ON;-DLIBCXX_USE_COMPILER_RT=ON;-DLIBCXXABI_USE_COMPILER_RT=ON;-DLIBCXXABI_USE_LLVM_UNWINDER=ON However, we primarily test against the Homebrew installation, so if building LLVM and Clang yourself you may hit some issues. pre-commit We use pre-commit to run various checks . This will automatically run important checks, including formatting. Our recommended way of installing is: pip3 install pre-commit # From within each carbon-language git repository: pre-commit install NOTE: There are other ways of installing listed at pre-commit.com , but pip is recommended for reliability. When you have changes to commit to git, a standard pre-commit workflow can look like: # Let pre-commit fix style issues. pre-commit run # Add modifications made by pre-commit. git add . # Commit the changes git commit When modifying or adding pre-commit hooks, please run pre-commit run --all-files to see what changes. Optional tools Carbon-maintained Carbon-maintained tools are provided by the carbon-lang repository, rather than a separate install. They are noted here mainly to help findability. new_proposal.py new_proposal.py is a helper for generating the PR and proposal file for a new proposal. It's documented in the proposal template . NOTE : This requires installing the gh CLI . pr_comments.py pr_comments.py is a helper for scanning comments in GitHub. It's particularly intended to help find threads which need to be resolved. Options can be seen with -h . A couple key options to be aware of are: --long : Prints long output, with the full comment. --comments-after LOGIN : Only print threads where the final comment is not from the given user. For example, use when looking for threads that you still need to respond to. --comments-from LOGIN : Only print threads with comments from the given user. For example, use when looking for threads that you've commented on. This script may be run directly if gql is installed: pip install gql ./github_tools/pr_comments.py <PR#> It may also be run using bazel , without installing gql : bazel run //github_tools:pr_comments -- <PR#> GitHub gh CLI The gh CLI supports some GitHub queries, and is used by some scripts. Our recommended way of installing is: brew install github/gh/gh GitHub Desktop GitHub Desktop provides a UI for managing git repositories. See the page for installation instructions. rs-git-fsmonitor and Watchman WARNING : Bugs in rs-git-fsmonitor and/or Watchman can result in pre-commit deleting files. If you see files being deleted, disable rs-git-fsmonitor with git config --unset core.fsmonitor . rs-git-fsmonitor is a file system monitor that uses Watchman to speed up git on large repositories, such as carbon-lang . Our recommended way of installing is: Linux: If you don't have Rust's Cargo package manager, install it first with the official install command . ```bash brew install watchman cargo install --git https://github.com/jgavris/rs-git-fsmonitor.git Configure the git repository to use fsmonitor. git config core.fsmonitor rs-git-fsmonitor ``` MacOS: ```bash brew tap jgavris/rs-git-fsmonitor \\ https://github.com/jgavris/rs-git-fsmonitor.git brew install rs-git-fsmonitor Configure the git repository to use fsmonitor. git config core.fsmonitor rs-git-fsmonitor ``` Vim vim-prettier vim-prettier is a vim integration for Prettier . If you use vim-prettier, the .prettierrc.yaml should still apply as long as config_precedence is set to the default file-override . However, we may need to add additional settings where the vim-prettier default diverges from prettier , as we notice them. Our recommended way of installing is to use the canonical instructions . Visual Studio Code Visual Studio Code is an IDE used by several of us. We provide recommended extensions to assist Carbon development. Some settings changes must be made separately: Python \u203a Formatting: Provider: black Our recommended way of installing is to use the canonical download . WARNING: Visual Studio Code modifies the PATH environment variable, particularly in the terminals it creates. The PATH difference can cause bazel to detect different startup options, discarding its build cache. As a consequence, it's recommended to use either normal terminals or Visual Studio Code to run bazel , not both in combination. Visual Studio Code can still be used for other purposes, such as editing files, without interfering with bazel . pre-commit enabled tools If you're using pre-commit, it will run these tools. Installing and running them manually is entirely optional , as they can be run without being installed through pre-commit run , but install instructions are still noted here for direct execution. black We use Black to format Python code. Although Prettier is used for most languages, it doesn't support Python. Our recommended way of installing is: pip install black codespell We use codespell to spellcheck common errors. This won't catch every error; we're trying to balance true and false positives. Our recommended way of installing is: pip install codespell Prettier We use Prettier for formatting. There is an rc file for configuration. Our recommended way of installing is to use the canonical instructions .","title":"Contribution tools"},{"location":"project/contribution_tools/#contribution-tools","text":"The Carbon language project has a number of tools used to assist in preparing contributions.","title":"Contribution tools"},{"location":"project/contribution_tools/#table-of-contents","text":"Tool setup flow Package managers Linux and MacOS Homebrew python3 and pip3 Main tools Bazel and Bazelisk Clang and LLVM Manual installations (not recommended) pre-commit Optional tools Carbon-maintained new_proposal.py pr_comments.py GitHub gh CLI GitHub Desktop rs-git-fsmonitor and Watchman Vim vim-prettier Visual Studio Code pre-commit enabled tools black codespell Prettier","title":"Table of contents"},{"location":"project/contribution_tools/#tool-setup-flow","text":"In order to set up a machine and git repository for developing on Carbon, a typical tool setup flow is: Install package managers . Install main tools and any desired optional tools . Set up the git repository: In GitHub, create a fork for development at https://github.com/carbon-language/carbon-lang. gh repo clone USER/carbon-lang , or otherwise clone the fork. cd carbon-lang to go into the cloned fork's directory. pre-commit install to set up pre-commit in the clone. Validate your installation by invoking `bazel test //...:all' from the project root. All tests should pass.","title":"Tool setup flow"},{"location":"project/contribution_tools/#package-managers","text":"Instructions for installing tools can be helpful for installing tooling. These instructions will try to rely on a minimum of managers.","title":"Package managers"},{"location":"project/contribution_tools/#linux-and-macos","text":"","title":"Linux and MacOS"},{"location":"project/contribution_tools/#homebrew","text":"Homebrew is a package manager, and can help install several tools that we recommend. Our recommended way of installing is to run the canonical install command . To get the latest version of brew packages, it will be necessary to periodically run brew upgrade .","title":"Homebrew"},{"location":"project/contribution_tools/#python3-and-pip3","text":"Carbon requires Python 3.9 or newer. The included pip3 should typically be used for Python package installation rather than other package managers. NOTE : Carbon will focus support on Homebrew installs of Python 3.9, but it may not be necessary if you have Python 3.9 installed another way. If you're trying to use a non-Homebrew Python but have issues involving Carbon and Python, please try Homebrew's Python. Our recommended way of installing is: brew install python@3.9 pip3 install -U pip NOTE : pip3 runs may print deprecation warnings referencing https://github.com/Homebrew/homebrew-core/issues/76621. These will need to be addressed in the future, but as of August 2021 can be ignored. To get the latest version of pip3 packages, it will be necessary to periodically run pip3 list --outdated , then pip3 install -U <package> to upgrade desired packages. Keep in mind when upgrading that version dependencies may mean packages should be outdated, and not be upgraded.","title":"python3 and pip3"},{"location":"project/contribution_tools/#main-tools","text":"These tools are key for contributions, primarily focused on validating contributions.","title":"Main tools"},{"location":"project/contribution_tools/#bazel-and-bazelisk","text":"Bazel is Carbon's standard build system. Bazelisk is recommended for installing Bazel. Our recommended way of installing is: brew install bazelisk","title":"Bazel and Bazelisk"},{"location":"project/contribution_tools/#clang-and-llvm","text":"Clang and LLVM are used to compile and link Carbon as part of its build. Bazel will also download and build against a specific upstream LLVM commit. While the Bazel uses upstream LLVM sources, the project expects the LLVM 12 release (or newer) to be installed with Clang and other tools in your PATH for use in building Carbon itself. Our recommended way of installing is: brew install llvm On MacOS only (not Linux), llvm is keg-only; bear in mind this requires updating PATH for it because it's not part of the standard Homebrew path. Read the output of brew install for the necessary path changes, or add something to your PATH like: export PATH=\"$(brew --prefix llvm)/bin:${PATH}\" Carbon expects the PATH to include the installed tooling. If set, CC should also point at clang . Our build environment will detect the clang binary using CC then PATH , and will expect the rest of the LLVM toolchain to be available in the same directory as clang . However, various scripts and tools assume that the LLVM toolchain will be in PATH , particularly for tools like clang-format and clang-tidy . TODO: We'd like to use apt , but standard LLVM Debian packages are not configured correctly for our needs. We are currently aware of two libc++ issues, 43604 and 46321 .","title":"Clang and LLVM"},{"location":"project/contribution_tools/#manual-installations-not-recommended","text":"You can also build and install LLVM yourself if you prefer. The essential CMake options to pass in order for this to work reliably include: -DLLVM_ENABLE_PROJECTS=clang;clang-tools-extra;lld -DLLVM_ENABLE_RUNTIMES=compiler-rt;libcxx;libcxxabi;libunwind -DRUNTIMES_CMAKE_ARGS=-DLLVM_ENABLE_PER_TARGET_RUNTIME_DIR=OFF;-DCMAKE_POSITION_INDEPENDENT_CODE=ON;-DLIBCXX_ENABLE_STATIC_ABI_LIBRARY=ON;-DLIBCXX_STATICALLY_LINK_ABI_IN_SHARED_LIBRARY=OFF;-DLIBCXX_STATICALLY_LINK_ABI_IN_STATIC_LIBRARY=ON;-DLIBCXX_USE_COMPILER_RT=ON;-DLIBCXXABI_USE_COMPILER_RT=ON;-DLIBCXXABI_USE_LLVM_UNWINDER=ON However, we primarily test against the Homebrew installation, so if building LLVM and Clang yourself you may hit some issues.","title":"Manual installations (not recommended)"},{"location":"project/contribution_tools/#pre-commit","text":"We use pre-commit to run various checks . This will automatically run important checks, including formatting. Our recommended way of installing is: pip3 install pre-commit # From within each carbon-language git repository: pre-commit install NOTE: There are other ways of installing listed at pre-commit.com , but pip is recommended for reliability. When you have changes to commit to git, a standard pre-commit workflow can look like: # Let pre-commit fix style issues. pre-commit run # Add modifications made by pre-commit. git add . # Commit the changes git commit When modifying or adding pre-commit hooks, please run pre-commit run --all-files to see what changes.","title":"pre-commit"},{"location":"project/contribution_tools/#optional-tools","text":"","title":"Optional tools"},{"location":"project/contribution_tools/#carbon-maintained","text":"Carbon-maintained tools are provided by the carbon-lang repository, rather than a separate install. They are noted here mainly to help findability.","title":"Carbon-maintained"},{"location":"project/contribution_tools/#new_proposalpy","text":"new_proposal.py is a helper for generating the PR and proposal file for a new proposal. It's documented in the proposal template . NOTE : This requires installing the gh CLI .","title":"new_proposal.py"},{"location":"project/contribution_tools/#pr_commentspy","text":"pr_comments.py is a helper for scanning comments in GitHub. It's particularly intended to help find threads which need to be resolved. Options can be seen with -h . A couple key options to be aware of are: --long : Prints long output, with the full comment. --comments-after LOGIN : Only print threads where the final comment is not from the given user. For example, use when looking for threads that you still need to respond to. --comments-from LOGIN : Only print threads with comments from the given user. For example, use when looking for threads that you've commented on. This script may be run directly if gql is installed: pip install gql ./github_tools/pr_comments.py <PR#> It may also be run using bazel , without installing gql : bazel run //github_tools:pr_comments -- <PR#>","title":"pr_comments.py"},{"location":"project/contribution_tools/#github","text":"","title":"GitHub"},{"location":"project/contribution_tools/#gh-cli","text":"The gh CLI supports some GitHub queries, and is used by some scripts. Our recommended way of installing is: brew install github/gh/gh","title":"gh CLI"},{"location":"project/contribution_tools/#github-desktop","text":"GitHub Desktop provides a UI for managing git repositories. See the page for installation instructions.","title":"GitHub Desktop"},{"location":"project/contribution_tools/#rs-git-fsmonitor-and-watchman","text":"WARNING : Bugs in rs-git-fsmonitor and/or Watchman can result in pre-commit deleting files. If you see files being deleted, disable rs-git-fsmonitor with git config --unset core.fsmonitor . rs-git-fsmonitor is a file system monitor that uses Watchman to speed up git on large repositories, such as carbon-lang . Our recommended way of installing is: Linux: If you don't have Rust's Cargo package manager, install it first with the official install command . ```bash brew install watchman cargo install --git https://github.com/jgavris/rs-git-fsmonitor.git","title":"rs-git-fsmonitor and Watchman"},{"location":"project/contribution_tools/#configure-the-git-repository-to-use-fsmonitor","text":"git config core.fsmonitor rs-git-fsmonitor ``` MacOS: ```bash brew tap jgavris/rs-git-fsmonitor \\ https://github.com/jgavris/rs-git-fsmonitor.git brew install rs-git-fsmonitor","title":"Configure the git repository to use fsmonitor."},{"location":"project/contribution_tools/#configure-the-git-repository-to-use-fsmonitor_1","text":"git config core.fsmonitor rs-git-fsmonitor ```","title":"Configure the git repository to use fsmonitor."},{"location":"project/contribution_tools/#vim","text":"","title":"Vim"},{"location":"project/contribution_tools/#vim-prettier","text":"vim-prettier is a vim integration for Prettier . If you use vim-prettier, the .prettierrc.yaml should still apply as long as config_precedence is set to the default file-override . However, we may need to add additional settings where the vim-prettier default diverges from prettier , as we notice them. Our recommended way of installing is to use the canonical instructions .","title":"vim-prettier"},{"location":"project/contribution_tools/#visual-studio-code","text":"Visual Studio Code is an IDE used by several of us. We provide recommended extensions to assist Carbon development. Some settings changes must be made separately: Python \u203a Formatting: Provider: black Our recommended way of installing is to use the canonical download . WARNING: Visual Studio Code modifies the PATH environment variable, particularly in the terminals it creates. The PATH difference can cause bazel to detect different startup options, discarding its build cache. As a consequence, it's recommended to use either normal terminals or Visual Studio Code to run bazel , not both in combination. Visual Studio Code can still be used for other purposes, such as editing files, without interfering with bazel .","title":"Visual Studio Code"},{"location":"project/contribution_tools/#pre-commit-enabled-tools","text":"If you're using pre-commit, it will run these tools. Installing and running them manually is entirely optional , as they can be run without being installed through pre-commit run , but install instructions are still noted here for direct execution.","title":"pre-commit enabled tools"},{"location":"project/contribution_tools/#black","text":"We use Black to format Python code. Although Prettier is used for most languages, it doesn't support Python. Our recommended way of installing is: pip install black","title":"black"},{"location":"project/contribution_tools/#codespell","text":"We use codespell to spellcheck common errors. This won't catch every error; we're trying to balance true and false positives. Our recommended way of installing is: pip install codespell","title":"codespell"},{"location":"project/contribution_tools/#prettier","text":"We use Prettier for formatting. There is an rc file for configuration. Our recommended way of installing is to use the canonical instructions .","title":"Prettier"},{"location":"project/cpp_style_guide/","text":"C++ style guide Table of contents Background Baseline Carbon-local guidance General naming rules File names Syntax and formatting Copyable and movable types Static and global variables Foundational libraries and data types Suggested .clang-format contents Background C++ code in the Carbon project should use a consistent and well documented style guide. Where possible, this should be enacted and enforced with tooling to avoid toil both for authors of C++ code in the Carbon project and for code reviewers. However, we are not in the business of innovating significantly in the space of writing clean and maintainable C++ code, and so we work primarily to reuse existing best practices and guidelines. Baseline The baseline style guidance is the Google C++ style guide . Carbon-local guidance We provide some local guidance beyond the baseline. This are typically motived either by specific value provided to the project, or to give simpler and more strict guidance for Carbon's narrow use of C++. General naming rules Carbon's C++ code tries to match the proposed Carbon naming convention as closely as is reasonable in C++ in order to better understand and familiarize ourselves with the practice of using this convention. It happens that this is fairly similar to the naming convention in the Google style guide and largely serves to simplify it. Known, compile-time constants use UpperCamelCase , referencing Proper Nouns. This includes namespaces, type names, functions, member functions (except as noted below), template parameters, constexpr variables, enumerators, etc. Note that virtual member functions should be named with UpperCamelCase . The distinction between a virtual function and a non-virtual function should be invisible, especially at the call site, as that is an internal implementation detail. We want to be able to freely change that without updating the name. Member functions may use snake_case names if they do nothing besides return a reference to a data member (or assign a value to a data member, in the case of set_ methods), or if their behavior (including performance) would be unsurprising to a caller who assumes they are implemented that way. All other names use snake_case , including function parameters, and non-constant local and member variables. Private member variables should have a trailing _ . File names Always use snake_case for files, directories, and build system rules. Avoid - s in these as well. Use .cpp for source files, which is the most common open source extension and matches other places where \"C++\" is written without punctuation. Syntax and formatting These are minor issues where any of the options would be fine and we simply need to pick a consistent option. Where possible, clang-format should be used to enforce these. Always use trailing return type syntax for functions and methods. Place the pointer * adjacent to the type: TypeName* variable_name . Only declare one variable at a time (declaring multiple variables requires confusing repetition of part of the type). Write const before the type when at the outer level: const int N = 42; . Only use line comments (with // , not /* ... */ ), on a line by themselves, except for argument name comments , closing namespace comments , and similar structural comments. In particular, don't append comments about a line of code to the end of its line: ``` int bad = 42; // Don't comment here. // Instead comment here. int good = 42; // Closing namespace comments are structural, and both okay and expected. } // namespace MyNamespace ``` This dogfoods our planned commenting syntax for Carbon. It also provides a single, consistent placement rule. It also provides more resilience against automated refactorings. Those changes often make code longer, which forces ever more difficult formatting decisions, and can easily spread one line across multiple lines, leaving it impossible to know where to place the comment. Comments on their own line preceding such code, while still imprecise, are at least less confusing over the course of such refactorings. Use the using -based type alias syntax instead of typedef . Don't use using to support unqualified lookup on std types; for example, using std::vector; . This also applies to other short namespaces, particularly llvm and clang . Writing std:: gives clearer diagnostics and avoids any possible ambiguity, particularly for ADL. An exception is made for functions like std::swap that are intentionally called using ADL. This pattern should be written as { using std::swap; swap(thing1, thing2); } . Follow the rules for initialization outlined in Abseil's tip #88 . To summarize, omitting some details from the article: Use assignment syntax ( = ) when initializing directly with the intended value (or with a braced initializer directly specifying that value). Use the traditional constructor syntax (with parentheses) when the initialization is performing some active logic, rather than simply composing values together. Use {} initialization without the = only if the above options don't compile. Never mix {} initialization and auto . Always use braces for conditional, switch , and loop statements, even when the body is a single statement. Within a switch statement, use braces after a case label when necessary to create a scope for a variable. Always break the line immediately after an open brace except for empty loop bodies. For internal linkage of definitions of functions and variables, prefer static over anonymous namespaces. static minimizes the context necessary to notice the internal linkage of a definition. Anonymous namespaces are still necessary for classes and enums. Tests are an exception and should typically be wrapped with namespace Carbon::Testing { namespace { ... } } to keep everything internal. Copyable and movable types Types should have value semantics and support both move and copy where possible. Types that cannot be copied should still be movable where possible. If supported, moving should be as efficient as possible. Static and global variables Global and static variables, whether at file, class, or function scope, should be declared constexpr . Foundational libraries and data types Generally prefer LLVM libraries and data structures to standard C++ ones. These are optimized significantly for performance, especially when used without exception handling or safety requirements, and when used in patterns that tend to occur while building compilers. They also minimize the vocabulary type friction when using actual LLVM and Clang APIs. Do not add third-party library dependencies to any code that might conceivably be used as part of the compiler or runtime. Compilers and runtime libraries have unique constraints on their licensing. For simplicity, we want all transitive dependencies of these layers to be under the LLVM license that the Carbon project as a whole uses (as well as LLVM itself). Suggested .clang-format contents See this repository's .clang-format file .","title":"C++ style guide"},{"location":"project/cpp_style_guide/#c-style-guide","text":"","title":"C++ style guide"},{"location":"project/cpp_style_guide/#table-of-contents","text":"Background Baseline Carbon-local guidance General naming rules File names Syntax and formatting Copyable and movable types Static and global variables Foundational libraries and data types Suggested .clang-format contents","title":"Table of contents"},{"location":"project/cpp_style_guide/#background","text":"C++ code in the Carbon project should use a consistent and well documented style guide. Where possible, this should be enacted and enforced with tooling to avoid toil both for authors of C++ code in the Carbon project and for code reviewers. However, we are not in the business of innovating significantly in the space of writing clean and maintainable C++ code, and so we work primarily to reuse existing best practices and guidelines.","title":"Background"},{"location":"project/cpp_style_guide/#baseline","text":"The baseline style guidance is the Google C++ style guide .","title":"Baseline"},{"location":"project/cpp_style_guide/#carbon-local-guidance","text":"We provide some local guidance beyond the baseline. This are typically motived either by specific value provided to the project, or to give simpler and more strict guidance for Carbon's narrow use of C++.","title":"Carbon-local guidance"},{"location":"project/cpp_style_guide/#general-naming-rules","text":"Carbon's C++ code tries to match the proposed Carbon naming convention as closely as is reasonable in C++ in order to better understand and familiarize ourselves with the practice of using this convention. It happens that this is fairly similar to the naming convention in the Google style guide and largely serves to simplify it. Known, compile-time constants use UpperCamelCase , referencing Proper Nouns. This includes namespaces, type names, functions, member functions (except as noted below), template parameters, constexpr variables, enumerators, etc. Note that virtual member functions should be named with UpperCamelCase . The distinction between a virtual function and a non-virtual function should be invisible, especially at the call site, as that is an internal implementation detail. We want to be able to freely change that without updating the name. Member functions may use snake_case names if they do nothing besides return a reference to a data member (or assign a value to a data member, in the case of set_ methods), or if their behavior (including performance) would be unsurprising to a caller who assumes they are implemented that way. All other names use snake_case , including function parameters, and non-constant local and member variables. Private member variables should have a trailing _ .","title":"General naming rules"},{"location":"project/cpp_style_guide/#file-names","text":"Always use snake_case for files, directories, and build system rules. Avoid - s in these as well. Use .cpp for source files, which is the most common open source extension and matches other places where \"C++\" is written without punctuation.","title":"File names"},{"location":"project/cpp_style_guide/#syntax-and-formatting","text":"These are minor issues where any of the options would be fine and we simply need to pick a consistent option. Where possible, clang-format should be used to enforce these. Always use trailing return type syntax for functions and methods. Place the pointer * adjacent to the type: TypeName* variable_name . Only declare one variable at a time (declaring multiple variables requires confusing repetition of part of the type). Write const before the type when at the outer level: const int N = 42; . Only use line comments (with // , not /* ... */ ), on a line by themselves, except for argument name comments , closing namespace comments , and similar structural comments. In particular, don't append comments about a line of code to the end of its line: ``` int bad = 42; // Don't comment here. // Instead comment here. int good = 42; // Closing namespace comments are structural, and both okay and expected. } // namespace MyNamespace ``` This dogfoods our planned commenting syntax for Carbon. It also provides a single, consistent placement rule. It also provides more resilience against automated refactorings. Those changes often make code longer, which forces ever more difficult formatting decisions, and can easily spread one line across multiple lines, leaving it impossible to know where to place the comment. Comments on their own line preceding such code, while still imprecise, are at least less confusing over the course of such refactorings. Use the using -based type alias syntax instead of typedef . Don't use using to support unqualified lookup on std types; for example, using std::vector; . This also applies to other short namespaces, particularly llvm and clang . Writing std:: gives clearer diagnostics and avoids any possible ambiguity, particularly for ADL. An exception is made for functions like std::swap that are intentionally called using ADL. This pattern should be written as { using std::swap; swap(thing1, thing2); } . Follow the rules for initialization outlined in Abseil's tip #88 . To summarize, omitting some details from the article: Use assignment syntax ( = ) when initializing directly with the intended value (or with a braced initializer directly specifying that value). Use the traditional constructor syntax (with parentheses) when the initialization is performing some active logic, rather than simply composing values together. Use {} initialization without the = only if the above options don't compile. Never mix {} initialization and auto . Always use braces for conditional, switch , and loop statements, even when the body is a single statement. Within a switch statement, use braces after a case label when necessary to create a scope for a variable. Always break the line immediately after an open brace except for empty loop bodies. For internal linkage of definitions of functions and variables, prefer static over anonymous namespaces. static minimizes the context necessary to notice the internal linkage of a definition. Anonymous namespaces are still necessary for classes and enums. Tests are an exception and should typically be wrapped with namespace Carbon::Testing { namespace { ... } } to keep everything internal.","title":"Syntax and formatting"},{"location":"project/cpp_style_guide/#copyable-and-movable-types","text":"Types should have value semantics and support both move and copy where possible. Types that cannot be copied should still be movable where possible. If supported, moving should be as efficient as possible.","title":"Copyable and movable types"},{"location":"project/cpp_style_guide/#static-and-global-variables","text":"Global and static variables, whether at file, class, or function scope, should be declared constexpr .","title":"Static and global variables"},{"location":"project/cpp_style_guide/#foundational-libraries-and-data-types","text":"Generally prefer LLVM libraries and data structures to standard C++ ones. These are optimized significantly for performance, especially when used without exception handling or safety requirements, and when used in patterns that tend to occur while building compilers. They also minimize the vocabulary type friction when using actual LLVM and Clang APIs. Do not add third-party library dependencies to any code that might conceivably be used as part of the compiler or runtime. Compilers and runtime libraries have unique constraints on their licensing. For simplicity, we want all transitive dependencies of these layers to be under the LLVM license that the Carbon project as a whole uses (as well as LLVM itself).","title":"Foundational libraries and data types"},{"location":"project/cpp_style_guide/#suggested-clang-format-contents","text":"See this repository's .clang-format file .","title":"Suggested .clang-format contents"},{"location":"project/design_style_guide/","text":"Language design style guide Table of contents Background General Linking Document structure Overview and detailed design Alternatives considered References Background The language design documentation in the Carbon project should use a consistent style and tone, and should read as if it were written by a single author. This document describes structural, stylistic, and formatting conventions for the language design, where they have been established. General The language design documentation follows the style conventions for Carbon documentation. Linking Links to issues and to complete proposals should use the text #nnnn , where nnnn is the issue number, optionally followed by the proposal title, and should link to the issue or pull request on GitHub. For example, [#123: widget painting](https://github.com/carbon-language/carbon-lang/pull/123) . Links to specific sections of a proposal should link to the repository copy of the proposal file, using the section title or other appropriate link text. For example, [Painting details](/proposals/p0123.md#painting-details) Document structure Documents within the language design should usually be divided into the following sections, with suitable level-two ( ## ) headings: Table of contents (auto-generated) TODO (optional) Overview Zero or more detailed design sections Alternatives considered References Overview and detailed design The overview should describe the high-level concepts of this area of the design, following BLUF principles. Where the overview does not fully cover the detailed design, additional sections can be added as needed to more completely describe the design. The aim of these sections is to describe the design choices that have been made, how those choices fit into the overall design of Carbon, the rationale for those choices, and how and why those choices differ from other languages to which Carbon is likely to be compared, particularly C++, Rust, and Swift. Alternatives considered This section should provide bullet points briefly describing alternative designs that were considered, along with references to the proposals in which those designs were discussed. For example: - [Paint widgets from bottom to top](/proposals/p0123.md#alternatives-considered). References This section should provide bullet points linking to the following: External documents providing background on the topic or additional useful information. Each proposal that contributed to the design described in this document. For example: - [Wikipedia example page](https://en.wikipedia.org/wiki/Wikipedia:Example) - Proposal [#123: widget painting](https://github.com/carbon-language/carbon-lang/pull/123). Links to related parts of the design should be included inline, where relevant, not in the references section.","title":"Language design style guide"},{"location":"project/design_style_guide/#language-design-style-guide","text":"","title":"Language design style guide"},{"location":"project/design_style_guide/#table-of-contents","text":"Background General Linking Document structure Overview and detailed design Alternatives considered References","title":"Table of contents"},{"location":"project/design_style_guide/#background","text":"The language design documentation in the Carbon project should use a consistent style and tone, and should read as if it were written by a single author. This document describes structural, stylistic, and formatting conventions for the language design, where they have been established.","title":"Background"},{"location":"project/design_style_guide/#general","text":"The language design documentation follows the style conventions for Carbon documentation.","title":"General"},{"location":"project/design_style_guide/#linking","text":"Links to issues and to complete proposals should use the text #nnnn , where nnnn is the issue number, optionally followed by the proposal title, and should link to the issue or pull request on GitHub. For example, [#123: widget painting](https://github.com/carbon-language/carbon-lang/pull/123) . Links to specific sections of a proposal should link to the repository copy of the proposal file, using the section title or other appropriate link text. For example, [Painting details](/proposals/p0123.md#painting-details)","title":"Linking"},{"location":"project/design_style_guide/#document-structure","text":"Documents within the language design should usually be divided into the following sections, with suitable level-two ( ## ) headings: Table of contents (auto-generated) TODO (optional) Overview Zero or more detailed design sections Alternatives considered References","title":"Document structure"},{"location":"project/design_style_guide/#overview-and-detailed-design","text":"The overview should describe the high-level concepts of this area of the design, following BLUF principles. Where the overview does not fully cover the detailed design, additional sections can be added as needed to more completely describe the design. The aim of these sections is to describe the design choices that have been made, how those choices fit into the overall design of Carbon, the rationale for those choices, and how and why those choices differ from other languages to which Carbon is likely to be compared, particularly C++, Rust, and Swift.","title":"Overview and detailed design"},{"location":"project/design_style_guide/#alternatives-considered","text":"This section should provide bullet points briefly describing alternative designs that were considered, along with references to the proposals in which those designs were discussed. For example: - [Paint widgets from bottom to top](/proposals/p0123.md#alternatives-considered).","title":"Alternatives considered"},{"location":"project/design_style_guide/#references","text":"This section should provide bullet points linking to the following: External documents providing background on the topic or additional useful information. Each proposal that contributed to the design described in this document. For example: - [Wikipedia example page](https://en.wikipedia.org/wiki/Wikipedia:Example) - Proposal [#123: widget painting](https://github.com/carbon-language/carbon-lang/pull/123). Links to related parts of the design should be included inline, where relevant, not in the references section.","title":"References"},{"location":"project/difficulties_improving_cpp/","text":"Difficulties improving C++ C++ is the dominant programming language for the performance critical software our goals prioritize. The most direct way to deliver a modern and excellent developer experience for those use cases and developers would be to improve C++. Improving C++ to deliver the kind of experience developers expect from a programming language today is difficult in part because C++ has decades of technical debt accumulated in the design of the language. It inherited the legacy of C, including textual preprocessing and inclusion . At the time, this was essential to C++'s success by giving it instant and high quality access to a large C ecosystem. However, over time this has resulted in significant technical debt ranging from integer promotion rules to complex syntax with \" the most vexing parse \". C++ has also prioritized backwards compatibility including both syntax and ABI . This is heavily motivated by preserving its access to existing C/C++ ecosystems, and forms one of the foundations of common Linux package management approaches. A consequence is that rather than changing or replacing language designs to simplify and improve the language, features have overwhelmingly been added over time. This both creates technical debt due to complicated feature interaction, and fails to benefit from on cleanup opportunities in the form of replacing or removing legacy features. Carbon is exploring significant backwards incompatible changes. It doesn't inherit the legacy of C or C++ directly, and instead is starting with solid foundations, like a modern generics system, modular code organization, and consistent, simple syntax. Then, it builds a simplified and improved language around those foundational components that remains both interoperable with and migratable from C++, while giving up transparent backwards compatibility. This is fundamentally a successor language approach , rather than an attempt to incrementally evolve C++ to achieve these improvements. Another challenge to improving C++ in these ways is the current evolution process and direction. A key example of this is the committee's struggle to converge on a clear set of high-level and long-term goals and priorities aligned with ours . When pushed to address the technical debt caused by not breaking the ABI , C++'s process did not reach any definitive conclusion . This both failed to meaningfully change C++'s direction and priorities towards improvements rather than backwards compatibility, and demonstrates how the process can fail to make directional decisions. Beyond C++'s evolution direction, the mechanics of the process also make improving C++ difficult. C++'s process is oriented around standardization rather than design : it uses a multiyear waterfall committee process. Access to the committee and standard is restricted and expensive, attendance is necessary to have a voice, and decisions are made by live votes of those present. The committee structure is designed to ensure representation of nations and companies, rather than building an inclusive and welcoming team and community of experts and people actively contributing to the language. Carbon has a more accessible and efficient evolution process built on open-source principles, processes, and tools. Throughout the project, we explicitly and clearly lay out our goals and priorities and how those directly shape our decisions. We also have a clear governance structure that can make decisions rapidly when needed. The open-source model enables the Carbon project to expand its scope beyond just the language. We will build a holistic collection of tools that provide a rich developer experience, ranging from the compiler and standard library to IDE tools and more. We will even try to close a huge gap in the C++ ecosystem with a built-in package manager. Carbon is particularly focused on a specific set of goals . These will not align with every user of C++, but have significant interest across a wide range of users that are capable and motivated to evolve and modernize their codebase. Given the difficulties posed by C++'s technical debt, sustained priority of backwards compatibility, and evolution process, we wanted to explore an alternative approach to achieve these goals -- through a backwards-incompatible successor language, designed with robust support for interoperability with and migration from C++. We hope other efforts to incrementally improve C++ continue, and would love to share ideas where we can.","title":"Difficulties improving C++"},{"location":"project/difficulties_improving_cpp/#difficulties-improving-c","text":"C++ is the dominant programming language for the performance critical software our goals prioritize. The most direct way to deliver a modern and excellent developer experience for those use cases and developers would be to improve C++. Improving C++ to deliver the kind of experience developers expect from a programming language today is difficult in part because C++ has decades of technical debt accumulated in the design of the language. It inherited the legacy of C, including textual preprocessing and inclusion . At the time, this was essential to C++'s success by giving it instant and high quality access to a large C ecosystem. However, over time this has resulted in significant technical debt ranging from integer promotion rules to complex syntax with \" the most vexing parse \". C++ has also prioritized backwards compatibility including both syntax and ABI . This is heavily motivated by preserving its access to existing C/C++ ecosystems, and forms one of the foundations of common Linux package management approaches. A consequence is that rather than changing or replacing language designs to simplify and improve the language, features have overwhelmingly been added over time. This both creates technical debt due to complicated feature interaction, and fails to benefit from on cleanup opportunities in the form of replacing or removing legacy features. Carbon is exploring significant backwards incompatible changes. It doesn't inherit the legacy of C or C++ directly, and instead is starting with solid foundations, like a modern generics system, modular code organization, and consistent, simple syntax. Then, it builds a simplified and improved language around those foundational components that remains both interoperable with and migratable from C++, while giving up transparent backwards compatibility. This is fundamentally a successor language approach , rather than an attempt to incrementally evolve C++ to achieve these improvements. Another challenge to improving C++ in these ways is the current evolution process and direction. A key example of this is the committee's struggle to converge on a clear set of high-level and long-term goals and priorities aligned with ours . When pushed to address the technical debt caused by not breaking the ABI , C++'s process did not reach any definitive conclusion . This both failed to meaningfully change C++'s direction and priorities towards improvements rather than backwards compatibility, and demonstrates how the process can fail to make directional decisions. Beyond C++'s evolution direction, the mechanics of the process also make improving C++ difficult. C++'s process is oriented around standardization rather than design : it uses a multiyear waterfall committee process. Access to the committee and standard is restricted and expensive, attendance is necessary to have a voice, and decisions are made by live votes of those present. The committee structure is designed to ensure representation of nations and companies, rather than building an inclusive and welcoming team and community of experts and people actively contributing to the language. Carbon has a more accessible and efficient evolution process built on open-source principles, processes, and tools. Throughout the project, we explicitly and clearly lay out our goals and priorities and how those directly shape our decisions. We also have a clear governance structure that can make decisions rapidly when needed. The open-source model enables the Carbon project to expand its scope beyond just the language. We will build a holistic collection of tools that provide a rich developer experience, ranging from the compiler and standard library to IDE tools and more. We will even try to close a huge gap in the C++ ecosystem with a built-in package manager. Carbon is particularly focused on a specific set of goals . These will not align with every user of C++, but have significant interest across a wide range of users that are capable and motivated to evolve and modernize their codebase. Given the difficulties posed by C++'s technical debt, sustained priority of backwards compatibility, and evolution process, we wanted to explore an alternative approach to achieve these goals -- through a backwards-incompatible successor language, designed with robust support for interoperability with and migration from C++. We hope other efforts to incrementally improve C++ continue, and would love to share ideas where we can.","title":"Difficulties improving C++"},{"location":"project/evolution/","text":"Evolution and governance Table of contents Overview Proposals Life of a proposal Proposal roles Proposal authors Community Active contributors Carbon leads When to write a proposal Proposal PRs What goes in the proposal document Open questions Review and RFC on proposal PRs Blocking issues Discussion on blocking issues Governance structure Carbon leads Subteams Painter Adding and removing governance members Acknowledgements Overview Carbon's evolution process uses proposals to evaluate and approve significant changes to the project or language. This process is designed to: Ensure these kinds of changes can receive feedback from the entire community. Resolve questions and decide direction efficiently. Create a clear log of rationale for why the project and language have evolved in particular directions. When there are questions, concerns, or issues with a proposal that need to be resolved, Carbon uses its governance system of Carbon leads to decide how to move forward. Leads are fundamentally responsible for encouraging Carbon's ongoing and healthy evolution and so also take on the critical steps of the evolution process for proposals. Proposals These are primarily structured as GitHub pull requests that use a somewhat more formal document structure and process to ensure changes to the project or language are well explained, justified, and reviewed by the community. Life of a proposal Proposals consist of a PR (pull request) in GitHub that adds a document to the proposals/ directory following the template . Proposal PRs start out in draft mode. When proposal PRs are ready, click on \"Ready for review\" , and change the Proposals project column to \"RFC\". The project column should be available as a dropdown under \"Projects\" on the PR. This will result in a Carbon lead being assigned to review the PR. This also signifies an RFC (request for comment) from the entire community. Contributors are encouraged to react with a thumbs-up to proposal PRs if they are generally interested and supportive of the high level direction based on title and summary. Similarly, other reactions are encouraged to help surface contributor's sentiment. We use GitHub issues to discuss and track blocking issues with proposals, such as open questions or alternative approaches that may need further consideration. These are assigned to carbon-leads to decide. A Carbon lead will be assigned to a proposal PR. They are responsible for the basic review (or delegating that) as well as ultimately approving the PR. The assigned lead should ensure that there is a reasonable degree of consensus among the contributors outside of the identified blocking issues. Contributors should have a reasonable chance to raise concerns, and where needed they should become blocking issues. Community consensus isn't intended to be perfect though, and is ultimately a judgement call by the lead. When things are missed or mistakes are made here, we should just revert or fix-forward as usual. Once a reasonable degree of community consensus is reached and the assigned lead finishes code review, the lead should approve the PR. Any outstanding high-level concerns should be handled with blocking issues. Optionally, the assigned lead can file a blocking issue for a one week final comment period when they approve. This is rarely needed, and only when it is both useful and important for the proposal to give extra time for community comments. The leads are responsible for resolving any blocking issues for a proposal PR, including the one week comment period where resolving it indicates comments arrived which require the proposal to undergo further review. The proposal PR can be merged once the assigned lead approves, all blocking issues have been decided, and any related decisions are incorporated. If the leads choose to defer or reject the proposal, the reviewing lead should explain why and close the PR. Proposal roles It is also useful to see what the process looks like for different roles within the community. These perspectives are also the most critical to keep simple and easily understood. Proposal authors For proposal authors, this should feel like a code review, with some broken out issues for longer discussion: Create a proposal document and draft PR following the template . new_proposal.py helps create templated PRs. If you have open questions, filing blocking issues while preparing the PR can help resolve them quickly. When ready, click on \"Ready for review\" in GitHub, and change the Proposals project column to \"RFC\". The project column should be available as a dropdown under \"Projects\" on the PR. This will result in the PR being assigned to an individual for review. This will also send the proposal as a broad RFC to the community. While setting the \"RFC\" project column will also add the \"RFC\" label, the reverse is not true. Unfortunately, this is currently a limit of GitHub automation. Similarly, Adding the \"RFC\" project column currently does not set \"Ready for review\". Address comments where you can and they make sense. If you don't see an obvious way to address comments, that's OK. It's great to engage a bit with the commenter to clarify their comment or why you don't see an obvious way to address it, just like you would in code review . If the commenter feels this is important, they can move it to a blocking issue for a longer discussion and resolution from the leads. You don't need to try to resolve everything yourself. Incorporate any changes needed based on the resolution of blocking issues. Once the leads have provided a resolution, it's important to make progress with that direction. When you both have approval from the assigned lead and the last blocking issue is addressed, merge! If you end up making significant changes when incorporating resolved issues after the approval from the assigned lead, circle back for a fresh approval before merging, just like you would with code review. Community We use the Proposals dashboard to track proposals that are in RFC. Anyone that is interested can participate once a proposal is ready for review and in RFC. It's OK to only comment when particularly interested in a proposal, or when asked by one of the leads to help ensure thorough review. Not everyone needs to participate heavily in every RFC. PRs that are in \"draft\" status in GitHub are considered works-in-progress. Check with the author before spending time reviewing these, and generally avoid distracting the author with comments unless they ask for them. The proposal may be actively undergoing edits. Read the proposal and leave comments to try to help make the proposal an improvement for Carbon. Note that progress and improvement are more important than perfection here! Try to make comments on proposals constructive . Suggest how the proposal could be better if at all possible. If there is an open question or a critical blocking issue that needs to get resolved, move it to its own issue that the PR depends on, and focus the discussion there. The issue should focus on surfacing the important aspects of the tradeoff represented by the issue or open question, not on advocacy. Active contributors Everyone actively contributing to the evolution of Carbon should try to regularly: Give a thumbs-up or other reaction on any interesting PRs out for RFC to help surface the community's sentiment around the high level idea or direction. Don't worry about \"approving\" or the detailed text of the proposal here. If interested and time permitting, dive into some RFCs and provide community feedback . Carbon leads Carbon leads are responsible for making decisions rapidly and ensuring proposal PRs land: Rapidly resolve all blocking issues raised across any proposals. When assigned a specific proposal PR: Make sure it gets both constructive general comments and good code review. Ideally, you should directly participate in the code review, but it's fine to ask others to help. However, ultimately you have to review and approve the PR. Escalate any blocking issues without a resolution that are slowing down the proposal to the other leads. Evaluate whether the community has had a reasonable chance to raise concerns and there is sufficient consensus to move forward given the decisions on the blocking issues. This doesn't need to be perfect though. Here too, we prioritize progress over perfection. We can revert or fix-forward mistakes whenever necessary, especially for low-risk changes. In rare cases, an extended final comment period can be used when warranted for a proposal. Once ready, approve and help the author merge the proposal. When to write a proposal Any substantive change to Carbon -- whether the language, project, infrastructure, or otherwise -- should be done through an evolution proposal. The meaning of \"substantive\" is subjective, but will generally include: Any semantic or syntactic language change that isn't fixing a bug. Major changes to project infrastructure, including additions and removals. Rolling back an accepted proposal, even if never executed. Changes which generally will not require a proposal are: Fixing typos or bugs that don't change the meaning or intent. Rephrasing or refactoring documentation for easier reading. Minor infrastructure updates, improvements, setting changes, tweaks. If you're not sure whether to write a proposal, please err on the side of writing a proposal. A team can always ask for a change to be made directly if they believe it doesn't need review. Conversely, a reviewer might also ask that a pull request instead go through the full evolution process. Proposal PRs A proposal PR should use the proposal label, have a descriptive title, and easily understood initial summary comment. Authors and leads are encouraged to edit both as necessary to ensure they give the best high-level understanding of the proposal possible. A proposal PR will include a \"P-numbered\" proposal document , proposals/pNNNN.md , where NNNN is the pull request number. This file should be based on the proposal template file . When writing a proposal, try to keep it brief and focused to maximize the community's engagement in it. Beyond the above structure, try to use Inverted Pyramid or BLUF writing style to help readers rapidly skim the material. What goes in the proposal document The purpose of the proposal document is to present the case for deciding to adopt the proposal. Any information that feeds into making that decision, and that should not be maintained as part of our living design documentation, belongs in the proposal document. This includes background material to introduce the problem, comparisons to any alternative designs that were considered and any other current proposals in the same area, records of informal polls taken to determine community preferences, and rationale for the decision based on the project's goals. The proposal PR can contain related changes to the Carbon project, such as updates to the design documentation. Those changes form part of the proposal, and need not be additionally described in the proposal document beyond a mention in the \"Proposal\" section that such changes exist. For example: ## Proposal See the proposed changes to the design documents. Readers of proposals are expected to consult the PR or the git commit that merged the PR in order to understand the proposed changes. The author of a proposal is not required to include changes to the design documentation as part of a proposal, and it may in some cases be preferable to decouple the proposal process from updating the design. When accepted, the proposal would then be implemented through a series of future PRs to the rest of the project, and the proposal document should describe what is being proposed in enough detail to validate that those future PRs properly implement the proposed direction. Open questions Feel free to factor out open questions in a proposal to issues that you assign to the leads to resolve. You can even do this before sending the proposal for review. Even after it's resolved, an open question issue can be reopened if new information comes up during the RFC. When opening issues, add them to the \"Issues for leads\" project under \"Questions\". Carbon leads use this to locate and prioritize the issue for resolution. Review and RFC on proposal PRs When a proposal PR is assigned to the carbon-leads GitHub group , one of them will be assigned the PR. They are responsible for helping land that proposal, or explaining why the project won't move forward in that direction. The assigned lead is also ultimately responsible for the code review on the PR. Proposals sent for review are also sent as an RFC to the entire community. All active Carbon contributors are strongly encouraged to regularly skim the title and summary comment of proposals under RFC that are interesting to them. They should use GitHub reactions, including at least a thumbs-up, to show their interest and enthusiasm about the proposal, and help encourage the author. Writing proposals is extremely hard work , and we need to clearly show both interest in the proposed direction of Carbon and appreciation for the work put into the proposal. This is not about approving the proposal, or any of its details. It is completely fine and coherent to both give a thumbs-up to a proposal and provide a serious, blocking issue that needs to be resolved. Anyone in the community is welcome to participate in the RFC in detail if interested. However, not everyone needs to participate in every RFC. If a proposal is already getting actively and thoroughly reviewed, feel free to focus your time on other proposals with fewer commenters. Even if there are issues or problems discovered later, we can always fix them with follow-up proposals. Both code review and high-level design comments are welcome. If an open question comes up or a high-level blocking issue is uncovered, feel free to move it to its own GitHub issue and assign it to the leads to resolve. That issue is also a good place to focus discussion on that specific topic rather than the main PR. The assigned lead should approve proposals once the following criteria are met: It looks good from a code review perspective. At least three thumbs-up reactions showing general community interest. The community has had a sufficient opportunity to review the proposed change, given its scope and complexity. Any remaining blocking issues are reasonably likely to resolve in a way that allows the proposal to move forward. It is fine if some are not fully decided, but a lead shouldn't approve a proposal that's unlikely to move forward. The last two criteria are fundamentally judgement calls for the lead to make, and we don't try to formulate a rigid or fixed bar for them. If resolving the blocking issues requires significant changes, the author should also get a fresh approval from the assigned lead after those changes, just like they would with code review. The assigned lead may also request a final comment period for the community when approving. This signals to the community that the proposal is likely to be merged once the blocking issues are resolved, and any remaining concerns need to be surfaced. The goal is to help uncover concerns that were hidden until it was clear that the proposal is likely to move forward. However, requesting a final comment period is not the default; the assigned lead should only do this when there is some reason to expect further community comment is especially important to solicit. Common cases to consider are contentious, complex, or dramatic changes to the language or project. Ultimately, whether this is important is a judgement call for the lead. This will be modeled by filing a blocking issue that resolves in one week when approving. This issue will also explain the motivation for requesting a final comment period. Blocking issues We use blocking GitHub issues to track open questions or other discussions that the leads are asked to resolve. Any time a blocking issue is filed, that issue forms both the primary discussion thread and where the leads signal how it is resolved. We use issues both to track that there is a specific resolution expected and that there may be dependencies. We add blocking issues to the \"Issues for leads\" project under \"Blocking issues\". Carbon leads use this to locate and prioritize the issue for resolution. These issues can be created at any time and by any one. Issues can be created while the proposal is being drafted in order to help inform specific content that should go into the proposal. It is even fine to create an issue first, even before a proposal exists, as an open question about whether to produce a particular proposal, or what a proposal that is being planned should say. For issues which don't (yet) have a specific proposal PR associated with them, at some point the leads may ask that a proposal be created to help collect in a more cohesive place a written overview of the issue and related information, but this process need not be strictly or rigidly bound to having proposal text. Avoid using issues for things that are just requests or suggestions on a proposal PR. If in doubt, start off with a simple comment on the PR and see if there is any disagreement -- everyone may already be aligned and agree. When a comment does seem worth turning into an issue, don't worry about that as the author or the commenter. Getting the leads to resolve disagreement isn't a bad thing for anyone involved. This should be seen as a friendly way to move the discussion out to its own forum where it'll get resolved, and focus the PR on improving the proposal and getting it ready to merge. When an issue is created from a discussion on a PR, and after the discussion on the issue all the original parties come to a happy agreement, it's totally OK to close the issue and move back to the code review in the PR. Anyone who would prefer the leads to still chime in can re-open the issue and the leads will follow up, even if it's only to get confirmation that everyone did end up happy with the resolution. At the end of the day, while it's fine to resolve an issue that everyone actually ended up agreeing about (maybe once some confusion is addressed), ultimately the leads are responsible for resolving these issues and there is no pressure on anyone else to do so. Discussion on blocking issues Discussion on these issues, especially contentious ones, should endeavor to focus on surfacing information and highlighting the nature of the tradeoff implied by the decisions available. This is in contrast to focusing on advocacy or persuasion. The goal of the issues shouldn't be to persuade or convince the leads to make a specific decision, but to give the leads the information they need to make the best decision for Carbon. It's fine that some people have a specific belief of which decision would be best; however, framing their contributions to the discussion as surfacing the information that underpins that belief will make the discussion more constructive, welcoming, and effective. Overall, everyone should strive to focus on data-based arguments to the extent they can, minimizing their use of appeals to emotion or excessive rhetoric. None of this should preclude gathering information like polls of opinion among groups, or signaling agreement. Where community members stand and how many agree with that stance on any issue is information, and useful to surface. Governance structure Carbon leads Carbon leads are responsible for reviewing proposals and setting Carbon's roadmap and managing evolution. This team should broadly understand both the users of Carbon and the project itself in order to factor different needs, concerns, and pressures into a consensus decision-making process . While leads may approve proposals individually, they should decide on issues raised to them using blocking consensus with a quorum of two. Carbon's current leads are: chandlerc KateGregory zygoloid Subteams As Carbon grows, the leads may decide to form subteams that provide leadership for specific areas. These subteams are expected to largely organize in a similar fashion to the Carbon leads, with a more narrow focus and scope. Subteam decisions may be escalated to the Carbon leads. Painter Whenever possible, we want Carbon to make syntax and other decisions based on understanding its users, data, and the underlying goals of the language. However, there will be times when those don't provide a clear cut rationale for any particular decision -- all of the options are fine/good and someone simply needs to choose which color to paint the bikeshed. The goal of the painter role is to have a simple way to quickly decide these points. Leads and teams may defer a decision to the painter if there is a consensus that it is merely a bikeshed in need of paint. They may also open an issue to revisit the color with data and/or user studies of some kind. This allows progress to be unblocked while also ensuring we return to issues later and attempt to find more definite rationale. The painter is a single person in order to keep decisions around taste or aesthetics reasonably consistent. The current painter is: chandlerc Adding and removing governance members Any member of Carbon governance may step down or be replaced when they are no longer able to contribute effectively. The Carbon leads can nominate and decide on adding, removing, or replacing members using the usual evolution processes. Acknowledgements Our governance and evolution process is influenced by the Rust , Swift , and C++ processes. Many thanks to these communities for providing a basis.","title":"Evolution and governance"},{"location":"project/evolution/#evolution-and-governance","text":"","title":"Evolution and governance"},{"location":"project/evolution/#table-of-contents","text":"Overview Proposals Life of a proposal Proposal roles Proposal authors Community Active contributors Carbon leads When to write a proposal Proposal PRs What goes in the proposal document Open questions Review and RFC on proposal PRs Blocking issues Discussion on blocking issues Governance structure Carbon leads Subteams Painter Adding and removing governance members Acknowledgements","title":"Table of contents"},{"location":"project/evolution/#overview","text":"Carbon's evolution process uses proposals to evaluate and approve significant changes to the project or language. This process is designed to: Ensure these kinds of changes can receive feedback from the entire community. Resolve questions and decide direction efficiently. Create a clear log of rationale for why the project and language have evolved in particular directions. When there are questions, concerns, or issues with a proposal that need to be resolved, Carbon uses its governance system of Carbon leads to decide how to move forward. Leads are fundamentally responsible for encouraging Carbon's ongoing and healthy evolution and so also take on the critical steps of the evolution process for proposals.","title":"Overview"},{"location":"project/evolution/#proposals","text":"These are primarily structured as GitHub pull requests that use a somewhat more formal document structure and process to ensure changes to the project or language are well explained, justified, and reviewed by the community.","title":"Proposals"},{"location":"project/evolution/#life-of-a-proposal","text":"Proposals consist of a PR (pull request) in GitHub that adds a document to the proposals/ directory following the template . Proposal PRs start out in draft mode. When proposal PRs are ready, click on \"Ready for review\" , and change the Proposals project column to \"RFC\". The project column should be available as a dropdown under \"Projects\" on the PR. This will result in a Carbon lead being assigned to review the PR. This also signifies an RFC (request for comment) from the entire community. Contributors are encouraged to react with a thumbs-up to proposal PRs if they are generally interested and supportive of the high level direction based on title and summary. Similarly, other reactions are encouraged to help surface contributor's sentiment. We use GitHub issues to discuss and track blocking issues with proposals, such as open questions or alternative approaches that may need further consideration. These are assigned to carbon-leads to decide. A Carbon lead will be assigned to a proposal PR. They are responsible for the basic review (or delegating that) as well as ultimately approving the PR. The assigned lead should ensure that there is a reasonable degree of consensus among the contributors outside of the identified blocking issues. Contributors should have a reasonable chance to raise concerns, and where needed they should become blocking issues. Community consensus isn't intended to be perfect though, and is ultimately a judgement call by the lead. When things are missed or mistakes are made here, we should just revert or fix-forward as usual. Once a reasonable degree of community consensus is reached and the assigned lead finishes code review, the lead should approve the PR. Any outstanding high-level concerns should be handled with blocking issues. Optionally, the assigned lead can file a blocking issue for a one week final comment period when they approve. This is rarely needed, and only when it is both useful and important for the proposal to give extra time for community comments. The leads are responsible for resolving any blocking issues for a proposal PR, including the one week comment period where resolving it indicates comments arrived which require the proposal to undergo further review. The proposal PR can be merged once the assigned lead approves, all blocking issues have been decided, and any related decisions are incorporated. If the leads choose to defer or reject the proposal, the reviewing lead should explain why and close the PR.","title":"Life of a proposal"},{"location":"project/evolution/#proposal-roles","text":"It is also useful to see what the process looks like for different roles within the community. These perspectives are also the most critical to keep simple and easily understood.","title":"Proposal roles"},{"location":"project/evolution/#proposal-authors","text":"For proposal authors, this should feel like a code review, with some broken out issues for longer discussion: Create a proposal document and draft PR following the template . new_proposal.py helps create templated PRs. If you have open questions, filing blocking issues while preparing the PR can help resolve them quickly. When ready, click on \"Ready for review\" in GitHub, and change the Proposals project column to \"RFC\". The project column should be available as a dropdown under \"Projects\" on the PR. This will result in the PR being assigned to an individual for review. This will also send the proposal as a broad RFC to the community. While setting the \"RFC\" project column will also add the \"RFC\" label, the reverse is not true. Unfortunately, this is currently a limit of GitHub automation. Similarly, Adding the \"RFC\" project column currently does not set \"Ready for review\". Address comments where you can and they make sense. If you don't see an obvious way to address comments, that's OK. It's great to engage a bit with the commenter to clarify their comment or why you don't see an obvious way to address it, just like you would in code review . If the commenter feels this is important, they can move it to a blocking issue for a longer discussion and resolution from the leads. You don't need to try to resolve everything yourself. Incorporate any changes needed based on the resolution of blocking issues. Once the leads have provided a resolution, it's important to make progress with that direction. When you both have approval from the assigned lead and the last blocking issue is addressed, merge! If you end up making significant changes when incorporating resolved issues after the approval from the assigned lead, circle back for a fresh approval before merging, just like you would with code review.","title":"Proposal authors"},{"location":"project/evolution/#community","text":"We use the Proposals dashboard to track proposals that are in RFC. Anyone that is interested can participate once a proposal is ready for review and in RFC. It's OK to only comment when particularly interested in a proposal, or when asked by one of the leads to help ensure thorough review. Not everyone needs to participate heavily in every RFC. PRs that are in \"draft\" status in GitHub are considered works-in-progress. Check with the author before spending time reviewing these, and generally avoid distracting the author with comments unless they ask for them. The proposal may be actively undergoing edits. Read the proposal and leave comments to try to help make the proposal an improvement for Carbon. Note that progress and improvement are more important than perfection here! Try to make comments on proposals constructive . Suggest how the proposal could be better if at all possible. If there is an open question or a critical blocking issue that needs to get resolved, move it to its own issue that the PR depends on, and focus the discussion there. The issue should focus on surfacing the important aspects of the tradeoff represented by the issue or open question, not on advocacy.","title":"Community"},{"location":"project/evolution/#active-contributors","text":"Everyone actively contributing to the evolution of Carbon should try to regularly: Give a thumbs-up or other reaction on any interesting PRs out for RFC to help surface the community's sentiment around the high level idea or direction. Don't worry about \"approving\" or the detailed text of the proposal here. If interested and time permitting, dive into some RFCs and provide community feedback .","title":"Active contributors"},{"location":"project/evolution/#carbon-leads","text":"Carbon leads are responsible for making decisions rapidly and ensuring proposal PRs land: Rapidly resolve all blocking issues raised across any proposals. When assigned a specific proposal PR: Make sure it gets both constructive general comments and good code review. Ideally, you should directly participate in the code review, but it's fine to ask others to help. However, ultimately you have to review and approve the PR. Escalate any blocking issues without a resolution that are slowing down the proposal to the other leads. Evaluate whether the community has had a reasonable chance to raise concerns and there is sufficient consensus to move forward given the decisions on the blocking issues. This doesn't need to be perfect though. Here too, we prioritize progress over perfection. We can revert or fix-forward mistakes whenever necessary, especially for low-risk changes. In rare cases, an extended final comment period can be used when warranted for a proposal. Once ready, approve and help the author merge the proposal.","title":"Carbon leads"},{"location":"project/evolution/#when-to-write-a-proposal","text":"Any substantive change to Carbon -- whether the language, project, infrastructure, or otherwise -- should be done through an evolution proposal. The meaning of \"substantive\" is subjective, but will generally include: Any semantic or syntactic language change that isn't fixing a bug. Major changes to project infrastructure, including additions and removals. Rolling back an accepted proposal, even if never executed. Changes which generally will not require a proposal are: Fixing typos or bugs that don't change the meaning or intent. Rephrasing or refactoring documentation for easier reading. Minor infrastructure updates, improvements, setting changes, tweaks. If you're not sure whether to write a proposal, please err on the side of writing a proposal. A team can always ask for a change to be made directly if they believe it doesn't need review. Conversely, a reviewer might also ask that a pull request instead go through the full evolution process.","title":"When to write a proposal"},{"location":"project/evolution/#proposal-prs","text":"A proposal PR should use the proposal label, have a descriptive title, and easily understood initial summary comment. Authors and leads are encouraged to edit both as necessary to ensure they give the best high-level understanding of the proposal possible. A proposal PR will include a \"P-numbered\" proposal document , proposals/pNNNN.md , where NNNN is the pull request number. This file should be based on the proposal template file . When writing a proposal, try to keep it brief and focused to maximize the community's engagement in it. Beyond the above structure, try to use Inverted Pyramid or BLUF writing style to help readers rapidly skim the material.","title":"Proposal PRs"},{"location":"project/evolution/#what-goes-in-the-proposal-document","text":"The purpose of the proposal document is to present the case for deciding to adopt the proposal. Any information that feeds into making that decision, and that should not be maintained as part of our living design documentation, belongs in the proposal document. This includes background material to introduce the problem, comparisons to any alternative designs that were considered and any other current proposals in the same area, records of informal polls taken to determine community preferences, and rationale for the decision based on the project's goals. The proposal PR can contain related changes to the Carbon project, such as updates to the design documentation. Those changes form part of the proposal, and need not be additionally described in the proposal document beyond a mention in the \"Proposal\" section that such changes exist. For example: ## Proposal See the proposed changes to the design documents. Readers of proposals are expected to consult the PR or the git commit that merged the PR in order to understand the proposed changes. The author of a proposal is not required to include changes to the design documentation as part of a proposal, and it may in some cases be preferable to decouple the proposal process from updating the design. When accepted, the proposal would then be implemented through a series of future PRs to the rest of the project, and the proposal document should describe what is being proposed in enough detail to validate that those future PRs properly implement the proposed direction.","title":"What goes in the proposal document"},{"location":"project/evolution/#open-questions","text":"Feel free to factor out open questions in a proposal to issues that you assign to the leads to resolve. You can even do this before sending the proposal for review. Even after it's resolved, an open question issue can be reopened if new information comes up during the RFC. When opening issues, add them to the \"Issues for leads\" project under \"Questions\". Carbon leads use this to locate and prioritize the issue for resolution.","title":"Open questions"},{"location":"project/evolution/#review-and-rfc-on-proposal-prs","text":"When a proposal PR is assigned to the carbon-leads GitHub group , one of them will be assigned the PR. They are responsible for helping land that proposal, or explaining why the project won't move forward in that direction. The assigned lead is also ultimately responsible for the code review on the PR. Proposals sent for review are also sent as an RFC to the entire community. All active Carbon contributors are strongly encouraged to regularly skim the title and summary comment of proposals under RFC that are interesting to them. They should use GitHub reactions, including at least a thumbs-up, to show their interest and enthusiasm about the proposal, and help encourage the author. Writing proposals is extremely hard work , and we need to clearly show both interest in the proposed direction of Carbon and appreciation for the work put into the proposal. This is not about approving the proposal, or any of its details. It is completely fine and coherent to both give a thumbs-up to a proposal and provide a serious, blocking issue that needs to be resolved. Anyone in the community is welcome to participate in the RFC in detail if interested. However, not everyone needs to participate in every RFC. If a proposal is already getting actively and thoroughly reviewed, feel free to focus your time on other proposals with fewer commenters. Even if there are issues or problems discovered later, we can always fix them with follow-up proposals. Both code review and high-level design comments are welcome. If an open question comes up or a high-level blocking issue is uncovered, feel free to move it to its own GitHub issue and assign it to the leads to resolve. That issue is also a good place to focus discussion on that specific topic rather than the main PR. The assigned lead should approve proposals once the following criteria are met: It looks good from a code review perspective. At least three thumbs-up reactions showing general community interest. The community has had a sufficient opportunity to review the proposed change, given its scope and complexity. Any remaining blocking issues are reasonably likely to resolve in a way that allows the proposal to move forward. It is fine if some are not fully decided, but a lead shouldn't approve a proposal that's unlikely to move forward. The last two criteria are fundamentally judgement calls for the lead to make, and we don't try to formulate a rigid or fixed bar for them. If resolving the blocking issues requires significant changes, the author should also get a fresh approval from the assigned lead after those changes, just like they would with code review. The assigned lead may also request a final comment period for the community when approving. This signals to the community that the proposal is likely to be merged once the blocking issues are resolved, and any remaining concerns need to be surfaced. The goal is to help uncover concerns that were hidden until it was clear that the proposal is likely to move forward. However, requesting a final comment period is not the default; the assigned lead should only do this when there is some reason to expect further community comment is especially important to solicit. Common cases to consider are contentious, complex, or dramatic changes to the language or project. Ultimately, whether this is important is a judgement call for the lead. This will be modeled by filing a blocking issue that resolves in one week when approving. This issue will also explain the motivation for requesting a final comment period.","title":"Review and RFC on proposal PRs"},{"location":"project/evolution/#blocking-issues","text":"We use blocking GitHub issues to track open questions or other discussions that the leads are asked to resolve. Any time a blocking issue is filed, that issue forms both the primary discussion thread and where the leads signal how it is resolved. We use issues both to track that there is a specific resolution expected and that there may be dependencies. We add blocking issues to the \"Issues for leads\" project under \"Blocking issues\". Carbon leads use this to locate and prioritize the issue for resolution. These issues can be created at any time and by any one. Issues can be created while the proposal is being drafted in order to help inform specific content that should go into the proposal. It is even fine to create an issue first, even before a proposal exists, as an open question about whether to produce a particular proposal, or what a proposal that is being planned should say. For issues which don't (yet) have a specific proposal PR associated with them, at some point the leads may ask that a proposal be created to help collect in a more cohesive place a written overview of the issue and related information, but this process need not be strictly or rigidly bound to having proposal text. Avoid using issues for things that are just requests or suggestions on a proposal PR. If in doubt, start off with a simple comment on the PR and see if there is any disagreement -- everyone may already be aligned and agree. When a comment does seem worth turning into an issue, don't worry about that as the author or the commenter. Getting the leads to resolve disagreement isn't a bad thing for anyone involved. This should be seen as a friendly way to move the discussion out to its own forum where it'll get resolved, and focus the PR on improving the proposal and getting it ready to merge. When an issue is created from a discussion on a PR, and after the discussion on the issue all the original parties come to a happy agreement, it's totally OK to close the issue and move back to the code review in the PR. Anyone who would prefer the leads to still chime in can re-open the issue and the leads will follow up, even if it's only to get confirmation that everyone did end up happy with the resolution. At the end of the day, while it's fine to resolve an issue that everyone actually ended up agreeing about (maybe once some confusion is addressed), ultimately the leads are responsible for resolving these issues and there is no pressure on anyone else to do so.","title":"Blocking issues"},{"location":"project/evolution/#discussion-on-blocking-issues","text":"Discussion on these issues, especially contentious ones, should endeavor to focus on surfacing information and highlighting the nature of the tradeoff implied by the decisions available. This is in contrast to focusing on advocacy or persuasion. The goal of the issues shouldn't be to persuade or convince the leads to make a specific decision, but to give the leads the information they need to make the best decision for Carbon. It's fine that some people have a specific belief of which decision would be best; however, framing their contributions to the discussion as surfacing the information that underpins that belief will make the discussion more constructive, welcoming, and effective. Overall, everyone should strive to focus on data-based arguments to the extent they can, minimizing their use of appeals to emotion or excessive rhetoric. None of this should preclude gathering information like polls of opinion among groups, or signaling agreement. Where community members stand and how many agree with that stance on any issue is information, and useful to surface.","title":"Discussion on blocking issues"},{"location":"project/evolution/#governance-structure","text":"","title":"Governance structure"},{"location":"project/evolution/#carbon-leads_1","text":"Carbon leads are responsible for reviewing proposals and setting Carbon's roadmap and managing evolution. This team should broadly understand both the users of Carbon and the project itself in order to factor different needs, concerns, and pressures into a consensus decision-making process . While leads may approve proposals individually, they should decide on issues raised to them using blocking consensus with a quorum of two. Carbon's current leads are: chandlerc KateGregory zygoloid","title":"Carbon leads"},{"location":"project/evolution/#subteams","text":"As Carbon grows, the leads may decide to form subteams that provide leadership for specific areas. These subteams are expected to largely organize in a similar fashion to the Carbon leads, with a more narrow focus and scope. Subteam decisions may be escalated to the Carbon leads.","title":"Subteams"},{"location":"project/evolution/#painter","text":"Whenever possible, we want Carbon to make syntax and other decisions based on understanding its users, data, and the underlying goals of the language. However, there will be times when those don't provide a clear cut rationale for any particular decision -- all of the options are fine/good and someone simply needs to choose which color to paint the bikeshed. The goal of the painter role is to have a simple way to quickly decide these points. Leads and teams may defer a decision to the painter if there is a consensus that it is merely a bikeshed in need of paint. They may also open an issue to revisit the color with data and/or user studies of some kind. This allows progress to be unblocked while also ensuring we return to issues later and attempt to find more definite rationale. The painter is a single person in order to keep decisions around taste or aesthetics reasonably consistent. The current painter is: chandlerc","title":"Painter"},{"location":"project/evolution/#adding-and-removing-governance-members","text":"Any member of Carbon governance may step down or be replaced when they are no longer able to contribute effectively. The Carbon leads can nominate and decide on adding, removing, or replacing members using the usual evolution processes.","title":"Adding and removing governance members"},{"location":"project/evolution/#acknowledgements","text":"Our governance and evolution process is influenced by the Rust , Swift , and C++ processes. Many thanks to these communities for providing a basis.","title":"Acknowledgements"},{"location":"project/faq/","text":"Project FAQ Table of contents What is Carbon? What is Carbon's status? How soon can we use Carbon? Why make Carbon public while it's still an experiment? How complete is Carbon's design? How many people are involved in Carbon? Is there a demo? Why build Carbon? Why is performance critical? What level of C++ interoperability is expected? What would migrating C++ code to Carbon look like? What alternatives did you consider? Why did they not work? Why not improve C++? Why not fork C++? Why not Rust? If you can use Rust, ignore Carbon Why is adopting Rust difficult for C++ codebases? Why not a garbage collected language, like Java, Kotlin, or Go? How will Carbon work? What compiler infrastructure is Carbon using? How will Carbon's bidirectional C++ interoperability work? How do Carbon generics differ from templates? What is Carbon's memory model? How will Carbon achieve memory safety? How will the Carbon project work? Where does development occur? How does Carbon make decisions? What happens when a decision was wrong? What license does Carbon use? Why make Carbon open source? Why does Carbon have a CLA? Who pays for Carbon's infrastructure? What is Carbon? The Carbon Language is an experimental successor to C++. It is an effort to explore a possible future direction for the C++ language given the difficulties improving C++ . What is Carbon's status? Carbon is still an experiment. There remain significant open questions that we need to answer before the project can consider becoming a production effort. For now, we're focused on exploring this direction and gaining information to begin answering these questions. Project status Roadmap How soon can we use Carbon? Carbon is still years away \u2014 even if the experiment succeeds, it's unlikely that it will be ready for serious or production use in the next few years. Everything here is part of a long-term investigation. Why make Carbon public while it's still an experiment? One of the critical questions we need to answer as part of this experiment is whether the direction we're exploring with Carbon has both broad and significant interest for the industry at large. We feel like this is best answered by developing the language openly, publicly, and with broad participation. How complete is Carbon's design? We've resolved several of the most challenging language design technical decisions we anticipated based on experience with C++ and its constraints, particularly around generics and inheritance. Beyond those two areas, we have initial designs for class types, inheritance, operator overloading, syntactic and lexical structure, and modular code organization. We are aiming to complete the initial 0.1 language design around the end of 2022 although there are a large number of variables in that timeline. See our roadmap for details. References: Carbon design overview How do Carbon generics differ from templates? Roadmap How many people are involved in Carbon? Prior to going public, Carbon has had a couple dozen people involved. GitHub Insights provides activity metrics. Is there a demo? Yes! A prototype interpreter demo explorer can be used to execute simple examples. For example: $ bazel run //explorer -- ./explorer/testdata/basic_syntax/print.carbon Example source files can be found under /explorer/testdata . Carbon can also be explored interactively on https://carbon.compiler-explorer.com . Why build Carbon? See the project README for an overview of the motivation for Carbon. This section dives into specific questions in that space. Why is performance critical? Performance is critical for many users today. A few reasons are: Cost savings : Organizations with large-scale compute needs care about software performance because it reduces hardware needs. Reliable latency : Environments with specific latency needs or concerns with bounding tail latency need to be able to control and improve their latency. Resource constraints : Many systems have constrained CPU or memory resources that require precise control over resource usage and performance. What level of C++ interoperability is expected? Carbon code will be able to call C++, and the other way around, without overhead. You will be able to migrate a single library to Carbon within a C++ application, or write new Carbon on top of their existing C++ investment. While Carbon's interoperability may not cover every last case, most C++ style guides (such as the C++ Core Guidelines or Google C++ Style Guide) steer developers away from complex C++ code that's more likely to cause issues, and we expect the vast majority of code to interoperate well. For example, considering a pure C++ application: It's possible to migrate a single function to Carbon: References: Interoperability philosophy and goals How will Carbon's bidirectional C++ interoperability work? What would migrating C++ code to Carbon look like? Migration support is a key long-term goal for Carbon . If a migration occurs, we anticipate: Migration tools that automatically translate C++ libraries to Carbon at the file or library level with minimal human assistance. Bidirectional C++ interoperability that allows teams to migrate libraries in any order they choose without performance concerns or maintaining interoperability wrappers. Test-driven verification that migrations are correct. What alternatives did you consider? Why did they not work? Why not improve C++? A lot of effort has been invested into improving C++, but C++ is difficult to improve . For example, although P2137 was not accepted, it formed the basis for Carbon's goals . Why not fork C++? While we would like to see C++ improve, we don't think that forking C++ is the right path to achieving that goal. A fork could create confusion about what code works with standard C++. We believe a successor programming language is a better approach because it gives more freedom for Carbon's design while retaining the existing C++ ecosystem investments. Why not Rust? If you can use Rust, ignore Carbon If you want to use Rust, and it is technically and economically viable for your project, you should use Rust. In fact, if you can use Rust or any other established programming language, you should. Carbon is for organizations and projects that heavily depend on C++; for example, projects that have a lot of C++ code or use many third-party C++ libraries. We believe that Rust is an excellent choice for writing software within the pure Rust ecosystem. Software written in Rust has properties that neither C++ nor Carbon have. When you need to call other languages from Rust, RPCs are a good option. Rust is also good for using APIs implemented in a different language in-process, when the cost of maintaining the FFI boundary is reasonable. When the foreign language API is large, constantly changes, uses advanced C++ features, or makes architectural choices that are incompatible with safe Rust , maintaining a C++/Rust FFI may not be economically viable today (but it is an area of active research: cxx , autocxx , Crubit ). The Carbon community is looking for a language that existing, large, monolithic C++ codebases can incrementally adopt and have a prospect of migrating away from C++ completely. We would be very happy if Rust could be this language. However, we are not certain that: Idiomatic, safe Rust can seamlessly integrate into an existing C++ codebase, similarly to how TypeScript code can be added to a large existing JavaScript codebase. Developers can incrementally migrate existing C++ code to Rust, just like they can migrate JavaScript to TypeScript one file at a time, while keeping the project working. See Carbon's goals for an in-depth discussion of Carbon's vision for C++/Carbon interop and migration. Why is adopting Rust difficult for C++ codebases? Large existing C++ codebases almost certainly made architectural choices that are incompatible with safe Rust. Specifically: Seamless interop where existing, unmodified C++ APIs are made callable from safe Rust requires the C++ code to follow borrow checking rules at the API boundary. To reduce the amount of Rust-side compile-time checking that makes interop difficult, C++ APIs can be exposed to Rust with pointers instead of references. However, that forces users to write unsafe Rust, which can be even more tricky to write than C++ because it has new kinds of UB compared to C++; for example, stacked borrows violations . Seamless interop where safe Rust APIs are made callable from C++ requires C++ users to follow Rust borrow checking rules. Incremental migration of C++ to safe Rust means that C++ code gets converted to Rust without major changes to the architecture, data structures, or APIs. However Rust imposes stricter rules than C++, disallowing some design choices that were valid in C++. Therefore, the original C++ code must follow Rust rules before attempting a conversion. Original C++ code must be structured in such a way that the resulting Rust code passes borrow checking. C++ APIs and data structures are not designed with this in mind. Migrating C++ to unsafe Rust would still require the code to follow Rust's reference exclusivity and stacked borrows rules. Why not a garbage collected language, like Java, Kotlin, or Go? If you can use one of these languages, you absolutely should. Garbage collection provides dramatically simpler memory management for developers, but at the expense of performance. The performance cost can range from direct runtime overhead to significant complexity and loss of control over performance. This trade-off makes sense for many applications, and we actively encourage using these languages in those cases. However, we need a solution for C++ use-cases that require its full performance, low-level control, and access to hardware. How will Carbon work? What compiler infrastructure is Carbon using? Carbon is being built using LLVM, and is expected to have Clang dependencies for interoperability . How will Carbon's bidirectional C++ interoperability work? The Carbon toolchain will compile both Carbon and C++ code together, in order to make the interoperability seamless . For example, for import Cpp library \"<vector>\" , Carbon will: Call into Clang to load the AST of the vector header file. Analyze the AST for public APIs, which will be turned into names that can be accessed from Carbon; for example, std::vector is Cpp.std.vector in Carbon. Use Clang to instantiate the Cpp.std.vector template when parameterized references occur in Carbon code. In other words, C++ templates will be instantiated using standard C++ mechanisms, and the instantiated versions are called by Carbon code. Some code, such as #define preprocessor macros, will not work as well. C++ allows arbitrary content in a #define , and that can be difficult to translate. As a consequence, this is likely to be a limitation of interoperability and left to migration. How do Carbon generics differ from templates? Carbon's generic programming support will handle both templates (matching C++) and checked generics (common in other languages: Rust, Swift, Go, Kotlin, Java, and so on). The key difference between the two is that template arguments can only finish type-checking during instantiation, whereas generics specify an interface with which arguments can finish type-checking without instantiation. This has a couple important benefits: Type-checking errors for generics happen earlier, making it easier for the compiler to produce helpful diagnostics. Generic functions can generate less compiled output, allowing compilation with many uses to be faster. For comparison, template instantiations are a major factor for C++ compilation latency. Although Carbon will prefer generics over templates, templates are provided for migration of C++ code. References: Generics: Goals: Better compiler experience Generics: Terminology: Generic versus template parameters What is Carbon's memory model? Carbon will match C++'s memory model closely in order to maintain zero-overhead interoperability. There may be some changes made as part of supporting memory safety, but performance and interoperability will constrain flexibility in this space. How will Carbon achieve memory safety? See memory safety in the project README . References: Lifetime annotations for C++ Carbon principle: Safety strategy How will the Carbon project work? Where does development occur? Carbon is using GitHub for its repository and code reviews. Most non-review discussion occurs on our Discord server . If you're interested in contributing, you can find more information in our Contributing file . How does Carbon make decisions? Any interested developer may propose and discuss changes to Carbon. The Carbon leads are responsible for reviewing proposals and surrounding discussion, then making decisions based on the discussion. As Carbon grows, we expect to add feature teams to distribute responsibility. The intent of this setup is that Carbon remains a community-driven project, avoiding situations where any single organization controls Carbon's direction. References: Contributing Evolution process What happens when a decision was wrong? Carbon's evolution process is iterative: when we make poor decisions, we'll work to fix them. If we realize a mistake quickly, it may make sense to just roll back the decision. Otherwise, a fix will need to follow the normal evolution process, with a proposal explaining why the decision was wrong and proposing a better path forward. What license does Carbon use? Carbon is under the Apache License v2.0 with LLVM Exceptions . We want Carbon to be available under a permissive open source license. As a programming language with compiler and runtime library considerations, our project has the same core needs as the LLVM project for its license and we build on their work to address these by combining the Apache License with the LLVM Exceptions . Why make Carbon open source? We believe it is important for a programming language like Carbon, if it is successful, to be developed by and for a broad community. We feel that the open source model is the most effective and successful approach for doing this. We're closely modeled on LLVM and other similar open source projects, and want to follow their good examples. We've structured the project to be attractive for industry players big and small to participate in, but also to be resilient and independent long-term. The open source model, particularly as followed by Apache and LLVM, also provides a strong foundation for handling hard problems like intellectual property and licensing with a broad and diverse group of contributors. Why does Carbon have a CLA? Carbon uses a CLA (Contributor License Agreement) in case we need to fix issues with the license structure in the future, something which has proven to be important in other projects. Any changes to the license of Carbon would be made very carefully and subject to the exact same decision making process as any other change to the overall project direction. Initially, Carbon is bootstrapping using Google's CLA. We are planning to create an open source foundation and transfer all Carbon-related rights to it; our goal is for the foundation setup to be similar to other open source projects, such as LLVM or Kubernetes. Who pays for Carbon's infrastructure? Carbon is currently bootstrapping infrastructure with the help of Google. As soon as a foundation is ready to oversee infrastructure, such as continuous integration and the CLA, we plan to transfer them so they are run by the community.","title":"Project FAQ"},{"location":"project/faq/#project-faq","text":"","title":"Project FAQ"},{"location":"project/faq/#table-of-contents","text":"What is Carbon? What is Carbon's status? How soon can we use Carbon? Why make Carbon public while it's still an experiment? How complete is Carbon's design? How many people are involved in Carbon? Is there a demo? Why build Carbon? Why is performance critical? What level of C++ interoperability is expected? What would migrating C++ code to Carbon look like? What alternatives did you consider? Why did they not work? Why not improve C++? Why not fork C++? Why not Rust? If you can use Rust, ignore Carbon Why is adopting Rust difficult for C++ codebases? Why not a garbage collected language, like Java, Kotlin, or Go? How will Carbon work? What compiler infrastructure is Carbon using? How will Carbon's bidirectional C++ interoperability work? How do Carbon generics differ from templates? What is Carbon's memory model? How will Carbon achieve memory safety? How will the Carbon project work? Where does development occur? How does Carbon make decisions? What happens when a decision was wrong? What license does Carbon use? Why make Carbon open source? Why does Carbon have a CLA? Who pays for Carbon's infrastructure?","title":"Table of contents"},{"location":"project/faq/#what-is-carbon","text":"The Carbon Language is an experimental successor to C++. It is an effort to explore a possible future direction for the C++ language given the difficulties improving C++ .","title":"What is Carbon?"},{"location":"project/faq/#what-is-carbons-status","text":"Carbon is still an experiment. There remain significant open questions that we need to answer before the project can consider becoming a production effort. For now, we're focused on exploring this direction and gaining information to begin answering these questions. Project status Roadmap","title":"What is Carbon's status?"},{"location":"project/faq/#how-soon-can-we-use-carbon","text":"Carbon is still years away \u2014 even if the experiment succeeds, it's unlikely that it will be ready for serious or production use in the next few years. Everything here is part of a long-term investigation.","title":"How soon can we use Carbon?"},{"location":"project/faq/#why-make-carbon-public-while-its-still-an-experiment","text":"One of the critical questions we need to answer as part of this experiment is whether the direction we're exploring with Carbon has both broad and significant interest for the industry at large. We feel like this is best answered by developing the language openly, publicly, and with broad participation.","title":"Why make Carbon public while it's still an experiment?"},{"location":"project/faq/#how-complete-is-carbons-design","text":"We've resolved several of the most challenging language design technical decisions we anticipated based on experience with C++ and its constraints, particularly around generics and inheritance. Beyond those two areas, we have initial designs for class types, inheritance, operator overloading, syntactic and lexical structure, and modular code organization. We are aiming to complete the initial 0.1 language design around the end of 2022 although there are a large number of variables in that timeline. See our roadmap for details. References: Carbon design overview How do Carbon generics differ from templates? Roadmap","title":"How complete is Carbon's design?"},{"location":"project/faq/#how-many-people-are-involved-in-carbon","text":"Prior to going public, Carbon has had a couple dozen people involved. GitHub Insights provides activity metrics.","title":"How many people are involved in Carbon?"},{"location":"project/faq/#is-there-a-demo","text":"Yes! A prototype interpreter demo explorer can be used to execute simple examples. For example: $ bazel run //explorer -- ./explorer/testdata/basic_syntax/print.carbon Example source files can be found under /explorer/testdata . Carbon can also be explored interactively on https://carbon.compiler-explorer.com .","title":"Is there a demo?"},{"location":"project/faq/#why-build-carbon","text":"See the project README for an overview of the motivation for Carbon. This section dives into specific questions in that space.","title":"Why build Carbon?"},{"location":"project/faq/#why-is-performance-critical","text":"Performance is critical for many users today. A few reasons are: Cost savings : Organizations with large-scale compute needs care about software performance because it reduces hardware needs. Reliable latency : Environments with specific latency needs or concerns with bounding tail latency need to be able to control and improve their latency. Resource constraints : Many systems have constrained CPU or memory resources that require precise control over resource usage and performance.","title":"Why is performance critical?"},{"location":"project/faq/#what-level-of-c-interoperability-is-expected","text":"Carbon code will be able to call C++, and the other way around, without overhead. You will be able to migrate a single library to Carbon within a C++ application, or write new Carbon on top of their existing C++ investment. While Carbon's interoperability may not cover every last case, most C++ style guides (such as the C++ Core Guidelines or Google C++ Style Guide) steer developers away from complex C++ code that's more likely to cause issues, and we expect the vast majority of code to interoperate well. For example, considering a pure C++ application: It's possible to migrate a single function to Carbon: References: Interoperability philosophy and goals How will Carbon's bidirectional C++ interoperability work?","title":"What level of C++ interoperability is expected?"},{"location":"project/faq/#what-would-migrating-c-code-to-carbon-look-like","text":"Migration support is a key long-term goal for Carbon . If a migration occurs, we anticipate: Migration tools that automatically translate C++ libraries to Carbon at the file or library level with minimal human assistance. Bidirectional C++ interoperability that allows teams to migrate libraries in any order they choose without performance concerns or maintaining interoperability wrappers. Test-driven verification that migrations are correct.","title":"What would migrating C++ code to Carbon look like?"},{"location":"project/faq/#what-alternatives-did-you-consider-why-did-they-not-work","text":"","title":"What alternatives did you consider? Why did they not work?"},{"location":"project/faq/#why-not-improve-c","text":"A lot of effort has been invested into improving C++, but C++ is difficult to improve . For example, although P2137 was not accepted, it formed the basis for Carbon's goals .","title":"Why not improve C++?"},{"location":"project/faq/#why-not-fork-c","text":"While we would like to see C++ improve, we don't think that forking C++ is the right path to achieving that goal. A fork could create confusion about what code works with standard C++. We believe a successor programming language is a better approach because it gives more freedom for Carbon's design while retaining the existing C++ ecosystem investments.","title":"Why not fork C++?"},{"location":"project/faq/#why-not-rust","text":"","title":"Why not Rust?"},{"location":"project/faq/#if-you-can-use-rust-ignore-carbon","text":"If you want to use Rust, and it is technically and economically viable for your project, you should use Rust. In fact, if you can use Rust or any other established programming language, you should. Carbon is for organizations and projects that heavily depend on C++; for example, projects that have a lot of C++ code or use many third-party C++ libraries. We believe that Rust is an excellent choice for writing software within the pure Rust ecosystem. Software written in Rust has properties that neither C++ nor Carbon have. When you need to call other languages from Rust, RPCs are a good option. Rust is also good for using APIs implemented in a different language in-process, when the cost of maintaining the FFI boundary is reasonable. When the foreign language API is large, constantly changes, uses advanced C++ features, or makes architectural choices that are incompatible with safe Rust , maintaining a C++/Rust FFI may not be economically viable today (but it is an area of active research: cxx , autocxx , Crubit ). The Carbon community is looking for a language that existing, large, monolithic C++ codebases can incrementally adopt and have a prospect of migrating away from C++ completely. We would be very happy if Rust could be this language. However, we are not certain that: Idiomatic, safe Rust can seamlessly integrate into an existing C++ codebase, similarly to how TypeScript code can be added to a large existing JavaScript codebase. Developers can incrementally migrate existing C++ code to Rust, just like they can migrate JavaScript to TypeScript one file at a time, while keeping the project working. See Carbon's goals for an in-depth discussion of Carbon's vision for C++/Carbon interop and migration.","title":"If you can use Rust, ignore Carbon"},{"location":"project/faq/#why-is-adopting-rust-difficult-for-c-codebases","text":"Large existing C++ codebases almost certainly made architectural choices that are incompatible with safe Rust. Specifically: Seamless interop where existing, unmodified C++ APIs are made callable from safe Rust requires the C++ code to follow borrow checking rules at the API boundary. To reduce the amount of Rust-side compile-time checking that makes interop difficult, C++ APIs can be exposed to Rust with pointers instead of references. However, that forces users to write unsafe Rust, which can be even more tricky to write than C++ because it has new kinds of UB compared to C++; for example, stacked borrows violations . Seamless interop where safe Rust APIs are made callable from C++ requires C++ users to follow Rust borrow checking rules. Incremental migration of C++ to safe Rust means that C++ code gets converted to Rust without major changes to the architecture, data structures, or APIs. However Rust imposes stricter rules than C++, disallowing some design choices that were valid in C++. Therefore, the original C++ code must follow Rust rules before attempting a conversion. Original C++ code must be structured in such a way that the resulting Rust code passes borrow checking. C++ APIs and data structures are not designed with this in mind. Migrating C++ to unsafe Rust would still require the code to follow Rust's reference exclusivity and stacked borrows rules.","title":"Why is adopting Rust difficult for C++ codebases?"},{"location":"project/faq/#why-not-a-garbage-collected-language-like-java-kotlin-or-go","text":"If you can use one of these languages, you absolutely should. Garbage collection provides dramatically simpler memory management for developers, but at the expense of performance. The performance cost can range from direct runtime overhead to significant complexity and loss of control over performance. This trade-off makes sense for many applications, and we actively encourage using these languages in those cases. However, we need a solution for C++ use-cases that require its full performance, low-level control, and access to hardware.","title":"Why not a garbage collected language, like Java, Kotlin, or Go?"},{"location":"project/faq/#how-will-carbon-work","text":"","title":"How will Carbon work?"},{"location":"project/faq/#what-compiler-infrastructure-is-carbon-using","text":"Carbon is being built using LLVM, and is expected to have Clang dependencies for interoperability .","title":"What compiler infrastructure is Carbon using?"},{"location":"project/faq/#how-will-carbons-bidirectional-c-interoperability-work","text":"The Carbon toolchain will compile both Carbon and C++ code together, in order to make the interoperability seamless . For example, for import Cpp library \"<vector>\" , Carbon will: Call into Clang to load the AST of the vector header file. Analyze the AST for public APIs, which will be turned into names that can be accessed from Carbon; for example, std::vector is Cpp.std.vector in Carbon. Use Clang to instantiate the Cpp.std.vector template when parameterized references occur in Carbon code. In other words, C++ templates will be instantiated using standard C++ mechanisms, and the instantiated versions are called by Carbon code. Some code, such as #define preprocessor macros, will not work as well. C++ allows arbitrary content in a #define , and that can be difficult to translate. As a consequence, this is likely to be a limitation of interoperability and left to migration.","title":"How will Carbon's bidirectional C++ interoperability work?"},{"location":"project/faq/#how-do-carbon-generics-differ-from-templates","text":"Carbon's generic programming support will handle both templates (matching C++) and checked generics (common in other languages: Rust, Swift, Go, Kotlin, Java, and so on). The key difference between the two is that template arguments can only finish type-checking during instantiation, whereas generics specify an interface with which arguments can finish type-checking without instantiation. This has a couple important benefits: Type-checking errors for generics happen earlier, making it easier for the compiler to produce helpful diagnostics. Generic functions can generate less compiled output, allowing compilation with many uses to be faster. For comparison, template instantiations are a major factor for C++ compilation latency. Although Carbon will prefer generics over templates, templates are provided for migration of C++ code. References: Generics: Goals: Better compiler experience Generics: Terminology: Generic versus template parameters","title":"How do Carbon generics differ from templates?"},{"location":"project/faq/#what-is-carbons-memory-model","text":"Carbon will match C++'s memory model closely in order to maintain zero-overhead interoperability. There may be some changes made as part of supporting memory safety, but performance and interoperability will constrain flexibility in this space.","title":"What is Carbon's memory model?"},{"location":"project/faq/#how-will-carbon-achieve-memory-safety","text":"See memory safety in the project README . References: Lifetime annotations for C++ Carbon principle: Safety strategy","title":"How will Carbon achieve memory safety?"},{"location":"project/faq/#how-will-the-carbon-project-work","text":"","title":"How will the Carbon project work?"},{"location":"project/faq/#where-does-development-occur","text":"Carbon is using GitHub for its repository and code reviews. Most non-review discussion occurs on our Discord server . If you're interested in contributing, you can find more information in our Contributing file .","title":"Where does development occur?"},{"location":"project/faq/#how-does-carbon-make-decisions","text":"Any interested developer may propose and discuss changes to Carbon. The Carbon leads are responsible for reviewing proposals and surrounding discussion, then making decisions based on the discussion. As Carbon grows, we expect to add feature teams to distribute responsibility. The intent of this setup is that Carbon remains a community-driven project, avoiding situations where any single organization controls Carbon's direction. References: Contributing Evolution process","title":"How does Carbon make decisions?"},{"location":"project/faq/#what-happens-when-a-decision-was-wrong","text":"Carbon's evolution process is iterative: when we make poor decisions, we'll work to fix them. If we realize a mistake quickly, it may make sense to just roll back the decision. Otherwise, a fix will need to follow the normal evolution process, with a proposal explaining why the decision was wrong and proposing a better path forward.","title":"What happens when a decision was wrong?"},{"location":"project/faq/#what-license-does-carbon-use","text":"Carbon is under the Apache License v2.0 with LLVM Exceptions . We want Carbon to be available under a permissive open source license. As a programming language with compiler and runtime library considerations, our project has the same core needs as the LLVM project for its license and we build on their work to address these by combining the Apache License with the LLVM Exceptions .","title":"What license does Carbon use?"},{"location":"project/faq/#why-make-carbon-open-source","text":"We believe it is important for a programming language like Carbon, if it is successful, to be developed by and for a broad community. We feel that the open source model is the most effective and successful approach for doing this. We're closely modeled on LLVM and other similar open source projects, and want to follow their good examples. We've structured the project to be attractive for industry players big and small to participate in, but also to be resilient and independent long-term. The open source model, particularly as followed by Apache and LLVM, also provides a strong foundation for handling hard problems like intellectual property and licensing with a broad and diverse group of contributors.","title":"Why make Carbon open source?"},{"location":"project/faq/#why-does-carbon-have-a-cla","text":"Carbon uses a CLA (Contributor License Agreement) in case we need to fix issues with the license structure in the future, something which has proven to be important in other projects. Any changes to the license of Carbon would be made very carefully and subject to the exact same decision making process as any other change to the overall project direction. Initially, Carbon is bootstrapping using Google's CLA. We are planning to create an open source foundation and transfer all Carbon-related rights to it; our goal is for the foundation setup to be similar to other open source projects, such as LLVM or Kubernetes.","title":"Why does Carbon have a CLA?"},{"location":"project/faq/#who-pays-for-carbons-infrastructure","text":"Carbon is currently bootstrapping infrastructure with the help of Google. As soon as a foundation is ready to oversee infrastructure, such as continuous integration and the CLA, we plan to transfer them so they are run by the community.","title":"Who pays for Carbon's infrastructure?"},{"location":"project/goals/","text":"Goals Table of contents Overview Project goals Community and culture Language tools and ecosystem Language goals and priorities Performance-critical software Software and language evolution Code that is easy to read, understand, and write Practical safety and testing mechanisms Fast and scalable development Modern OS platforms, hardware architectures, and environments Interoperability with and migration from existing C++ code Non-goals Stable language and library ABI Backwards or forwards compatibility Legacy compiled libraries without source code or ability to rebuild Support for existing compilation and linking models Idiomatic migration of non-modern, non-idiomatic C++ code Prioritization beyond goals Acknowledgements Overview Carbon is an experiment to explore a possible, distant future for the C++ programming language designed around a specific set of goals, priorities, and use cases. A programming language is a tool, and different tools are good for different purposes. We think there is great value in priorities that differentiate Carbon from other programming languages. Stating Carbon\u2019s priorities clearly and explicitly shapes the design of Carbon, and helps the entire community effectively evaluate and use the language. Carbon's language goals have historically been best addressed by C++, and there are large ecosystems and codebases written using C++ to these ends. Carbon should be attractive and easy for C++ developers to try out and incrementally adopt, even in individual libraries both using and used from C++ code. We expect this depends on having high-performance bidirectional interoperability with C++, excellent migration tooling, and an easy ramp-up for experienced C++ software developers. Principles are provided to clarify these goals. Principles do not supersede goals and priorities. Project goals Community and culture Carbon has an overarching goal of promoting a healthy and vibrant community with an inclusive, welcoming, and pragmatic culture. While this may not directly affect Carbon's design, it affects how Carbon's design occurs. We cannot build a good language without a good community. As the saying goes, \"culture eats strategy for breakfast\" . Carbon's community, including both maintainers and users, needs to last for years and be capable of scaling up. It needs to support people working on Carbon across a wide range of companies as their full time job, but also people contributing in small fractions of their time, or as students, teachers, or as a hobby. There are several key ingredients to achieving this. The community and project needs a code of conduct. We want Carbon's community to be welcoming and respectful, with a deep commitment to psychological safety. We need consistent expectations for how every community member should behave, regardless of their position in the community. These expectations around conduct and behavior need to be clearly articulated both to set expectations for people joining, and to help remind and anchor us on consistent standards. It is also important that we hold ourselves accountable to these expectations and have real and meaningful mechanisms to moderate the community. When behavior steps outside of our expectations, we need tools, process, and policy for how we will recognize and correct it. An open, inclusive process for Carbon changes. The community needs to be able to effectively engage in the direction and evolution of the project and language, while keeping the process efficient and effective. That means we need an open, inclusive process where everyone feels comfortable participating. Community members should understand how and why decisions are made, and have the ability to both influence them before they occur and give feedback afterward. We want to use this process to also ensure we stick to our language priorities and have clear rationales for all of our technical designs and decisions. Being inclusive is different from including everyone. We want to avoid excluding or marginalizing members of the community. However, we expect to inevitably make choices that benefit some Carbon community members more than others. We will provide justification for these decisions, but achieving Carbon's goals -- including that of a healthy community -- will be the guiding rule. Language tools and ecosystem Programming languages do not succeed in a vacuum. The Carbon project cannot merely design a language in order to succeed, it must tackle the full ecosystem of tooling that makes developers effective using the language. This includes not only a compiler and standard library, but also a broad range of other tools that enable developers to be more effective, efficient, or productive. We will provide a reference implementation. This helps the language have a strong and consistent experience for developers and a clear onboarding process. It also enables us to carefully consider implementation considerations throughout the design of the language. However, we do not want this to be seen as a replacement for a formal specification at any point. Carbon will have a formal specification. Fully specifying the language enables other implementations and allows us to clearly document the expected behavior of the reference implementation. This does not mean the specification defines what is \"correct\"; instead, the specification and reference implementation should complement each other. Any divergence is a bug that must be resolved, and the specification and reference should always converge. Carbon should not have designs or specifications which do not match the practical implementation, even if that means updating designs to reflect implementation realities. Having the specification will enable better analysis of the language as a whole and the production of other partial or full implementations which match the behavior of the reference implementation. Approachable, developer-facing documentation. Developers shouldn't be expected to read through the specification to ramp up with Carbon. User guides and other documentation will be provided to make it easy to learn how to use Carbon. Compelling adoption tooling. We want to provide a compelling suite of tools out-of-the-box in order to encourage adoption of Carbon at scale where it can augment existing C++ codebases. For example, we expect a C++ -> Carbon code translator will be important. Tooling for updating code when Carbon evolves. As Carbon evolves over time, we expect to provide tooling to help automate and scale migrating existing Carbon code to the new version. The goal is to enable more rapid evolution of the language without the churn tax and version skew becoming unsustainable. Developer tooling. We need developers to be productive reading and writing Carbon code. We expect to provide a broad suite of development oriented tools ranging from refactoring tools to LSP implementations and editor integrations. We also plan to provide machine readable forms of many parts of the language, such as a grammar, to ensure consistency between tools and enable the development of tools by others. Infrastructure to enable package management and other library ecosystem support. The goal is to support what the ecosystem needs, regardless of the exact form this ends up taking. Language goals and priorities We are designing Carbon to support: Performance-critical software Software and language evolution Code that is easy to read, understand, and write Practical safety and testing mechanisms Fast and scalable development Modern OS platforms, hardware architectures, and environments Interoperability with and migration from existing C++ code Many languages share subsets of these goals, but what distinguishes Carbon is their combination. Where it is necessary to make tradeoffs between these goals, we intend to prioritize them in this order. Each goal is broad, and has several facets to consider when making decisions. Below, we discuss all of these goals in more detail to give a deeper understanding of both the nature and motivation of these goals. Performance-critical software All software consumes resources: time, memory, compute, power, binary size, and so on. In many cases, raw resource usage is not the biggest concern. Instead, algorithmic efficiency or business logic dominates these concerns. However, there exists software where its rate of resource consumption -- its performance -- is critical to its successful operation. Another way to think about when performance is critical: would a performance regression be considered a bug? Would it even be noticed? Our goal is to support software where its performance with respect to some set of resource constraints is critical to its successful operation. This overarching goal can be decomposed into a few specific aspects. Provide the developer control over every aspect of performance. When faced with some performance problem, the developer should always have tools within Carbon to address it. This does not mean that the developer is necessarily concerned with ultimate performance at every moment, but in the most constrained scenarios they must be able to \"open up the hood\" without switching to another language. Idiomatic code should be fast. Developers should not regularly be required to choose between performance and readability. Although performance tuning may in rare cases require complex or surprising code, Carbon's design should ensure regular, idiomatic code usually results in high performance. Code should perform predictably. The reader and writer of code should be able to easily understand its expected performance, given sufficient background knowledge of the environment in which it will run. This need not be precise, but instead can use heuristics and guidelines to avoid surprise. The key priority is that performance, whether good or bad, is unsurprising to developers. Even pleasant surprises, when too frequent, can become a problem due to establishing brittle baseline performance that cannot be reliably sustained. Leave no room for a lower level language. Developers should not need to leave the rules and structure of Carbon, whether to gain control over performance problems or to gain access to hardware facilities. Software and language evolution Titus Winters writes in \"Non-Atomic Refactoring and Software Sustainability\": What is the difference between programming and software engineering? These are nebulous concepts and thus there are many possible answers, but my favorite definition is this: Software engineering is programming integrated over time. All of the hard parts of engineering come from dealing with time: compatibility over time, dealing with changes to underlying infrastructure and dependencies, and working with legacy code or data. Fundamentally, it is a different task to produce a programming solution to a problem (that solves the current [instance] of the problem) versus an engineering solution (that solves current instances, future instances that we can predict, and - through flexibility - allows updates to solve future instances we may not be able to predict). Carbon will prioritize being a \"software engineering\" language, in the above sense. We specifically are interested in dealing with the time-oriented aspects of software built in this language. We need to be prepared for substantive changes in priority over the next decade, on par with the changes experienced in the 2010s: 10x scaling of software organizations, mobile, cloud, diversification of platforms and architectures, and so on. Support maintaining and evolving software written in Carbon for decades. The life expectancy of some software will be long and the software will not be static or unchanging in that time. Mistakes will be made and need to be corrected. New functionality will be introduced and old functionality retired and removed. The design of Carbon must support and ease every step of this process. This ranges from emphasizing testing and continuous integration to tooling and the ability to make non-atomic changes. It also includes constraints on the design of Carbon itself: we should avoid, or at least minimize, language features that encourage unchangeable constructs. For example, any feature with a contract that cannot be strengthened or weakened without breaking the expected usage patterns is inherently hostile to refactoring. Analogously, features or conventions that require simultaneously updating all users of an API when extending it are inherently hostile towards long-term maintenance of software. Support maintaining and evolving the language itself for decades. We will not get the design of most language features correct on our first, second, or 73rd try. As a consequence, there must be a built-in plan and ability to move Carbon forward at a reasonable pace and with a reasonable cost. Simultaneously, an evolving language must not leave software behind to languish, but bring software forward. This requirement should not imply compatibility, but instead some migratability, likely tool-assisted. Be mindful of legacy. Globally, there may be as many as 50 billion lines of C++ code. Any evolution of Carbon that fails to account for human investment/training and legacy code, representing significant capital, is doomed from the start. Note that our priority is restricted to legacy source code; we do not prioritize full support of legacy object code. While that still leaves many options open, such as dedicated and potentially slower features, it does limit the degree to which legacy use cases beyond source code should shape the Carbon design. Code that is easy to read, understand, and write While this is perhaps the least unique among programming languages of the goals we list here, we feel it is important to state it, explain all of what we mean by it, and fit it into our prioritization scheme. Software has inherent complexity that burdens developers, especially at scale and over time. Carbon will strive to minimize that burden for reading, understanding, and writing code. The behavior of code should be easily understood, especially by those unfamiliar with the software system. Consider developers attempting to diagnose a serious outage under time pressure -- every second spent trying to understand the language is one not spent understanding the problem . While the source code of our software may be read far more often by machines, humans are the most expensive readers and writers of software. As a consequence, we need to optimize for human reading, understanding, and writing of software, in that order. Excellent ergonomics. Human capabilities and limitations in the domains of perception, memory, reasoning, and decision-making affect interactions between humans and systems. Ergonomic language design takes human factors into account to increase productivity and comfort, and reduce errors and fatigue, making Carbon more suitable for humans to use. We can also say that ergonomic designs are accessible to humans. \"Readability\" is a related, but a more focused concept, connected to only the process of reading code. \"Ergonomics\" covers all activities where humans interact with Carbon: reading, writing, designing, discussing, reviewing, and refactoring code, as well as learning and teaching Carbon. A few examples: Carbon should not use symbols that are difficult to type, see, or differentiate from similar symbols in commonly used contexts. Syntax should be easily parsed and scanned by any human in any development environment, not just a machine or a human aided by semantic hints from an IDE. Code with similar behavior should use similar syntax, and code with different behavior should use different syntax. Behavior in this context should include both the functionality and performance of the code. This is part of conceptual integrity. Explicitness must be balanced against conciseness, as verbosity and ceremony add cognitive overhead for the reader, while explicitness reduces the amount of outside context the reader must have or assume. Common yet complex tasks, such as parallel code, should be well-supported in ways that are easy to reason about. Ordinary tasks should not require extraordinary care, because humans cannot consistently avoid making mistakes for an extended amount of time. Support tooling at every layer of the development experience, including IDEs. The design and implementation of Carbon should make it easy to create such tools and make them effective. Carbon should avoid syntax and textual structures that are difficult to recognize and mechanically change without losing meaning. Support software outside of the primary use cases well. There are surprisingly high costs for developers to switch languages. Even when the primary goal is to support performance-critical software, other kinds of software should not be penalized unnecessarily. \"The right tool for the job is often the tool you are already using -- adding new tools has a higher cost than many people appreciate.\" -- John Carmack Focus on encouraging appropriate usage of features rather than restricting misuse. Adding arbitrary restrictions to prevent misuse of otherwise general features of the language can create problems when they end up interfering with unexpected or rare but still appropriate usages. Instead, Carbon should focus on enabling appropriate and effective usage of features, and creating incentives around those. What seems initially like a \"misuse\" of a feature may be critical for some rare or future use case. Put differently, we will not always be able to prevent developers from misusing features or writing unnecessarily complex code, and that is okay. We should instead focus on helping reduce the rate that this occurs accidentally, and enabling tooling and diagnostics that warn about dangerous or surprising patterns. The behavior and semantics of code should be clearly and simply specified whenever possible. Leaving behavior undefined for some cases of invalid, buggy, or non-portable code may be necessary, but it comes at a very high cost and should be avoided. Every case where behavior is left undefined should be clearly spelled out with a strong rationale for this tradeoff. The code patterns without defined behavior should be teachable and understandable by developers. Finally, there must be mechanisms available to detect undefined behavior, at best statically, and at worst dynamically with high probability and at minimal cost. Adhere to the principle of least surprise. Defaults should match typical usage patterns. Implicit features should be unsurprising and expected, while explicit syntax should inform the reader about any behavior which might otherwise be surprising. The core concepts of implicit versus explicit syntax are well articulated in the Rust community , although we may come to different conclusions regarding the principles. Design features to be simple to implement. Syntax, structure, and language features should be chosen while keeping the implementation complexity manageable. Simplicity of implementation reduces bugs, and will in most cases make the features easier to understand. It's also often the best way to ensure predictable performance, although supporting peak performance may require options for more complex implementation behavior. Practical safety and testing mechanisms Our goal is to add as much language-level safety and security to Carbon as possible, using a hybrid strategy to balance other goals. We will do as many safety checks as we can at compile time. We will also provide dynamic runtime checking and a strong testing methodology ranging from unit tests through integration and system tests all the way to coverage-directed fuzz testing. We have specific criteria that are important for this strategy to be successful: Make unsafe or risky aspects of Carbon code explicit and syntactically visible. This will allow the software to use the precise flexibility needed and to minimize its exposure, while still aiding the reader. It can also help the reader more by indicating the specific nature of risk faced by a given construct. More simply, safe things shouldn't look like unsafe things and unsafe things should be easily recognized when reading code. Common patterns of unsafe or risky code must support static checking. Waiting until a dynamic check is too late to prevent the most common errors. A canonical example here are thread-safety annotations for basic mutex lock management to allow static checking. This handles the common patterns, and we use dynamic checks, such as TSan and deadlock detection, to handle edge cases. All unsafe or risky operations and interfaces must support some dynamic checking. Developers need some way to test and verify that their code using any such interface is in fact correct. Uncheckable unsafety removes any ability for the developer to gain confidence. This means we need to design features with unsafe or risky aspects with dynamic checking in mind. A concrete example of this can be seen in facilities that allow indexing into an array: such facilities should be designed to have the bounds of the array available to implement bounds checking when desirable. Fast and scalable development Software development iteration has a critical \"edit, test, debug\" cycle. Developers will use IDEs, editors, compilers, and other tools that need different levels of parsing. For small projects, raw parsing speed is essential; for large software systems, scalability of parsing is also necessary. Syntax should parse with bounded, small look-ahead. Syntax that requires unbounded look-ahead or fully general backtracking adds significant complexity to parsing and makes it harder to provide high quality error messages. The result is both slower iteration and more iterations, a multiplicative negative impact on productivity. Humans aren't immune either; they can be confused by constructs that appear to mean one thing but actually mean another. Instead, we should design for syntax that is fast to parse, with easy and reliable error messages. No semantic or contextual information used when parsing. The more context, and especially the more semantic context, required for merely parsing code, the fewer options available to improve the performance of tools and compilation. Cross-file context has an especially damaging effect on the potential distributed build graph options. Without these options, we will again be unable to provide fast developer iteration as the codebase scales up. Support separate compilation, including parallel and distributed strategies. Iteration requires frequent rebuilds of software as part of the edit/test/debug cycle of development. The language design should enable low-latency build strategies, particularly when relatively little has changed. This minimally requires separate compilation of source files, and potentially other incremental build strategies. Separate compilation also enables better scalability options for build systems of large software. Modern OS platforms, hardware architectures, and environments Carbon must have strong support for all of the major, modern OS platforms, the hardware architectures they run on, and the environments in which their software runs. Carbon must also continue supporting these over time, even as which ones are major or modern evolve and change. Provide native support for the programming models of those platforms and environments. This goes beyond enabling compile-time translations from one abstraction to several implementations. While enabling high-level synchronization primitives like mutexes and futures is good, the underlying atomic operations provided by the hardware must also be directly available. Similarly, lowering parallel constructs into a specific implementation, such as SIMD or SPMD, is good but insufficient. Multiple parallel implementations must be directly addressable in Carbon. The need for native support repeats across the landscape of OS platform, hardware, and environment distinctions; for example, concurrency versus parallelism, and desktop versus mobile. Conversely, Carbon cannot prioritize support for historical platforms. To use a hockey metaphor, we should not skate to where the puck is, much less where the puck was twenty years ago. We have existing systems to support those platforms where necessary. Instead, Carbon should be forward-leaning in its platform support. As these platforms evolve over time, Carbon will have to evolve as well to continue to effectively prioritize the modern and major platforms. For examples, please see Carbon's success criteria . Interoperability with and migration from existing C++ code We want developers working within existing C++ ecosystems to easily start using Carbon, without starting from scratch. Adopting Carbon should not require complete rewrites, new programming models, or building an entire new stack/ecosystem. This means integrating into the existing C++ ecosystem by supporting incremental migration from C++ to Carbon, which in turn requires high-quality interoperability with existing C++ code. We must be able to move existing large C++ codebases -- some with hundreds of millions of lines of code and tens of thousands of active developers -- onto Carbon. C++ developers must also successfully switch to Carbon development. Any migration of this scale will take years, will need to be incremental, and some libraries -- particularly third-party -- may remain in C and C++. It must be possible to migrate a C++ library to Carbon without simultaneously migrating all of the libraries it depends on or all of the libraries that depend on it. We believe incremental migrations require: Familiarity for experienced C++ developers with a gentle learning curve. We need a feasible plan for retraining a C++ workforce to become proficient in Carbon. If long and significant study is required to be minimally proficient, meaning able to read, superficially understand, and do limited debugging or modifications, then the inertia of C++ will inevitably win. Further, we need a gentle and easily traversed learning curve to basic productivity in order for the transition to not become a chore or otherwise unsustainable for teams and individuals. Expressivity comparable to C++. If an algorithm or data structure or system architecture can naturally be written in C++, it should also be possible to write it naturally in Carbon. Automated source-to-source migration of large segments of large-scale idiomatic C++ code bases with high fidelity. We will prioritize having very low human interaction to achieve high fidelity migration results. We do not require all C++ code to be migratable in this fashion, and the resulting Carbon may be non-idiomatic. We can add reasonable constraints here if those constraints are already well established best practices for C++ development, including design patterns, testing coverage, or usage of sanitizers. Over many years, as Carbon evolves and codebases have had time to migrate, the results of the tooling may also drift further from idiomatic Carbon and have less desirable results. Support for bi-directional interoperability with existing C++ code. We need Carbon code to be able to call into C and C++ libraries with both reasonable API clarity and high performance. We will also need some ability to implement C++ interfaces with business logic in Carbon, although this direction can tolerate slightly more constraints both in supported features and performance overhead. In all cases, the particular performance overhead imposed by moving between C++ and Carbon will need to be easily exposed and understood by developers. While a given piece code only needs to be migrated once, we expect interoperability to be invoked continuously to support migrated code and will thus remain important for most developers. Non-goals There are common or expected goals of many programming languages that we explicitly call out as non-goals for Carbon. That doesn't make these things bad in any way, but reflects the fact that they do not provide meaningful value to us and come with serious costs and/or risks. Stable language and library ABI We would prefer to provide better, dedicated mechanisms to decompose software subsystems in ways that scale over time rather than providing a stable ABI across the Carbon language and libraries. Our experience is that providing broad ABI-level stability for high-level constructs is a significant and permanent burden on their design. It becomes an impediment to evolution, which is one of our stated goals. This doesn't preclude having low-level language features or tools to create specific and curated stable ABIs, or even serializable protocols. Using any such facilities will also cause developers to explicitly state where they are relying on ABI and isolating it in source from code which does not need that stability. However, these facilities would only expose a restricted set of language features to avoid coupling the high-level language to particular stabilized interfaces. There is a wide range of such facilities that should be explored, from serialization-based systems like protobufs or pickling in Python , to other approaches like COM or Swift's \"resilience\" model. The specific approach should be designed around the goals outlined above in order to fit the Carbon language. Backwards or forwards compatibility Our goals are focused on migration from one version of Carbon to the next rather than compatibility between them. This is rooted in our experience with evolving software over time more generally and a live-at-head model . Any transition, whether based on backward compatibility or a migration plan, will require some manual intervention despite our best efforts, due to Hyrum's Law , and so we should acknowledge that upgrades require active migrations. Legacy compiled libraries without source code or ability to rebuild We consider it a non-goal to support legacy code for which the source code is no longer available, though we do sympathize with such use cases and would like the tooling mentioned above to allow easier bridging between ABIs in these cases. Similarly, plugin ABIs aren\u2019t our particular concern, yet we\u2019re interested in seeing tooling which can help bridge between programs and plugins which use different ABIs. Support for existing compilation and linking models While it is essential to have interoperability with C++, we are willing to change the compilation and linking model of C++ itself to enable this if necessary. Compilation models and linking models should be designed to suit the needs of Carbon and its use cases, tools, and environments, not what happens to have been implemented thus far in compilers and linkers. As a concrete example, Carbon will not support platforms that cannot update their compiler and linker alongside the language. Idiomatic migration of non-modern, non-idiomatic C++ code While large-scale, tool-assisted migration of C++ code to Carbon is an explicit goal, handling all C++ code with this is expressly not a goal. There is likely a great deal of C++ code that works merely by chance or has serious flaws that prevent us from understanding the developer's intent. While we may be able to provide a minimally \"correct\" migration to very unfriendly code, mechanically reproducing exact C++ semantics even if bizarre, even this is not guaranteed and improving on it is not a goal. Migration support will prioritize code that adheres to reasonable C++ best practices, such as avoiding undefined behavior, maintaining good test coverage, and validating tests with sanitizers. Prioritization beyond goals The features, tools, and other efforts of Carbon should be prioritized based on a clearly articulated rationale. This may be based on this document's overarching goals and priorities, or if those don't offer enough clarity, we will fall back on rationale such as a required implementation order or a cost-benefit analysis. Cost-benefit will drive many choices. We expect to measure both cost, including complexity, and benefit using the impact on the project and language as a whole. Benefit accumulates over time, which means providing incremental solutions earlier will typically increase total benefit. It is also reasonable for the rationale of a decision to factor in both effort already invested, and effort ready to commit to the feature. This should not overwhelm any fundamental cost-benefit analysis. However, given two equally impactful features, we should focus on the solution that is moving the fastest. Domain-motivated libraries and features are an example. For these, the cost function will typically be the effort required to specify and implement the feature. Benefit will stem from the number of users and how much utility the feature provides. We don't expect to have concrete numbers for these, but we expect prioritization decisions between features to be expressed using this framework. Acknowledgements Carbon's goals are heavily based on \"Goals and priorities for C++\" . Many thanks to the authors and contributors for helping us formulate our goals and priorities.","title":"Goals"},{"location":"project/goals/#goals","text":"","title":"Goals"},{"location":"project/goals/#table-of-contents","text":"Overview Project goals Community and culture Language tools and ecosystem Language goals and priorities Performance-critical software Software and language evolution Code that is easy to read, understand, and write Practical safety and testing mechanisms Fast and scalable development Modern OS platforms, hardware architectures, and environments Interoperability with and migration from existing C++ code Non-goals Stable language and library ABI Backwards or forwards compatibility Legacy compiled libraries without source code or ability to rebuild Support for existing compilation and linking models Idiomatic migration of non-modern, non-idiomatic C++ code Prioritization beyond goals Acknowledgements","title":"Table of contents"},{"location":"project/goals/#overview","text":"Carbon is an experiment to explore a possible, distant future for the C++ programming language designed around a specific set of goals, priorities, and use cases. A programming language is a tool, and different tools are good for different purposes. We think there is great value in priorities that differentiate Carbon from other programming languages. Stating Carbon\u2019s priorities clearly and explicitly shapes the design of Carbon, and helps the entire community effectively evaluate and use the language. Carbon's language goals have historically been best addressed by C++, and there are large ecosystems and codebases written using C++ to these ends. Carbon should be attractive and easy for C++ developers to try out and incrementally adopt, even in individual libraries both using and used from C++ code. We expect this depends on having high-performance bidirectional interoperability with C++, excellent migration tooling, and an easy ramp-up for experienced C++ software developers. Principles are provided to clarify these goals. Principles do not supersede goals and priorities.","title":"Overview"},{"location":"project/goals/#project-goals","text":"","title":"Project goals"},{"location":"project/goals/#community-and-culture","text":"Carbon has an overarching goal of promoting a healthy and vibrant community with an inclusive, welcoming, and pragmatic culture. While this may not directly affect Carbon's design, it affects how Carbon's design occurs. We cannot build a good language without a good community. As the saying goes, \"culture eats strategy for breakfast\" . Carbon's community, including both maintainers and users, needs to last for years and be capable of scaling up. It needs to support people working on Carbon across a wide range of companies as their full time job, but also people contributing in small fractions of their time, or as students, teachers, or as a hobby. There are several key ingredients to achieving this. The community and project needs a code of conduct. We want Carbon's community to be welcoming and respectful, with a deep commitment to psychological safety. We need consistent expectations for how every community member should behave, regardless of their position in the community. These expectations around conduct and behavior need to be clearly articulated both to set expectations for people joining, and to help remind and anchor us on consistent standards. It is also important that we hold ourselves accountable to these expectations and have real and meaningful mechanisms to moderate the community. When behavior steps outside of our expectations, we need tools, process, and policy for how we will recognize and correct it. An open, inclusive process for Carbon changes. The community needs to be able to effectively engage in the direction and evolution of the project and language, while keeping the process efficient and effective. That means we need an open, inclusive process where everyone feels comfortable participating. Community members should understand how and why decisions are made, and have the ability to both influence them before they occur and give feedback afterward. We want to use this process to also ensure we stick to our language priorities and have clear rationales for all of our technical designs and decisions. Being inclusive is different from including everyone. We want to avoid excluding or marginalizing members of the community. However, we expect to inevitably make choices that benefit some Carbon community members more than others. We will provide justification for these decisions, but achieving Carbon's goals -- including that of a healthy community -- will be the guiding rule.","title":"Community and culture"},{"location":"project/goals/#language-tools-and-ecosystem","text":"Programming languages do not succeed in a vacuum. The Carbon project cannot merely design a language in order to succeed, it must tackle the full ecosystem of tooling that makes developers effective using the language. This includes not only a compiler and standard library, but also a broad range of other tools that enable developers to be more effective, efficient, or productive. We will provide a reference implementation. This helps the language have a strong and consistent experience for developers and a clear onboarding process. It also enables us to carefully consider implementation considerations throughout the design of the language. However, we do not want this to be seen as a replacement for a formal specification at any point. Carbon will have a formal specification. Fully specifying the language enables other implementations and allows us to clearly document the expected behavior of the reference implementation. This does not mean the specification defines what is \"correct\"; instead, the specification and reference implementation should complement each other. Any divergence is a bug that must be resolved, and the specification and reference should always converge. Carbon should not have designs or specifications which do not match the practical implementation, even if that means updating designs to reflect implementation realities. Having the specification will enable better analysis of the language as a whole and the production of other partial or full implementations which match the behavior of the reference implementation. Approachable, developer-facing documentation. Developers shouldn't be expected to read through the specification to ramp up with Carbon. User guides and other documentation will be provided to make it easy to learn how to use Carbon. Compelling adoption tooling. We want to provide a compelling suite of tools out-of-the-box in order to encourage adoption of Carbon at scale where it can augment existing C++ codebases. For example, we expect a C++ -> Carbon code translator will be important. Tooling for updating code when Carbon evolves. As Carbon evolves over time, we expect to provide tooling to help automate and scale migrating existing Carbon code to the new version. The goal is to enable more rapid evolution of the language without the churn tax and version skew becoming unsustainable. Developer tooling. We need developers to be productive reading and writing Carbon code. We expect to provide a broad suite of development oriented tools ranging from refactoring tools to LSP implementations and editor integrations. We also plan to provide machine readable forms of many parts of the language, such as a grammar, to ensure consistency between tools and enable the development of tools by others. Infrastructure to enable package management and other library ecosystem support. The goal is to support what the ecosystem needs, regardless of the exact form this ends up taking.","title":"Language tools and ecosystem"},{"location":"project/goals/#language-goals-and-priorities","text":"We are designing Carbon to support: Performance-critical software Software and language evolution Code that is easy to read, understand, and write Practical safety and testing mechanisms Fast and scalable development Modern OS platforms, hardware architectures, and environments Interoperability with and migration from existing C++ code Many languages share subsets of these goals, but what distinguishes Carbon is their combination. Where it is necessary to make tradeoffs between these goals, we intend to prioritize them in this order. Each goal is broad, and has several facets to consider when making decisions. Below, we discuss all of these goals in more detail to give a deeper understanding of both the nature and motivation of these goals.","title":"Language goals and priorities"},{"location":"project/goals/#performance-critical-software","text":"All software consumes resources: time, memory, compute, power, binary size, and so on. In many cases, raw resource usage is not the biggest concern. Instead, algorithmic efficiency or business logic dominates these concerns. However, there exists software where its rate of resource consumption -- its performance -- is critical to its successful operation. Another way to think about when performance is critical: would a performance regression be considered a bug? Would it even be noticed? Our goal is to support software where its performance with respect to some set of resource constraints is critical to its successful operation. This overarching goal can be decomposed into a few specific aspects. Provide the developer control over every aspect of performance. When faced with some performance problem, the developer should always have tools within Carbon to address it. This does not mean that the developer is necessarily concerned with ultimate performance at every moment, but in the most constrained scenarios they must be able to \"open up the hood\" without switching to another language. Idiomatic code should be fast. Developers should not regularly be required to choose between performance and readability. Although performance tuning may in rare cases require complex or surprising code, Carbon's design should ensure regular, idiomatic code usually results in high performance. Code should perform predictably. The reader and writer of code should be able to easily understand its expected performance, given sufficient background knowledge of the environment in which it will run. This need not be precise, but instead can use heuristics and guidelines to avoid surprise. The key priority is that performance, whether good or bad, is unsurprising to developers. Even pleasant surprises, when too frequent, can become a problem due to establishing brittle baseline performance that cannot be reliably sustained. Leave no room for a lower level language. Developers should not need to leave the rules and structure of Carbon, whether to gain control over performance problems or to gain access to hardware facilities.","title":"Performance-critical software"},{"location":"project/goals/#software-and-language-evolution","text":"Titus Winters writes in \"Non-Atomic Refactoring and Software Sustainability\": What is the difference between programming and software engineering? These are nebulous concepts and thus there are many possible answers, but my favorite definition is this: Software engineering is programming integrated over time. All of the hard parts of engineering come from dealing with time: compatibility over time, dealing with changes to underlying infrastructure and dependencies, and working with legacy code or data. Fundamentally, it is a different task to produce a programming solution to a problem (that solves the current [instance] of the problem) versus an engineering solution (that solves current instances, future instances that we can predict, and - through flexibility - allows updates to solve future instances we may not be able to predict). Carbon will prioritize being a \"software engineering\" language, in the above sense. We specifically are interested in dealing with the time-oriented aspects of software built in this language. We need to be prepared for substantive changes in priority over the next decade, on par with the changes experienced in the 2010s: 10x scaling of software organizations, mobile, cloud, diversification of platforms and architectures, and so on. Support maintaining and evolving software written in Carbon for decades. The life expectancy of some software will be long and the software will not be static or unchanging in that time. Mistakes will be made and need to be corrected. New functionality will be introduced and old functionality retired and removed. The design of Carbon must support and ease every step of this process. This ranges from emphasizing testing and continuous integration to tooling and the ability to make non-atomic changes. It also includes constraints on the design of Carbon itself: we should avoid, or at least minimize, language features that encourage unchangeable constructs. For example, any feature with a contract that cannot be strengthened or weakened without breaking the expected usage patterns is inherently hostile to refactoring. Analogously, features or conventions that require simultaneously updating all users of an API when extending it are inherently hostile towards long-term maintenance of software. Support maintaining and evolving the language itself for decades. We will not get the design of most language features correct on our first, second, or 73rd try. As a consequence, there must be a built-in plan and ability to move Carbon forward at a reasonable pace and with a reasonable cost. Simultaneously, an evolving language must not leave software behind to languish, but bring software forward. This requirement should not imply compatibility, but instead some migratability, likely tool-assisted. Be mindful of legacy. Globally, there may be as many as 50 billion lines of C++ code. Any evolution of Carbon that fails to account for human investment/training and legacy code, representing significant capital, is doomed from the start. Note that our priority is restricted to legacy source code; we do not prioritize full support of legacy object code. While that still leaves many options open, such as dedicated and potentially slower features, it does limit the degree to which legacy use cases beyond source code should shape the Carbon design.","title":"Software and language evolution"},{"location":"project/goals/#code-that-is-easy-to-read-understand-and-write","text":"While this is perhaps the least unique among programming languages of the goals we list here, we feel it is important to state it, explain all of what we mean by it, and fit it into our prioritization scheme. Software has inherent complexity that burdens developers, especially at scale and over time. Carbon will strive to minimize that burden for reading, understanding, and writing code. The behavior of code should be easily understood, especially by those unfamiliar with the software system. Consider developers attempting to diagnose a serious outage under time pressure -- every second spent trying to understand the language is one not spent understanding the problem . While the source code of our software may be read far more often by machines, humans are the most expensive readers and writers of software. As a consequence, we need to optimize for human reading, understanding, and writing of software, in that order. Excellent ergonomics. Human capabilities and limitations in the domains of perception, memory, reasoning, and decision-making affect interactions between humans and systems. Ergonomic language design takes human factors into account to increase productivity and comfort, and reduce errors and fatigue, making Carbon more suitable for humans to use. We can also say that ergonomic designs are accessible to humans. \"Readability\" is a related, but a more focused concept, connected to only the process of reading code. \"Ergonomics\" covers all activities where humans interact with Carbon: reading, writing, designing, discussing, reviewing, and refactoring code, as well as learning and teaching Carbon. A few examples: Carbon should not use symbols that are difficult to type, see, or differentiate from similar symbols in commonly used contexts. Syntax should be easily parsed and scanned by any human in any development environment, not just a machine or a human aided by semantic hints from an IDE. Code with similar behavior should use similar syntax, and code with different behavior should use different syntax. Behavior in this context should include both the functionality and performance of the code. This is part of conceptual integrity. Explicitness must be balanced against conciseness, as verbosity and ceremony add cognitive overhead for the reader, while explicitness reduces the amount of outside context the reader must have or assume. Common yet complex tasks, such as parallel code, should be well-supported in ways that are easy to reason about. Ordinary tasks should not require extraordinary care, because humans cannot consistently avoid making mistakes for an extended amount of time. Support tooling at every layer of the development experience, including IDEs. The design and implementation of Carbon should make it easy to create such tools and make them effective. Carbon should avoid syntax and textual structures that are difficult to recognize and mechanically change without losing meaning. Support software outside of the primary use cases well. There are surprisingly high costs for developers to switch languages. Even when the primary goal is to support performance-critical software, other kinds of software should not be penalized unnecessarily. \"The right tool for the job is often the tool you are already using -- adding new tools has a higher cost than many people appreciate.\" -- John Carmack Focus on encouraging appropriate usage of features rather than restricting misuse. Adding arbitrary restrictions to prevent misuse of otherwise general features of the language can create problems when they end up interfering with unexpected or rare but still appropriate usages. Instead, Carbon should focus on enabling appropriate and effective usage of features, and creating incentives around those. What seems initially like a \"misuse\" of a feature may be critical for some rare or future use case. Put differently, we will not always be able to prevent developers from misusing features or writing unnecessarily complex code, and that is okay. We should instead focus on helping reduce the rate that this occurs accidentally, and enabling tooling and diagnostics that warn about dangerous or surprising patterns. The behavior and semantics of code should be clearly and simply specified whenever possible. Leaving behavior undefined for some cases of invalid, buggy, or non-portable code may be necessary, but it comes at a very high cost and should be avoided. Every case where behavior is left undefined should be clearly spelled out with a strong rationale for this tradeoff. The code patterns without defined behavior should be teachable and understandable by developers. Finally, there must be mechanisms available to detect undefined behavior, at best statically, and at worst dynamically with high probability and at minimal cost. Adhere to the principle of least surprise. Defaults should match typical usage patterns. Implicit features should be unsurprising and expected, while explicit syntax should inform the reader about any behavior which might otherwise be surprising. The core concepts of implicit versus explicit syntax are well articulated in the Rust community , although we may come to different conclusions regarding the principles. Design features to be simple to implement. Syntax, structure, and language features should be chosen while keeping the implementation complexity manageable. Simplicity of implementation reduces bugs, and will in most cases make the features easier to understand. It's also often the best way to ensure predictable performance, although supporting peak performance may require options for more complex implementation behavior.","title":"Code that is easy to read, understand, and write"},{"location":"project/goals/#practical-safety-and-testing-mechanisms","text":"Our goal is to add as much language-level safety and security to Carbon as possible, using a hybrid strategy to balance other goals. We will do as many safety checks as we can at compile time. We will also provide dynamic runtime checking and a strong testing methodology ranging from unit tests through integration and system tests all the way to coverage-directed fuzz testing. We have specific criteria that are important for this strategy to be successful: Make unsafe or risky aspects of Carbon code explicit and syntactically visible. This will allow the software to use the precise flexibility needed and to minimize its exposure, while still aiding the reader. It can also help the reader more by indicating the specific nature of risk faced by a given construct. More simply, safe things shouldn't look like unsafe things and unsafe things should be easily recognized when reading code. Common patterns of unsafe or risky code must support static checking. Waiting until a dynamic check is too late to prevent the most common errors. A canonical example here are thread-safety annotations for basic mutex lock management to allow static checking. This handles the common patterns, and we use dynamic checks, such as TSan and deadlock detection, to handle edge cases. All unsafe or risky operations and interfaces must support some dynamic checking. Developers need some way to test and verify that their code using any such interface is in fact correct. Uncheckable unsafety removes any ability for the developer to gain confidence. This means we need to design features with unsafe or risky aspects with dynamic checking in mind. A concrete example of this can be seen in facilities that allow indexing into an array: such facilities should be designed to have the bounds of the array available to implement bounds checking when desirable.","title":"Practical safety and testing mechanisms"},{"location":"project/goals/#fast-and-scalable-development","text":"Software development iteration has a critical \"edit, test, debug\" cycle. Developers will use IDEs, editors, compilers, and other tools that need different levels of parsing. For small projects, raw parsing speed is essential; for large software systems, scalability of parsing is also necessary. Syntax should parse with bounded, small look-ahead. Syntax that requires unbounded look-ahead or fully general backtracking adds significant complexity to parsing and makes it harder to provide high quality error messages. The result is both slower iteration and more iterations, a multiplicative negative impact on productivity. Humans aren't immune either; they can be confused by constructs that appear to mean one thing but actually mean another. Instead, we should design for syntax that is fast to parse, with easy and reliable error messages. No semantic or contextual information used when parsing. The more context, and especially the more semantic context, required for merely parsing code, the fewer options available to improve the performance of tools and compilation. Cross-file context has an especially damaging effect on the potential distributed build graph options. Without these options, we will again be unable to provide fast developer iteration as the codebase scales up. Support separate compilation, including parallel and distributed strategies. Iteration requires frequent rebuilds of software as part of the edit/test/debug cycle of development. The language design should enable low-latency build strategies, particularly when relatively little has changed. This minimally requires separate compilation of source files, and potentially other incremental build strategies. Separate compilation also enables better scalability options for build systems of large software.","title":"Fast and scalable development"},{"location":"project/goals/#modern-os-platforms-hardware-architectures-and-environments","text":"Carbon must have strong support for all of the major, modern OS platforms, the hardware architectures they run on, and the environments in which their software runs. Carbon must also continue supporting these over time, even as which ones are major or modern evolve and change. Provide native support for the programming models of those platforms and environments. This goes beyond enabling compile-time translations from one abstraction to several implementations. While enabling high-level synchronization primitives like mutexes and futures is good, the underlying atomic operations provided by the hardware must also be directly available. Similarly, lowering parallel constructs into a specific implementation, such as SIMD or SPMD, is good but insufficient. Multiple parallel implementations must be directly addressable in Carbon. The need for native support repeats across the landscape of OS platform, hardware, and environment distinctions; for example, concurrency versus parallelism, and desktop versus mobile. Conversely, Carbon cannot prioritize support for historical platforms. To use a hockey metaphor, we should not skate to where the puck is, much less where the puck was twenty years ago. We have existing systems to support those platforms where necessary. Instead, Carbon should be forward-leaning in its platform support. As these platforms evolve over time, Carbon will have to evolve as well to continue to effectively prioritize the modern and major platforms. For examples, please see Carbon's success criteria .","title":"Modern OS platforms, hardware architectures, and environments"},{"location":"project/goals/#interoperability-with-and-migration-from-existing-c-code","text":"We want developers working within existing C++ ecosystems to easily start using Carbon, without starting from scratch. Adopting Carbon should not require complete rewrites, new programming models, or building an entire new stack/ecosystem. This means integrating into the existing C++ ecosystem by supporting incremental migration from C++ to Carbon, which in turn requires high-quality interoperability with existing C++ code. We must be able to move existing large C++ codebases -- some with hundreds of millions of lines of code and tens of thousands of active developers -- onto Carbon. C++ developers must also successfully switch to Carbon development. Any migration of this scale will take years, will need to be incremental, and some libraries -- particularly third-party -- may remain in C and C++. It must be possible to migrate a C++ library to Carbon without simultaneously migrating all of the libraries it depends on or all of the libraries that depend on it. We believe incremental migrations require: Familiarity for experienced C++ developers with a gentle learning curve. We need a feasible plan for retraining a C++ workforce to become proficient in Carbon. If long and significant study is required to be minimally proficient, meaning able to read, superficially understand, and do limited debugging or modifications, then the inertia of C++ will inevitably win. Further, we need a gentle and easily traversed learning curve to basic productivity in order for the transition to not become a chore or otherwise unsustainable for teams and individuals. Expressivity comparable to C++. If an algorithm or data structure or system architecture can naturally be written in C++, it should also be possible to write it naturally in Carbon. Automated source-to-source migration of large segments of large-scale idiomatic C++ code bases with high fidelity. We will prioritize having very low human interaction to achieve high fidelity migration results. We do not require all C++ code to be migratable in this fashion, and the resulting Carbon may be non-idiomatic. We can add reasonable constraints here if those constraints are already well established best practices for C++ development, including design patterns, testing coverage, or usage of sanitizers. Over many years, as Carbon evolves and codebases have had time to migrate, the results of the tooling may also drift further from idiomatic Carbon and have less desirable results. Support for bi-directional interoperability with existing C++ code. We need Carbon code to be able to call into C and C++ libraries with both reasonable API clarity and high performance. We will also need some ability to implement C++ interfaces with business logic in Carbon, although this direction can tolerate slightly more constraints both in supported features and performance overhead. In all cases, the particular performance overhead imposed by moving between C++ and Carbon will need to be easily exposed and understood by developers. While a given piece code only needs to be migrated once, we expect interoperability to be invoked continuously to support migrated code and will thus remain important for most developers.","title":"Interoperability with and migration from existing C++ code"},{"location":"project/goals/#non-goals","text":"There are common or expected goals of many programming languages that we explicitly call out as non-goals for Carbon. That doesn't make these things bad in any way, but reflects the fact that they do not provide meaningful value to us and come with serious costs and/or risks.","title":"Non-goals"},{"location":"project/goals/#stable-language-and-library-abi","text":"We would prefer to provide better, dedicated mechanisms to decompose software subsystems in ways that scale over time rather than providing a stable ABI across the Carbon language and libraries. Our experience is that providing broad ABI-level stability for high-level constructs is a significant and permanent burden on their design. It becomes an impediment to evolution, which is one of our stated goals. This doesn't preclude having low-level language features or tools to create specific and curated stable ABIs, or even serializable protocols. Using any such facilities will also cause developers to explicitly state where they are relying on ABI and isolating it in source from code which does not need that stability. However, these facilities would only expose a restricted set of language features to avoid coupling the high-level language to particular stabilized interfaces. There is a wide range of such facilities that should be explored, from serialization-based systems like protobufs or pickling in Python , to other approaches like COM or Swift's \"resilience\" model. The specific approach should be designed around the goals outlined above in order to fit the Carbon language.","title":"Stable language and library ABI"},{"location":"project/goals/#backwards-or-forwards-compatibility","text":"Our goals are focused on migration from one version of Carbon to the next rather than compatibility between them. This is rooted in our experience with evolving software over time more generally and a live-at-head model . Any transition, whether based on backward compatibility or a migration plan, will require some manual intervention despite our best efforts, due to Hyrum's Law , and so we should acknowledge that upgrades require active migrations.","title":"Backwards or forwards compatibility"},{"location":"project/goals/#legacy-compiled-libraries-without-source-code-or-ability-to-rebuild","text":"We consider it a non-goal to support legacy code for which the source code is no longer available, though we do sympathize with such use cases and would like the tooling mentioned above to allow easier bridging between ABIs in these cases. Similarly, plugin ABIs aren\u2019t our particular concern, yet we\u2019re interested in seeing tooling which can help bridge between programs and plugins which use different ABIs.","title":"Legacy compiled libraries without source code or ability to rebuild"},{"location":"project/goals/#support-for-existing-compilation-and-linking-models","text":"While it is essential to have interoperability with C++, we are willing to change the compilation and linking model of C++ itself to enable this if necessary. Compilation models and linking models should be designed to suit the needs of Carbon and its use cases, tools, and environments, not what happens to have been implemented thus far in compilers and linkers. As a concrete example, Carbon will not support platforms that cannot update their compiler and linker alongside the language.","title":"Support for existing compilation and linking models"},{"location":"project/goals/#idiomatic-migration-of-non-modern-non-idiomatic-c-code","text":"While large-scale, tool-assisted migration of C++ code to Carbon is an explicit goal, handling all C++ code with this is expressly not a goal. There is likely a great deal of C++ code that works merely by chance or has serious flaws that prevent us from understanding the developer's intent. While we may be able to provide a minimally \"correct\" migration to very unfriendly code, mechanically reproducing exact C++ semantics even if bizarre, even this is not guaranteed and improving on it is not a goal. Migration support will prioritize code that adheres to reasonable C++ best practices, such as avoiding undefined behavior, maintaining good test coverage, and validating tests with sanitizers.","title":"Idiomatic migration of non-modern, non-idiomatic C++ code"},{"location":"project/goals/#prioritization-beyond-goals","text":"The features, tools, and other efforts of Carbon should be prioritized based on a clearly articulated rationale. This may be based on this document's overarching goals and priorities, or if those don't offer enough clarity, we will fall back on rationale such as a required implementation order or a cost-benefit analysis. Cost-benefit will drive many choices. We expect to measure both cost, including complexity, and benefit using the impact on the project and language as a whole. Benefit accumulates over time, which means providing incremental solutions earlier will typically increase total benefit. It is also reasonable for the rationale of a decision to factor in both effort already invested, and effort ready to commit to the feature. This should not overwhelm any fundamental cost-benefit analysis. However, given two equally impactful features, we should focus on the solution that is moving the fastest. Domain-motivated libraries and features are an example. For these, the cost function will typically be the effort required to specify and implement the feature. Benefit will stem from the number of users and how much utility the feature provides. We don't expect to have concrete numbers for these, but we expect prioritization decisions between features to be expressed using this framework.","title":"Prioritization beyond goals"},{"location":"project/goals/#acknowledgements","text":"Carbon's goals are heavily based on \"Goals and priorities for C++\" . Many thanks to the authors and contributors for helping us formulate our goals and priorities.","title":"Acknowledgements"},{"location":"project/groups/","text":"Groups These are groups used by the Carbon Language project, listed here for central tracking. We use a mix of: Groups to assist contacting key contributors on appropriate systems: GitHub teams Discord roles Google groups , usually as Google Drive ACLs. We generally won't use these as contact lists, unless specifically mentioned. Please prefer other community forums. All contributors GitHub organization GitHub team: Contributors with label access : Mirrors the GitHub organization for write access. Manually updated . Discord access Google group : Grants Google Drive access. Team-specific access Any team-specific access will typically be managed by a team owner or admin, when somebody joins the respective team. Admins GitHub owners Discord role: admin Carbon leads GitHub team Discord role: carbon-leads Conduct team For most purposes, the Core team should be contacted about conduct issues. Google group : Primarily a contact list. Implementation team This team is responsible for development of Carbon's primary, reference implementation and toolchain. It also oversees other related implementation work within the Carbon project, from tooling of the language spec to test suites. There may be some overlap with admins -- any issue can be resolved by escalating to the Carbon leads . Notably, this team is not responsible for the design of the language itself, only for its implementation. Github team Discord role: implementation-team","title":"Groups"},{"location":"project/groups/#groups","text":"These are groups used by the Carbon Language project, listed here for central tracking. We use a mix of: Groups to assist contacting key contributors on appropriate systems: GitHub teams Discord roles Google groups , usually as Google Drive ACLs. We generally won't use these as contact lists, unless specifically mentioned. Please prefer other community forums.","title":"Groups"},{"location":"project/groups/#all-contributors","text":"GitHub organization GitHub team: Contributors with label access : Mirrors the GitHub organization for write access. Manually updated . Discord access Google group : Grants Google Drive access.","title":"All contributors"},{"location":"project/groups/#team-specific-access","text":"Any team-specific access will typically be managed by a team owner or admin, when somebody joins the respective team.","title":"Team-specific access"},{"location":"project/groups/#admins","text":"GitHub owners Discord role: admin","title":"Admins"},{"location":"project/groups/#carbon-leads","text":"GitHub team Discord role: carbon-leads","title":"Carbon leads"},{"location":"project/groups/#conduct-team","text":"For most purposes, the Core team should be contacted about conduct issues. Google group : Primarily a contact list.","title":"Conduct team"},{"location":"project/groups/#implementation-team","text":"This team is responsible for development of Carbon's primary, reference implementation and toolchain. It also oversees other related implementation work within the Carbon project, from tooling of the language spec to test suites. There may be some overlap with admins -- any issue can be resolved by escalating to the Carbon leads . Notably, this team is not responsible for the design of the language itself, only for its implementation. Github team Discord role: implementation-team","title":"Implementation team"},{"location":"project/pull_request_workflow/","text":"Trunk-based pull-request GitHub workflow Table of contents Trunk based development Green tests Always use pull requests (with review) rather than pushing directly Small, incremental changes Stacking dependent pull requests Managing pull requests with multiple commits Linear history Carbon repositories follow a few basic principles: Development directly on the trunk branch and revert to green . Always use pull requests, rather than pushing directly. Changes should be small, incremental, and review-optimized. Preserve linear history by rebasing or squashing pull requests rather than using unsquashed merge commits. These principles try to optimize for several different uses or activities with version control: Continuous integration and bisection to identify failures and revert to green. Code review both at the time of commit and follow-up review after commit. Understanding how things evolve over time, which can manifest in different ways: When were things introduced? How does the main branch and project evolve over time? How was a bug or surprising thing introduced? Note that this document focuses on the mechanical workflow and branch management. Details of the code review process are in their own document . Trunk based development We work in a simple trunk-based development model. This means all development activity takes place on a single common trunk branch in the repository (our default branch). We focus on small, incremental changes rather than feature branches or the \"scaled\" variations of this workflow. Green tests The trunk branch should always stay \"green\". That means that if tests fail or if we discover bugs or errors, we revert to a \"green\" state by default, where the failure or bug is no longer present. Fixing forward is fine if that will be comparably fast and efficient. The goal isn't to dogmatically avoid fixing forward, but to prioritize getting back to green quickly. We hope to eventually tool this through automatic continuous-integration powered submit queues, but even those can fail and the principle remains. Always use pull requests (with review) rather than pushing directly We want to ensure that changes to Carbon are always reviewed, and the simplest way to do this is to consistently follow a pull request workflow. Even if the change seems trivial, still go through a pull request -- it'll likely be trivial to review. Always wait for someone else to review your pull request rather than just merging it, even if you have permission to do so. Our GitHub repositories are configured to require pull requests and review before they are merged, so this rule is enforced automatically. Small, incremental changes Developing in small, incremental changes improves code review time, continuous integration, and bisection. This means we typically squash pull requests into a single commit when landing. We use two fundamental guides for deciding how to split up pull requests: Ensure that each pull request builds and passes any tests cleanly when you request review and when it lands. This will ensure bisection and continuous integration can effectively process them. Without violating the first point, try to get each pull request to be \"just right\": not too big, not too small. You don't want to separate a pattern of tightly related changes into separate requests when they're easier to review as a set or batch, and you don't want to bundle unrelated changes together. Typically you should try to keep the pull request as small as you can without breaking apart tightly coupled changes. However, listen to your code reviewer if they ask to split things up or combine them. While the default is to squash pull requests into a single commit, during the review you typically want to leave the development history undisturbed until the end so that comments on any particular increment aren't lost. We typically use the GitHub squash-and-merge functionality to land things. Stacking dependent pull requests Carbon uses pull requests in the common, distributed GitHub model where you first fork the repository, typically into your own private GitHub fork, and then develop on feature branches in that fork. When a branch is ready for review, it is turned into a pull request against the official repository. This flow should always be where you start when contributing to Carbon, and it scales well even with many independent changes in flight. However, a common limitation to hit is when you want to create a stack of dependent , small, and incremental changes and allow them to be reviewed in parallel. Each of these should be its own pull request to facilitate our desire for small and incremental changes and review. Unfortunately, GitHub has very poor support for managing the review of these stacked pull requests. Specifically, one pull request cannot serve as the base for another pull request, so each pull request will include all of the commits and diffs of the preceding pull requests in the stack. We suggest a specific workflow to address this (note, commit access is required): Create your initial pull request from a branch of your fork, nothing special is needed at this step. Let's say you have a branch feature-basic in your clone of your fork, and that the origin remote is your fork. Push the branch to your fork: shell git checkout feature-basic git push origin And create a pull request for it using the gh tool: shell gh pr create Let's imagine this creates a pull request N in the upstream repository. If you end up needing to create a subsequent pull request based on the first one, we need to create a branch in the upstream repository that tracks the first pull request and serves as the base for the subsequent pull request. Assuming your fork $USER/carbon-lang is remote origin and carbon-language/carbon-lang is remote upstream in your repository: shell git checkout feature-basic git push upstream HEAD:pull-N-feature-basic Everyone marked as a contributor to Carbon is allowed to push branches if the name matches pull-* , skipping pull request review processes. They can be force pushed as necessary and deleted. These branch names should only be used for this ephemeral purpose. All other branch names are protected. Create your stacked branch on your fork: shell git checkout -b next-feature-extension git commit -a -m 'Some initial work on the next feature.' git push origin Create the pull request using the upstream branch tracking your prior pull request as the base: shell gh pr create --base pull-N-feature-basic This creates a baseline for the new, stacked pull request that you have manually synced to your prior pull request. Each time you update the original pull request by pushing more commits to the feature-basic branch on your origin , you'll want to re-push to the upstream tracking branch as well: shell git checkout feature-basic git commit -a -m 'Address some code review feedback...' git push git push upstream HEAD:pull-N-feature-basic Then merge those changes into your subsequent pull request: shell git checkout next-feature-extension git merge feature-basic git push The merge will prevent disrupting the history of next-feature-extension where you may have code review comments on specific commits, while still allowing the pull request diff view to show the new delta after incorporating the new baseline. Follow a similar process as in 5 above for merging updates from the main branch of upstream : ```shell git checkout trunk git pull --rebase upstream Update your fork (optional). git push Merge changes from upstream into your bracnh without disrpting history. git checkout feature-basic git merge trunk Push to the first PR on your fork. git push Synchronize the upstream tracking branch for the first PR. git push upstream HEAD:pull-N-feature-basic Merge changes from the the first PR (now including changes from trunk) without disrupting history. git checkout next-feature-extension git merge feature-basic And push to the second PR on your fork. git push ``` When the first pull request lands in the main upstream branch, merge those changes from upstream trunk into the stacked branch: ```shell Pick up the first PR's changes from upstream trunk. git checkout trunk git pull --rebase upstream Merge those changes into the stacked PR branch. git checkout next-feature-extension git merge trunk git push ``` Then update the stacked PR's base branch to be carbon-language:trunk rather than the upstream tracking branch. To do this, go to the page for the PR on GitHub, click the \"Edit\" button to the right of the PR title, and then select trunk from the \"base\" drop-down box below the PR title. Once that's done, delete the upstream tracking branch: shell git push upstream --delete pull-N-feature-basic When landing the second, stacked pull request, it will require actively rebasing or squashing due to the complex merge history used while updating. Additional notes: If you need to create a third or more stacked pull requests, simply repeat the steps starting from #2 above for each pull request in the stack, but starting from the prior pull request's branch. If you want to split the two pull requests so they become independent, you can explicitly edit the base branch of a pull request in the GitHub UI. The result will be two pull requests with an overlapping initial sequence of commits. You can then restructure each one to make sense independently. Managing pull requests with multiple commits Sometimes, it will make sense to land a series of separate commits for a single pull request through rebasing. This can happen when there is important overarching context that should feed into the review, but the changes can be usefully decomposed when landing them. When following this model, each commit you intend to end up on the trunk branch needs to follow the same fundamental rules as the pull request above: they should each build and pass tests when landed in order, and they should have well written, cohesive commit messages. Prior to landing the pull request, you are expected to rebase it to produce this final commit sequence, either interactively or not. This kind of rebase rewrites the history in Git, which can make it hard to track the resolution of code review comments. Typically, only do this as a cleanup step when the review has finished, or when it won't otherwise disrupt code review. It is healthy and expected to add \"addressing review comments\" commits during the review and then squashing them away before the pull request is merged. Linear history We want the history of the trunk branch of each repository to be as simple and easy to understand as possible. While Git has strong support for managing complex history and merge patterns, we find understanding and reasoning about the history -- especially for humans -- to be at least somewhat simplified by sticking to a linear progression. As a consequence, we either squash pull requests or rebase them when merging them.","title":"Trunk-based pull-request GitHub workflow"},{"location":"project/pull_request_workflow/#trunk-based-pull-request-github-workflow","text":"","title":"Trunk-based pull-request GitHub workflow"},{"location":"project/pull_request_workflow/#table-of-contents","text":"Trunk based development Green tests Always use pull requests (with review) rather than pushing directly Small, incremental changes Stacking dependent pull requests Managing pull requests with multiple commits Linear history Carbon repositories follow a few basic principles: Development directly on the trunk branch and revert to green . Always use pull requests, rather than pushing directly. Changes should be small, incremental, and review-optimized. Preserve linear history by rebasing or squashing pull requests rather than using unsquashed merge commits. These principles try to optimize for several different uses or activities with version control: Continuous integration and bisection to identify failures and revert to green. Code review both at the time of commit and follow-up review after commit. Understanding how things evolve over time, which can manifest in different ways: When were things introduced? How does the main branch and project evolve over time? How was a bug or surprising thing introduced? Note that this document focuses on the mechanical workflow and branch management. Details of the code review process are in their own document .","title":"Table of contents"},{"location":"project/pull_request_workflow/#trunk-based-development","text":"We work in a simple trunk-based development model. This means all development activity takes place on a single common trunk branch in the repository (our default branch). We focus on small, incremental changes rather than feature branches or the \"scaled\" variations of this workflow.","title":"Trunk based development"},{"location":"project/pull_request_workflow/#green-tests","text":"The trunk branch should always stay \"green\". That means that if tests fail or if we discover bugs or errors, we revert to a \"green\" state by default, where the failure or bug is no longer present. Fixing forward is fine if that will be comparably fast and efficient. The goal isn't to dogmatically avoid fixing forward, but to prioritize getting back to green quickly. We hope to eventually tool this through automatic continuous-integration powered submit queues, but even those can fail and the principle remains.","title":"Green tests"},{"location":"project/pull_request_workflow/#always-use-pull-requests-with-review-rather-than-pushing-directly","text":"We want to ensure that changes to Carbon are always reviewed, and the simplest way to do this is to consistently follow a pull request workflow. Even if the change seems trivial, still go through a pull request -- it'll likely be trivial to review. Always wait for someone else to review your pull request rather than just merging it, even if you have permission to do so. Our GitHub repositories are configured to require pull requests and review before they are merged, so this rule is enforced automatically.","title":"Always use pull requests (with review) rather than pushing directly"},{"location":"project/pull_request_workflow/#small-incremental-changes","text":"Developing in small, incremental changes improves code review time, continuous integration, and bisection. This means we typically squash pull requests into a single commit when landing. We use two fundamental guides for deciding how to split up pull requests: Ensure that each pull request builds and passes any tests cleanly when you request review and when it lands. This will ensure bisection and continuous integration can effectively process them. Without violating the first point, try to get each pull request to be \"just right\": not too big, not too small. You don't want to separate a pattern of tightly related changes into separate requests when they're easier to review as a set or batch, and you don't want to bundle unrelated changes together. Typically you should try to keep the pull request as small as you can without breaking apart tightly coupled changes. However, listen to your code reviewer if they ask to split things up or combine them. While the default is to squash pull requests into a single commit, during the review you typically want to leave the development history undisturbed until the end so that comments on any particular increment aren't lost. We typically use the GitHub squash-and-merge functionality to land things.","title":"Small, incremental changes"},{"location":"project/pull_request_workflow/#stacking-dependent-pull-requests","text":"Carbon uses pull requests in the common, distributed GitHub model where you first fork the repository, typically into your own private GitHub fork, and then develop on feature branches in that fork. When a branch is ready for review, it is turned into a pull request against the official repository. This flow should always be where you start when contributing to Carbon, and it scales well even with many independent changes in flight. However, a common limitation to hit is when you want to create a stack of dependent , small, and incremental changes and allow them to be reviewed in parallel. Each of these should be its own pull request to facilitate our desire for small and incremental changes and review. Unfortunately, GitHub has very poor support for managing the review of these stacked pull requests. Specifically, one pull request cannot serve as the base for another pull request, so each pull request will include all of the commits and diffs of the preceding pull requests in the stack. We suggest a specific workflow to address this (note, commit access is required): Create your initial pull request from a branch of your fork, nothing special is needed at this step. Let's say you have a branch feature-basic in your clone of your fork, and that the origin remote is your fork. Push the branch to your fork: shell git checkout feature-basic git push origin And create a pull request for it using the gh tool: shell gh pr create Let's imagine this creates a pull request N in the upstream repository. If you end up needing to create a subsequent pull request based on the first one, we need to create a branch in the upstream repository that tracks the first pull request and serves as the base for the subsequent pull request. Assuming your fork $USER/carbon-lang is remote origin and carbon-language/carbon-lang is remote upstream in your repository: shell git checkout feature-basic git push upstream HEAD:pull-N-feature-basic Everyone marked as a contributor to Carbon is allowed to push branches if the name matches pull-* , skipping pull request review processes. They can be force pushed as necessary and deleted. These branch names should only be used for this ephemeral purpose. All other branch names are protected. Create your stacked branch on your fork: shell git checkout -b next-feature-extension git commit -a -m 'Some initial work on the next feature.' git push origin Create the pull request using the upstream branch tracking your prior pull request as the base: shell gh pr create --base pull-N-feature-basic This creates a baseline for the new, stacked pull request that you have manually synced to your prior pull request. Each time you update the original pull request by pushing more commits to the feature-basic branch on your origin , you'll want to re-push to the upstream tracking branch as well: shell git checkout feature-basic git commit -a -m 'Address some code review feedback...' git push git push upstream HEAD:pull-N-feature-basic Then merge those changes into your subsequent pull request: shell git checkout next-feature-extension git merge feature-basic git push The merge will prevent disrupting the history of next-feature-extension where you may have code review comments on specific commits, while still allowing the pull request diff view to show the new delta after incorporating the new baseline. Follow a similar process as in 5 above for merging updates from the main branch of upstream : ```shell git checkout trunk git pull --rebase upstream","title":"Stacking dependent pull requests"},{"location":"project/pull_request_workflow/#update-your-fork-optional","text":"git push","title":"Update your fork (optional)."},{"location":"project/pull_request_workflow/#merge-changes-from-upstream-into-your-bracnh-without-disrpting-history","text":"git checkout feature-basic git merge trunk","title":"Merge changes from upstream into your bracnh without disrpting history."},{"location":"project/pull_request_workflow/#push-to-the-first-pr-on-your-fork","text":"git push","title":"Push to the first PR on your fork."},{"location":"project/pull_request_workflow/#synchronize-the-upstream-tracking-branch-for-the-first-pr","text":"git push upstream HEAD:pull-N-feature-basic","title":"Synchronize the upstream tracking branch for the first PR."},{"location":"project/pull_request_workflow/#merge-changes-from-the-the-first-pr-now-including-changes-from-trunk","text":"","title":"Merge changes from the the first PR (now including changes from trunk)"},{"location":"project/pull_request_workflow/#without-disrupting-history","text":"git checkout next-feature-extension git merge feature-basic","title":"without disrupting history."},{"location":"project/pull_request_workflow/#and-push-to-the-second-pr-on-your-fork","text":"git push ``` When the first pull request lands in the main upstream branch, merge those changes from upstream trunk into the stacked branch: ```shell","title":"And push to the second PR on your fork."},{"location":"project/pull_request_workflow/#pick-up-the-first-prs-changes-from-upstream-trunk","text":"git checkout trunk git pull --rebase upstream","title":"Pick up the first PR's changes from upstream trunk."},{"location":"project/pull_request_workflow/#merge-those-changes-into-the-stacked-pr-branch","text":"git checkout next-feature-extension git merge trunk git push ``` Then update the stacked PR's base branch to be carbon-language:trunk rather than the upstream tracking branch. To do this, go to the page for the PR on GitHub, click the \"Edit\" button to the right of the PR title, and then select trunk from the \"base\" drop-down box below the PR title. Once that's done, delete the upstream tracking branch: shell git push upstream --delete pull-N-feature-basic When landing the second, stacked pull request, it will require actively rebasing or squashing due to the complex merge history used while updating. Additional notes: If you need to create a third or more stacked pull requests, simply repeat the steps starting from #2 above for each pull request in the stack, but starting from the prior pull request's branch. If you want to split the two pull requests so they become independent, you can explicitly edit the base branch of a pull request in the GitHub UI. The result will be two pull requests with an overlapping initial sequence of commits. You can then restructure each one to make sense independently.","title":"Merge those changes into the stacked PR branch."},{"location":"project/pull_request_workflow/#managing-pull-requests-with-multiple-commits","text":"Sometimes, it will make sense to land a series of separate commits for a single pull request through rebasing. This can happen when there is important overarching context that should feed into the review, but the changes can be usefully decomposed when landing them. When following this model, each commit you intend to end up on the trunk branch needs to follow the same fundamental rules as the pull request above: they should each build and pass tests when landed in order, and they should have well written, cohesive commit messages. Prior to landing the pull request, you are expected to rebase it to produce this final commit sequence, either interactively or not. This kind of rebase rewrites the history in Git, which can make it hard to track the resolution of code review comments. Typically, only do this as a cleanup step when the review has finished, or when it won't otherwise disrupt code review. It is healthy and expected to add \"addressing review comments\" commits during the review and then squashing them away before the pull request is merged.","title":"Managing pull requests with multiple commits"},{"location":"project/pull_request_workflow/#linear-history","text":"We want the history of the trunk branch of each repository to be as simple and easy to understand as possible. While Git has strong support for managing complex history and merge patterns, we find understanding and reasoning about the history -- especially for humans -- to be at least somewhat simplified by sticking to a linear progression. As a consequence, we either squash pull requests or rebase them when merging them.","title":"Linear history"},{"location":"project/roadmap/","text":"Roadmap Table of contents Objective for 2022: make Carbon public, finish 0.1 language Completing the language design Going public Key results in 2022 Broaden participation so no organization is >50% Example ports of C++ libraries to Carbon (100% of woff2, 99% of RE2) Language design covers the syntax and semantics of the example port code. Demo implementation of core features with working examples Carbon explorer implementation of core features with test cases Beyond 2022 Potential 2023 goals: finish 0.2 language, stop experimenting Potential 2024-2025 goals: ship 1.0 language & organization Objective for 2022: make Carbon public, finish 0.1 language We have two primary goals for 2022: Shift the experiment to being public. Reach the point where the core language design is substantially complete. Completing the language design By the end of 2022, the core Carbon language design should be substantially complete, including designs for expressions and statements, classes, generics and templates, core built-in types and interfaces such as integers and pointers, and interoperability with C++. The design choices made to reach this point are expected to be experimental, and many of them may need revisiting before we reach 1.0, but the broad shape of the language should be clear at this point, and it should be possible to write non-trivial Carbon programs. An initial rough framework for the core standard library functionality should be provided, as necessary to support the core language components. A largely complete implementation of the core language design should be available in Carbon explorer. The toolchain should be able to parse the core language design, with some support for name lookup and type-checking. We should have begun writing non-trivial portions of the standard library, such as common higher-level data structures and algorithms. Going public At some point in 2022 we should shift the experiment to be public. This will allow us to significantly expand both those directly involved and contributing to Carbon but also those able to evaluate and give us feedback. We don't expect Carbon to shift away from an experiment until after it becomes public and after we have been able to collect and incorporate a reasonable amount of feedback from the broader industry and community. This feedback will be central in determining whether Carbon should continue past the experimental stage. Key results in 2022 There are several milestones that we believe are on the critical path to successfully achieving our main goal for the year, and point to concrete areas of focus for the project. Broaden participation so no organization is >50% Our goal is that no single organization makes up >50% of participation in the Carbon project, to ensure that we are including as broad and representative a set of perspectives in the evolution of Carbon as possible. As a proxy for the amount of participation, we will count the number of active participants from each organization in 2022, with the aim that each organization is represented by less than 50% of all active participatnts. There are many ways in which someone could be an active participant, and when the leads come to reflect on this at the end of the year, we expect this to be a judgment call. We will consider at least the following when measuring our success on this objective: Pull requests authored and reviewed, including proposals, code changes, and documentation changes. Contribution to discussions, including Discord, teleconferences, and GitHub issues. Example ports of C++ libraries to Carbon (100% of woff2 , 99% of RE2 ) The first part of this result is that all of the woff2 library is ported to Carbon in a way that exports the same C++ API. There should be no gaps in this port given that woff2 has a very simple C++ API and uses few C++ language features. RE2 is a larger library using significantly more language features. For that part of the result, fewer than 1% of its C++ lines of code should be missing a semantically meaningful port into Carbon code. An important nuance of this goal is that it doesn't include building a complete Carbon standard library beyond the most basic necessary types. The intent is to exercise and show the interoperability layers of Carbon by re-using the C++ standard library in many cases and exporting a compatible C++ API to both woff2 and RE2's current API. While this key result isn't directly tied to the main objective, we believe it represents a critical milestone for being able to achieve this objective. It both measures our progress solidifying Carbon's design and demonstrating the value proposition of Carbon. Note that both woff2 and RE2 libraries are chosen somewhat arbitrarily and could easily be replaced with a different, more effective libraries to achieve the fundamental result of demonstrating a compelling body of cohesive design and the overarching value proposition. Language design covers the syntax and semantics of the example port code. We should have a clear understanding of the syntax and semantics used by these example ports. We should be able to demonstrate that self-contained portions of the ported code work correctly using Carbon explorer. Demo implementation of core features with working examples A core set of Carbon features should be implemented sufficiently to build working examples of those features and run them successfully. These features could include: User-defined types, functions, namespaces, packages, and importing. Basic generic functions and types using interfaces. Initial/simple implementation of safety checking including at least bounds checking, simple lifetime checking, and simple initialization checking. Sum types sufficient for optional-types to model nullable pointers. Pattern matching sufficient for basic function overloading on types and arity, as well as unwrapping of optional types for guard statements. Stretch goals if we can hit the above: Instantiating a basic C++ template through interop layer for use within Carbon. The demo implementation should also provide demos outside of specific language features including: Basic benchmarking of the different phases of compilation (lexing, parsing, etc). A basic REPL command line. Stretch goals if we can hit the above: Automatic code formatter on top of the implementation infrastructure. A compiler explorer fork with REPL integrated. Benchmarking at this stage isn't expected to include extensive optimization. Instead, it should focus on letting us track large/high-level impact on different phases as they are developed or features are added. They may also help illustrate initial high-level performance characteristics of the implementation, but the long term focus should be on end-to-end user metrics. Automatic code formatting could be achieved many ways, but it seems useful to ensure the language and implementation both support use cases like formatting. Carbon explorer implementation of core features with test cases This should include both a human readable rendering of the formal semantics as well as an execution environment to run test cases through those semantics. The implementation should cover enough of the core language that example code, such as the above ports of woff2 and RE2 and the Carbon standard library, can be verified with Carbon explorer. Beyond 2022 Longer term goals are hard to pin down and always subject to change, but we want to give an idea of what kinds of things are expected at a high level further out in order to illustrate how the goals and priorities we have in 2022 feed into subsequent years. Potential 2023 goals: finish 0.2 language, stop experimenting Once Carbon is moving quickly and getting public feedback, we should be able to conclude the experiment. We should know if this is the right direction for moving C++ forward for a large enough portion of the industry and community, and whether the value proposition of this direction outweighs the cost. However, there will still be a lot of work to make Carbon into a production quality language, even if the experiment concludes successfully. Some concrete goals that might show up in this time frame: Self-hosting toolchain, including sufficient Carbon standard library support. Expand design of standard library to include, at least directionally, critical and complex areas. For example: concurrency/parallelism and networking/IO. Migration tooling sufficient to use with real-world libraries and systems. This might be used to help with self-hosting Carbon, as well as by initial early adopters evaluating Carbon. Create a foundation or similar organization to manage the Carbon project, separate from any corporate entities that fund work on Carbon. Potential 2024-2025 goals: ship 1.0 language & organization A major milestone will be the first version of a production language. We should also have finished transferring all governance of Carbon to an independent open source organization at that point. However, we won't know what a more realistic or clear schedule for these milestones will be until we get closer. Another important aspect of our goals in this time frame is expanding them to encompass the broader ecosystem of the language: End-to-end developer tooling and experience. Teaching and training material. Package management. Etc.","title":"Roadmap"},{"location":"project/roadmap/#roadmap","text":"","title":"Roadmap"},{"location":"project/roadmap/#table-of-contents","text":"Objective for 2022: make Carbon public, finish 0.1 language Completing the language design Going public Key results in 2022 Broaden participation so no organization is >50% Example ports of C++ libraries to Carbon (100% of woff2, 99% of RE2) Language design covers the syntax and semantics of the example port code. Demo implementation of core features with working examples Carbon explorer implementation of core features with test cases Beyond 2022 Potential 2023 goals: finish 0.2 language, stop experimenting Potential 2024-2025 goals: ship 1.0 language & organization","title":"Table of contents"},{"location":"project/roadmap/#objective-for-2022-make-carbon-public-finish-01-language","text":"We have two primary goals for 2022: Shift the experiment to being public. Reach the point where the core language design is substantially complete.","title":"Objective for 2022: make Carbon public, finish 0.1 language"},{"location":"project/roadmap/#completing-the-language-design","text":"By the end of 2022, the core Carbon language design should be substantially complete, including designs for expressions and statements, classes, generics and templates, core built-in types and interfaces such as integers and pointers, and interoperability with C++. The design choices made to reach this point are expected to be experimental, and many of them may need revisiting before we reach 1.0, but the broad shape of the language should be clear at this point, and it should be possible to write non-trivial Carbon programs. An initial rough framework for the core standard library functionality should be provided, as necessary to support the core language components. A largely complete implementation of the core language design should be available in Carbon explorer. The toolchain should be able to parse the core language design, with some support for name lookup and type-checking. We should have begun writing non-trivial portions of the standard library, such as common higher-level data structures and algorithms.","title":"Completing the language design"},{"location":"project/roadmap/#going-public","text":"At some point in 2022 we should shift the experiment to be public. This will allow us to significantly expand both those directly involved and contributing to Carbon but also those able to evaluate and give us feedback. We don't expect Carbon to shift away from an experiment until after it becomes public and after we have been able to collect and incorporate a reasonable amount of feedback from the broader industry and community. This feedback will be central in determining whether Carbon should continue past the experimental stage.","title":"Going public"},{"location":"project/roadmap/#key-results-in-2022","text":"There are several milestones that we believe are on the critical path to successfully achieving our main goal for the year, and point to concrete areas of focus for the project.","title":"Key results in 2022"},{"location":"project/roadmap/#broaden-participation-so-no-organization-is-50","text":"Our goal is that no single organization makes up >50% of participation in the Carbon project, to ensure that we are including as broad and representative a set of perspectives in the evolution of Carbon as possible. As a proxy for the amount of participation, we will count the number of active participants from each organization in 2022, with the aim that each organization is represented by less than 50% of all active participatnts. There are many ways in which someone could be an active participant, and when the leads come to reflect on this at the end of the year, we expect this to be a judgment call. We will consider at least the following when measuring our success on this objective: Pull requests authored and reviewed, including proposals, code changes, and documentation changes. Contribution to discussions, including Discord, teleconferences, and GitHub issues.","title":"Broaden participation so no organization is &gt;50%"},{"location":"project/roadmap/#example-ports-of-c-libraries-to-carbon-100-of-woff2-99-of-re2","text":"The first part of this result is that all of the woff2 library is ported to Carbon in a way that exports the same C++ API. There should be no gaps in this port given that woff2 has a very simple C++ API and uses few C++ language features. RE2 is a larger library using significantly more language features. For that part of the result, fewer than 1% of its C++ lines of code should be missing a semantically meaningful port into Carbon code. An important nuance of this goal is that it doesn't include building a complete Carbon standard library beyond the most basic necessary types. The intent is to exercise and show the interoperability layers of Carbon by re-using the C++ standard library in many cases and exporting a compatible C++ API to both woff2 and RE2's current API. While this key result isn't directly tied to the main objective, we believe it represents a critical milestone for being able to achieve this objective. It both measures our progress solidifying Carbon's design and demonstrating the value proposition of Carbon. Note that both woff2 and RE2 libraries are chosen somewhat arbitrarily and could easily be replaced with a different, more effective libraries to achieve the fundamental result of demonstrating a compelling body of cohesive design and the overarching value proposition.","title":"Example ports of C++ libraries to Carbon (100% of woff2, 99% of RE2)"},{"location":"project/roadmap/#language-design-covers-the-syntax-and-semantics-of-the-example-port-code","text":"We should have a clear understanding of the syntax and semantics used by these example ports. We should be able to demonstrate that self-contained portions of the ported code work correctly using Carbon explorer.","title":"Language design covers the syntax and semantics of the example port code."},{"location":"project/roadmap/#demo-implementation-of-core-features-with-working-examples","text":"A core set of Carbon features should be implemented sufficiently to build working examples of those features and run them successfully. These features could include: User-defined types, functions, namespaces, packages, and importing. Basic generic functions and types using interfaces. Initial/simple implementation of safety checking including at least bounds checking, simple lifetime checking, and simple initialization checking. Sum types sufficient for optional-types to model nullable pointers. Pattern matching sufficient for basic function overloading on types and arity, as well as unwrapping of optional types for guard statements. Stretch goals if we can hit the above: Instantiating a basic C++ template through interop layer for use within Carbon. The demo implementation should also provide demos outside of specific language features including: Basic benchmarking of the different phases of compilation (lexing, parsing, etc). A basic REPL command line. Stretch goals if we can hit the above: Automatic code formatter on top of the implementation infrastructure. A compiler explorer fork with REPL integrated. Benchmarking at this stage isn't expected to include extensive optimization. Instead, it should focus on letting us track large/high-level impact on different phases as they are developed or features are added. They may also help illustrate initial high-level performance characteristics of the implementation, but the long term focus should be on end-to-end user metrics. Automatic code formatting could be achieved many ways, but it seems useful to ensure the language and implementation both support use cases like formatting.","title":"Demo implementation of core features with working examples"},{"location":"project/roadmap/#carbon-explorer-implementation-of-core-features-with-test-cases","text":"This should include both a human readable rendering of the formal semantics as well as an execution environment to run test cases through those semantics. The implementation should cover enough of the core language that example code, such as the above ports of woff2 and RE2 and the Carbon standard library, can be verified with Carbon explorer.","title":"Carbon explorer implementation of core features with test cases"},{"location":"project/roadmap/#beyond-2022","text":"Longer term goals are hard to pin down and always subject to change, but we want to give an idea of what kinds of things are expected at a high level further out in order to illustrate how the goals and priorities we have in 2022 feed into subsequent years.","title":"Beyond 2022"},{"location":"project/roadmap/#potential-2023-goals-finish-02-language-stop-experimenting","text":"Once Carbon is moving quickly and getting public feedback, we should be able to conclude the experiment. We should know if this is the right direction for moving C++ forward for a large enough portion of the industry and community, and whether the value proposition of this direction outweighs the cost. However, there will still be a lot of work to make Carbon into a production quality language, even if the experiment concludes successfully. Some concrete goals that might show up in this time frame: Self-hosting toolchain, including sufficient Carbon standard library support. Expand design of standard library to include, at least directionally, critical and complex areas. For example: concurrency/parallelism and networking/IO. Migration tooling sufficient to use with real-world libraries and systems. This might be used to help with self-hosting Carbon, as well as by initial early adopters evaluating Carbon. Create a foundation or similar organization to manage the Carbon project, separate from any corporate entities that fund work on Carbon.","title":"Potential 2023 goals: finish 0.2 language, stop experimenting"},{"location":"project/roadmap/#potential-2024-2025-goals-ship-10-language-organization","text":"A major milestone will be the first version of a production language. We should also have finished transferring all governance of Carbon to an independent open source organization at that point. However, we won't know what a more realistic or clear schedule for these milestones will be until we get closer. Another important aspect of our goals in this time frame is expanding them to encompass the broader ecosystem of the language: End-to-end developer tooling and experience. Teaching and training material. Package management. Etc.","title":"Potential 2024-2025 goals: ship 1.0 language &amp; organization"},{"location":"project/roadmap_process/","text":"Roadmap process Carbon has an annual roadmap to align and focus the work of the teams and community. Teams will need to defer work on Carbon that may be good and make sense but doesn't align with the current focus and plan of the project. The core team will draft proposed roadmaps each year, going through the standard proposal review process. Objectives and key results will be based on goals , success criteria , and tactical features. It is expected that the core team will provide a draft decision and enter decision review first thing at the beginning of the year, concluding in an accepted plan of record for the overarching direction of the project for that year. Subteams may optionally follow a similar practice as needed for their area of Carbon. This roadmap is not strictly binding and does not need to cover everything that will happen. However, it can and should be used by teams to defer some proposals as needed to focus on proposals aligned with the current roadmap. The roadmap is also subject to change, just as any other document, with a fresh proposal. The core team should critically evaluate the direction and any new information on a quarterly basis during the initial phase of the project and adjust the roadmap as needed to stay focused on the most important things.","title":"Roadmap process"},{"location":"project/roadmap_process/#roadmap-process","text":"Carbon has an annual roadmap to align and focus the work of the teams and community. Teams will need to defer work on Carbon that may be good and make sense but doesn't align with the current focus and plan of the project. The core team will draft proposed roadmaps each year, going through the standard proposal review process. Objectives and key results will be based on goals , success criteria , and tactical features. It is expected that the core team will provide a draft decision and enter decision review first thing at the beginning of the year, concluding in an accepted plan of record for the overarching direction of the project for that year. Subteams may optionally follow a similar practice as needed for their area of Carbon. This roadmap is not strictly binding and does not need to cover everything that will happen. However, it can and should be used by teams to defer some proposals as needed to focus on proposals aligned with the current roadmap. The roadmap is also subject to change, just as any other document, with a fresh proposal. The core team should critically evaluate the direction and any new information on a quarterly basis during the initial phase of the project and adjust the roadmap as needed to stay focused on the most important things.","title":"Roadmap process"},{"location":"project/principles/","text":"Principles Some language goals will have widely-applicable, high-impact, and sometimes non-obvious corollaries. We collect concrete language design principles in this directory as a way to document and clarify these. Principles clarify, but do not supersede, goals and priorities. Principles should be used as a tool in making decisions, and to clarify to contributors how decisions are expected to be made. A key difference between a principle and the design of a language feature is that a principle should inform multiple designs, whereas a feature's design is typically more focused on achieving a specific goal or set of goals. The principle can help achieve consistency across those multiple designs. Note that these principles seek to establish both the approaches the project wants to pursue, as well as those we want to exclude. Errors are values Information accumulation Low context-sensitivity Prefer providing only one way to do a given thing Safety strategy One static open extension mechanism Success criteria","title":"Principles"},{"location":"project/principles/#principles","text":"Some language goals will have widely-applicable, high-impact, and sometimes non-obvious corollaries. We collect concrete language design principles in this directory as a way to document and clarify these. Principles clarify, but do not supersede, goals and priorities. Principles should be used as a tool in making decisions, and to clarify to contributors how decisions are expected to be made. A key difference between a principle and the design of a language feature is that a principle should inform multiple designs, whereas a feature's design is typically more focused on achieving a specific goal or set of goals. The principle can help achieve consistency across those multiple designs. Note that these principles seek to establish both the approaches the project wants to pursue, as well as those we want to exclude. Errors are values Information accumulation Low context-sensitivity Prefer providing only one way to do a given thing Safety strategy One static open extension mechanism Success criteria","title":"Principles"},{"location":"project/principles/error_handling/","text":"Principle: Errors are values Table of contents Background Principle Applications of these principles Background Most nontrivial programs contain functions that can fail , meaning that even if all their preconditions are met, they may not be able to perform their primary behavior. For example, a function that reads data from a remote server may fail if the server is unreachable, and a function that parses a string to return an integer may fail if the input string is not a properly-formatted integer. In many cases, the function author wants these failures to be recoverable , meaning that a direct or transitive caller can respond to the failure in some way that enables the program to continue running. Principle A Carbon function that needs to report recoverable failures should return a sum type whose alternatives represent the success case and failure cases, such as Optional(T) , Result(T, Error) , or Bool . The function's successful return value, and any metadata about the failure, should be embedded in the alternatives of the sum type, rather than reported by way of output parameters or other side channels. Carbon's design will prioritize making this form of error handling efficient and ergonomic. Applications of these principles Carbon errors, unlike exceptions in C++ and similar languages, will not be propagated implicitly. Instead, Carbon will very likely need to provide some explicit but syntactically lightweight means of propagating errors, such as Rust's ? operator, so that error-propagation boilerplate doesn't make it hard for readers to follow the logic of the success path. Carbon will not have a special syntax for specifying what kind of errors a function can emit, such as noexcept or dynamic exception specifications in C++, or throws in Java, because that information will be embedded in the function's return type. Similarly, Carbon errors will be statically typed, because Carbon return values are statically typed.","title":"Principle: Errors are values"},{"location":"project/principles/error_handling/#principle-errors-are-values","text":"","title":"Principle: Errors are values"},{"location":"project/principles/error_handling/#table-of-contents","text":"Background Principle Applications of these principles","title":"Table of contents"},{"location":"project/principles/error_handling/#background","text":"Most nontrivial programs contain functions that can fail , meaning that even if all their preconditions are met, they may not be able to perform their primary behavior. For example, a function that reads data from a remote server may fail if the server is unreachable, and a function that parses a string to return an integer may fail if the input string is not a properly-formatted integer. In many cases, the function author wants these failures to be recoverable , meaning that a direct or transitive caller can respond to the failure in some way that enables the program to continue running.","title":"Background"},{"location":"project/principles/error_handling/#principle","text":"A Carbon function that needs to report recoverable failures should return a sum type whose alternatives represent the success case and failure cases, such as Optional(T) , Result(T, Error) , or Bool . The function's successful return value, and any metadata about the failure, should be embedded in the alternatives of the sum type, rather than reported by way of output parameters or other side channels. Carbon's design will prioritize making this form of error handling efficient and ergonomic.","title":"Principle"},{"location":"project/principles/error_handling/#applications-of-these-principles","text":"Carbon errors, unlike exceptions in C++ and similar languages, will not be propagated implicitly. Instead, Carbon will very likely need to provide some explicit but syntactically lightweight means of propagating errors, such as Rust's ? operator, so that error-propagation boilerplate doesn't make it hard for readers to follow the logic of the success path. Carbon will not have a special syntax for specifying what kind of errors a function can emit, such as noexcept or dynamic exception specifications in C++, or throws in Java, because that information will be embedded in the function's return type. Similarly, Carbon errors will be statically typed, because Carbon return values are statically typed.","title":"Applications of these principles"},{"location":"project/principles/information_accumulation/","text":"Principle: Information accumulation Table of contents Background Principle Applications of this principle Exceptions Alternatives considered Background There are many different sources of information in a program, and a tool or a human interpreting code will not in general have full information, but will still want to draw conclusions about the code. Different languages take different approaches to this problem. For example: In C, information is accumulated linearly in each source file independently, and only information from earlier in the same file is available. A program can observe that information is incomplete at one point and complete at another. In C++, the behavior is largely similar to C, except: Within certain contexts in a class, information from later in the class definition is available. With C++20 modules, information from other source files can be made available. It is easier to observe -- perhaps even accidentally -- that information is accumulated incrementally. In Rust, all information from the entire crate is available everywhere within that crate, with exceptions for constructs like proc macros that can see the state of the program being incrementally built. In Swift, all information from the entire source file is available within that source file. Principle In Carbon, information is accumulated incrementally within each source file. Carbon programs are invalid if they would have a different meaning if more information were available. Carbon source files can be interpreted top-down, without referring to information that appears substantially later in a file. Source files are expected to be organized into a topological order where that makes sense, with forward declarations used to introduce names before they are first referenced when necessary. If a program attempts to use information that has not yet been provided, the program is invalid. There are multiple options for how this can be reported: The program can be rejected as soon as it tries to use information that might not be known yet. For the case where the information can only be provided in the same source file, an assumption about the information can be made at the point where it is needed, and the program can be rejected only if that assumption turns out to be incorrect. Disallowing programs from changing meaning in the context of more information ensures that the program is interpreted consistently or is rejected. This is especially important to the coherence of generics and templates. Applications of this principle As in C++, and unlike in Rust and Swift, name lookup only finds names declared earlier. Classes are incomplete until the end of their definition. Unlike in C++, any attempt to observe a property of an incomplete class that is not known until the class is complete renders the program invalid. When an impl needs to be resolved, only those impl s that have already been declared are considered. However, if a later impl would change the result of any earlier impl lookup, the program is invalid. Exceptions Because a class is not complete until its definition has been fully parsed, applying this rule would make it impossible to define most member functions within the class definition. In order to still provide the convenience of defining class member functions inline, such member function bodies are deferred and processed as if they appeared immediately after the end of the outermost enclosing class, like in C++. Alternatives considered Allow information to be used before it is provided globally , within a file , or within a top-level declaration . Do not allow inline method bodies to use members before they are declared Do not allow separate declaration and definition","title":"Principle: Information accumulation"},{"location":"project/principles/information_accumulation/#principle-information-accumulation","text":"","title":"Principle: Information accumulation"},{"location":"project/principles/information_accumulation/#table-of-contents","text":"Background Principle Applications of this principle Exceptions Alternatives considered","title":"Table of contents"},{"location":"project/principles/information_accumulation/#background","text":"There are many different sources of information in a program, and a tool or a human interpreting code will not in general have full information, but will still want to draw conclusions about the code. Different languages take different approaches to this problem. For example: In C, information is accumulated linearly in each source file independently, and only information from earlier in the same file is available. A program can observe that information is incomplete at one point and complete at another. In C++, the behavior is largely similar to C, except: Within certain contexts in a class, information from later in the class definition is available. With C++20 modules, information from other source files can be made available. It is easier to observe -- perhaps even accidentally -- that information is accumulated incrementally. In Rust, all information from the entire crate is available everywhere within that crate, with exceptions for constructs like proc macros that can see the state of the program being incrementally built. In Swift, all information from the entire source file is available within that source file.","title":"Background"},{"location":"project/principles/information_accumulation/#principle","text":"In Carbon, information is accumulated incrementally within each source file. Carbon programs are invalid if they would have a different meaning if more information were available. Carbon source files can be interpreted top-down, without referring to information that appears substantially later in a file. Source files are expected to be organized into a topological order where that makes sense, with forward declarations used to introduce names before they are first referenced when necessary. If a program attempts to use information that has not yet been provided, the program is invalid. There are multiple options for how this can be reported: The program can be rejected as soon as it tries to use information that might not be known yet. For the case where the information can only be provided in the same source file, an assumption about the information can be made at the point where it is needed, and the program can be rejected only if that assumption turns out to be incorrect. Disallowing programs from changing meaning in the context of more information ensures that the program is interpreted consistently or is rejected. This is especially important to the coherence of generics and templates.","title":"Principle"},{"location":"project/principles/information_accumulation/#applications-of-this-principle","text":"As in C++, and unlike in Rust and Swift, name lookup only finds names declared earlier. Classes are incomplete until the end of their definition. Unlike in C++, any attempt to observe a property of an incomplete class that is not known until the class is complete renders the program invalid. When an impl needs to be resolved, only those impl s that have already been declared are considered. However, if a later impl would change the result of any earlier impl lookup, the program is invalid.","title":"Applications of this principle"},{"location":"project/principles/information_accumulation/#exceptions","text":"Because a class is not complete until its definition has been fully parsed, applying this rule would make it impossible to define most member functions within the class definition. In order to still provide the convenience of defining class member functions inline, such member function bodies are deferred and processed as if they appeared immediately after the end of the outermost enclosing class, like in C++.","title":"Exceptions"},{"location":"project/principles/information_accumulation/#alternatives-considered","text":"Allow information to be used before it is provided globally , within a file , or within a top-level declaration . Do not allow inline method bodies to use members before they are declared Do not allow separate declaration and definition","title":"Alternatives considered"},{"location":"project/principles/library_apis_only/","text":"Principle: All APIs are library APIs Table of contents Background Principle Applications of this principle Exceptions Alternatives considered Background Every major modern programming language comes with a standard library, which consists of APIs that are not part of the core language, but instead are written in the language (although their implementations may not be). However, different languages draw the boundary between language and library in different places. For example, Go's map type is built into the core language, whereas the C++ equivalent, std::unordered_map , is part of the standard library. In Swift, even fundamental types like integers and pointers are part of the standard library; there are no truly \"built in\" types. These decisions can have important consequences for the design of the language. For example, many important features of C++, such as move semantics, variadics, and coroutines, were motivated largely by their anticipated uses in a small set of standard library types. In a language with a different design philosophy, those types could have been built into the core language. This would probably have substantially simplified the language, and made those types available faster. However, that would have come at the cost of less flexibility for users outside the common case. Principle In Carbon, every public function is declared in some Carbon api file, and every public interface , impl , and first-class type is defined in some Carbon api file. In some cases, the bodies of public functions will not be defined as Carbon code, or will be defined as hybrid Carbon code using intrinsics that aren't available to ordinary Carbon code. However, we will try to minimize those situations. Thus, even \"built-in\" APIs can be used like user-defined APIs, by importing the appropriate library and using qualified names from that library, relying on the ordinary semantic rules for Carbon APIs. Applications of this principle We expect Carbon to have a special \"prelude\" library that is implicitly imported by all Carbon source files, and there might be a special name lookup rule to allow the names in the prelude to be used unqualified. However, in accordance with this principle, they will remain available to ordinary qualified name lookup as well. According to the resolutions of #543 and #750 , Carbon will have a substantial number of type keywords, such as i32 , f64 , and bool . However, these keywords will all be aliases for ordinary type names, such as Carbon.Int(32) , Carbon.Float(64) , and Carbon.Bool . Furthermore, all arithmetic and logical operators will be overloadable, so that those types can be defined as class types. The member function bodies for these types will be probably not be implemented in Carbon, but this principle applies only to function declarations, not function definitions. Similarly, a pointer type such as Foo* will be an alias for some library class type, for example Carbon.Ptr(Foo) . As a result, Carbon will support overloading pointer operations like -> and unary * . All Carbon operations that use function-style syntax, such as sizeof() and decltype() in C++, will be standard library functions. As above, in some cases we may choose to alias those functions with keywords, and the function bodies may not be defined in Carbon. Exceptions This principle applies to types only if they are first-class , meaning that they can be the types of run-time variables, function parameters, and return values. Carbon's type system will probably also include some types whose usage is more restricted, and this principle will not apply to them. Most importantly, function types might not be first-class types, in which case they need not be library types. The logic for translating a literal expression to a value of the appropriate type is arguably part of that type's public API, but will not be part of that type's class definition. Tuple types will probably not fully conform to this principle, because doing so would be circular: there is no way to name a tuple type that doesn't rely on tuple syntax, and no way to define a class body for a tuple type that doesn't contain tuple patterns. However, we will strive to ensure that it is possible to define a parameterized class type within Carbon that supports all the same operations as built-in tuple types. Alternatives considered Built-in primitive types","title":"Principle: All APIs are library APIs"},{"location":"project/principles/library_apis_only/#principle-all-apis-are-library-apis","text":"","title":"Principle: All APIs are library APIs"},{"location":"project/principles/library_apis_only/#table-of-contents","text":"Background Principle Applications of this principle Exceptions Alternatives considered","title":"Table of contents"},{"location":"project/principles/library_apis_only/#background","text":"Every major modern programming language comes with a standard library, which consists of APIs that are not part of the core language, but instead are written in the language (although their implementations may not be). However, different languages draw the boundary between language and library in different places. For example, Go's map type is built into the core language, whereas the C++ equivalent, std::unordered_map , is part of the standard library. In Swift, even fundamental types like integers and pointers are part of the standard library; there are no truly \"built in\" types. These decisions can have important consequences for the design of the language. For example, many important features of C++, such as move semantics, variadics, and coroutines, were motivated largely by their anticipated uses in a small set of standard library types. In a language with a different design philosophy, those types could have been built into the core language. This would probably have substantially simplified the language, and made those types available faster. However, that would have come at the cost of less flexibility for users outside the common case.","title":"Background"},{"location":"project/principles/library_apis_only/#principle","text":"In Carbon, every public function is declared in some Carbon api file, and every public interface , impl , and first-class type is defined in some Carbon api file. In some cases, the bodies of public functions will not be defined as Carbon code, or will be defined as hybrid Carbon code using intrinsics that aren't available to ordinary Carbon code. However, we will try to minimize those situations. Thus, even \"built-in\" APIs can be used like user-defined APIs, by importing the appropriate library and using qualified names from that library, relying on the ordinary semantic rules for Carbon APIs.","title":"Principle"},{"location":"project/principles/library_apis_only/#applications-of-this-principle","text":"We expect Carbon to have a special \"prelude\" library that is implicitly imported by all Carbon source files, and there might be a special name lookup rule to allow the names in the prelude to be used unqualified. However, in accordance with this principle, they will remain available to ordinary qualified name lookup as well. According to the resolutions of #543 and #750 , Carbon will have a substantial number of type keywords, such as i32 , f64 , and bool . However, these keywords will all be aliases for ordinary type names, such as Carbon.Int(32) , Carbon.Float(64) , and Carbon.Bool . Furthermore, all arithmetic and logical operators will be overloadable, so that those types can be defined as class types. The member function bodies for these types will be probably not be implemented in Carbon, but this principle applies only to function declarations, not function definitions. Similarly, a pointer type such as Foo* will be an alias for some library class type, for example Carbon.Ptr(Foo) . As a result, Carbon will support overloading pointer operations like -> and unary * . All Carbon operations that use function-style syntax, such as sizeof() and decltype() in C++, will be standard library functions. As above, in some cases we may choose to alias those functions with keywords, and the function bodies may not be defined in Carbon.","title":"Applications of this principle"},{"location":"project/principles/library_apis_only/#exceptions","text":"This principle applies to types only if they are first-class , meaning that they can be the types of run-time variables, function parameters, and return values. Carbon's type system will probably also include some types whose usage is more restricted, and this principle will not apply to them. Most importantly, function types might not be first-class types, in which case they need not be library types. The logic for translating a literal expression to a value of the appropriate type is arguably part of that type's public API, but will not be part of that type's class definition. Tuple types will probably not fully conform to this principle, because doing so would be circular: there is no way to name a tuple type that doesn't rely on tuple syntax, and no way to define a class body for a tuple type that doesn't contain tuple patterns. However, we will strive to ensure that it is possible to define a parameterized class type within Carbon that supports all the same operations as built-in tuple types.","title":"Exceptions"},{"location":"project/principles/library_apis_only/#alternatives-considered","text":"Built-in primitive types","title":"Alternatives considered"},{"location":"project/principles/low_context_sensitivity/","text":"Principle: Low context-sensitivity Table of contents Principle Mitigations of context-sensitive costs Visual aids Contextual validity rather than meaning Reduced cost of mistakes Compiler-checked context Applications of the principle Imports and namespaces Name shadowing Flow-sensitive typing Coherence of names and generics Performance Principle Carbon should favor designs and mechanisms that are not sensitive to context. Instead, we should favor constructs that are not ambiguous so that they don't need context for disambiguation. This is in service to the goal that Carbon code is easy to read, understand, and write . In particular, this is about prioritizing reading and understanding over writing. We should be willing to trade off conciseness, which still benefits reading as well as writing, for a sufficiently impactful reduction in the amount of context needed to read and understand code. Context can be expensive in different ways, for example: It can be large : it might require looking through a lot of lines of code to find all of the relevant contextual information. It can be distant : the further away from the current declaration or definition, the more expensive it is to find contextual information. This can scale from a separate definition in the same file, to a separate file, or even to a separate package. It can be unpredictable : it might require careful searching of large bodies of code to locate the contextual information if its location cannot be predicted. It can be subtle : the contextual clues might be easily missed or mistaken. Code that isn't context sensitive is easier to copy or move between contexts, like files or functions. It is code that needs fewer changes when it is refactored, in support of software evolution . In general, we should start with more restrictive constructs that limit ambiguity and see if we can make them work. If we find those restrictions are burdensome, we will then have more information to inform the next step. Ideally we would address those use cases with simple tools that solve multiple problems. The goal is to make a bunch of orthogonal mechanisms, each of which are easily understood and act in unsurprising ways. If that next step is to loosen restrictions, that is generally easier to do while maintaining compatibility with existing code than adding new restrictions. Mitigations of context-sensitive costs There are several ways that the potential costs of context-sensitive code can be mitigated. These techniques can and should be leveraged to help minimize and mitigate the contextual costs of Carbon features, and in some cases may provide a path to a feature that would otherwise be prohibitively costly. Visual aids A direct way to reduce contextual costs is through lexical and syntactic structures that form visual aids. These can both reinforce what the context is and aid the reader in the expensive aspect of navigating the context. For example, representing contexts with indentation, or IDE highlighting of matching parentheses and braces. These visual hints make it easier for developers to notice contextual elements. Contextual validity rather than meaning When the context only affects the validity of code, but not its meaning, the costs are significantly reduced. In that case, understanding the meaning or behavior of the code doesn't require context, and a developer can easily rely on the compiler to check the validity. A simple example of this is contextually valid syntax, which is relatively common and inexpensive. However, reusing the same syntax with different contexts with different meanings shifts the contextual information from simple validity to impacting the meaning of code. Reduced cost of mistakes Another mitigation for the costs of context-sensitive code is when the cost of a mistake due to the context is low. Some simple examples: Context-sensitivity in comments is less expensive in general than in code. In places where the general meaning is clear, developers can safely and reliably work with that general understanding, and the context only provides a minor refinement. Compiler-checked context Another way the costs of mistakes can be reduced is when the compiler can reliably detect them. This is the fundamental idea behind statically type-checked languages: the compiler enforcement reduces the contextual cost of knowing what the types are. How early and effectively the compiler can detect the mistakes also plays a role in reducing this cost, which is part of the value proposition for definition-checked generics. An example of this situation in Rust is that the same syntax is used for a move and a copy of the value in a variable. Those cases are distinguished by whether the type implements a specific trait, which may not be readily ascertained. The compiler verifies that the code never uses a variable that is no longer valid due to having been moved from, which is expected to catch the problems that could arise from this difference. Otherwise the semantic difference between a move and a copy is considered in Rust to be low-enough stakes for there to be no need to signal that difference in the code. However, the reasoning that makes this example a good design on balance for Rust doesn't necessarily apply to Carbon. The compiler is checking to prevent errors , but it can't reliably check for unpredictable performance . Given Carbon's priorities, that might make this level of contextual information still too expensive. More background on this area of Rust specifically is presented in their blog post on language ergonomics . Applications of the principle There are many parts of Carbon that could potentially be analyzed through this lens, and we can't enumerate them all here. This section focuses on several examples to help illustrate how the principle is likely to be relevant to Carbon. They focus on either cases that showcase the principle in effect or cases which make challenging tradeoffs of the costs in the principle. Imports and namespaces There are several parts of the way imports and namespaces are designed in Carbon that reflect applications of this principle: Adding an import or reordering imports should never change behavior of existing code. This means the reader doesn't have to look through all the imports to understand how code behaves. This is also important for tooling, which should not have to worry about unwanted side effects when adding or sorting imports. Carbon doesn't provide an analogy to C++'s using namespace or a \"wildcard imports\" mechanisms that merge the names from one namespace into another. Either would introduce ambiguity in where a name is coming from, making the code more context-sensitive. Carbon doesn't support large blocks of code inside a namespace declaration , where the reader would have to search for the beginning of the block to see what namespace applies. Name shadowing We should limit how names can be reused with shadowing rules, so the meaning of a name doesn't change in surprising ways between scopes. Further, if you find a matching declaration you don't have to keep searching to see if there is another that hides the one you found. This both expands the context you have to consider, and is an opportunity to make a mistake identifying the correct context, potentially leading to misunderstanding of the code. Flow-sensitive typing This principle is an argument against flow-sensitive typing , where the type of a name can change depending on control flow. For example, Midori used this for optional types . If we were to support this in Carbon, you could unwrap an optional value by testing it against None . var x: Optional(Int) = ...; if (x != None) { // x has type Int. PrintInt(x); } // x is back to type Optional(Int). This can be taken farther, this example has x taking on three different types: var x: Optional(Optional(Int)) = ...; if (x != None) { // x has type Optional(Int). if (x != None) { // x has type Int. PrintInt(x); } // x has type Optional(Int). } // x has type Optional(Optional(Int)). The concern here is that the context is very subtle. The type of x is affected by otherwise ordinary-looking if statements and closing braces ( } ). While we might not want to completely eliminate the possibility of flow-sensitive typing in Carbon, it would have to overcome a large hurdle. We would only want a flow-sensitive feature if it delivered sufficiently large usability, consistency, or expressivity gains. Coherence of names and generics Carbon packages are designed to ensure all declared names belong to exactly one package and the compiler can enforcement Carbon's equivalent one-definition rule (ODR) . This avoids an issue in C++ where the ODR is not reliably checked by the compiler, which can leave the correctness of programs dependent on both distant and subtle contextual information. Similarly, Carbon generics should have coherence, like Rust , where types have a single implementation of an interface. And this should be enforced by the compiler, using rules like Rust's orphan rules . Performance Since Carbon's number one goal is performance , it is important that the performance characteristics of code be predictable and readily determined by readers. This argues that those characteristics should not depend on expensive context. For example, Carbon should not provide a dynamic_cast facility with the same capabilities of C++'s where distant aspects of the inheritance structure can cause surprising performance differences. Similarly, Carbon should try to ensure normal looking method calls and data member access don't have the surprising performance costs caused by virtual inheritance in C++. More generally, Carbon should avoid features with hidden costs, particularly when they scale based on subtle aspects of the context where those features are used.","title":"Principle: Low context-sensitivity"},{"location":"project/principles/low_context_sensitivity/#principle-low-context-sensitivity","text":"","title":"Principle: Low context-sensitivity"},{"location":"project/principles/low_context_sensitivity/#table-of-contents","text":"Principle Mitigations of context-sensitive costs Visual aids Contextual validity rather than meaning Reduced cost of mistakes Compiler-checked context Applications of the principle Imports and namespaces Name shadowing Flow-sensitive typing Coherence of names and generics Performance","title":"Table of contents"},{"location":"project/principles/low_context_sensitivity/#principle","text":"Carbon should favor designs and mechanisms that are not sensitive to context. Instead, we should favor constructs that are not ambiguous so that they don't need context for disambiguation. This is in service to the goal that Carbon code is easy to read, understand, and write . In particular, this is about prioritizing reading and understanding over writing. We should be willing to trade off conciseness, which still benefits reading as well as writing, for a sufficiently impactful reduction in the amount of context needed to read and understand code. Context can be expensive in different ways, for example: It can be large : it might require looking through a lot of lines of code to find all of the relevant contextual information. It can be distant : the further away from the current declaration or definition, the more expensive it is to find contextual information. This can scale from a separate definition in the same file, to a separate file, or even to a separate package. It can be unpredictable : it might require careful searching of large bodies of code to locate the contextual information if its location cannot be predicted. It can be subtle : the contextual clues might be easily missed or mistaken. Code that isn't context sensitive is easier to copy or move between contexts, like files or functions. It is code that needs fewer changes when it is refactored, in support of software evolution . In general, we should start with more restrictive constructs that limit ambiguity and see if we can make them work. If we find those restrictions are burdensome, we will then have more information to inform the next step. Ideally we would address those use cases with simple tools that solve multiple problems. The goal is to make a bunch of orthogonal mechanisms, each of which are easily understood and act in unsurprising ways. If that next step is to loosen restrictions, that is generally easier to do while maintaining compatibility with existing code than adding new restrictions.","title":"Principle"},{"location":"project/principles/low_context_sensitivity/#mitigations-of-context-sensitive-costs","text":"There are several ways that the potential costs of context-sensitive code can be mitigated. These techniques can and should be leveraged to help minimize and mitigate the contextual costs of Carbon features, and in some cases may provide a path to a feature that would otherwise be prohibitively costly.","title":"Mitigations of context-sensitive costs"},{"location":"project/principles/low_context_sensitivity/#visual-aids","text":"A direct way to reduce contextual costs is through lexical and syntactic structures that form visual aids. These can both reinforce what the context is and aid the reader in the expensive aspect of navigating the context. For example, representing contexts with indentation, or IDE highlighting of matching parentheses and braces. These visual hints make it easier for developers to notice contextual elements.","title":"Visual aids"},{"location":"project/principles/low_context_sensitivity/#contextual-validity-rather-than-meaning","text":"When the context only affects the validity of code, but not its meaning, the costs are significantly reduced. In that case, understanding the meaning or behavior of the code doesn't require context, and a developer can easily rely on the compiler to check the validity. A simple example of this is contextually valid syntax, which is relatively common and inexpensive. However, reusing the same syntax with different contexts with different meanings shifts the contextual information from simple validity to impacting the meaning of code.","title":"Contextual validity rather than meaning"},{"location":"project/principles/low_context_sensitivity/#reduced-cost-of-mistakes","text":"Another mitigation for the costs of context-sensitive code is when the cost of a mistake due to the context is low. Some simple examples: Context-sensitivity in comments is less expensive in general than in code. In places where the general meaning is clear, developers can safely and reliably work with that general understanding, and the context only provides a minor refinement.","title":"Reduced cost of mistakes"},{"location":"project/principles/low_context_sensitivity/#compiler-checked-context","text":"Another way the costs of mistakes can be reduced is when the compiler can reliably detect them. This is the fundamental idea behind statically type-checked languages: the compiler enforcement reduces the contextual cost of knowing what the types are. How early and effectively the compiler can detect the mistakes also plays a role in reducing this cost, which is part of the value proposition for definition-checked generics. An example of this situation in Rust is that the same syntax is used for a move and a copy of the value in a variable. Those cases are distinguished by whether the type implements a specific trait, which may not be readily ascertained. The compiler verifies that the code never uses a variable that is no longer valid due to having been moved from, which is expected to catch the problems that could arise from this difference. Otherwise the semantic difference between a move and a copy is considered in Rust to be low-enough stakes for there to be no need to signal that difference in the code. However, the reasoning that makes this example a good design on balance for Rust doesn't necessarily apply to Carbon. The compiler is checking to prevent errors , but it can't reliably check for unpredictable performance . Given Carbon's priorities, that might make this level of contextual information still too expensive. More background on this area of Rust specifically is presented in their blog post on language ergonomics .","title":"Compiler-checked context"},{"location":"project/principles/low_context_sensitivity/#applications-of-the-principle","text":"There are many parts of Carbon that could potentially be analyzed through this lens, and we can't enumerate them all here. This section focuses on several examples to help illustrate how the principle is likely to be relevant to Carbon. They focus on either cases that showcase the principle in effect or cases which make challenging tradeoffs of the costs in the principle.","title":"Applications of the principle"},{"location":"project/principles/low_context_sensitivity/#imports-and-namespaces","text":"There are several parts of the way imports and namespaces are designed in Carbon that reflect applications of this principle: Adding an import or reordering imports should never change behavior of existing code. This means the reader doesn't have to look through all the imports to understand how code behaves. This is also important for tooling, which should not have to worry about unwanted side effects when adding or sorting imports. Carbon doesn't provide an analogy to C++'s using namespace or a \"wildcard imports\" mechanisms that merge the names from one namespace into another. Either would introduce ambiguity in where a name is coming from, making the code more context-sensitive. Carbon doesn't support large blocks of code inside a namespace declaration , where the reader would have to search for the beginning of the block to see what namespace applies.","title":"Imports and namespaces"},{"location":"project/principles/low_context_sensitivity/#name-shadowing","text":"We should limit how names can be reused with shadowing rules, so the meaning of a name doesn't change in surprising ways between scopes. Further, if you find a matching declaration you don't have to keep searching to see if there is another that hides the one you found. This both expands the context you have to consider, and is an opportunity to make a mistake identifying the correct context, potentially leading to misunderstanding of the code.","title":"Name shadowing"},{"location":"project/principles/low_context_sensitivity/#flow-sensitive-typing","text":"This principle is an argument against flow-sensitive typing , where the type of a name can change depending on control flow. For example, Midori used this for optional types . If we were to support this in Carbon, you could unwrap an optional value by testing it against None . var x: Optional(Int) = ...; if (x != None) { // x has type Int. PrintInt(x); } // x is back to type Optional(Int). This can be taken farther, this example has x taking on three different types: var x: Optional(Optional(Int)) = ...; if (x != None) { // x has type Optional(Int). if (x != None) { // x has type Int. PrintInt(x); } // x has type Optional(Int). } // x has type Optional(Optional(Int)). The concern here is that the context is very subtle. The type of x is affected by otherwise ordinary-looking if statements and closing braces ( } ). While we might not want to completely eliminate the possibility of flow-sensitive typing in Carbon, it would have to overcome a large hurdle. We would only want a flow-sensitive feature if it delivered sufficiently large usability, consistency, or expressivity gains.","title":"Flow-sensitive typing"},{"location":"project/principles/low_context_sensitivity/#coherence-of-names-and-generics","text":"Carbon packages are designed to ensure all declared names belong to exactly one package and the compiler can enforcement Carbon's equivalent one-definition rule (ODR) . This avoids an issue in C++ where the ODR is not reliably checked by the compiler, which can leave the correctness of programs dependent on both distant and subtle contextual information. Similarly, Carbon generics should have coherence, like Rust , where types have a single implementation of an interface. And this should be enforced by the compiler, using rules like Rust's orphan rules .","title":"Coherence of names and generics"},{"location":"project/principles/low_context_sensitivity/#performance","text":"Since Carbon's number one goal is performance , it is important that the performance characteristics of code be predictable and readily determined by readers. This argues that those characteristics should not depend on expensive context. For example, Carbon should not provide a dynamic_cast facility with the same capabilities of C++'s where distant aspects of the inheritance structure can cause surprising performance differences. Similarly, Carbon should try to ensure normal looking method calls and data member access don't have the surprising performance costs caused by virtual inheritance in C++. More generally, Carbon should avoid features with hidden costs, particularly when they scale based on subtle aspects of the context where those features are used.","title":"Performance"},{"location":"project/principles/one_way/","text":"Principle: Prefer providing only one way to do a given thing Table of contents Background Principle Applications of this principle Caveats Specialized syntax Non-obvious alternatives In evolution Alternatives considered Background It's common in programming languages to provide multiple, similar ways of doing the same thing. Sometimes this reflects the legacy of a language, and difficulties in evolving in ways that would require changes to developer-authored code, thereby retaining backwards compatibility. Other times it reflects a desire to provide both verbose and concise versions of the same syntax. We are concerned with both forms. We also are cautious about creating alternatives that may give rise to a paradox of choice , wherein options are similar enough that developers actively spend time analyzing tarde-offs, and the time spent that way outweighs the potential benefits of a correct choice. Where multiple, similar implementation options exist, it can sometimes give rise to style guidelines to indicate a preferential choice; sometimes because one option is objectively better, but sometimes because making a choice is better than not making one. Even with a style guide, developers may diverge in style by accident or intent, choosing different coding patterns simply because either option works. It can also become an issue as developers move between an organization that they need to learn a new style guide, and relearn habits. A couple examples of this in other languages are: In Perl, \"There is more than one way to do it.\" In Python, \"There should be one -- and preferably only one -- obvious way to do it.\" Principle In Carbon, we will prefer providing only one way to do a given thing. That is, given a syntax scenario where multiple design options are available, we will tend to provide one option rather than than providing several and letting users choose. This echoes Python's principle. Minimizing choices serves several goals: Language tools should be easier to write and maintain with the lower language complexity implied by less duplication of functionality. Software and language evolution processes should find it easier to both consider existing syntax and avoid creation of new syntax conflicts. Understandability of code should be promoted if developers have less syntax they need to understand. This can be expected to improve code quality and productivity so long as the resulting code structures aren't overly complicated. By minimizing the overlap of language features, we hope to make work easier for both Carbon's maintainers and developers. Applications of this principle We can observe the application of this principle by comparing several language features to C++. There, improving understandability is frequently the primary motivation: Where C++ allows logical operators to be written with either symbols (for example, && ) or text (for example, and ), Carbon will only support one form (in this case, text ). Where C++ allows hexadecimal numeric literals to be either lowercase ( 0xaa ) or uppercase ( 0xAA ), and with x optionally uppercase as well, Carbon will only allow the 0xAA casing . Where C++ provides both struct and class with the only difference is access control defaults, Carbon will only provide one ( class , albeit with default public visibility diverging from C++). However, sometimes language tools are the primary motivation. For example, where C++ allows braces to be omitted for single-statement control flow blocks, Carbon will require braces . This offers a syntax simplification that should allow for better error detection. Caveats Specialized syntax Sometimes overlap will occur because a specialized syntax offers particular benefits, typically as a matter of convenience for either a common use-case or a particularly complex and important use-case. Some examples of why and where this occurs are: For performance , it may at times be necessary to provide a specialized syntax that better supports optimization than a generic syntax. For understandability of code , there may be times that a particular use-case is common enough that simplifying its syntax provides substantial benefit. For example, for (var x: auto in list) could typically be written with as a while loop, but range-based for loops are considered to improve understandability. However, C++'s for (;;) syntax is sufficiently close to while that we expect to use while to address the corresponding use-cases. For migration and interoperability , it may be pragmatic to provide both an ideal way of doing things for new Carbon code, and a separate approach that is more C++-compatible for migration. For example, consider generics and templates: generics are considered to be the preferred form for new code, but templates are considered a necessity for migration of C++ code. This is not an evolution situation because we do not anticipate ever removing templates. Non-obvious alternatives Echoing Python, there may be non-obvious alternative ways of doing a given thing, such as using while (condition) { DoSomething(); break; } in place of if (condition) { DoSomething(); } . As a more complex example, lambdas could be implemented using other code constructs; this would require significantly more code and hinder understandability. This kind of overlap may exist, but will hopefully be considered sufficiently non-idiomatic that examples won't be common in code. If a choice would not likely be based mainly on coding styles, it's likely sufficiently distinct that this principle won't apply. In evolution For evolution , it will often be necessary to temporarily provide an \"old\" and \"new\" way of doing things simultaneously. For example, if renaming a language feature, it may be appropriate to provide the same functionality under two identifiers. However, one should be marked as deprecated and eventually removed. We should be cautious of adding new, overlapping features without a plan to remove the corresponding legacy version. Alternatives considered Provide multiple ways of doing a given thing","title":"Principle: Prefer providing only one way to do a given thing"},{"location":"project/principles/one_way/#principle-prefer-providing-only-one-way-to-do-a-given-thing","text":"","title":"Principle: Prefer providing only one way to do a given thing"},{"location":"project/principles/one_way/#table-of-contents","text":"Background Principle Applications of this principle Caveats Specialized syntax Non-obvious alternatives In evolution Alternatives considered","title":"Table of contents"},{"location":"project/principles/one_way/#background","text":"It's common in programming languages to provide multiple, similar ways of doing the same thing. Sometimes this reflects the legacy of a language, and difficulties in evolving in ways that would require changes to developer-authored code, thereby retaining backwards compatibility. Other times it reflects a desire to provide both verbose and concise versions of the same syntax. We are concerned with both forms. We also are cautious about creating alternatives that may give rise to a paradox of choice , wherein options are similar enough that developers actively spend time analyzing tarde-offs, and the time spent that way outweighs the potential benefits of a correct choice. Where multiple, similar implementation options exist, it can sometimes give rise to style guidelines to indicate a preferential choice; sometimes because one option is objectively better, but sometimes because making a choice is better than not making one. Even with a style guide, developers may diverge in style by accident or intent, choosing different coding patterns simply because either option works. It can also become an issue as developers move between an organization that they need to learn a new style guide, and relearn habits. A couple examples of this in other languages are: In Perl, \"There is more than one way to do it.\" In Python, \"There should be one -- and preferably only one -- obvious way to do it.\"","title":"Background"},{"location":"project/principles/one_way/#principle","text":"In Carbon, we will prefer providing only one way to do a given thing. That is, given a syntax scenario where multiple design options are available, we will tend to provide one option rather than than providing several and letting users choose. This echoes Python's principle. Minimizing choices serves several goals: Language tools should be easier to write and maintain with the lower language complexity implied by less duplication of functionality. Software and language evolution processes should find it easier to both consider existing syntax and avoid creation of new syntax conflicts. Understandability of code should be promoted if developers have less syntax they need to understand. This can be expected to improve code quality and productivity so long as the resulting code structures aren't overly complicated. By minimizing the overlap of language features, we hope to make work easier for both Carbon's maintainers and developers.","title":"Principle"},{"location":"project/principles/one_way/#applications-of-this-principle","text":"We can observe the application of this principle by comparing several language features to C++. There, improving understandability is frequently the primary motivation: Where C++ allows logical operators to be written with either symbols (for example, && ) or text (for example, and ), Carbon will only support one form (in this case, text ). Where C++ allows hexadecimal numeric literals to be either lowercase ( 0xaa ) or uppercase ( 0xAA ), and with x optionally uppercase as well, Carbon will only allow the 0xAA casing . Where C++ provides both struct and class with the only difference is access control defaults, Carbon will only provide one ( class , albeit with default public visibility diverging from C++). However, sometimes language tools are the primary motivation. For example, where C++ allows braces to be omitted for single-statement control flow blocks, Carbon will require braces . This offers a syntax simplification that should allow for better error detection.","title":"Applications of this principle"},{"location":"project/principles/one_way/#caveats","text":"","title":"Caveats"},{"location":"project/principles/one_way/#specialized-syntax","text":"Sometimes overlap will occur because a specialized syntax offers particular benefits, typically as a matter of convenience for either a common use-case or a particularly complex and important use-case. Some examples of why and where this occurs are: For performance , it may at times be necessary to provide a specialized syntax that better supports optimization than a generic syntax. For understandability of code , there may be times that a particular use-case is common enough that simplifying its syntax provides substantial benefit. For example, for (var x: auto in list) could typically be written with as a while loop, but range-based for loops are considered to improve understandability. However, C++'s for (;;) syntax is sufficiently close to while that we expect to use while to address the corresponding use-cases. For migration and interoperability , it may be pragmatic to provide both an ideal way of doing things for new Carbon code, and a separate approach that is more C++-compatible for migration. For example, consider generics and templates: generics are considered to be the preferred form for new code, but templates are considered a necessity for migration of C++ code. This is not an evolution situation because we do not anticipate ever removing templates.","title":"Specialized syntax"},{"location":"project/principles/one_way/#non-obvious-alternatives","text":"Echoing Python, there may be non-obvious alternative ways of doing a given thing, such as using while (condition) { DoSomething(); break; } in place of if (condition) { DoSomething(); } . As a more complex example, lambdas could be implemented using other code constructs; this would require significantly more code and hinder understandability. This kind of overlap may exist, but will hopefully be considered sufficiently non-idiomatic that examples won't be common in code. If a choice would not likely be based mainly on coding styles, it's likely sufficiently distinct that this principle won't apply.","title":"Non-obvious alternatives"},{"location":"project/principles/one_way/#in-evolution","text":"For evolution , it will often be necessary to temporarily provide an \"old\" and \"new\" way of doing things simultaneously. For example, if renaming a language feature, it may be appropriate to provide the same functionality under two identifiers. However, one should be marked as deprecated and eventually removed. We should be cautious of adding new, overlapping features without a plan to remove the corresponding legacy version.","title":"In evolution"},{"location":"project/principles/one_way/#alternatives-considered","text":"Provide multiple ways of doing a given thing","title":"Alternatives considered"},{"location":"project/principles/safety_strategy/","text":"Safety strategy Table of contents Background What \"safety\" means in Carbon Safety guarantees versus hardening Philosophy Principles Details Incremental work when safety requires work Using build modes to manage safety checks Debug Performance Hardened Managing bugs without compile-time safety Caveats Probabilistic techniques likely cannot stop attacks Alternatives considered Guaranteed memory safety programming models Guaranteed compile-time memory safety using borrow checking Guaranteed run-time memory safety using reference counting Guaranteed run-time memory safety using garbage collection Build mode names Performance versus safety in the hardened build mode Add more build modes Background Carbon's goal is to provide practical safety and testing mechanisms . What \"safety\" means in Carbon Safety is protection from software bugs, whether the protection is required by the language or merely an implementation option. Application-specific logic errors can be prevented by testing, but can lead to security vulnerabilities in production. Safety categories will be referred to using names based on the type of security vulnerability they protect against. A key subset of safety categories Carbon should address are: Memory safety protects against invalid memory accesses. Carbon uses two main subcategories for memory safety: Spatial memory safety protects against accessing an address that's out of bounds for the source. This includes array boundaries, as well as dereferencing invalid pointers such as uninitialized pointers, NULL in C++, or manufactured pointer addresses. Temporal memory safety protects against accessing an address that has been deallocated. This includes use-after-free for heap and use-after-return for stack addresses. Type safety protects against accessing valid memory with an incorrect type, also known as \"type confusion\". Data race safety protects against racing memory access: when a thread accesses (read or write) a memory location concurrently with a different writing thread and without synchronizing. Safety guarantees versus hardening In providing safety, the underlying goal is to prevent attacks from turning a logic error into a security vulnerability . The three ways of doing this can be thought of in terms of how they prevent attacks: Safety guarantees prevent bugs. They offer a strong requirement that a particular security vulnerability cannot exist. Compile-time safety checks are always a safety guarantee, but safety guarantees may also be done at runtime. For example: At compile-time, range-based for loops offer a spatial safety guarantee that out-of-bounds issues cannot exist in the absence of concurrent modification of the sequence. At runtime, garbage collected languages offer a temporal safety guarantee because objects cannot be freed while they're still accessible. Error detection checks for common logic errors at runtime. For example: An array lookup function might offer spatial memory error detection by verifying that the passed index is in-bounds. A program can implement reference counting to detect a temporal memory error by checking whether any references remain when memory is freed. Safety hardening mitigates bugs, typically by minimizing the feasibility of an attack. For example: Control Flow Integrity (CFI) monitors for behavior which can subvert the program's control flow. In Clang , it is optimized for use in release builds. Typically CFI analysis will only detect a subset of attacks because it can't track each possible code path separately. It should still reduce the feasibility of both spatial memory, temporal memory, and type attacks. Memory tagging makes each attempt at an invalid read or write operation have a high probability of trapping, while still not detecting the underlying bug in every case. Realistic attacks require many such operations, so memory tagging may stop attacks in some environments. Alternatively, the trap might be asynchronous, leaving only a tiny window of time prior to the attack being detected and program terminated. These are probabilistic hardening and reduces the feasibility of both spatial and temporal memory attacks. Under both error detection and safety hardening, even if a safety is protected, the underlying bugs will still exist and will need to be fixed. For example, program termination could be used for a denial-of-service attack. Philosophy Carbon's practical safety and testing mechanisms will emphasize guaranteed safety where feasible without creating barriers to Carbon's other goals , particularly performance and interoperability. This limits Carbon's options for guaranteed safety, and as a result there will be more reliance upon error detection and safety hardening. The language's design should incentivize safe programming, although it will not be required. When writing code, Carbon developers should expect to receive safety without needing to add safety annotations. Carbon will have optional safety annotations for purposes such as optimizing safety checks or providing information that improves coverage of safety checks. Carbon will favor compile-time safety checks because catching issues early will make applications more reliable. Runtime checks, either error detection or safety hardening, will be enabled where safety cannot be proven at compile-time. There will be three high-level use cases or directions that Carbon addresses through different build modes that prioritize safety checks differently: A debug oriented build mode that prioritizes detecting bugs and reporting errors helpfully. A performance oriented build mode that skips any dynamic safety checks to reduce overhead. A hardened oriented build mode that prioritizes ensuring sufficient safety to prevent security vulnerabilities, although it may not allow detecting all of the bugs. These high level build modes may be tuned, either to select specific nuanced approach for achieving the high level goal, or to configure orthogonal constraints such as whether to prioritize binary size or execution speed. However, there is a strong desire to avoid requiring more fundamental build modes to achieve the necessary coverage of detecting bugs and shipping software. These build modes are also not expected to be interchangeable or compatible with each other within a single executable -- they must be a global selection. Although expensive safety checks could be provided through additional build modes, Carbon will favor safety checks that can be combined into these three build modes rather than adding more. Over time, safety should evolve using a hybrid compile-time and runtime safety approach to eventually provide a similar level of safety to a language that puts more emphasis on guaranteed safety, such as Rust . However, while Carbon may encourage developers to modify code in support of more efficient safety checks, it will remain important to improve the safety of code for developers who cannot invest into safety-specific code modifications. Principles Safety must be easy to ramp-up with , even if it means new developers don't receive the full safety that Carbon can offer. Developers should benefit from Carbon's safety without needing to learn and apply Carbon-specific design patterns. Some safety should be enabled by default, without safety-specific work, although some safety will require work to opt in. Developers concerned with performance should only need to disable safety in rare edge-cases. Where there is a choice between safety approaches, the safe option should be incentivized by making it equally easy or easier to use. If there is a default, it should be the safe option. It should be identifiable when the unsafe option is used. Incentives will prioritize, in order: Guaranteed safety. Error detection. Safety hardening. Unsafe and unmitigated code. Language design choices should favor more efficient implementations of safety checks. They should also allow favor automation of testing and fuzzing. Safety in Carbon must work with interoperable or migrated C++ code , so that C++ developers can readily take advantage of Carbon's improvements. Safety mechanisms will ideally be designed to apply to automatically migrated C++ code. Providing immediate safety improvements to Carbon adopters will help motivate adoption. In the other direction, safety mechanisms must not force manual rewriting of C++ code in order to migrate, either by creating design incompatibilities or performance degradations. Automated migration of C++ code to Carbon must work for most developers, even if it forces Carbon's safety design to take a different approach. Carbon's safety should degrade gracefully when Carbon code calls C++ code, although this may require use of the Carbon toolchain to compile the C++ code. Applications should be expected to use interoperability. Although some safety features will be Carbon-specific, safety should not stop at the language boundary. The rules for determining whether code will pass compile-time safety checking should be articulable, documented, and easy to understand. Compile-time safety checks should not change significantly across different build modes. The purpose of the build modes is to determine code generation. Each build mode will prioritize performance and safety differently: The debug build mode will produce development-focused binaries that prioritize fast iteration on code with safety checks that assist in identification and debugging of errors. The performance build mode will produce release-focused binaries that prioritize performance over safety. The hardened build mode will produce release-focused binaries that prioritize safety that is resistant to attacks at the cost of performance. Safety checks should try to be identical across build modes. There will be differences, typically due to performance overhead and detection rate trade-offs of safety check algorithms. The number of build modes will be limited, and should be expected to remain at the named three. Most developers will use two build modes in their work: debug for development, and either performance or hardened for releases. It's important to focus on checks that are cheap enough to run as part of normal development. Users are not expected to want to run additional development build modes for additional sanitizers. Limiting the number of build modes simplifies support for both Carbon maintainers, who can focus on a more limited set of configurations, and Carbon developers, who can easily choose which is better for their use-case. Each distinct safety-related build mode (debug, performance, and hardened) cannot be combined with others in the same binary. Cross-binary interfaces will exist in Carbon, and will need to be used by developers interested in combining libraries built under different build modes. Although runtime safety checks should prevent logic errors from turning into security vulnerabilities, the underlying logic errors will still be bugs. For example, some safety checks would result in application termination; this prevents execution of unexpected code and still needs to be fixed. Developers need a strong testing methodology to engineer correct software. Carbon will encourage testing and then leverage it with the checking build modes to find and fix bugs and vulnerabilities. Details Incremental work when safety requires work Carbon is prioritizing usability of the language, particularly minimizing retraining of C++ developers and easing migration of C++ codebases, over the kind of provable safety that some other languages pursue, particularly Rust. A key motivation of Carbon is to move C++ developers to a better, safer language. However, if Carbon requires manually rewriting or redesigning C++ code in order to maintain performance, it creates additional pressure on C++ developers to learn and spend time on safety. Safety will often not be the top priority for developers; as a result, Carbon must be thoughtful about how and when it forces developers to think about safety. Relying on multiple build modes to provide safety should fit into normal development workflows. Carbon can also have features to enable additional safety, so long as developers can start using Carbon in their applications without learning new paradigms. Where possible, safety checks shouldn't require work on the part of Carbon developers. A safety check that requires no code edits or can be handled by automated migration may be opt-out, as there is negligible cost to developers. One which requires local code changes should be opt-in because costs will scale with codebase size. Safety check approaches which would require substantial redesign by developers will be disfavored based on adoption cost, even if the alternative is a less-comprehensive approach. Using build modes to manage safety checks Carbon will likely start in a state where most safety checks are done at runtime. However, runtime detection of safety violations remains expensive. In order to make as many safety checks as possible available to developers, Carbon will adopt a strategy based on three build modes that target key use-cases. Debug The debug build mode targets developers who are iterating on code and running tests. It will emphasize detection and debugability, especially for safety issues. It needs to perform well enough to be run frequently by developers, but will make performance sacrifices to catch more safety issues. This mode should have runtime checks for the most common safety issues, but it can make trade-offs that improve performance in exchange for less frequent, but still reliable, detection. Developers should do most of their testing in this build mode. The debug build mode will place a premium on the debugability of safety violations. Where safety checks rely on hardening instead of guaranteed safety, violations should be detected with a high probability per single occurrence of the bug. Detected bugs will be accompanied by a detailed diagnostic report to ease classification and root cause identification. Performance The performance build mode targets the typical application that wants high performance from Carbon code, where performance considers processing time, memory, and disk space. Trade-offs will be made that maximize the performance. Only safety techniques that don't measurably impact application hot path performance will be enabled by default. This is a very high bar, but is crucial for meeting Carbon's performance goals, as well as allowing migration of existing C++ systems which may not have been designed with Carbon's safety semantics in mind. Hardened The hardened build mode targets applications where developers want strong safety against attacks in exchange for worse performance. It will work to prevent attacks in ways that attackers cannot work around , even if it means using techniques that create significant performance costs. Managing bugs without compile-time safety Carbon's reliance on runtime checks will allow developers to manage their security risk. Developers will still need to reliably find and fix the inevitable bugs, including both safety violations and regular business logic bugs. The cornerstone of managing bugs will be strong testing methodologies, with built-in support from Carbon. Strong testing is more than good test coverage. It means a combination of: Ensuring unsafe or risky operations and interfaces can easily be recognized by developers. Using static analysis tools to detect common bugs, and ensuring they're integrated into build and code review workflows. These could be viewed as static testing of code. Writing good test coverage, including unit, integration, and system tests. Implementing coverage-directed fuzz testing to discover bugs outside of manually authored test coverage, especially for interfaces handling untrusted data. Fuzz testing is a robust way to catch bugs when APIs may be used in ways developers don't consider. Running continuous integration, including automatic and continuous running of these tests. The checked development build mode should be validated, as well as any additional build modes necessary to cover different forms of behavior checking. Easing automated testing and fuzzing through language features. For example, if the language encourages value types and pure functions of some sort, they can be automatically fuzzed. These practices are necessary for reliable, large-scale software engineering. Maintaining correctness of business logic over time requires continuous and thorough testing. Without it, such software systems cannot be changed and evolved over time reliably. Carbon will re-use these practices in conjunction with checking build modes to mitigate the limitations of Carbon's safety guarantees without imposing overhead on production systems. When a developer chooses to use Carbon, adhering to this kind of testing methodology is essential for maintaining safety. As a consequence, Carbon's ecosystem, including the language, tools, and libraries, will need to directly work to remove barriers and encourage the development of these methodologies. The reliance on testing may make Carbon a poor choice in some environments; in environments where such testing rigor is infeasible, a language with a greater degree of static checking may be better suited. Caveats Probabilistic techniques likely cannot stop attacks It's expected that probabilistic techniques that can be applied at the language level are attackable through a variety of techniques: The attacker might be able to attack repeatedly until it gets through. The attacker may be able to determine when the attack would be detected and only run the attack when it would not be. The attacker might be able control the test condition to make detection much less likely or avoid detection completely. For example, if detection is based on the last 4 bits of a memory address, an attacker may be able to generate memory allocations, viewing the address and only attacking when there's a collision. Hardware vulnerabilities may make these attacks easier than they might otherwise appear. Future hardware vulnerabilities are difficult to predict. Note this statement focuses on what can be applied to the language level. Using a secure hash algorithm, such as SHA256, may be used to offer probabilistic defense in other situations. However, the overhead of a secure hash algorithm's calculation is significant in the context of most things that Carbon may do at the language level. Combining these issues, although it may seem like a probabilistic safety check could be proven to reliably detect attackers, it's likely infeasible to do so. For the various build modes, this means: The debug build mode will not typically be accessible to attackers, so where a probabilistic technique provides a better developer experience, it will be preferred. The performance build mode will often avoid safety checks in order to reach peak performance. As a consequence, even the weak protection of a probabilistic safety check may be used in order to provide some protection. The hardened build mode will prefer non-probabilistic techniques that cannot be attacked. Alternatives considered Guaranteed memory safety programming models Multiple approaches that would offer guaranteed memory safety have been considered, mainly based on other languages which offer related approaches. Carbon will likely rely more on error detection and hardening because of what the models would mean for Carbon's performance and C++ migration language goals. Guaranteed compile-time memory safety using borrow checking Rust offers a good example of an approach for compile-time safety based on borrow checking, which provides guaranteed safety. For code which can't implement borrow checking, runtime safety using reference counting is available and provides reliable error detection. This approach still allows for unsafe blocks , as well as types that offer runtime safety while wrapping unsafe interfaces. Carbon could use a similar approach for guaranteed safety by default. Advantages: Guaranteed safety, including against data races, is provided for the binaries. The emphasis on compile-time safety limits the scope of the runtime memory safety costs. With Rust, there is early evidence that there's a significant impact in reducing bugs generally. Imitating Rust's techniques would allow building on the huge work of the Rust community, reducing the risks of implementing similar in Carbon. Careful use of narrow unsafe escape hatches can be effectively encapsulated behind otherwise safe APIs. Disadvantages: Rust's approach to compile-time safety requires use of design patterns and idioms that are substantially different from C++. Conversion of C++ code to Rust results in either rewrites of code, or use of runtime safety checks that impair performance. Requires fully modeling lifetime and exclusivity in the type system. Data structures must be redesigned to avoid sharing mutable state. Increases complexity of node and pointer based data structures, such as linked lists. Imitating Rust's techniques may prove insufficient for achieving Carbon's compiler performance goals . Rust compilation performance suggests its borrow checking performance is slow, although it's difficult to determine how significant this is or whether it could be improved. The Rust compiler is slow , although much has been done to improve it . Details of type checking, particularly requiring parsing of function bodies to type check signatures, as well as wide use of monomorphization are likely significant contributors to Rust compilation performance. LLVM codegen is also a significant cost for Rust compilation performance. With Fuchsia as an example, in December 2020, borrow checking and type checking combined account for around 10% of Rust compile CPU time, or 25% of end-to-end compile time. The current cost of borrow checking is obscured both because of the combination with type checking, and because Fuchsia disables some compiler parallelization due to build system incompatibility. The complexity of using Rust's compile-time safety may incentivize unnecessary runtime checking of safety properties. For example, using RefCell or Rc to avoid changing designs to fit compile-time safety models. Some of the most essential safety tools that ease the ergonomic burden of the Rust-style lifetime model ( Rc ) introduce semantic differences that cannot then be eliminated in a context where performance is the dominant priority. It's possible to modify the Rust model several ways in order to reduce the burden on C++ developers: Don't offer safety guarantees for data races, eliminating RefCell . This would likely not avoid the need for Rc or Arc , and wouldn't substantially reduce the complexity. Require manual destruction of Rc , allowing safety checks to be disabled in the performance build mode to eliminate overhead. This still requires redesigning C++ code to take advantage of Rc . The possibility of incorrect manual destruction means that the safety issue is being turned into a bug, which means it is hardening and no longer a safety guarantee. Carbon can provide equivalent hardening through techniques such as MarkUs , which does not require redesigning C++ code. Overall, Carbon is making a compromise around safety in order to give a path for C++ to evolve. C++ developers must be comfortable migrating their codebases, and able to do so in a largely automated manner. In order to achieve automated migration, Carbon cannot require fundamental redesigns of migrated C++ code. While a migration tool could in theory mark all migrated code as unsafe , Carbon should use a safety strategy that degrades gracefully and offers improvements for C++ code, whether migrated or not. That does not mean Carbon will never adopt guaranteed safety by default, only that performance and migration of C++ code takes priority, and any design will need to be considered in the context of other goals. It should still be possible to adopt guaranteed safety later, although it will require identifying a migration path. Guaranteed run-time memory safety using reference counting Reference counting is a common memory safety model, with Swift as a popular example. Advantages: Simple model for safety, particularly as compared with Rust. Safe for all of the most common and important classes of memory safety bugs. Disadvantages: Safety based on reference counting introduces significant performance costs, and tools for controlling these costs are difficult. Safety based on garbage collection has less direct performance overhead, but has a greater unpredictability of performance. Significant design differences versus C++ still result, as the distinction between value types and \"class types\" becomes extremely important. Class types are held by a reference counted pointer and are thus lifetime safe. In order to mitigate the performance overhead, Swift does have a proposal to add an option for unique ownership, although the specifics are not designed yet. The unique ownership approach is expected to require unowned and unsafe access, so it would not considered to improve the safety trade-offs. Swift was designated by Apple as the replacement for Objective-C. The safety versus performance trade-offs that it makes fit Apple's priorities. Carbon's performance goals should lead to different trade-off decisions with a higher priority on peak performance, which effectively rules out broad use of reference counting. Guaranteed run-time memory safety using garbage collection Garbage collection is a common memory safety model, with Java as a popular example. Advantages: This approach is among the most robust and well studied models, with decades of practical usage and analysis for security properties. Extremely suitable for efficient implementation on top of a virtual machine, such as the JVM. Disadvantages: Extremely high complexity to fully understand the implications of complex cases like data races. Performance overhead is significant in terms of what Carbon would like to consider. Garbage collection remains a difficult performance problem, even for the JVM and its extensive optimizations. The complexity of the implementation makes it difficult to predict performance; for example, Java applications experience latency spikes when garbage collection runs. Java is a good choice for many applications, but Carbon is working to focus on a set of performance priorities that would be difficult to achieve with a garbage collector. Build mode names The build mode concepts are difficult to name. Other names that were evaluated, and are ultimately similar, are: \"Debug\" is a common term for the intended use of this build mode. Also, tooling including Visual Studio frequently uses the debug term for describing similar. \"Development\" was also considered, but this term is less specific and would be better for describing all non-release builds together. For example, a \"fast build\" mode might be added that disables safety checks to improve iteration time, like might be controlled by way of C++'s NDEBUG option. \"Performance\" aligns with the phrasing of the language performance goal. \"Optimized\" implies that other modes would not be fully optimized, but hardened should be optimized. \"Fast\" would suggest that speed is the only aspect of performance being optimizing for, but \"performance\" also optimizes for memory usage and binary size. \"Hardened\" is the choice for succinctly describing the additional safety measures that will be taken, and is a well-known term in the safety space. It could be incorrectly inferred that \"performance\" has no hardening, but the preference is to clearly indicate the priority of the \"hardened\" build mode. \"Safe\" implies something closer to guaranteed safety. However, safety bugs should be expected to result in program termination, which can still be used in other attacks, such as Denial-of-Service. \"Mitigated\" is an overloaded term, and it may not be succinctly clear that it's about security mitigations. Some terms which were considered and don't fit well into the above groups are: \"Release\" is avoided because both \"performance\" and \"hardened\" could be considered to be \"release\" build modes. The names \"performance\" and \"hardened\" may lead to misinterpretations, with some developers who should use \"hardened\" using \"performance\" because they are worried about giving up too much performance, and the other way around. The terms try to balance the utility of well-known terminology with the succinctness of a short phrase for build modes, and that limits the expressivity. Some confusion is expected, and documentation as well as real-world experience (for example, a developer who cares about latency benchmarking both builds) should be expected to help mitigate mix-ups. Performance versus safety in the hardened build mode The performance cost of safety techniques are expected to be non-linear with respect to detection rates. For example, a particular vulnerability such as heap use-after-free may be detectable with 99% accuracy at 20% performance cost, but 100% accuracy at 50% performance cost. At present, build modes should be expected to evaluate such a scenario as: The debug build mode would choose the 99% accurate approach. Detecting safety issues is valuable for debugging. The probabilistic detection rate won't meaningfully affect accuracy of tests. The lower performance cost improves developer velocity. The performance build mode would decline detection. Safety checks with a measurable performance cost should be declined. The hardened build mode would choose the 100% accurate approach. Safety must be non-probabilistic in order to reliably prevent attacks. Significant performance hits are acceptable. This means the hardened build mode may be slower than the debug build mode. In order to achieve better performance, the hardened build mode could make trade-offs closer to the debug build mode. Rather than relying on non-probabilistic techniques, it could instead offer a probability-based chance of detecting a given attack. Advantages: Probabilistic safety should come at lower performance cost (including CPU, memory, and disk space). This will sometimes be significant, and as a result of multiple checks, could be the difference between the hardened build mode being 50% slower than the performance build mode and being 200% slower. Disadvantages: Probabilistic techniques likely cannot stop attacks . Attackers may be able to repeat attacks until they succeed. The variables upon which the probability is based, such as memory addresses, may be manipulable by the attacker. As a consequence, a determined attacker may be able to manipulate probabilities and not even be detected. Although performance is Carbon's top goal , the hardened build mode exists to satisfy developers and environments that value safety more than performance. The hardened build mode will rely on non-probabilistic safety at significant performance cost because other approaches will be insufficient to guard against determined attackers. Add more build modes More build modes could be added to this principle, or the principle could encourage the idea that specific designs may add more. To explain why three build modes: The concept of debug and release (sometimes called opt) are common. For example, in Visual Studio . In Carbon, this could be considered to translate to the \"debug\" and \"performance\" build modes by default. The hardened build mode is added in order to emphasize security. Although hardened could be implemented as a set of options passed to the standard release build mode, the preference is to focus on it as an important feature. An example of why another build mode may be needed is ThreadSanitizer , which is noted as having 5-15x slowdown and 5-10x memory overhead. This is infeasible for normal use, but could be useful for some users in a separate build mode. A trade-off that's possible for Carbon is instead using an approach similar to KCSAN which offers relatively inexpensive but lower-probability race detection. Although options to these build modes may be supported to customize deployments, the preference is to focus on a small set and make them behave well. For example, if a separate build mode is added for ThreadSanitizer, it should be considered a temporary solution until it can be merged into the debug build mode. Advantages: Grants more flexibility for using build modes as a solution to problems. With safety checks, this would allow providing safety checks that are high overhead but also high detection rate as separate build modes. With other systems, there could be non-safety performance versus behavior trade-offs. Disadvantages: Having standard modes simplifies validation of interactions between various safety checks. Safety is the only reason that's been considered for adding build modes. As more build modes are added, the chance of developers being confused and choosing the wrong build mode for their application increases. Any long-term additions to the set of build modes will need to update this principle, raising the visibility and requiring more consideration of such an addition. If build modes are added for non-safety-related reasons, this may lead to moving build modes out of the safety strategy. Experiment : This can be considered an experiment. Carbon may eventually add more than the initial three build modes, although the reticence to add more is likely to remain.","title":"Safety strategy"},{"location":"project/principles/safety_strategy/#safety-strategy","text":"","title":"Safety strategy"},{"location":"project/principles/safety_strategy/#table-of-contents","text":"Background What \"safety\" means in Carbon Safety guarantees versus hardening Philosophy Principles Details Incremental work when safety requires work Using build modes to manage safety checks Debug Performance Hardened Managing bugs without compile-time safety Caveats Probabilistic techniques likely cannot stop attacks Alternatives considered Guaranteed memory safety programming models Guaranteed compile-time memory safety using borrow checking Guaranteed run-time memory safety using reference counting Guaranteed run-time memory safety using garbage collection Build mode names Performance versus safety in the hardened build mode Add more build modes","title":"Table of contents"},{"location":"project/principles/safety_strategy/#background","text":"Carbon's goal is to provide practical safety and testing mechanisms .","title":"Background"},{"location":"project/principles/safety_strategy/#what-safety-means-in-carbon","text":"Safety is protection from software bugs, whether the protection is required by the language or merely an implementation option. Application-specific logic errors can be prevented by testing, but can lead to security vulnerabilities in production. Safety categories will be referred to using names based on the type of security vulnerability they protect against. A key subset of safety categories Carbon should address are: Memory safety protects against invalid memory accesses. Carbon uses two main subcategories for memory safety: Spatial memory safety protects against accessing an address that's out of bounds for the source. This includes array boundaries, as well as dereferencing invalid pointers such as uninitialized pointers, NULL in C++, or manufactured pointer addresses. Temporal memory safety protects against accessing an address that has been deallocated. This includes use-after-free for heap and use-after-return for stack addresses. Type safety protects against accessing valid memory with an incorrect type, also known as \"type confusion\". Data race safety protects against racing memory access: when a thread accesses (read or write) a memory location concurrently with a different writing thread and without synchronizing.","title":"What \"safety\" means in Carbon"},{"location":"project/principles/safety_strategy/#safety-guarantees-versus-hardening","text":"In providing safety, the underlying goal is to prevent attacks from turning a logic error into a security vulnerability . The three ways of doing this can be thought of in terms of how they prevent attacks: Safety guarantees prevent bugs. They offer a strong requirement that a particular security vulnerability cannot exist. Compile-time safety checks are always a safety guarantee, but safety guarantees may also be done at runtime. For example: At compile-time, range-based for loops offer a spatial safety guarantee that out-of-bounds issues cannot exist in the absence of concurrent modification of the sequence. At runtime, garbage collected languages offer a temporal safety guarantee because objects cannot be freed while they're still accessible. Error detection checks for common logic errors at runtime. For example: An array lookup function might offer spatial memory error detection by verifying that the passed index is in-bounds. A program can implement reference counting to detect a temporal memory error by checking whether any references remain when memory is freed. Safety hardening mitigates bugs, typically by minimizing the feasibility of an attack. For example: Control Flow Integrity (CFI) monitors for behavior which can subvert the program's control flow. In Clang , it is optimized for use in release builds. Typically CFI analysis will only detect a subset of attacks because it can't track each possible code path separately. It should still reduce the feasibility of both spatial memory, temporal memory, and type attacks. Memory tagging makes each attempt at an invalid read or write operation have a high probability of trapping, while still not detecting the underlying bug in every case. Realistic attacks require many such operations, so memory tagging may stop attacks in some environments. Alternatively, the trap might be asynchronous, leaving only a tiny window of time prior to the attack being detected and program terminated. These are probabilistic hardening and reduces the feasibility of both spatial and temporal memory attacks. Under both error detection and safety hardening, even if a safety is protected, the underlying bugs will still exist and will need to be fixed. For example, program termination could be used for a denial-of-service attack.","title":"Safety guarantees versus hardening"},{"location":"project/principles/safety_strategy/#philosophy","text":"Carbon's practical safety and testing mechanisms will emphasize guaranteed safety where feasible without creating barriers to Carbon's other goals , particularly performance and interoperability. This limits Carbon's options for guaranteed safety, and as a result there will be more reliance upon error detection and safety hardening. The language's design should incentivize safe programming, although it will not be required. When writing code, Carbon developers should expect to receive safety without needing to add safety annotations. Carbon will have optional safety annotations for purposes such as optimizing safety checks or providing information that improves coverage of safety checks. Carbon will favor compile-time safety checks because catching issues early will make applications more reliable. Runtime checks, either error detection or safety hardening, will be enabled where safety cannot be proven at compile-time. There will be three high-level use cases or directions that Carbon addresses through different build modes that prioritize safety checks differently: A debug oriented build mode that prioritizes detecting bugs and reporting errors helpfully. A performance oriented build mode that skips any dynamic safety checks to reduce overhead. A hardened oriented build mode that prioritizes ensuring sufficient safety to prevent security vulnerabilities, although it may not allow detecting all of the bugs. These high level build modes may be tuned, either to select specific nuanced approach for achieving the high level goal, or to configure orthogonal constraints such as whether to prioritize binary size or execution speed. However, there is a strong desire to avoid requiring more fundamental build modes to achieve the necessary coverage of detecting bugs and shipping software. These build modes are also not expected to be interchangeable or compatible with each other within a single executable -- they must be a global selection. Although expensive safety checks could be provided through additional build modes, Carbon will favor safety checks that can be combined into these three build modes rather than adding more. Over time, safety should evolve using a hybrid compile-time and runtime safety approach to eventually provide a similar level of safety to a language that puts more emphasis on guaranteed safety, such as Rust . However, while Carbon may encourage developers to modify code in support of more efficient safety checks, it will remain important to improve the safety of code for developers who cannot invest into safety-specific code modifications.","title":"Philosophy"},{"location":"project/principles/safety_strategy/#principles","text":"Safety must be easy to ramp-up with , even if it means new developers don't receive the full safety that Carbon can offer. Developers should benefit from Carbon's safety without needing to learn and apply Carbon-specific design patterns. Some safety should be enabled by default, without safety-specific work, although some safety will require work to opt in. Developers concerned with performance should only need to disable safety in rare edge-cases. Where there is a choice between safety approaches, the safe option should be incentivized by making it equally easy or easier to use. If there is a default, it should be the safe option. It should be identifiable when the unsafe option is used. Incentives will prioritize, in order: Guaranteed safety. Error detection. Safety hardening. Unsafe and unmitigated code. Language design choices should favor more efficient implementations of safety checks. They should also allow favor automation of testing and fuzzing. Safety in Carbon must work with interoperable or migrated C++ code , so that C++ developers can readily take advantage of Carbon's improvements. Safety mechanisms will ideally be designed to apply to automatically migrated C++ code. Providing immediate safety improvements to Carbon adopters will help motivate adoption. In the other direction, safety mechanisms must not force manual rewriting of C++ code in order to migrate, either by creating design incompatibilities or performance degradations. Automated migration of C++ code to Carbon must work for most developers, even if it forces Carbon's safety design to take a different approach. Carbon's safety should degrade gracefully when Carbon code calls C++ code, although this may require use of the Carbon toolchain to compile the C++ code. Applications should be expected to use interoperability. Although some safety features will be Carbon-specific, safety should not stop at the language boundary. The rules for determining whether code will pass compile-time safety checking should be articulable, documented, and easy to understand. Compile-time safety checks should not change significantly across different build modes. The purpose of the build modes is to determine code generation. Each build mode will prioritize performance and safety differently: The debug build mode will produce development-focused binaries that prioritize fast iteration on code with safety checks that assist in identification and debugging of errors. The performance build mode will produce release-focused binaries that prioritize performance over safety. The hardened build mode will produce release-focused binaries that prioritize safety that is resistant to attacks at the cost of performance. Safety checks should try to be identical across build modes. There will be differences, typically due to performance overhead and detection rate trade-offs of safety check algorithms. The number of build modes will be limited, and should be expected to remain at the named three. Most developers will use two build modes in their work: debug for development, and either performance or hardened for releases. It's important to focus on checks that are cheap enough to run as part of normal development. Users are not expected to want to run additional development build modes for additional sanitizers. Limiting the number of build modes simplifies support for both Carbon maintainers, who can focus on a more limited set of configurations, and Carbon developers, who can easily choose which is better for their use-case. Each distinct safety-related build mode (debug, performance, and hardened) cannot be combined with others in the same binary. Cross-binary interfaces will exist in Carbon, and will need to be used by developers interested in combining libraries built under different build modes. Although runtime safety checks should prevent logic errors from turning into security vulnerabilities, the underlying logic errors will still be bugs. For example, some safety checks would result in application termination; this prevents execution of unexpected code and still needs to be fixed. Developers need a strong testing methodology to engineer correct software. Carbon will encourage testing and then leverage it with the checking build modes to find and fix bugs and vulnerabilities.","title":"Principles"},{"location":"project/principles/safety_strategy/#details","text":"","title":"Details"},{"location":"project/principles/safety_strategy/#incremental-work-when-safety-requires-work","text":"Carbon is prioritizing usability of the language, particularly minimizing retraining of C++ developers and easing migration of C++ codebases, over the kind of provable safety that some other languages pursue, particularly Rust. A key motivation of Carbon is to move C++ developers to a better, safer language. However, if Carbon requires manually rewriting or redesigning C++ code in order to maintain performance, it creates additional pressure on C++ developers to learn and spend time on safety. Safety will often not be the top priority for developers; as a result, Carbon must be thoughtful about how and when it forces developers to think about safety. Relying on multiple build modes to provide safety should fit into normal development workflows. Carbon can also have features to enable additional safety, so long as developers can start using Carbon in their applications without learning new paradigms. Where possible, safety checks shouldn't require work on the part of Carbon developers. A safety check that requires no code edits or can be handled by automated migration may be opt-out, as there is negligible cost to developers. One which requires local code changes should be opt-in because costs will scale with codebase size. Safety check approaches which would require substantial redesign by developers will be disfavored based on adoption cost, even if the alternative is a less-comprehensive approach.","title":"Incremental work when safety requires work"},{"location":"project/principles/safety_strategy/#using-build-modes-to-manage-safety-checks","text":"Carbon will likely start in a state where most safety checks are done at runtime. However, runtime detection of safety violations remains expensive. In order to make as many safety checks as possible available to developers, Carbon will adopt a strategy based on three build modes that target key use-cases.","title":"Using build modes to manage safety checks"},{"location":"project/principles/safety_strategy/#debug","text":"The debug build mode targets developers who are iterating on code and running tests. It will emphasize detection and debugability, especially for safety issues. It needs to perform well enough to be run frequently by developers, but will make performance sacrifices to catch more safety issues. This mode should have runtime checks for the most common safety issues, but it can make trade-offs that improve performance in exchange for less frequent, but still reliable, detection. Developers should do most of their testing in this build mode. The debug build mode will place a premium on the debugability of safety violations. Where safety checks rely on hardening instead of guaranteed safety, violations should be detected with a high probability per single occurrence of the bug. Detected bugs will be accompanied by a detailed diagnostic report to ease classification and root cause identification.","title":"Debug"},{"location":"project/principles/safety_strategy/#performance","text":"The performance build mode targets the typical application that wants high performance from Carbon code, where performance considers processing time, memory, and disk space. Trade-offs will be made that maximize the performance. Only safety techniques that don't measurably impact application hot path performance will be enabled by default. This is a very high bar, but is crucial for meeting Carbon's performance goals, as well as allowing migration of existing C++ systems which may not have been designed with Carbon's safety semantics in mind.","title":"Performance"},{"location":"project/principles/safety_strategy/#hardened","text":"The hardened build mode targets applications where developers want strong safety against attacks in exchange for worse performance. It will work to prevent attacks in ways that attackers cannot work around , even if it means using techniques that create significant performance costs.","title":"Hardened"},{"location":"project/principles/safety_strategy/#managing-bugs-without-compile-time-safety","text":"Carbon's reliance on runtime checks will allow developers to manage their security risk. Developers will still need to reliably find and fix the inevitable bugs, including both safety violations and regular business logic bugs. The cornerstone of managing bugs will be strong testing methodologies, with built-in support from Carbon. Strong testing is more than good test coverage. It means a combination of: Ensuring unsafe or risky operations and interfaces can easily be recognized by developers. Using static analysis tools to detect common bugs, and ensuring they're integrated into build and code review workflows. These could be viewed as static testing of code. Writing good test coverage, including unit, integration, and system tests. Implementing coverage-directed fuzz testing to discover bugs outside of manually authored test coverage, especially for interfaces handling untrusted data. Fuzz testing is a robust way to catch bugs when APIs may be used in ways developers don't consider. Running continuous integration, including automatic and continuous running of these tests. The checked development build mode should be validated, as well as any additional build modes necessary to cover different forms of behavior checking. Easing automated testing and fuzzing through language features. For example, if the language encourages value types and pure functions of some sort, they can be automatically fuzzed. These practices are necessary for reliable, large-scale software engineering. Maintaining correctness of business logic over time requires continuous and thorough testing. Without it, such software systems cannot be changed and evolved over time reliably. Carbon will re-use these practices in conjunction with checking build modes to mitigate the limitations of Carbon's safety guarantees without imposing overhead on production systems. When a developer chooses to use Carbon, adhering to this kind of testing methodology is essential for maintaining safety. As a consequence, Carbon's ecosystem, including the language, tools, and libraries, will need to directly work to remove barriers and encourage the development of these methodologies. The reliance on testing may make Carbon a poor choice in some environments; in environments where such testing rigor is infeasible, a language with a greater degree of static checking may be better suited.","title":"Managing bugs without compile-time safety"},{"location":"project/principles/safety_strategy/#caveats","text":"","title":"Caveats"},{"location":"project/principles/safety_strategy/#probabilistic-techniques-likely-cannot-stop-attacks","text":"It's expected that probabilistic techniques that can be applied at the language level are attackable through a variety of techniques: The attacker might be able to attack repeatedly until it gets through. The attacker may be able to determine when the attack would be detected and only run the attack when it would not be. The attacker might be able control the test condition to make detection much less likely or avoid detection completely. For example, if detection is based on the last 4 bits of a memory address, an attacker may be able to generate memory allocations, viewing the address and only attacking when there's a collision. Hardware vulnerabilities may make these attacks easier than they might otherwise appear. Future hardware vulnerabilities are difficult to predict. Note this statement focuses on what can be applied to the language level. Using a secure hash algorithm, such as SHA256, may be used to offer probabilistic defense in other situations. However, the overhead of a secure hash algorithm's calculation is significant in the context of most things that Carbon may do at the language level. Combining these issues, although it may seem like a probabilistic safety check could be proven to reliably detect attackers, it's likely infeasible to do so. For the various build modes, this means: The debug build mode will not typically be accessible to attackers, so where a probabilistic technique provides a better developer experience, it will be preferred. The performance build mode will often avoid safety checks in order to reach peak performance. As a consequence, even the weak protection of a probabilistic safety check may be used in order to provide some protection. The hardened build mode will prefer non-probabilistic techniques that cannot be attacked.","title":"Probabilistic techniques likely cannot stop attacks"},{"location":"project/principles/safety_strategy/#alternatives-considered","text":"","title":"Alternatives considered"},{"location":"project/principles/safety_strategy/#guaranteed-memory-safety-programming-models","text":"Multiple approaches that would offer guaranteed memory safety have been considered, mainly based on other languages which offer related approaches. Carbon will likely rely more on error detection and hardening because of what the models would mean for Carbon's performance and C++ migration language goals.","title":"Guaranteed memory safety programming models"},{"location":"project/principles/safety_strategy/#guaranteed-compile-time-memory-safety-using-borrow-checking","text":"Rust offers a good example of an approach for compile-time safety based on borrow checking, which provides guaranteed safety. For code which can't implement borrow checking, runtime safety using reference counting is available and provides reliable error detection. This approach still allows for unsafe blocks , as well as types that offer runtime safety while wrapping unsafe interfaces. Carbon could use a similar approach for guaranteed safety by default. Advantages: Guaranteed safety, including against data races, is provided for the binaries. The emphasis on compile-time safety limits the scope of the runtime memory safety costs. With Rust, there is early evidence that there's a significant impact in reducing bugs generally. Imitating Rust's techniques would allow building on the huge work of the Rust community, reducing the risks of implementing similar in Carbon. Careful use of narrow unsafe escape hatches can be effectively encapsulated behind otherwise safe APIs. Disadvantages: Rust's approach to compile-time safety requires use of design patterns and idioms that are substantially different from C++. Conversion of C++ code to Rust results in either rewrites of code, or use of runtime safety checks that impair performance. Requires fully modeling lifetime and exclusivity in the type system. Data structures must be redesigned to avoid sharing mutable state. Increases complexity of node and pointer based data structures, such as linked lists. Imitating Rust's techniques may prove insufficient for achieving Carbon's compiler performance goals . Rust compilation performance suggests its borrow checking performance is slow, although it's difficult to determine how significant this is or whether it could be improved. The Rust compiler is slow , although much has been done to improve it . Details of type checking, particularly requiring parsing of function bodies to type check signatures, as well as wide use of monomorphization are likely significant contributors to Rust compilation performance. LLVM codegen is also a significant cost for Rust compilation performance. With Fuchsia as an example, in December 2020, borrow checking and type checking combined account for around 10% of Rust compile CPU time, or 25% of end-to-end compile time. The current cost of borrow checking is obscured both because of the combination with type checking, and because Fuchsia disables some compiler parallelization due to build system incompatibility. The complexity of using Rust's compile-time safety may incentivize unnecessary runtime checking of safety properties. For example, using RefCell or Rc to avoid changing designs to fit compile-time safety models. Some of the most essential safety tools that ease the ergonomic burden of the Rust-style lifetime model ( Rc ) introduce semantic differences that cannot then be eliminated in a context where performance is the dominant priority. It's possible to modify the Rust model several ways in order to reduce the burden on C++ developers: Don't offer safety guarantees for data races, eliminating RefCell . This would likely not avoid the need for Rc or Arc , and wouldn't substantially reduce the complexity. Require manual destruction of Rc , allowing safety checks to be disabled in the performance build mode to eliminate overhead. This still requires redesigning C++ code to take advantage of Rc . The possibility of incorrect manual destruction means that the safety issue is being turned into a bug, which means it is hardening and no longer a safety guarantee. Carbon can provide equivalent hardening through techniques such as MarkUs , which does not require redesigning C++ code. Overall, Carbon is making a compromise around safety in order to give a path for C++ to evolve. C++ developers must be comfortable migrating their codebases, and able to do so in a largely automated manner. In order to achieve automated migration, Carbon cannot require fundamental redesigns of migrated C++ code. While a migration tool could in theory mark all migrated code as unsafe , Carbon should use a safety strategy that degrades gracefully and offers improvements for C++ code, whether migrated or not. That does not mean Carbon will never adopt guaranteed safety by default, only that performance and migration of C++ code takes priority, and any design will need to be considered in the context of other goals. It should still be possible to adopt guaranteed safety later, although it will require identifying a migration path.","title":"Guaranteed compile-time memory safety using borrow checking"},{"location":"project/principles/safety_strategy/#guaranteed-run-time-memory-safety-using-reference-counting","text":"Reference counting is a common memory safety model, with Swift as a popular example. Advantages: Simple model for safety, particularly as compared with Rust. Safe for all of the most common and important classes of memory safety bugs. Disadvantages: Safety based on reference counting introduces significant performance costs, and tools for controlling these costs are difficult. Safety based on garbage collection has less direct performance overhead, but has a greater unpredictability of performance. Significant design differences versus C++ still result, as the distinction between value types and \"class types\" becomes extremely important. Class types are held by a reference counted pointer and are thus lifetime safe. In order to mitigate the performance overhead, Swift does have a proposal to add an option for unique ownership, although the specifics are not designed yet. The unique ownership approach is expected to require unowned and unsafe access, so it would not considered to improve the safety trade-offs. Swift was designated by Apple as the replacement for Objective-C. The safety versus performance trade-offs that it makes fit Apple's priorities. Carbon's performance goals should lead to different trade-off decisions with a higher priority on peak performance, which effectively rules out broad use of reference counting.","title":"Guaranteed run-time memory safety using reference counting"},{"location":"project/principles/safety_strategy/#guaranteed-run-time-memory-safety-using-garbage-collection","text":"Garbage collection is a common memory safety model, with Java as a popular example. Advantages: This approach is among the most robust and well studied models, with decades of practical usage and analysis for security properties. Extremely suitable for efficient implementation on top of a virtual machine, such as the JVM. Disadvantages: Extremely high complexity to fully understand the implications of complex cases like data races. Performance overhead is significant in terms of what Carbon would like to consider. Garbage collection remains a difficult performance problem, even for the JVM and its extensive optimizations. The complexity of the implementation makes it difficult to predict performance; for example, Java applications experience latency spikes when garbage collection runs. Java is a good choice for many applications, but Carbon is working to focus on a set of performance priorities that would be difficult to achieve with a garbage collector.","title":"Guaranteed run-time memory safety using garbage collection"},{"location":"project/principles/safety_strategy/#build-mode-names","text":"The build mode concepts are difficult to name. Other names that were evaluated, and are ultimately similar, are: \"Debug\" is a common term for the intended use of this build mode. Also, tooling including Visual Studio frequently uses the debug term for describing similar. \"Development\" was also considered, but this term is less specific and would be better for describing all non-release builds together. For example, a \"fast build\" mode might be added that disables safety checks to improve iteration time, like might be controlled by way of C++'s NDEBUG option. \"Performance\" aligns with the phrasing of the language performance goal. \"Optimized\" implies that other modes would not be fully optimized, but hardened should be optimized. \"Fast\" would suggest that speed is the only aspect of performance being optimizing for, but \"performance\" also optimizes for memory usage and binary size. \"Hardened\" is the choice for succinctly describing the additional safety measures that will be taken, and is a well-known term in the safety space. It could be incorrectly inferred that \"performance\" has no hardening, but the preference is to clearly indicate the priority of the \"hardened\" build mode. \"Safe\" implies something closer to guaranteed safety. However, safety bugs should be expected to result in program termination, which can still be used in other attacks, such as Denial-of-Service. \"Mitigated\" is an overloaded term, and it may not be succinctly clear that it's about security mitigations. Some terms which were considered and don't fit well into the above groups are: \"Release\" is avoided because both \"performance\" and \"hardened\" could be considered to be \"release\" build modes. The names \"performance\" and \"hardened\" may lead to misinterpretations, with some developers who should use \"hardened\" using \"performance\" because they are worried about giving up too much performance, and the other way around. The terms try to balance the utility of well-known terminology with the succinctness of a short phrase for build modes, and that limits the expressivity. Some confusion is expected, and documentation as well as real-world experience (for example, a developer who cares about latency benchmarking both builds) should be expected to help mitigate mix-ups.","title":"Build mode names"},{"location":"project/principles/safety_strategy/#performance-versus-safety-in-the-hardened-build-mode","text":"The performance cost of safety techniques are expected to be non-linear with respect to detection rates. For example, a particular vulnerability such as heap use-after-free may be detectable with 99% accuracy at 20% performance cost, but 100% accuracy at 50% performance cost. At present, build modes should be expected to evaluate such a scenario as: The debug build mode would choose the 99% accurate approach. Detecting safety issues is valuable for debugging. The probabilistic detection rate won't meaningfully affect accuracy of tests. The lower performance cost improves developer velocity. The performance build mode would decline detection. Safety checks with a measurable performance cost should be declined. The hardened build mode would choose the 100% accurate approach. Safety must be non-probabilistic in order to reliably prevent attacks. Significant performance hits are acceptable. This means the hardened build mode may be slower than the debug build mode. In order to achieve better performance, the hardened build mode could make trade-offs closer to the debug build mode. Rather than relying on non-probabilistic techniques, it could instead offer a probability-based chance of detecting a given attack. Advantages: Probabilistic safety should come at lower performance cost (including CPU, memory, and disk space). This will sometimes be significant, and as a result of multiple checks, could be the difference between the hardened build mode being 50% slower than the performance build mode and being 200% slower. Disadvantages: Probabilistic techniques likely cannot stop attacks . Attackers may be able to repeat attacks until they succeed. The variables upon which the probability is based, such as memory addresses, may be manipulable by the attacker. As a consequence, a determined attacker may be able to manipulate probabilities and not even be detected. Although performance is Carbon's top goal , the hardened build mode exists to satisfy developers and environments that value safety more than performance. The hardened build mode will rely on non-probabilistic safety at significant performance cost because other approaches will be insufficient to guard against determined attackers.","title":"Performance versus safety in the hardened build mode"},{"location":"project/principles/safety_strategy/#add-more-build-modes","text":"More build modes could be added to this principle, or the principle could encourage the idea that specific designs may add more. To explain why three build modes: The concept of debug and release (sometimes called opt) are common. For example, in Visual Studio . In Carbon, this could be considered to translate to the \"debug\" and \"performance\" build modes by default. The hardened build mode is added in order to emphasize security. Although hardened could be implemented as a set of options passed to the standard release build mode, the preference is to focus on it as an important feature. An example of why another build mode may be needed is ThreadSanitizer , which is noted as having 5-15x slowdown and 5-10x memory overhead. This is infeasible for normal use, but could be useful for some users in a separate build mode. A trade-off that's possible for Carbon is instead using an approach similar to KCSAN which offers relatively inexpensive but lower-probability race detection. Although options to these build modes may be supported to customize deployments, the preference is to focus on a small set and make them behave well. For example, if a separate build mode is added for ThreadSanitizer, it should be considered a temporary solution until it can be merged into the debug build mode. Advantages: Grants more flexibility for using build modes as a solution to problems. With safety checks, this would allow providing safety checks that are high overhead but also high detection rate as separate build modes. With other systems, there could be non-safety performance versus behavior trade-offs. Disadvantages: Having standard modes simplifies validation of interactions between various safety checks. Safety is the only reason that's been considered for adding build modes. As more build modes are added, the chance of developers being confused and choosing the wrong build mode for their application increases. Any long-term additions to the set of build modes will need to update this principle, raising the visibility and requiring more consideration of such an addition. If build modes are added for non-safety-related reasons, this may lead to moving build modes out of the safety strategy. Experiment : This can be considered an experiment. Carbon may eventually add more than the initial three build modes, although the reticence to add more is likely to remain.","title":"Add more build modes"},{"location":"project/principles/static_open_extension/","text":"Principle: One static open extension mechanism Table of contents Background Principle Alternatives considered Background In C++, a single function may be overloaded with definitions in multiple files. The ADL name lookup rule even allows an unqualified call to resolve to functions defined in different namespaces. These rules are used to define extension points with static dispatch for operator overloading and functions like swap . Nothing in C++ restricts the signatures of function overloads. This means that if overloading is used as an extension point to define an operation for a variety of types, there is no way to type check generic code that tries to invoke that operation over those types. Further, in C++, all non-member functions can be found in this way, if they are declared in the same namespace as a type that could be associated with an argument list in a call. There is no straightforward opt-out mechanism in a function declaration, and while there are opt-out mechanisms at call sites, they are rarely used. As a consequence, many non-member functions with the same name can form part of an overload set, even if they provide unrelated functionality, and there is no indication in the code of which functions in different namespaces are intended to expose the same capability. Principle In Carbon, interface s are the only static open extension mechanism. Each type may define its own implementation of each interface. Generic code can be written that works with any type implementing the interface. That code can be type checked independent of which type the generic code is instantiated with by using the fact that the interface specifies the signatures of the calls. To keep the language simple, this is the only static open extension mechanism in Carbon. This means that function overloading is limited in Carbon to only signatures defined together in the same library. It also means that to interoperate with C++, the operators and swap need to have corresponding interfaces on the Carbon side. The main advantage of interfaces as an open extension mechanism over open overloading is allowing generics to be type checked separately. In addition, they are less context sensitive . Generics are coherent , while open function overloading can resolve names differently depending on what is imported. Closed overloading in Carbon also simplifies what gets exported to C++ from Carbon. Interface implementations express intent by being explicit, in contrast to how adding a function to a cross-file overload set can be accidental. Interfaces provide an way to group functions together, and express the constraint that all of the functions in the group are implemented. Consider a random-access iterator, which has a number of methods. If a C++ template function only accesses some of those methods which happens to match the subset defined for a type, the code will work temporarily but fail later when the code is changed to use a different subset. This helps achieve the Carbon Goal of code that is easy to read, understand, and write . Alternatives considered Another approach to operator overloading is to use methods with a specific name. In C++ these start with the operator keyword . Python uses method with names starting and ending with double underscores . Interfaces are more flexible about where implementations may be defined. For example, with special method names, + on a Vector(T) class could only be defined as part of the Vector(T) definition. With interfaces, additionally + for Vector(MyType) could be implemented with MyType . C++ provides this flexibility by also permitting non-method operator overloads, but this brings with it the cost of selecting a best-matching operator from a potentially very large open overload set.","title":"Principle: One static open extension mechanism"},{"location":"project/principles/static_open_extension/#principle-one-static-open-extension-mechanism","text":"","title":"Principle: One static open extension mechanism"},{"location":"project/principles/static_open_extension/#table-of-contents","text":"Background Principle Alternatives considered","title":"Table of contents"},{"location":"project/principles/static_open_extension/#background","text":"In C++, a single function may be overloaded with definitions in multiple files. The ADL name lookup rule even allows an unqualified call to resolve to functions defined in different namespaces. These rules are used to define extension points with static dispatch for operator overloading and functions like swap . Nothing in C++ restricts the signatures of function overloads. This means that if overloading is used as an extension point to define an operation for a variety of types, there is no way to type check generic code that tries to invoke that operation over those types. Further, in C++, all non-member functions can be found in this way, if they are declared in the same namespace as a type that could be associated with an argument list in a call. There is no straightforward opt-out mechanism in a function declaration, and while there are opt-out mechanisms at call sites, they are rarely used. As a consequence, many non-member functions with the same name can form part of an overload set, even if they provide unrelated functionality, and there is no indication in the code of which functions in different namespaces are intended to expose the same capability.","title":"Background"},{"location":"project/principles/static_open_extension/#principle","text":"In Carbon, interface s are the only static open extension mechanism. Each type may define its own implementation of each interface. Generic code can be written that works with any type implementing the interface. That code can be type checked independent of which type the generic code is instantiated with by using the fact that the interface specifies the signatures of the calls. To keep the language simple, this is the only static open extension mechanism in Carbon. This means that function overloading is limited in Carbon to only signatures defined together in the same library. It also means that to interoperate with C++, the operators and swap need to have corresponding interfaces on the Carbon side. The main advantage of interfaces as an open extension mechanism over open overloading is allowing generics to be type checked separately. In addition, they are less context sensitive . Generics are coherent , while open function overloading can resolve names differently depending on what is imported. Closed overloading in Carbon also simplifies what gets exported to C++ from Carbon. Interface implementations express intent by being explicit, in contrast to how adding a function to a cross-file overload set can be accidental. Interfaces provide an way to group functions together, and express the constraint that all of the functions in the group are implemented. Consider a random-access iterator, which has a number of methods. If a C++ template function only accesses some of those methods which happens to match the subset defined for a type, the code will work temporarily but fail later when the code is changed to use a different subset. This helps achieve the Carbon Goal of code that is easy to read, understand, and write .","title":"Principle"},{"location":"project/principles/static_open_extension/#alternatives-considered","text":"Another approach to operator overloading is to use methods with a specific name. In C++ these start with the operator keyword . Python uses method with names starting and ending with double underscores . Interfaces are more flexible about where implementations may be defined. For example, with special method names, + on a Vector(T) class could only be defined as part of the Vector(T) definition. With interfaces, additionally + for Vector(MyType) could be implemented with MyType . C++ provides this flexibility by also permitting non-method operator overloads, but this brings with it the cost of selecting a best-matching operator from a potentially very large open overload set.","title":"Alternatives considered"},{"location":"project/principles/success_criteria/","text":"Principle: Success criteria Table of contents Principle Applications of these principles Modern OS platforms, hardware architectures, and environments OS platforms Hardware architectures Historical platforms Interoperability with and migration from existing C++ code Migration tooling Principle Carbon's goals set a high-level path for where Carbon should head. However, given priorities, it's not always clear how specific features or details may end up being evaluated. Carbon's success criteria are specific, measurable, key results that we expect to use to see how Carbon is doing against its goals. Success criteria will be considered as part of Carbon's roadmap process , missing them will be considered significant, and extra scrutiny will be applied on proposals that would require diminishing them. These success criteria are not exhaustive, but they are a bar that we aim to exceed . Applications of these principles TODO: Add more metrics for various goals. Modern OS platforms, hardware architectures, and environments References: goal This should not be considered an exhaustive list of important platforms. OS platforms Our priority OS platforms are modern versions of: Linux, including common distributions, Android and ChromeOS FreeBSD Windows macOS and iOS Fuchsia WebAssembly Bare metal Hardware architectures We expect to prioritize 64-bit little endian hardware, including: x86-64 AArch64, also known as ARM 64-bit PPC64LE, also known as Power ISA, 64-bit, Little Endian RV64I, also known as RISC-V 64-bit We believe Carbon should strive to support some GPUs, other restricted computational hardware and environments, and embedded environments. While this should absolutely include future and emerging hardware and platforms, those shouldn't disproportionately shape the fundamental library and language design while they remain relatively new and rapidly evolving. Historical platforms Example historical platforms that we will not prioritize support for are: Byte sizes other than 8 bits, or non-power-of-two word sizes. Source code encodings other than UTF-8. Big- or mixed-endian, at least for computation; accessing encoded data remains useful. Non-2's-complement integer formats. Non-IEEE 754 binary floating point format and semantics for default single- and double-precision floating point types. Source code in file systems that don\u2019t support file extensions or nested directories. Interoperability with and migration from existing C++ code References: goal Migration tooling Migrations must be mostly automatic. To that end, given an arbitrary large codebase following best practices, we aim to have less than 2% of files require human interaction. This criterion includes: Addressing performance bugs unique to Carbon, introduced by migration tooling. Converting complex code which migration tooling does not handle. This criterion does not include: Cleaning up coding style to idiomatic Carbon. For example, heavy use of C++ preprocessor macros may result in expanded code where there is no equivalent Carbon metaprogramming construct.","title":"Principle: Success criteria"},{"location":"project/principles/success_criteria/#principle-success-criteria","text":"","title":"Principle: Success criteria"},{"location":"project/principles/success_criteria/#table-of-contents","text":"Principle Applications of these principles Modern OS platforms, hardware architectures, and environments OS platforms Hardware architectures Historical platforms Interoperability with and migration from existing C++ code Migration tooling","title":"Table of contents"},{"location":"project/principles/success_criteria/#principle","text":"Carbon's goals set a high-level path for where Carbon should head. However, given priorities, it's not always clear how specific features or details may end up being evaluated. Carbon's success criteria are specific, measurable, key results that we expect to use to see how Carbon is doing against its goals. Success criteria will be considered as part of Carbon's roadmap process , missing them will be considered significant, and extra scrutiny will be applied on proposals that would require diminishing them. These success criteria are not exhaustive, but they are a bar that we aim to exceed .","title":"Principle"},{"location":"project/principles/success_criteria/#applications-of-these-principles","text":"TODO: Add more metrics for various goals.","title":"Applications of these principles"},{"location":"project/principles/success_criteria/#modern-os-platforms-hardware-architectures-and-environments","text":"References: goal This should not be considered an exhaustive list of important platforms.","title":"Modern OS platforms, hardware architectures, and environments"},{"location":"project/principles/success_criteria/#os-platforms","text":"Our priority OS platforms are modern versions of: Linux, including common distributions, Android and ChromeOS FreeBSD Windows macOS and iOS Fuchsia WebAssembly Bare metal","title":"OS platforms"},{"location":"project/principles/success_criteria/#hardware-architectures","text":"We expect to prioritize 64-bit little endian hardware, including: x86-64 AArch64, also known as ARM 64-bit PPC64LE, also known as Power ISA, 64-bit, Little Endian RV64I, also known as RISC-V 64-bit We believe Carbon should strive to support some GPUs, other restricted computational hardware and environments, and embedded environments. While this should absolutely include future and emerging hardware and platforms, those shouldn't disproportionately shape the fundamental library and language design while they remain relatively new and rapidly evolving.","title":"Hardware architectures"},{"location":"project/principles/success_criteria/#historical-platforms","text":"Example historical platforms that we will not prioritize support for are: Byte sizes other than 8 bits, or non-power-of-two word sizes. Source code encodings other than UTF-8. Big- or mixed-endian, at least for computation; accessing encoded data remains useful. Non-2's-complement integer formats. Non-IEEE 754 binary floating point format and semantics for default single- and double-precision floating point types. Source code in file systems that don\u2019t support file extensions or nested directories.","title":"Historical platforms"},{"location":"project/principles/success_criteria/#interoperability-with-and-migration-from-existing-c-code","text":"References: goal","title":"Interoperability with and migration from existing C++ code"},{"location":"project/principles/success_criteria/#migration-tooling","text":"Migrations must be mostly automatic. To that end, given an arbitrary large codebase following best practices, we aim to have less than 2% of files require human interaction. This criterion includes: Addressing performance bugs unique to Carbon, introduced by migration tooling. Converting complex code which migration tooling does not handle. This criterion does not include: Cleaning up coding style to idiomatic Carbon. For example, heavy use of C++ preprocessor macros may result in expanded code where there is no equivalent Carbon metaprogramming construct.","title":"Migration tooling"},{"location":"spec/","text":"Spec Eventually, this will be the home of a formal specification for the Carbon Language. We are committed to having a specification that is sufficiently detailed to allow independent implementations of the language. While we plan to have a reference implementation, we think having a specification as well is an important tool to ensure that the behavior of the language is well understood and holds together. The work-in-progress specification is available here: Language specification Library specification","title":"Spec"},{"location":"spec/#spec","text":"Eventually, this will be the home of a formal specification for the Carbon Language. We are committed to having a specification that is sufficiently detailed to allow independent implementations of the language. While we plan to have a reference implementation, we think having a specification as well is an important tool to ensure that the behavior of the language is well understood and holds together. The work-in-progress specification is available here: Language specification Library specification","title":"Spec"},{"location":"spec/lang/","text":"Carbon language specification Program structure A program is a collection of one or more linkage units that are linked together. A Carbon linkage unit is the result of translating a source file. A foreign linkage unit is an artifact produced by a translation process for some other programming language. A linkage unit is either a Carbon linkage unit or a foreign linkage unit. A source file is a sequence of Unicode code points. Note: Source files are typically stored on disk in files with a .carbon file extension, encoded in UTF-8. Conformance A program is valid if it contains no constructs that violate \"shall\" constraints in this specification. Otherwise, the program is invalid . An implementation is conforming if it accepts all valid programs, it rejects all invalid programs for which a diagnostic is required, and the execution semantics of all accepted programs is as specified in this specification. Translation Translation of a source file into a Carbon linkage unit proceeds as follows: Lexical analysis decomposes the sequence of code points into a sequence of lexical elements. Whitespace and text comments are discarded, leaving a sequence of tokens . The tokens are parsed into an abstract syntax tree. Unqualified names are bound to declarations in the abstract syntax tree. A translated form of each imported library is located and loaded. Semantic analysis is performed: types are determined and semantic checks are performed for all non-template-dependent constructs in the abstract syntax tree, constant expressions are evaluated, and templates are instantiated and semantically analyzed. Note: After semantic analysis, an implementation may optionally > monomorphize generics by a process similar to template instantiation. The resulting linkage unit comprises all entities in the translated source file that are either external or are reachable from an external entity. Note: A linkage unit can include non-monomorphized generics, but never includes templates. Constant evaluation can eliminate references to entities. Linkage Two declarations declare the same entity if both declarations are in the same library and the same scope and declare the same name . TODO: Linkage rules for foreign entities. TODO: Ability to declare file-local entities. All declarations of an entity shall use the same type. Every entity that is reachable from a linkage unit in a program shall be defined by a linkage unit in the program; no diagnostic is required unless an entity that can be referenced during the execution of the program is not defined. There shall not be more than one definition of an entity in a program.","title":"Carbon language specification"},{"location":"spec/lang/#carbon-language-specification","text":"","title":"Carbon language specification"},{"location":"spec/lang/#program-structure","text":"A program is a collection of one or more linkage units that are linked together. A Carbon linkage unit is the result of translating a source file. A foreign linkage unit is an artifact produced by a translation process for some other programming language. A linkage unit is either a Carbon linkage unit or a foreign linkage unit. A source file is a sequence of Unicode code points. Note: Source files are typically stored on disk in files with a .carbon file extension, encoded in UTF-8.","title":"Program structure"},{"location":"spec/lang/#conformance","text":"A program is valid if it contains no constructs that violate \"shall\" constraints in this specification. Otherwise, the program is invalid . An implementation is conforming if it accepts all valid programs, it rejects all invalid programs for which a diagnostic is required, and the execution semantics of all accepted programs is as specified in this specification.","title":"Conformance"},{"location":"spec/lang/#translation","text":"Translation of a source file into a Carbon linkage unit proceeds as follows: Lexical analysis decomposes the sequence of code points into a sequence of lexical elements. Whitespace and text comments are discarded, leaving a sequence of tokens . The tokens are parsed into an abstract syntax tree. Unqualified names are bound to declarations in the abstract syntax tree. A translated form of each imported library is located and loaded. Semantic analysis is performed: types are determined and semantic checks are performed for all non-template-dependent constructs in the abstract syntax tree, constant expressions are evaluated, and templates are instantiated and semantically analyzed. Note: After semantic analysis, an implementation may optionally > monomorphize generics by a process similar to template instantiation. The resulting linkage unit comprises all entities in the translated source file that are either external or are reachable from an external entity. Note: A linkage unit can include non-monomorphized generics, but never includes templates. Constant evaluation can eliminate references to entities.","title":"Translation"},{"location":"spec/lang/#linkage","text":"Two declarations declare the same entity if both declarations are in the same library and the same scope and declare the same name . TODO: Linkage rules for foreign entities. TODO: Ability to declare file-local entities. All declarations of an entity shall use the same type. Every entity that is reachable from a linkage unit in a program shall be defined by a linkage unit in the program; no diagnostic is required unless an entity that can be referenced during the execution of the program is not defined. There shall not be more than one definition of an entity in a program.","title":"Linkage"},{"location":"spec/lang/execution/","text":"Execution Entry points TODO: Entry points (Carbon and foreign). fn Run() . Object model Sequential execution Threads and data races","title":"Execution"},{"location":"spec/lang/execution/#execution","text":"","title":"Execution"},{"location":"spec/lang/execution/#entry-points","text":"TODO: Entry points (Carbon and foreign). fn Run() .","title":"Entry points"},{"location":"spec/lang/execution/#object-model","text":"","title":"Object model"},{"location":"spec/lang/execution/#sequential-execution","text":"","title":"Sequential execution"},{"location":"spec/lang/execution/#threads-and-data-races","text":"","title":"Threads and data races"},{"location":"spec/lang/lex/","text":"Lexical analysis TODO Lexical elements The sequence of Unicode code points in a source file is partitioned into contiguous subsequences called lexical elements . Formation of lexical elements begins with the first code point in the source file and proceeds in code point order. At each step, the longest valid lexical element that can be formed from a prefix of the remaining code points is formed, even if this would result in a failure to form a later lexical element. Repeating this process shall convert the entire source file into lexical elements. Valid lexical elements are: TODO: Add a list of lexical elements once we've decided on them.","title":"Lexical analysis"},{"location":"spec/lang/lex/#lexical-analysis","text":"TODO","title":"Lexical analysis"},{"location":"spec/lang/lex/#lexical-elements","text":"The sequence of Unicode code points in a source file is partitioned into contiguous subsequences called lexical elements . Formation of lexical elements begins with the first code point in the source file and proceeds in code point order. At each step, the longest valid lexical element that can be formed from a prefix of the remaining code points is formed, even if this would result in a failure to form a later lexical element. Repeating this process shall convert the entire source file into lexical elements. Valid lexical elements are: TODO: Add a list of lexical elements once we've decided on them.","title":"Lexical elements"},{"location":"spec/lang/libs/","text":"Libraries and packages TODO","title":"Libraries and packages"},{"location":"spec/lang/libs/#libraries-and-packages","text":"TODO","title":"Libraries and packages"},{"location":"spec/lang/names/","text":"Names TODO Names A name is an identifier . Two names are the same if they comprise the same sequence of Unicode code points. TODO: Normalization? Scopes A scope is one of: The top level in a source file. A pattern scope. A block scope. A type definition. Every construct that declares a name binds the name to the declared entity within the innermost enclosing scope. Unqualified name lookup Unqualified name lookup associates a name with an entity. The associated entity is the entity to which the name is bound in the innermost enclosing scope in which the name is bound.","title":"Names"},{"location":"spec/lang/names/#names","text":"TODO","title":"Names"},{"location":"spec/lang/names/#names_1","text":"A name is an identifier . Two names are the same if they comprise the same sequence of Unicode code points. TODO: Normalization?","title":"Names"},{"location":"spec/lang/names/#scopes","text":"A scope is one of: The top level in a source file. A pattern scope. A block scope. A type definition. Every construct that declares a name binds the name to the declared entity within the innermost enclosing scope.","title":"Scopes"},{"location":"spec/lang/names/#unqualified-name-lookup","text":"Unqualified name lookup associates a name with an entity. The associated entity is the entity to which the name is bound in the innermost enclosing scope in which the name is bound.","title":"Unqualified name lookup"},{"location":"spec/lang/parsing/","text":"Parsing TODO","title":"Parsing"},{"location":"spec/lang/parsing/#parsing","text":"TODO","title":"Parsing"},{"location":"spec/lang/semantics/","text":"Semantic analysis TODO","title":"Semantic analysis"},{"location":"spec/lang/semantics/#semantic-analysis","text":"TODO","title":"Semantic analysis"},{"location":"spec/lib/","text":"Carbon standard library specification TODO","title":"Carbon standard library specification"},{"location":"spec/lib/#carbon-standard-library-specification","text":"TODO","title":"Carbon standard library specification"}]}